{
    "scipy.stats": "\n.. _statsrefmanual:\n\n==========================================\nStatistical functions (:mod:`scipy.stats`)\n==========================================\n\n.. currentmodule:: scipy.stats\n\nThis module contains a large number of probability distributions,\nsummary and frequency statistics, correlation functions and statistical\ntests, masked statistics, kernel density estimation, quasi-Monte Carlo\nfunctionality, and more.\n\nStatistics is a very large area, and there are topics that are out of scope\nfor SciPy and are covered by other packages. Some of the most important ones\nare:\n\n- `statsmodels <https://www.statsmodels.org/stable/index.html>`__:\n  regression, linear models, time series analysis, extensions to topics\n  also covered by ``scipy.stats``.\n- `Pandas <https://pandas.pydata.org/>`__: tabular data, time series\n  functionality, interfaces to other statistical languages.\n- `PyMC <https://docs.pymc.io/>`__: Bayesian statistical\n  modeling, probabilistic machine learning.\n- `scikit-learn <https://scikit-learn.org/>`__: classification, regression,\n  model selection.\n- `Seaborn <https://seaborn.pydata.org/>`__: statistical data visualization.\n- `rpy2 <https://rpy2.github.io/>`__: Python to R bridge.\n\n\nProbability distributions\n=========================\n\nEach univariate distribution is an instance of a subclass of `rv_continuous`\n(`rv_discrete` for discrete distributions):\n\n.. autosummary::\n   :toctree: generated/\n\n   rv_continuous\n   rv_discrete\n   rv_histogram\n\nContinuous distributions\n------------------------\n\n.. autosummary::\n   :toctree: generated/\n\n   alpha             -- Alpha\n   anglit            -- Anglit\n   arcsine           -- Arcsine\n   argus             -- Argus\n   beta              -- Beta\n   betaprime         -- Beta Prime\n   bradford          -- Bradford\n   burr              -- Burr (Type III)\n   burr12            -- Burr (Type XII)\n   cauchy            -- Cauchy\n   chi               -- Chi\n   chi2              -- Chi-squared\n   cosine            -- Cosine\n   crystalball       -- Crystalball\n   dgamma            -- Double Gamma\n   dweibull          -- Double Weibull\n   erlang            -- Erlang\n   expon             -- Exponential\n   exponnorm         -- Exponentially Modified Normal\n   exponweib         -- Exponentiated Weibull\n   exponpow          -- Exponential Power\n   f                 -- F (Snecdor F)\n   fatiguelife       -- Fatigue Life (Birnbaum-Saunders)\n   fisk              -- Fisk\n   foldcauchy        -- Folded Cauchy\n   foldnorm          -- Folded Normal\n   genlogistic       -- Generalized Logistic\n   gennorm           -- Generalized normal\n   genpareto         -- Generalized Pareto\n   genexpon          -- Generalized Exponential\n   genextreme        -- Generalized Extreme Value\n   gausshyper        -- Gauss Hypergeometric\n   gamma             -- Gamma\n   gengamma          -- Generalized gamma\n   genhalflogistic   -- Generalized Half Logistic\n   genhyperbolic     -- Generalized Hyperbolic\n   geninvgauss       -- Generalized Inverse Gaussian\n   gibrat            -- Gibrat\n   gompertz          -- Gompertz (Truncated Gumbel)\n   gumbel_r          -- Right Sided Gumbel, Log-Weibull, Fisher-Tippett, Extreme Value Type I\n   gumbel_l          -- Left Sided Gumbel, etc.\n   halfcauchy        -- Half Cauchy\n   halflogistic      -- Half Logistic\n   halfnorm          -- Half Normal\n   halfgennorm       -- Generalized Half Normal\n   hypsecant         -- Hyperbolic Secant\n   invgamma          -- Inverse Gamma\n   invgauss          -- Inverse Gaussian\n   invweibull        -- Inverse Weibull\n   irwinhall         -- Irwin-Hall\n   jf_skew_t         -- Jones and Faddy Skew-T\n   johnsonsb         -- Johnson SB\n   johnsonsu         -- Johnson SU\n   kappa4            -- Kappa 4 parameter\n   kappa3            -- Kappa 3 parameter\n   ksone             -- Distribution of Kolmogorov-Smirnov one-sided test statistic\n   kstwo             -- Distribution of Kolmogorov-Smirnov two-sided test statistic\n   kstwobign         -- Limiting Distribution of scaled Kolmogorov-Smirnov two-sided test statistic.\n   laplace           -- Laplace\n   laplace_asymmetric    -- Asymmetric Laplace\n   levy              -- Levy\n   levy_l\n   levy_stable\n   logistic          -- Logistic\n   loggamma          -- Log-Gamma\n   loglaplace        -- Log-Laplace (Log Double Exponential)\n   lognorm           -- Log-Normal\n   loguniform        -- Log-Uniform\n   lomax             -- Lomax (Pareto of the second kind)\n   maxwell           -- Maxwell\n   mielke            -- Mielke's Beta-Kappa\n   moyal             -- Moyal\n   nakagami          -- Nakagami\n   ncx2              -- Non-central chi-squared\n   ncf               -- Non-central F\n   nct               -- Non-central Student's T\n   norm              -- Normal (Gaussian)\n   norminvgauss      -- Normal Inverse Gaussian\n   pareto            -- Pareto\n   pearson3          -- Pearson type III\n   powerlaw          -- Power-function\n   powerlognorm      -- Power log normal\n   powernorm         -- Power normal\n   rdist             -- R-distribution\n   rayleigh          -- Rayleigh\n   rel_breitwigner   -- Relativistic Breit-Wigner\n   rice              -- Rice\n   recipinvgauss     -- Reciprocal Inverse Gaussian\n   semicircular      -- Semicircular\n   skewcauchy        -- Skew Cauchy\n   skewnorm          -- Skew normal\n   studentized_range    -- Studentized Range\n   t                 -- Student's T\n   trapezoid         -- Trapezoidal\n   triang            -- Triangular\n   truncexpon        -- Truncated Exponential\n   truncnorm         -- Truncated Normal\n   truncpareto       -- Truncated Pareto\n   truncweibull_min  -- Truncated minimum Weibull distribution\n   tukeylambda       -- Tukey-Lambda\n   uniform           -- Uniform\n   vonmises          -- Von-Mises (Circular)\n   vonmises_line     -- Von-Mises (Line)\n   wald              -- Wald\n   weibull_min       -- Minimum Weibull (see Frechet)\n   weibull_max       -- Maximum Weibull (see Frechet)\n   wrapcauchy        -- Wrapped Cauchy\n\nThe ``fit`` method of the univariate continuous distributions uses\nmaximum likelihood estimation to fit the distribution to a data set.\nThe ``fit`` method can accept regular data or *censored data*.\nCensored data is represented with instances of the `CensoredData`\nclass.\n\n.. autosummary::\n   :toctree: generated/\n\n   CensoredData\n\n\nMultivariate distributions\n--------------------------\n\n.. autosummary::\n   :toctree: generated/\n\n   multivariate_normal    -- Multivariate normal distribution\n   matrix_normal          -- Matrix normal distribution\n   dirichlet              -- Dirichlet\n   dirichlet_multinomial  -- Dirichlet multinomial distribution\n   wishart                -- Wishart\n   invwishart             -- Inverse Wishart\n   multinomial            -- Multinomial distribution\n   special_ortho_group    -- SO(N) group\n   ortho_group            -- O(N) group\n   unitary_group          -- U(N) group\n   random_correlation     -- random correlation matrices\n   multivariate_t         -- Multivariate t-distribution\n   multivariate_hypergeom -- Multivariate hypergeometric distribution\n   random_table           -- Distribution of random tables with given marginals\n   uniform_direction      -- Uniform distribution on S(N-1)\n   vonmises_fisher        -- Von Mises-Fisher distribution\n\n`scipy.stats.multivariate_normal` methods accept instances\nof the following class to represent the covariance.\n\n.. autosummary::\n   :toctree: generated/\n\n   Covariance             -- Representation of a covariance matrix\n\n\nDiscrete distributions\n----------------------\n\n.. autosummary::\n   :toctree: generated/\n\n   bernoulli                -- Bernoulli\n   betabinom                -- Beta-Binomial\n   betanbinom               -- Beta-Negative Binomial\n   binom                    -- Binomial\n   boltzmann                -- Boltzmann (Truncated Discrete Exponential)\n   dlaplace                 -- Discrete Laplacian\n   geom                     -- Geometric\n   hypergeom                -- Hypergeometric\n   logser                   -- Logarithmic (Log-Series, Series)\n   nbinom                   -- Negative Binomial\n   nchypergeom_fisher       -- Fisher's Noncentral Hypergeometric\n   nchypergeom_wallenius    -- Wallenius's Noncentral Hypergeometric\n   nhypergeom               -- Negative Hypergeometric\n   planck                   -- Planck (Discrete Exponential)\n   poisson                  -- Poisson\n   randint                  -- Discrete Uniform\n   skellam                  -- Skellam\n   yulesimon                -- Yule-Simon\n   zipf                     -- Zipf (Zeta)\n   zipfian                  -- Zipfian\n\n\nAn overview of statistical functions is given below.  Many of these functions\nhave a similar version in `scipy.stats.mstats` which work for masked arrays.\n\nSummary statistics\n==================\n\n.. autosummary::\n   :toctree: generated/\n\n   describe          -- Descriptive statistics\n   gmean             -- Geometric mean\n   hmean             -- Harmonic mean\n   pmean             -- Power mean\n   kurtosis          -- Fisher or Pearson kurtosis\n   mode              -- Modal value\n   moment            -- Central moment\n   expectile         -- Expectile\n   skew              -- Skewness\n   kstat             --\n   kstatvar          --\n   tmean             -- Truncated arithmetic mean\n   tvar              -- Truncated variance\n   tmin              --\n   tmax              --\n   tstd              --\n   tsem              --\n   variation         -- Coefficient of variation\n   find_repeats\n   rankdata\n   tiecorrect\n   trim_mean\n   gstd              -- Geometric Standard Deviation\n   iqr\n   sem\n   bayes_mvs\n   mvsdist\n   entropy\n   differential_entropy\n   median_abs_deviation\n\nFrequency statistics\n====================\n\n.. autosummary::\n   :toctree: generated/\n\n   cumfreq\n   percentileofscore\n   scoreatpercentile\n   relfreq\n\n.. autosummary::\n   :toctree: generated/\n\n   binned_statistic     -- Compute a binned statistic for a set of data.\n   binned_statistic_2d  -- Compute a 2-D binned statistic for a set of data.\n   binned_statistic_dd  -- Compute a d-D binned statistic for a set of data.\n\n.. _hypotests:\n\nHypothesis Tests and related functions\n======================================\nSciPy has many functions for performing hypothesis tests that return a\ntest statistic and a p-value, and several of them return confidence intervals\nand/or other related information.\n\nThe headings below are based on common uses of the functions within, but due to\nthe wide variety of statistical procedures, any attempt at coarse-grained\ncategorization will be imperfect. Also, note that tests within the same heading\nare not interchangeable in general (e.g. many have different distributional\nassumptions).\n\nOne Sample Tests / Paired Sample Tests\n--------------------------------------\nOne sample tests are typically used to assess whether a single sample was\ndrawn from a specified distribution or a distribution with specified properties\n(e.g. zero mean).\n\n.. autosummary::\n   :toctree: generated/\n\n   ttest_1samp\n   binomtest\n   quantile_test\n   skewtest\n   kurtosistest\n   normaltest\n   jarque_bera\n   shapiro\n   anderson\n   cramervonmises\n   ks_1samp\n   goodness_of_fit\n   chisquare\n   power_divergence\n\nPaired sample tests are often used to assess whether two samples were drawn\nfrom the same distribution; they differ from the independent sample tests below\nin that each observation in one sample is treated as paired with a\nclosely-related observation in the other sample (e.g. when environmental\nfactors are controlled between observations within a pair but not among pairs).\nThey can also be interpreted or used as one-sample tests (e.g. tests on the\nmean or median of *differences* between paired observations).\n\n.. autosummary::\n   :toctree: generated/\n\n   ttest_rel\n   wilcoxon\n\nAssociation/Correlation Tests\n-----------------------------\n\nThese tests are often used to assess whether there is a relationship (e.g.\nlinear) between paired observations in multiple samples or among the\ncoordinates of multivariate observations.\n\n.. autosummary::\n   :toctree: generated/\n\n   linregress\n   pearsonr\n   spearmanr\n   pointbiserialr\n   kendalltau\n   weightedtau\n   somersd\n   siegelslopes\n   theilslopes\n   page_trend_test\n   multiscale_graphcorr\n\nThese association tests and are to work with samples in the form of contingency\ntables. Supporting functions are available in `scipy.stats.contingency`.\n\n.. autosummary::\n   :toctree: generated/\n\n   chi2_contingency\n   fisher_exact\n   barnard_exact\n   boschloo_exact\n\nIndependent Sample Tests\n------------------------\nIndependent sample tests are typically used to assess whether multiple samples\nwere independently drawn from the same distribution or different distributions\nwith a shared property (e.g. equal means).\n\nSome tests are specifically for comparing two samples.\n\n.. autosummary::\n   :toctree: generated/\n\n   ttest_ind_from_stats\n   poisson_means_test\n   ttest_ind\n   mannwhitneyu\n   bws_test\n   ranksums\n   brunnermunzel\n   mood\n   ansari\n   cramervonmises_2samp\n   epps_singleton_2samp\n   ks_2samp\n   kstest\n\nOthers are generalized to multiple samples.\n\n.. autosummary::\n   :toctree: generated/\n\n   f_oneway\n   tukey_hsd\n   dunnett\n   kruskal\n   alexandergovern\n   fligner\n   levene\n   bartlett\n   median_test\n   friedmanchisquare\n   anderson_ksamp\n\nResampling and Monte Carlo Methods\n----------------------------------\nThe following functions can reproduce the p-value and confidence interval\nresults of most of the functions above, and often produce accurate results in a\nwider variety of conditions. They can also be used to perform hypothesis tests\nand generate confidence intervals for custom statistics. This flexibility comes\nat the cost of greater computational requirements and stochastic results.\n\n.. autosummary::\n   :toctree: generated/\n\n   monte_carlo_test\n   permutation_test\n   bootstrap\n   power\n\nInstances of the following object can be passed into some hypothesis test\nfunctions to perform a resampling or Monte Carlo version of the hypothesis\ntest.\n\n.. autosummary::\n   :toctree: generated/\n\n   MonteCarloMethod\n   PermutationMethod\n   BootstrapMethod\n\nMultiple Hypothesis Testing and Meta-Analysis\n---------------------------------------------\nThese functions are for assessing the results of individual tests as a whole.\nFunctions for performing specific multiple hypothesis tests (e.g. post hoc\ntests) are listed above.\n\n.. autosummary::\n   :toctree: generated/\n\n   combine_pvalues\n   false_discovery_control\n\n\nThe following functions are related to the tests above but do not belong in the\nabove categories.\n\nQuasi-Monte Carlo\n=================\n\n.. toctree::\n   :maxdepth: 4\n\n   stats.qmc\n\nContingency Tables\n==================\n\n.. toctree::\n   :maxdepth: 4\n\n   stats.contingency\n\nMasked statistics functions\n===========================\n\n.. toctree::\n\n   stats.mstats\n\n\nOther statistical functionality\n===============================\n\nTransformations\n---------------\n\n.. autosummary::\n   :toctree: generated/\n\n   boxcox\n   boxcox_normmax\n   boxcox_llf\n   yeojohnson\n   yeojohnson_normmax\n   yeojohnson_llf\n   obrientransform\n   sigmaclip\n   trimboth\n   trim1\n   zmap\n   zscore\n   gzscore\n\nStatistical distances\n---------------------\n\n.. autosummary::\n   :toctree: generated/\n\n   wasserstein_distance\n   wasserstein_distance_nd\n   energy_distance\n\nSampling\n--------\n\n.. toctree::\n   :maxdepth: 4\n\n   stats.sampling\n\nRandom variate generation / CDF Inversion\n-----------------------------------------\n\n.. autosummary::\n   :toctree: generated/\n\n   rvs_ratio_uniforms\n\nFitting / Survival Analysis\n---------------------------\n\n.. autosummary::\n   :toctree: generated/\n\n   fit\n   ecdf\n   logrank\n\nDirectional statistical functions\n---------------------------------\n\n.. autosummary::\n   :toctree: generated/\n\n   directional_stats\n   circmean\n   circvar\n   circstd\n\nSensitivity Analysis\n--------------------\n\n.. autosummary::\n   :toctree: generated/\n\n   sobol_indices\n\nPlot-tests\n----------\n\n.. autosummary::\n   :toctree: generated/\n\n   ppcc_max\n   ppcc_plot\n   probplot\n   boxcox_normplot\n   yeojohnson_normplot\n\nUnivariate and multivariate kernel density estimation\n-----------------------------------------------------\n\n.. autosummary::\n   :toctree: generated/\n\n   gaussian_kde\n\nWarnings / Errors used in :mod:`scipy.stats`\n--------------------------------------------\n\n.. autosummary::\n   :toctree: generated/\n\n   DegenerateDataWarning\n   ConstantInputWarning\n   NearConstantInputWarning\n   FitError\n\nResult classes used in :mod:`scipy.stats`\n-----------------------------------------\n\n.. warning::\n\n    These classes are private, but they are included here because instances\n    of them are returned by other statistical functions. User import and\n    instantiation is not supported.\n\n.. toctree::\n   :maxdepth: 2\n\n   stats._result_classes\n\n",
    "scipy.stats.BootstrapMethod": "Configuration information for a bootstrap confidence interval.\n\n    Instances of this class can be passed into the `method` parameter of some\n    confidence interval methods to generate a bootstrap confidence interval.\n\n    Attributes\n    ----------\n    n_resamples : int, optional\n        The number of resamples to perform. Default is 9999.\n    batch : int, optional\n        The number of resamples to process in each vectorized call to\n        the statistic. Batch sizes >>1 tend to be faster when the statistic\n        is vectorized, but memory usage scales linearly with the batch size.\n        Default is ``None``, which processes all resamples in a single batch.\n    random_state : {None, int, `numpy.random.Generator`,\n                    `numpy.random.RandomState`}, optional\n\n        Pseudorandom number generator state used to generate resamples.\n\n        If `random_state` is already a ``Generator`` or ``RandomState``\n        instance, then that instance is used.\n        If `random_state` is an int, a new ``RandomState`` instance is used,\n        seeded with `random_state`.\n        If `random_state` is ``None`` (default), the\n        `numpy.random.RandomState` singleton is used.\n\n    method : {'bca', 'percentile', 'basic'}\n        Whether to use the 'percentile' bootstrap ('percentile'), the 'basic'\n        (AKA 'reverse') bootstrap ('basic'), or the bias-corrected and\n        accelerated bootstrap ('BCa', default).\n    ",
    "scipy.stats.CensoredData": "\n    Instances of this class represent censored data.\n\n    Instances may be passed to the ``fit`` method of continuous\n    univariate SciPy distributions for maximum likelihood estimation.\n    The *only* method of the univariate continuous distributions that\n    understands `CensoredData` is the ``fit`` method.  An instance of\n    `CensoredData` can not be passed to methods such as ``pdf`` and\n    ``cdf``.\n\n    An observation is said to be *censored* when the precise value is unknown,\n    but it has a known upper and/or lower bound.  The conventional terminology\n    is:\n\n    * left-censored: an observation is below a certain value but it is\n      unknown by how much.\n    * right-censored: an observation is above a certain value but it is\n      unknown by how much.\n    * interval-censored: an observation lies somewhere on an interval between\n      two values.\n\n    Left-, right-, and interval-censored data can be represented by\n    `CensoredData`.\n\n    For convenience, the class methods ``left_censored`` and\n    ``right_censored`` are provided to create a `CensoredData`\n    instance from a single one-dimensional array of measurements\n    and a corresponding boolean array to indicate which measurements\n    are censored.  The class method ``interval_censored`` accepts two\n    one-dimensional arrays that hold the lower and upper bounds of the\n    intervals.\n\n    Parameters\n    ----------\n    uncensored : array_like, 1D\n        Uncensored observations.\n    left : array_like, 1D\n        Left-censored observations.\n    right : array_like, 1D\n        Right-censored observations.\n    interval : array_like, 2D, with shape (m, 2)\n        Interval-censored observations.  Each row ``interval[k, :]``\n        represents the interval for the kth interval-censored observation.\n\n    Notes\n    -----\n    In the input array `interval`, the lower bound of the interval may\n    be ``-inf``, and the upper bound may be ``inf``, but at least one must be\n    finite. When the lower bound is ``-inf``, the row represents a left-\n    censored observation, and when the upper bound is ``inf``, the row\n    represents a right-censored observation.  If the length of an interval\n    is 0 (i.e. ``interval[k, 0] == interval[k, 1]``, the observation is\n    treated as uncensored.  So one can represent all the types of censored\n    and uncensored data in ``interval``, but it is generally more convenient\n    to use `uncensored`, `left` and `right` for uncensored, left-censored and\n    right-censored observations, respectively.\n\n    Examples\n    --------\n    In the most general case, a censored data set may contain values that\n    are left-censored, right-censored, interval-censored, and uncensored.\n    For example, here we create a data set with five observations.  Two\n    are uncensored (values 1 and 1.5), one is a left-censored observation\n    of 0, one is a right-censored observation of 10 and one is\n    interval-censored in the interval [2, 3].\n\n    >>> import numpy as np\n    >>> from scipy.stats import CensoredData\n    >>> data = CensoredData(uncensored=[1, 1.5], left=[0], right=[10],\n    ...                     interval=[[2, 3]])\n    >>> print(data)\n    CensoredData(5 values: 2 not censored, 1 left-censored,\n    1 right-censored, 1 interval-censored)\n\n    Equivalently,\n\n    >>> data = CensoredData(interval=[[1, 1],\n    ...                               [1.5, 1.5],\n    ...                               [-np.inf, 0],\n    ...                               [10, np.inf],\n    ...                               [2, 3]])\n    >>> print(data)\n    CensoredData(5 values: 2 not censored, 1 left-censored,\n    1 right-censored, 1 interval-censored)\n\n    A common case is to have a mix of uncensored observations and censored\n    observations that are all right-censored (or all left-censored). For\n    example, consider an experiment in which six devices are started at\n    various times and left running until they fail.  Assume that time is\n    measured in hours, and the experiment is stopped after 30 hours, even\n    if all the devices have not failed by that time.  We might end up with\n    data such as this::\n\n        Device  Start-time  Fail-time  Time-to-failure\n           1         0         13           13\n           2         2         24           22\n           3         5         22           17\n           4         8         23           15\n           5        10        ***          >20\n           6        12        ***          >18\n\n    Two of the devices had not failed when the experiment was stopped;\n    the observations of the time-to-failure for these two devices are\n    right-censored.  We can represent this data with\n\n    >>> data = CensoredData(uncensored=[13, 22, 17, 15], right=[20, 18])\n    >>> print(data)\n    CensoredData(6 values: 4 not censored, 2 right-censored)\n\n    Alternatively, we can use the method `CensoredData.right_censored` to\n    create a representation of this data.  The time-to-failure observations\n    are put the list ``ttf``.  The ``censored`` list indicates which values\n    in ``ttf`` are censored.\n\n    >>> ttf = [13, 22, 17, 15, 20, 18]\n    >>> censored = [False, False, False, False, True, True]\n\n    Pass these lists to `CensoredData.right_censored` to create an\n    instance of `CensoredData`.\n\n    >>> data = CensoredData.right_censored(ttf, censored)\n    >>> print(data)\n    CensoredData(6 values: 4 not censored, 2 right-censored)\n\n    If the input data is interval censored and already stored in two\n    arrays, one holding the low end of the intervals and another\n    holding the high ends, the class method ``interval_censored`` can\n    be used to create the `CensoredData` instance.\n\n    This example creates an instance with four interval-censored values.\n    The intervals are [10, 11], [0.5, 1], [2, 3], and [12.5, 13.5].\n\n    >>> a = [10, 0.5, 2, 12.5]  # Low ends of the intervals\n    >>> b = [11, 1.0, 3, 13.5]  # High ends of the intervals\n    >>> data = CensoredData.interval_censored(low=a, high=b)\n    >>> print(data)\n    CensoredData(4 values: 0 not censored, 4 interval-censored)\n\n    Finally, we create and censor some data from the `weibull_min`\n    distribution, and then fit `weibull_min` to that data. We'll assume\n    that the location parameter is known to be 0.\n\n    >>> from scipy.stats import weibull_min\n    >>> rng = np.random.default_rng()\n\n    Create the random data set.\n\n    >>> x = weibull_min.rvs(2.5, loc=0, scale=30, size=250, random_state=rng)\n    >>> x[x > 40] = 40  # Right-censor values greater or equal to 40.\n\n    Create the `CensoredData` instance with the `right_censored` method.\n    The censored values are those where the value is 40.\n\n    >>> data = CensoredData.right_censored(x, x == 40)\n    >>> print(data)\n    CensoredData(250 values: 215 not censored, 35 right-censored)\n\n    35 values have been right-censored.\n\n    Fit `weibull_min` to the censored data.  We expect to shape and scale\n    to be approximately 2.5 and 30, respectively.\n\n    >>> weibull_min.fit(data, floc=0)\n    (2.3575922823897315, 0, 30.40650074451254)\n\n    ",
    "scipy.stats.ConstantInputWarning": "Warns when all values in data are exactly equal.",
    "scipy.stats.Covariance": "\n    Representation of a covariance matrix\n\n    Calculations involving covariance matrices (e.g. data whitening,\n    multivariate normal function evaluation) are often performed more\n    efficiently using a decomposition of the covariance matrix instead of the\n    covariance matrix itself. This class allows the user to construct an\n    object representing a covariance matrix using any of several\n    decompositions and perform calculations using a common interface.\n\n    .. note::\n\n        The `Covariance` class cannot be instantiated directly. Instead, use\n        one of the factory methods (e.g. `Covariance.from_diagonal`).\n\n    Examples\n    --------\n    The `Covariance` class is used by calling one of its\n    factory methods to create a `Covariance` object, then pass that\n    representation of the `Covariance` matrix as a shape parameter of a\n    multivariate distribution.\n\n    For instance, the multivariate normal distribution can accept an array\n    representing a covariance matrix:\n\n    >>> from scipy import stats\n    >>> import numpy as np\n    >>> d = [1, 2, 3]\n    >>> A = np.diag(d)  # a diagonal covariance matrix\n    >>> x = [4, -2, 5]  # a point of interest\n    >>> dist = stats.multivariate_normal(mean=[0, 0, 0], cov=A)\n    >>> dist.pdf(x)\n    4.9595685102808205e-08\n\n    but the calculations are performed in a very generic way that does not\n    take advantage of any special properties of the covariance matrix. Because\n    our covariance matrix is diagonal, we can use ``Covariance.from_diagonal``\n    to create an object representing the covariance matrix, and\n    `multivariate_normal` can use this to compute the probability density\n    function more efficiently.\n\n    >>> cov = stats.Covariance.from_diagonal(d)\n    >>> dist = stats.multivariate_normal(mean=[0, 0, 0], cov=cov)\n    >>> dist.pdf(x)\n    4.9595685102808205e-08\n\n    ",
    "scipy.stats.DegenerateDataWarning": "Warns when data is degenerate and results may not be reliable.",
    "scipy.stats.FitError": "Represents an error condition when fitting a distribution to data.",
    "scipy.stats.MonteCarloMethod": "Configuration information for a Monte Carlo hypothesis test.\n\n    Instances of this class can be passed into the `method` parameter of some\n    hypothesis test functions to perform a Monte Carlo version of the\n    hypothesis tests.\n\n    Attributes\n    ----------\n    n_resamples : int, optional\n        The number of Monte Carlo samples to draw. Default is 9999.\n    batch : int, optional\n        The number of Monte Carlo samples to process in each vectorized call to\n        the statistic. Batch sizes >>1 tend to be faster when the statistic\n        is vectorized, but memory usage scales linearly with the batch size.\n        Default is ``None``, which processes all samples in a single batch.\n    rvs : callable or tuple of callables, optional\n        A callable or sequence of callables that generates random variates\n        under the null hypothesis. Each element of `rvs` must be a callable\n        that accepts keyword argument ``size`` (e.g. ``rvs(size=(m, n))``) and\n        returns an N-d array sample of that shape. If `rvs` is a sequence, the\n        number of callables in `rvs` must match the number of samples passed\n        to the hypothesis test in which the `MonteCarloMethod` is used. Default\n        is ``None``, in which case the hypothesis test function chooses values\n        to match the standard version of the hypothesis test. For example,\n        the null hypothesis of `scipy.stats.pearsonr` is typically that the\n        samples are drawn from the standard normal distribution, so\n        ``rvs = (rng.normal, rng.normal)`` where\n        ``rng = np.random.default_rng()``.\n    ",
    "scipy.stats.NearConstantInputWarning": "Warns when all values in data are nearly equal.",
    "scipy.stats.PermutationMethod": "Configuration information for a permutation hypothesis test.\n\n    Instances of this class can be passed into the `method` parameter of some\n    hypothesis test functions to perform a permutation version of the\n    hypothesis tests.\n\n    Attributes\n    ----------\n    n_resamples : int, optional\n        The number of resamples to perform. Default is 9999.\n    batch : int, optional\n        The number of resamples to process in each vectorized call to\n        the statistic. Batch sizes >>1 tend to be faster when the statistic\n        is vectorized, but memory usage scales linearly with the batch size.\n        Default is ``None``, which processes all resamples in a single batch.\n    random_state : {None, int, `numpy.random.Generator`,\n                    `numpy.random.RandomState`}, optional\n\n        Pseudorandom number generator state used to generate resamples.\n\n        If `random_state` is already a ``Generator`` or ``RandomState``\n        instance, then that instance is used.\n        If `random_state` is an int, a new ``RandomState`` instance is used,\n        seeded with `random_state`.\n        If `random_state` is ``None`` (default), the\n        `numpy.random.RandomState` singleton is used.\n    ",
    "scipy.stats.alexandergovern": "    \n\n\nPerforms the Alexander Govern test.\n\nThe Alexander-Govern approximation tests the equality of k independent\nmeans in the face of heterogeneity of variance. The test is applied to\nsamples from two or more groups, possibly with differing sizes.\n\nParameters\n----------\nsample1, sample2, ... : array_like\n    The sample measurements for each group.  There must be at least\n    two samples, and each sample must contain at least two observations.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nres : AlexanderGovernResult\n    An object with attributes:\n    \n    statistic : float\n        The computed A statistic of the test.\n    pvalue : float\n        The associated p-value from the chi-squared distribution.\n\nWarns\n-----\n`~scipy.stats.ConstantInputWarning`\n    Raised if an input is a constant array.  The statistic is not defined\n    in this case, so ``np.nan`` is returned.\n\nSee Also\n--------\n\n:func:`f_oneway`\n    one-way ANOVA\n\n\nNotes\n-----\nThe use of this test relies on several assumptions.\n\n1. The samples are independent.\n2. Each sample is from a normally distributed population.\n3. Unlike `f_oneway`, this test does not assume on homoscedasticity,\n   instead relaxing the assumption of equal variances.\n\nInput samples must be finite, one dimensional, and with size greater than\none.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] Alexander, Ralph A., and Diane M. Govern. \"A New and Simpler\n       Approximation for ANOVA under Variance Heterogeneity.\" Journal\n       of Educational Statistics, vol. 19, no. 2, 1994, pp. 91-101.\n       JSTOR, www.jstor.org/stable/1165140. Accessed 12 Sept. 2020.\n\nExamples\n--------\n>>> from scipy.stats import alexandergovern\n\nHere are some data on annual percentage rate of interest charged on\nnew car loans at nine of the largest banks in four American cities\ntaken from the National Institute of Standards and Technology's\nANOVA dataset.\n\nWe use `alexandergovern` to test the null hypothesis that all cities\nhave the same mean APR against the alternative that the cities do not\nall have the same mean APR. We decide that a significance level of 5%\nis required to reject the null hypothesis in favor of the alternative.\n\n>>> atlanta = [13.75, 13.75, 13.5, 13.5, 13.0, 13.0, 13.0, 12.75, 12.5]\n>>> chicago = [14.25, 13.0, 12.75, 12.5, 12.5, 12.4, 12.3, 11.9, 11.9]\n>>> houston = [14.0, 14.0, 13.51, 13.5, 13.5, 13.25, 13.0, 12.5, 12.5]\n>>> memphis = [15.0, 14.0, 13.75, 13.59, 13.25, 12.97, 12.5, 12.25,\n...           11.89]\n>>> alexandergovern(atlanta, chicago, houston, memphis)\nAlexanderGovernResult(statistic=4.65087071883494,\n                      pvalue=0.19922132490385214)\n\nThe p-value is 0.1992, indicating a nearly 20% chance of observing\nsuch an extreme value of the test statistic under the null hypothesis.\nThis exceeds 5%, so we do not reject the null hypothesis in favor of\nthe alternative.\n",
    "scipy.stats.alpha": "An alpha continuous random variable.\n\n    As an instance of the `rv_continuous` class, `alpha` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, a, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, a, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, a, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, a, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, a, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, a, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, a, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, a, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(a, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, loc=0, scale=1)\n        Median of the distribution.\n    mean(a, loc=0, scale=1)\n        Mean of the distribution.\n    var(a, loc=0, scale=1)\n        Variance of the distribution.\n    std(a, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, a, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `alpha` ([1]_, [2]_) is:\n\n    .. math::\n\n        f(x, a) = \\frac{1}{x^2 \\Phi(a) \\sqrt{2\\pi}} *\n                  \\exp(-\\frac{1}{2} (a-1/x)^2)\n\n    where :math:`\\Phi` is the normal CDF, :math:`x > 0`, and :math:`a > 0`.\n\n    `alpha` takes ``a`` as a shape parameter.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``alpha.pdf(x, a, loc, scale)`` is identically\n    equivalent to ``alpha.pdf(y, a) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] Johnson, Kotz, and Balakrishnan, \"Continuous Univariate\n           Distributions, Volume 1\", Second Edition, John Wiley and Sons,\n           p. 173 (1994).\n    .. [2] Anthony A. Salvia, \"Reliability applications of the Alpha\n           Distribution\", IEEE Transactions on Reliability, Vol. R-34,\n           No. 3, pp. 251-252 (1985).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import alpha\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a = 3.57\n    >>> mean, var, skew, kurt = alpha.stats(a, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(alpha.ppf(0.01, a),\n    ...                 alpha.ppf(0.99, a), 100)\n    >>> ax.plot(x, alpha.pdf(x, a),\n    ...        'r-', lw=5, alpha=0.6, label='alpha pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = alpha(a)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = alpha.ppf([0.001, 0.5, 0.999], a)\n    >>> np.allclose([0.001, 0.5, 0.999], alpha.cdf(vals, a))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = alpha.rvs(a, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.anderson": "Anderson-Darling test for data coming from a particular distribution.\n\n    The Anderson-Darling test tests the null hypothesis that a sample is\n    drawn from a population that follows a particular distribution.\n    For the Anderson-Darling test, the critical values depend on\n    which distribution is being tested against.  This function works\n    for normal, exponential, logistic, weibull_min, or Gumbel (Extreme Value\n    Type I) distributions.\n\n    Parameters\n    ----------\n    x : array_like\n        Array of sample data.\n    dist : {'norm', 'expon', 'logistic', 'gumbel', 'gumbel_l', 'gumbel_r', 'extreme1', 'weibull_min'}, optional\n        The type of distribution to test against.  The default is 'norm'.\n        The names 'extreme1', 'gumbel_l' and 'gumbel' are synonyms for the\n        same distribution.\n\n    Returns\n    -------\n    result : AndersonResult\n        An object with the following attributes:\n\n        statistic : float\n            The Anderson-Darling test statistic.\n        critical_values : list\n            The critical values for this distribution.\n        significance_level : list\n            The significance levels for the corresponding critical values\n            in percents.  The function returns critical values for a\n            differing set of significance levels depending on the\n            distribution that is being tested against.\n        fit_result : `~scipy.stats._result_classes.FitResult`\n            An object containing the results of fitting the distribution to\n            the data.\n\n    See Also\n    --------\n    kstest : The Kolmogorov-Smirnov test for goodness-of-fit.\n\n    Notes\n    -----\n    Critical values provided are for the following significance levels:\n\n    normal/exponential\n        15%, 10%, 5%, 2.5%, 1%\n    logistic\n        25%, 10%, 5%, 2.5%, 1%, 0.5%\n    gumbel_l / gumbel_r\n        25%, 10%, 5%, 2.5%, 1%\n    weibull_min\n        50%, 25%, 15%, 10%, 5%, 2.5%, 1%, 0.5%\n\n    If the returned statistic is larger than these critical values then\n    for the corresponding significance level, the null hypothesis that\n    the data come from the chosen distribution can be rejected.\n    The returned statistic is referred to as 'A2' in the references.\n\n    For `weibull_min`, maximum likelihood estimation is known to be\n    challenging. If the test returns successfully, then the first order\n    conditions for a maximum likehood estimate have been verified and\n    the critical values correspond relatively well to the significance levels,\n    provided that the sample is sufficiently large (>10 observations [7]).\n    However, for some data - especially data with no left tail - `anderson`\n    is likely to result in an error message. In this case, consider\n    performing a custom goodness of fit test using\n    `scipy.stats.monte_carlo_test`.\n\n    References\n    ----------\n    .. [1] https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm\n    .. [2] Stephens, M. A. (1974). EDF Statistics for Goodness of Fit and\n           Some Comparisons, Journal of the American Statistical Association,\n           Vol. 69, pp. 730-737.\n    .. [3] Stephens, M. A. (1976). Asymptotic Results for Goodness-of-Fit\n           Statistics with Unknown Parameters, Annals of Statistics, Vol. 4,\n           pp. 357-369.\n    .. [4] Stephens, M. A. (1977). Goodness of Fit for the Extreme Value\n           Distribution, Biometrika, Vol. 64, pp. 583-588.\n    .. [5] Stephens, M. A. (1977). Goodness of Fit with Special Reference\n           to Tests for Exponentiality , Technical Report No. 262,\n           Department of Statistics, Stanford University, Stanford, CA.\n    .. [6] Stephens, M. A. (1979). Tests of Fit for the Logistic Distribution\n           Based on the Empirical Distribution Function, Biometrika, Vol. 66,\n           pp. 591-595.\n    .. [7] Richard A. Lockhart and Michael A. Stephens \"Estimation and Tests of\n           Fit for the Three-Parameter Weibull Distribution\"\n           Journal of the Royal Statistical Society.Series B(Methodological)\n           Vol. 56, No. 3 (1994), pp. 491-500, Table 0.\n\n    Examples\n    --------\n    Test the null hypothesis that a random sample was drawn from a normal\n    distribution (with unspecified mean and standard deviation).\n\n    >>> import numpy as np\n    >>> from scipy.stats import anderson\n    >>> rng = np.random.default_rng()\n    >>> data = rng.random(size=35)\n    >>> res = anderson(data)\n    >>> res.statistic\n    0.8398018749744764\n    >>> res.critical_values\n    array([0.527, 0.6  , 0.719, 0.839, 0.998])\n    >>> res.significance_level\n    array([15. , 10. ,  5. ,  2.5,  1. ])\n\n    The value of the statistic (barely) exceeds the critical value associated\n    with a significance level of 2.5%, so the null hypothesis may be rejected\n    at a significance level of 2.5%, but not at a significance level of 1%.\n\n    ",
    "scipy.stats.anderson_ksamp": "The Anderson-Darling test for k-samples.\n\n    The k-sample Anderson-Darling test is a modification of the\n    one-sample Anderson-Darling test. It tests the null hypothesis\n    that k-samples are drawn from the same population without having\n    to specify the distribution function of that population. The\n    critical values depend on the number of samples.\n\n    Parameters\n    ----------\n    samples : sequence of 1-D array_like\n        Array of sample data in arrays.\n    midrank : bool, optional\n        Type of Anderson-Darling test which is computed. Default\n        (True) is the midrank test applicable to continuous and\n        discrete populations. If False, the right side empirical\n        distribution is used.\n    method : PermutationMethod, optional\n        Defines the method used to compute the p-value. If `method` is an\n        instance of `PermutationMethod`, the p-value is computed using\n        `scipy.stats.permutation_test` with the provided configuration options\n        and other appropriate settings. Otherwise, the p-value is interpolated\n        from tabulated values.\n\n    Returns\n    -------\n    res : Anderson_ksampResult\n        An object containing attributes:\n\n        statistic : float\n            Normalized k-sample Anderson-Darling test statistic.\n        critical_values : array\n            The critical values for significance levels 25%, 10%, 5%, 2.5%, 1%,\n            0.5%, 0.1%.\n        pvalue : float\n            The approximate p-value of the test. If `method` is not\n            provided, the value is floored / capped at 0.1% / 25%.\n\n    Raises\n    ------\n    ValueError\n        If fewer than 2 samples are provided, a sample is empty, or no\n        distinct observations are in the samples.\n\n    See Also\n    --------\n    ks_2samp : 2 sample Kolmogorov-Smirnov test\n    anderson : 1 sample Anderson-Darling test\n\n    Notes\n    -----\n    [1]_ defines three versions of the k-sample Anderson-Darling test:\n    one for continuous distributions and two for discrete\n    distributions, in which ties between samples may occur. The\n    default of this routine is to compute the version based on the\n    midrank empirical distribution function. This test is applicable\n    to continuous and discrete data. If midrank is set to False, the\n    right side empirical distribution is used for a test for discrete\n    data. According to [1]_, the two discrete test statistics differ\n    only slightly if a few collisions due to round-off errors occur in\n    the test not adjusted for ties between samples.\n\n    The critical values corresponding to the significance levels from 0.01\n    to 0.25 are taken from [1]_. p-values are floored / capped\n    at 0.1% / 25%. Since the range of critical values might be extended in\n    future releases, it is recommended not to test ``p == 0.25``, but rather\n    ``p >= 0.25`` (analogously for the lower bound).\n\n    .. versionadded:: 0.14.0\n\n    References\n    ----------\n    .. [1] Scholz, F. W and Stephens, M. A. (1987), K-Sample\n           Anderson-Darling Tests, Journal of the American Statistical\n           Association, Vol. 82, pp. 918-924.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> res = stats.anderson_ksamp([rng.normal(size=50),\n    ... rng.normal(loc=0.5, size=30)])\n    >>> res.statistic, res.pvalue\n    (1.974403288713695, 0.04991293614572478)\n    >>> res.critical_values\n    array([0.325, 1.226, 1.961, 2.718, 3.752, 4.592, 6.546])\n\n    The null hypothesis that the two random samples come from the same\n    distribution can be rejected at the 5% level because the returned\n    test value is greater than the critical value for 5% (1.961) but\n    not at the 2.5% level. The interpolation gives an approximate\n    p-value of 4.99%.\n\n    >>> samples = [rng.normal(size=50), rng.normal(size=30),\n    ...            rng.normal(size=20)]\n    >>> res = stats.anderson_ksamp(samples)\n    >>> res.statistic, res.pvalue\n    (-0.29103725200789504, 0.25)\n    >>> res.critical_values\n    array([ 0.44925884,  1.3052767 ,  1.9434184 ,  2.57696569,  3.41634856,\n      4.07210043, 5.56419101])\n\n    The null hypothesis cannot be rejected for three samples from an\n    identical distribution. The reported p-value (25%) has been capped and\n    may not be very accurate (since it corresponds to the value 0.449\n    whereas the statistic is -0.291).\n\n    In such cases where the p-value is capped or when sample sizes are\n    small, a permutation test may be more accurate.\n\n    >>> method = stats.PermutationMethod(n_resamples=9999, random_state=rng)\n    >>> res = stats.anderson_ksamp(samples, method=method)\n    >>> res.pvalue\n    0.5254\n\n    ",
    "scipy.stats.anglit": "An anglit continuous random variable.\n\n    As an instance of the `rv_continuous` class, `anglit` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `anglit` is:\n\n    .. math::\n\n        f(x) = \\sin(2x + \\pi/2) = \\cos(2x)\n\n    for :math:`-\\pi/4 \\le x \\le \\pi/4`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``anglit.pdf(x, loc, scale)`` is identically\n    equivalent to ``anglit.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import anglit\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    \n    >>> mean, var, skew, kurt = anglit.stats(moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(anglit.ppf(0.01),\n    ...                 anglit.ppf(0.99), 100)\n    >>> ax.plot(x, anglit.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='anglit pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = anglit()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = anglit.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], anglit.cdf(vals))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = anglit.rvs(size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.ansari": "    \n\n\nPerform the Ansari-Bradley test for equal scale parameters.\n\nThe Ansari-Bradley test ([1]_, [2]_) is a non-parametric test\nfor the equality of the scale parameter of the distributions\nfrom which two samples were drawn. The null hypothesis states that\nthe ratio of the scale of the distribution underlying `x` to the scale\nof the distribution underlying `y` is 1.\n\nParameters\n----------\nx, y : array_like\n    Arrays of sample data.\nalternative : {'two-sided', 'less', 'greater'}, optional\n    Defines the alternative hypothesis. Default is 'two-sided'.\n    The following options are available:\n    \n    * 'two-sided': the ratio of scales is not equal to 1.\n    * 'less': the ratio of scales is less than 1.\n    * 'greater': the ratio of scales is greater than 1.\n    \n    .. versionadded:: 1.7.0\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nstatistic : float\n    The Ansari-Bradley test statistic.\npvalue : float\n    The p-value of the hypothesis test.\n\nSee Also\n--------\n\n:func:`fligner`\n    A non-parametric test for the equality of k variances\n:func:`mood`\n    A non-parametric test for the equality of two scale parameters\n\n\nNotes\n-----\nThe p-value given is exact when the sample sizes are both less than\n55 and there are no ties, otherwise a normal approximation for the\np-value is used.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] Ansari, A. R. and Bradley, R. A. (1960) Rank-sum tests for\n       dispersions, Annals of Mathematical Statistics, 31, 1174-1189.\n.. [2] Sprent, Peter and N.C. Smeeton.  Applied nonparametric\n       statistical methods.  3rd ed. Chapman and Hall/CRC. 2001.\n       Section 5.8.2.\n.. [3] Nathaniel E. Helwig \"Nonparametric Dispersion and Equality\n       Tests\" at http://users.stat.umn.edu/~helwig/notes/npde-Notes.pdf\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy.stats import ansari\n>>> rng = np.random.default_rng()\n\nFor these examples, we'll create three random data sets.  The first\ntwo, with sizes 35 and 25, are drawn from a normal distribution with\nmean 0 and standard deviation 2.  The third data set has size 25 and\nis drawn from a normal distribution with standard deviation 1.25.\n\n>>> x1 = rng.normal(loc=0, scale=2, size=35)\n>>> x2 = rng.normal(loc=0, scale=2, size=25)\n>>> x3 = rng.normal(loc=0, scale=1.25, size=25)\n\nFirst we apply `ansari` to `x1` and `x2`.  These samples are drawn\nfrom the same distribution, so we expect the Ansari-Bradley test\nshould not lead us to conclude that the scales of the distributions\nare different.\n\n>>> ansari(x1, x2)\nAnsariResult(statistic=541.0, pvalue=0.9762532927399098)\n\nWith a p-value close to 1, we cannot conclude that there is a\nsignificant difference in the scales (as expected).\n\nNow apply the test to `x1` and `x3`:\n\n>>> ansari(x1, x3)\nAnsariResult(statistic=425.0, pvalue=0.0003087020407974518)\n\nThe probability of observing such an extreme value of the statistic\nunder the null hypothesis of equal scales is only 0.03087%. We take this\nas evidence against the null hypothesis in favor of the alternative:\nthe scales of the distributions from which the samples were drawn\nare not equal.\n\nWe can use the `alternative` parameter to perform a one-tailed test.\nIn the above example, the scale of `x1` is greater than `x3` and so\nthe ratio of scales of `x1` and `x3` is greater than 1. This means\nthat the p-value when ``alternative='greater'`` should be near 0 and\nhence we should be able to reject the null hypothesis:\n\n>>> ansari(x1, x3, alternative='greater')\nAnsariResult(statistic=425.0, pvalue=0.0001543510203987259)\n\nAs we can see, the p-value is indeed quite low. Use of\n``alternative='less'`` should thus yield a large p-value:\n\n>>> ansari(x1, x3, alternative='less')\nAnsariResult(statistic=425.0, pvalue=0.9998643258449039)\n",
    "scipy.stats.arcsine": "An arcsine continuous random variable.\n\n    As an instance of the `rv_continuous` class, `arcsine` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `arcsine` is:\n\n    .. math::\n\n        f(x) = \\frac{1}{\\pi \\sqrt{x (1-x)}}\n\n    for :math:`0 < x < 1`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``arcsine.pdf(x, loc, scale)`` is identically\n    equivalent to ``arcsine.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import arcsine\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    \n    >>> mean, var, skew, kurt = arcsine.stats(moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(arcsine.ppf(0.01),\n    ...                 arcsine.ppf(0.99), 100)\n    >>> ax.plot(x, arcsine.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='arcsine pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = arcsine()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = arcsine.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], arcsine.cdf(vals))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = arcsine.rvs(size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.argus": "\n    Argus distribution\n\n    As an instance of the `rv_continuous` class, `argus` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(chi, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, chi, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, chi, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, chi, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, chi, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, chi, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, chi, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, chi, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, chi, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, chi, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(chi, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(chi, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(chi,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(chi, loc=0, scale=1)\n        Median of the distribution.\n    mean(chi, loc=0, scale=1)\n        Mean of the distribution.\n    var(chi, loc=0, scale=1)\n        Variance of the distribution.\n    std(chi, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, chi, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `argus` is:\n\n    .. math::\n\n        f(x, \\chi) = \\frac{\\chi^3}{\\sqrt{2\\pi} \\Psi(\\chi)} x \\sqrt{1-x^2}\n                     \\exp(-\\chi^2 (1 - x^2)/2)\n\n    for :math:`0 < x < 1` and :math:`\\chi > 0`, where\n\n    .. math::\n\n        \\Psi(\\chi) = \\Phi(\\chi) - \\chi \\phi(\\chi) - 1/2\n\n    with :math:`\\Phi` and :math:`\\phi` being the CDF and PDF of a standard\n    normal distribution, respectively.\n\n    `argus` takes :math:`\\chi` as shape a parameter. Details about sampling\n    from the ARGUS distribution can be found in [2]_.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``argus.pdf(x, chi, loc, scale)`` is identically\n    equivalent to ``argus.pdf(y, chi) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] \"ARGUS distribution\",\n           https://en.wikipedia.org/wiki/ARGUS_distribution\n    .. [2] Christoph Baumgarten \"Random variate generation by fast numerical\n           inversion in the varying parameter case.\" Research in Statistics,\n           vol. 1, 2023, doi:10.1080/27684520.2023.2279060.\n\n    .. versionadded:: 0.19.0\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import argus\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> chi = 1\n    >>> mean, var, skew, kurt = argus.stats(chi, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(argus.ppf(0.01, chi),\n    ...                 argus.ppf(0.99, chi), 100)\n    >>> ax.plot(x, argus.pdf(x, chi),\n    ...        'r-', lw=5, alpha=0.6, label='argus pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = argus(chi)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = argus.ppf([0.001, 0.5, 0.999], chi)\n    >>> np.allclose([0.001, 0.5, 0.999], argus.cdf(vals, chi))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = argus.rvs(chi, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n    ",
    "scipy.stats.barnard_exact": "Perform a Barnard exact test on a 2x2 contingency table.\n\n    Parameters\n    ----------\n    table : array_like of ints\n        A 2x2 contingency table.  Elements should be non-negative integers.\n\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the null and alternative hypotheses. Default is 'two-sided'.\n        Please see explanations in the Notes section below.\n\n    pooled : bool, optional\n        Whether to compute score statistic with pooled variance (as in\n        Student's t-test, for example) or unpooled variance (as in Welch's\n        t-test). Default is ``True``.\n\n    n : int, optional\n        Number of sampling points used in the construction of the sampling\n        method. Note that this argument will automatically be converted to\n        the next higher power of 2 since `scipy.stats.qmc.Sobol` is used to\n        select sample points. Default is 32. Must be positive. In most cases,\n        32 points is enough to reach good precision. More points comes at\n        performance cost.\n\n    Returns\n    -------\n    ber : BarnardExactResult\n        A result object with the following attributes.\n\n        statistic : float\n            The Wald statistic with pooled or unpooled variance, depending\n            on the user choice of `pooled`.\n\n        pvalue : float\n            P-value, the probability of obtaining a distribution at least as\n            extreme as the one that was actually observed, assuming that the\n            null hypothesis is true.\n\n    See Also\n    --------\n    chi2_contingency : Chi-square test of independence of variables in a\n        contingency table.\n    fisher_exact : Fisher exact test on a 2x2 contingency table.\n    boschloo_exact : Boschloo's exact test on a 2x2 contingency table,\n        which is an uniformly more powerful alternative to Fisher's exact test.\n\n    Notes\n    -----\n    Barnard's test is an exact test used in the analysis of contingency\n    tables. It examines the association of two categorical variables, and\n    is a more powerful alternative than Fisher's exact test\n    for 2x2 contingency tables.\n\n    Let's define :math:`X_0` a 2x2 matrix representing the observed sample,\n    where each column stores the binomial experiment, as in the example\n    below. Let's also define :math:`p_1, p_2` the theoretical binomial\n    probabilities for  :math:`x_{11}` and :math:`x_{12}`. When using\n    Barnard exact test, we can assert three different null hypotheses :\n\n    - :math:`H_0 : p_1 \\geq p_2` versus :math:`H_1 : p_1 < p_2`,\n      with `alternative` = \"less\"\n\n    - :math:`H_0 : p_1 \\leq p_2` versus :math:`H_1 : p_1 > p_2`,\n      with `alternative` = \"greater\"\n\n    - :math:`H_0 : p_1 = p_2` versus :math:`H_1 : p_1 \\neq p_2`,\n      with `alternative` = \"two-sided\" (default one)\n\n    In order to compute Barnard's exact test, we are using the Wald\n    statistic [3]_ with pooled or unpooled variance.\n    Under the default assumption that both variances are equal\n    (``pooled = True``), the statistic is computed as:\n\n    .. math::\n\n        T(X) = \\frac{\n            \\hat{p}_1 - \\hat{p}_2\n        }{\n            \\sqrt{\n                \\hat{p}(1 - \\hat{p})\n                (\\frac{1}{c_1} +\n                \\frac{1}{c_2})\n            }\n        }\n\n    with :math:`\\hat{p}_1, \\hat{p}_2` and :math:`\\hat{p}` the estimator of\n    :math:`p_1, p_2` and :math:`p`, the latter being the combined probability,\n    given the assumption that :math:`p_1 = p_2`.\n\n    If this assumption is invalid (``pooled = False``), the statistic is:\n\n    .. math::\n\n        T(X) = \\frac{\n            \\hat{p}_1 - \\hat{p}_2\n        }{\n            \\sqrt{\n                \\frac{\\hat{p}_1 (1 - \\hat{p}_1)}{c_1} +\n                \\frac{\\hat{p}_2 (1 - \\hat{p}_2)}{c_2}\n            }\n        }\n\n    The p-value is then computed as:\n\n    .. math::\n\n        \\sum\n            \\binom{c_1}{x_{11}}\n            \\binom{c_2}{x_{12}}\n            \\pi^{x_{11} + x_{12}}\n            (1 - \\pi)^{t - x_{11} - x_{12}}\n\n    where the sum is over all  2x2 contingency tables :math:`X` such that:\n    * :math:`T(X) \\leq T(X_0)` when `alternative` = \"less\",\n    * :math:`T(X) \\geq T(X_0)` when `alternative` = \"greater\", or\n    * :math:`T(X) \\geq |T(X_0)|` when `alternative` = \"two-sided\".\n    Above, :math:`c_1, c_2` are the sum of the columns 1 and 2,\n    and :math:`t` the total (sum of the 4 sample's element).\n\n    The returned p-value is the maximum p-value taken over the nuisance\n    parameter :math:`\\pi`, where :math:`0 \\leq \\pi \\leq 1`.\n\n    This function's complexity is :math:`O(n c_1 c_2)`, where `n` is the\n    number of sample points.\n\n    References\n    ----------\n    .. [1] Barnard, G. A. \"Significance Tests for 2x2 Tables\". *Biometrika*.\n           34.1/2 (1947): 123-138. :doi:`dpgkg3`\n\n    .. [2] Mehta, Cyrus R., and Pralay Senchaudhuri. \"Conditional versus\n           unconditional exact tests for comparing two binomials.\"\n           *Cytel Software Corporation* 675 (2003): 1-5.\n\n    .. [3] \"Wald Test\". *Wikipedia*. https://en.wikipedia.org/wiki/Wald_test\n\n    Examples\n    --------\n    An example use of Barnard's test is presented in [2]_.\n\n        Consider the following example of a vaccine efficacy study\n        (Chan, 1998). In a randomized clinical trial of 30 subjects, 15 were\n        inoculated with a recombinant DNA influenza vaccine and the 15 were\n        inoculated with a placebo. Twelve of the 15 subjects in the placebo\n        group (80%) eventually became infected with influenza whereas for the\n        vaccine group, only 7 of the 15 subjects (47%) became infected. The\n        data are tabulated as a 2 x 2 table::\n\n                Vaccine  Placebo\n            Yes     7        12\n            No      8        3\n\n    When working with statistical hypothesis testing, we usually use a\n    threshold probability or significance level upon which we decide\n    to reject the null hypothesis :math:`H_0`. Suppose we choose the common\n    significance level of 5%.\n\n    Our alternative hypothesis is that the vaccine will lower the chance of\n    becoming infected with the virus; that is, the probability :math:`p_1` of\n    catching the virus with the vaccine will be *less than* the probability\n    :math:`p_2` of catching the virus without the vaccine.  Therefore, we call\n    `barnard_exact` with the ``alternative=\"less\"`` option:\n\n    >>> import scipy.stats as stats\n    >>> res = stats.barnard_exact([[7, 12], [8, 3]], alternative=\"less\")\n    >>> res.statistic\n    -1.894\n    >>> res.pvalue\n    0.03407\n\n    Under the null hypothesis that the vaccine will not lower the chance of\n    becoming infected, the probability of obtaining test results at least as\n    extreme as the observed data is approximately 3.4%. Since this p-value is\n    less than our chosen significance level, we have evidence to reject\n    :math:`H_0` in favor of the alternative.\n\n    Suppose we had used Fisher's exact test instead:\n\n    >>> _, pvalue = stats.fisher_exact([[7, 12], [8, 3]], alternative=\"less\")\n    >>> pvalue\n    0.0640\n\n    With the same threshold significance of 5%, we would not have been able\n    to reject the null hypothesis in favor of the alternative. As stated in\n    [2]_, Barnard's test is uniformly more powerful than Fisher's exact test\n    because Barnard's test does not condition on any margin. Fisher's test\n    should only be used when both sets of marginals are fixed.\n\n    ",
    "scipy.stats.bartlett": "    \n\n\nPerform Bartlett's test for equal variances.\n\nBartlett's test tests the null hypothesis that all input samples\nare from populations with equal variances.  For samples\nfrom significantly non-normal populations, Levene's test\n`levene` is more robust.\n\nParameters\n----------\nsample1, sample2, ... : array_like\n    arrays of sample data.  Only 1d arrays are accepted, they may have\n    different lengths.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nstatistic : float\n    The test statistic.\npvalue : float\n    The p-value of the test.\n\nSee Also\n--------\n\n:func:`fligner`\n    A non-parametric test for the equality of k variances\n:func:`levene`\n    A robust parametric test for equality of k variances\n\n\nNotes\n-----\nConover et al. (1981) examine many of the existing parametric and\nnonparametric tests by extensive simulations and they conclude that the\ntests proposed by Fligner and Killeen (1976) and Levene (1960) appear to be\nsuperior in terms of robustness of departures from normality and power\n([3]_).\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1]  https://www.itl.nist.gov/div898/handbook/eda/section3/eda357.htm\n.. [2]  Snedecor, George W. and Cochran, William G. (1989), Statistical\n          Methods, Eighth Edition, Iowa State University Press.\n.. [3] Park, C. and Lindsay, B. G. (1999). Robust Scale Estimation and\n       Hypothesis Testing based on Quadratic Inference Function. Technical\n       Report #99-03, Center for Likelihood Studies, Pennsylvania State\n       University.\n.. [4] Bartlett, M. S. (1937). Properties of Sufficiency and Statistical\n       Tests. Proceedings of the Royal Society of London. Series A,\n       Mathematical and Physical Sciences, Vol. 160, No.901, pp. 268-282.\n.. [5] C.I. BLISS (1952), The Statistics of Bioassay: With Special\n       Reference to the Vitamins, pp 499-503,\n       :doi:`10.1016/C2013-0-12584-6`.\n.. [6] B. Phipson and G. K. Smyth. \"Permutation P-values Should Never Be\n       Zero: Calculating Exact P-values When Permutations Are Randomly\n       Drawn.\" Statistical Applications in Genetics and Molecular Biology\n       9.1 (2010).\n.. [7] Ludbrook, J., & Dudley, H. (1998). Why permutation tests are\n       superior to t and F tests in biomedical research. The American\n       Statistician, 52(2), 127-132.\n\nExamples\n--------\nIn [5]_, the influence of vitamin C on the tooth growth of guinea pigs\nwas investigated. In a control study, 60 subjects were divided into\nsmall dose, medium dose, and large dose groups that received\ndaily doses of 0.5, 1.0 and 2.0 mg of vitamin C, respectively.\nAfter 42 days, the tooth growth was measured.\n\nThe ``small_dose``, ``medium_dose``, and ``large_dose`` arrays below record\ntooth growth measurements of the three groups in microns.\n\n>>> import numpy as np\n>>> small_dose = np.array([\n...     4.2, 11.5, 7.3, 5.8, 6.4, 10, 11.2, 11.2, 5.2, 7,\n...     15.2, 21.5, 17.6, 9.7, 14.5, 10, 8.2, 9.4, 16.5, 9.7\n... ])\n>>> medium_dose = np.array([\n...     16.5, 16.5, 15.2, 17.3, 22.5, 17.3, 13.6, 14.5, 18.8, 15.5,\n...     19.7, 23.3, 23.6, 26.4, 20, 25.2, 25.8, 21.2, 14.5, 27.3\n... ])\n>>> large_dose = np.array([\n...     23.6, 18.5, 33.9, 25.5, 26.4, 32.5, 26.7, 21.5, 23.3, 29.5,\n...     25.5, 26.4, 22.4, 24.5, 24.8, 30.9, 26.4, 27.3, 29.4, 23\n... ])\n\nThe `bartlett` statistic is sensitive to differences in variances\nbetween the samples.\n\n>>> from scipy import stats\n>>> res = stats.bartlett(small_dose, medium_dose, large_dose)\n>>> res.statistic\n0.6654670663030519\n\nThe value of the statistic tends to be high when there is a large\ndifference in variances.\n\nWe can test for inequality of variance among the groups by comparing the\nobserved value of the statistic against the null distribution: the\ndistribution of statistic values derived under the null hypothesis that\nthe population variances of the three groups are equal.\n\nFor this test, the null distribution follows the chi-square distribution\nas shown below.\n\n>>> import matplotlib.pyplot as plt\n>>> k = 3  # number of samples\n>>> dist = stats.chi2(df=k-1)\n>>> val = np.linspace(0, 5, 100)\n>>> pdf = dist.pdf(val)\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> def plot(ax):  # we'll reuse this\n...     ax.plot(val, pdf, color='C0')\n...     ax.set_title(\"Bartlett Test Null Distribution\")\n...     ax.set_xlabel(\"statistic\")\n...     ax.set_ylabel(\"probability density\")\n...     ax.set_xlim(0, 5)\n...     ax.set_ylim(0, 1)\n>>> plot(ax)\n>>> plt.show()\n\nThe comparison is quantified by the p-value: the proportion of values in\nthe null distribution greater than or equal to the observed value of the\nstatistic.\n\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> plot(ax)\n>>> pvalue = dist.sf(res.statistic)\n>>> annotation = (f'p-value={pvalue:.3f}\\n(shaded area)')\n>>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n>>> _ = ax.annotate(annotation, (1.5, 0.22), (2.25, 0.3), arrowprops=props)\n>>> i = val >= res.statistic\n>>> ax.fill_between(val[i], y1=0, y2=pdf[i], color='C0')\n>>> plt.show()\n\n>>> res.pvalue\n0.71696121509966\n\nIf the p-value is \"small\" - that is, if there is a low probability of\nsampling data from distributions with identical variances that produces\nsuch an extreme value of the statistic - this may be taken as evidence\nagainst the null hypothesis in favor of the alternative: the variances of\nthe groups are not equal. Note that:\n\n- The inverse is not true; that is, the test is not used to provide\n  evidence for the null hypothesis.\n- The threshold for values that will be considered \"small\" is a choice that\n  should be made before the data is analyzed [6]_ with consideration of the\n  risks of both false positives (incorrectly rejecting the null hypothesis)\n  and false negatives (failure to reject a false null hypothesis).\n- Small p-values are not evidence for a *large* effect; rather, they can\n  only provide evidence for a \"significant\" effect, meaning that they are\n  unlikely to have occurred under the null hypothesis.\n\nNote that the chi-square distribution provides the null distribution\nwhen the observations are normally distributed. For small samples\ndrawn from non-normal populations, it may be more appropriate to\nperform a\npermutation test: Under the null hypothesis that all three samples were\ndrawn from the same population, each of the measurements is equally likely\nto have been observed in any of the three samples. Therefore, we can form\na randomized null distribution by calculating the statistic under many\nrandomly-generated partitionings of the observations into the three\nsamples.\n\n>>> def statistic(*samples):\n...     return stats.bartlett(*samples).statistic\n>>> ref = stats.permutation_test(\n...     (small_dose, medium_dose, large_dose), statistic,\n...     permutation_type='independent', alternative='greater'\n... )\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> plot(ax)\n>>> bins = np.linspace(0, 5, 25)\n>>> ax.hist(\n...     ref.null_distribution, bins=bins, density=True, facecolor=\"C1\"\n... )\n>>> ax.legend(['aymptotic approximation\\n(many observations)',\n...            'randomized null distribution'])\n>>> plot(ax)\n>>> plt.show()\n\n>>> ref.pvalue  # randomized test p-value\n0.5387  # may vary\n\nNote that there is significant disagreement between the p-value calculated\nhere and the asymptotic approximation returned by `bartlett` above.\nThe statistical inferences that can be drawn rigorously from a permutation\ntest are limited; nonetheless, they may be the preferred approach in many\ncircumstances [7]_.\n\nFollowing is another generic example where the null hypothesis would be\nrejected.\n\nTest whether the lists `a`, `b` and `c` come from populations\nwith equal variances.\n\n>>> a = [8.88, 9.12, 9.04, 8.98, 9.00, 9.08, 9.01, 8.85, 9.06, 8.99]\n>>> b = [8.88, 8.95, 9.29, 9.44, 9.15, 9.58, 8.36, 9.18, 8.67, 9.05]\n>>> c = [8.95, 9.12, 8.95, 8.85, 9.03, 8.84, 9.07, 8.98, 8.86, 8.98]\n>>> stat, p = stats.bartlett(a, b, c)\n>>> p\n1.1254782518834628e-05\n\nThe very small p-value suggests that the populations do not have equal\nvariances.\n\nThis is not surprising, given that the sample variance of `b` is much\nlarger than that of `a` and `c`:\n\n>>> [np.var(x, ddof=1) for x in [a, b, c]]\n[0.007054444444444413, 0.13073888888888888, 0.008890000000000002]\n",
    "scipy.stats.bayes_mvs": "\n    Bayesian confidence intervals for the mean, var, and std.\n\n    Parameters\n    ----------\n    data : array_like\n        Input data, if multi-dimensional it is flattened to 1-D by `bayes_mvs`.\n        Requires 2 or more data points.\n    alpha : float, optional\n        Probability that the returned confidence interval contains\n        the true parameter.\n\n    Returns\n    -------\n    mean_cntr, var_cntr, std_cntr : tuple\n        The three results are for the mean, variance and standard deviation,\n        respectively.  Each result is a tuple of the form::\n\n            (center, (lower, upper))\n\n        with `center` the mean of the conditional pdf of the value given the\n        data, and `(lower, upper)` a confidence interval, centered on the\n        median, containing the estimate to a probability ``alpha``.\n\n    See Also\n    --------\n    mvsdist\n\n    Notes\n    -----\n    Each tuple of mean, variance, and standard deviation estimates represent\n    the (center, (lower, upper)) with center the mean of the conditional pdf\n    of the value given the data and (lower, upper) is a confidence interval\n    centered on the median, containing the estimate to a probability\n    ``alpha``.\n\n    Converts data to 1-D and assumes all data has the same mean and variance.\n    Uses Jeffrey's prior for variance and std.\n\n    Equivalent to ``tuple((x.mean(), x.interval(alpha)) for x in mvsdist(dat))``\n\n    References\n    ----------\n    T.E. Oliphant, \"A Bayesian perspective on estimating mean, variance, and\n    standard-deviation from data\", https://scholarsarchive.byu.edu/facpub/278,\n    2006.\n\n    Examples\n    --------\n    First a basic example to demonstrate the outputs:\n\n    >>> from scipy import stats\n    >>> data = [6, 9, 12, 7, 8, 8, 13]\n    >>> mean, var, std = stats.bayes_mvs(data)\n    >>> mean\n    Mean(statistic=9.0, minmax=(7.103650222612533, 10.896349777387467))\n    >>> var\n    Variance(statistic=10.0, minmax=(3.176724206, 24.45910382))\n    >>> std\n    Std_dev(statistic=2.9724954732045084,\n            minmax=(1.7823367265645143, 4.945614605014631))\n\n    Now we generate some normally distributed random data, and get estimates of\n    mean and standard deviation with 95% confidence intervals for those\n    estimates:\n\n    >>> n_samples = 100000\n    >>> data = stats.norm.rvs(size=n_samples)\n    >>> res_mean, res_var, res_std = stats.bayes_mvs(data, alpha=0.95)\n\n    >>> import matplotlib.pyplot as plt\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> ax.hist(data, bins=100, density=True, label='Histogram of data')\n    >>> ax.vlines(res_mean.statistic, 0, 0.5, colors='r', label='Estimated mean')\n    >>> ax.axvspan(res_mean.minmax[0],res_mean.minmax[1], facecolor='r',\n    ...            alpha=0.2, label=r'Estimated mean (95% limits)')\n    >>> ax.vlines(res_std.statistic, 0, 0.5, colors='g', label='Estimated scale')\n    >>> ax.axvspan(res_std.minmax[0],res_std.minmax[1], facecolor='g', alpha=0.2,\n    ...            label=r'Estimated scale (95% limits)')\n\n    >>> ax.legend(fontsize=10)\n    >>> ax.set_xlim([-4, 4])\n    >>> ax.set_ylim([0, 0.5])\n    >>> plt.show()\n\n    ",
    "scipy.stats.bernoulli": "A Bernoulli discrete random variable.\n\n    As an instance of the `rv_discrete` class, `bernoulli` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(p, loc=0, size=1, random_state=None)\n        Random variates.\n    pmf(k, p, loc=0)\n        Probability mass function.\n    logpmf(k, p, loc=0)\n        Log of the probability mass function.\n    cdf(k, p, loc=0)\n        Cumulative distribution function.\n    logcdf(k, p, loc=0)\n        Log of the cumulative distribution function.\n    sf(k, p, loc=0)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(k, p, loc=0)\n        Log of the survival function.\n    ppf(q, p, loc=0)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, p, loc=0)\n        Inverse survival function (inverse of ``sf``).\n    stats(p, loc=0, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(p, loc=0)\n        (Differential) entropy of the RV.\n    expect(func, args=(p,), loc=0, lb=None, ub=None, conditional=False)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(p, loc=0)\n        Median of the distribution.\n    mean(p, loc=0)\n        Mean of the distribution.\n    var(p, loc=0)\n        Variance of the distribution.\n    std(p, loc=0)\n        Standard deviation of the distribution.\n    interval(confidence, p, loc=0)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability mass function for `bernoulli` is:\n\n    .. math::\n\n       f(k) = \\begin{cases}1-p  &\\text{if } k = 0\\\\\n                           p    &\\text{if } k = 1\\end{cases}\n\n    for :math:`k` in :math:`\\{0, 1\\}`, :math:`0 \\leq p \\leq 1`\n\n    `bernoulli` takes :math:`p` as shape parameter,\n    where :math:`p` is the probability of a single success\n    and :math:`1-p` is the probability of a single failure.\n\n    The probability mass function above is defined in the \"standardized\" form.\n    To shift distribution use the ``loc`` parameter.\n    Specifically, ``bernoulli.pmf(k, p, loc)`` is identically\n    equivalent to ``bernoulli.pmf(k - loc, p)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import bernoulli\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> p = 0.3\n    >>> mean, var, skew, kurt = bernoulli.stats(p, moments='mvsk')\n    \n    Display the probability mass function (``pmf``):\n    \n    >>> x = np.arange(bernoulli.ppf(0.01, p),\n    ...               bernoulli.ppf(0.99, p))\n    >>> ax.plot(x, bernoulli.pmf(x, p), 'bo', ms=8, label='bernoulli pmf')\n    >>> ax.vlines(x, 0, bernoulli.pmf(x, p), colors='b', lw=5, alpha=0.5)\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape and location. This returns a \"frozen\" RV object holding\n    the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pmf``:\n    \n    >>> rv = bernoulli(p)\n    >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n    ...         label='frozen pmf')\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> prob = bernoulli.cdf(x, p)\n    >>> np.allclose(x, bernoulli.ppf(prob, p))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = bernoulli.rvs(p, size=1000)\n\n    ",
    "scipy.stats.beta": "A beta continuous random variable.\n\n    As an instance of the `rv_continuous` class, `beta` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, a, b, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, a, b, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, a, b, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, a, b, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, a, b, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, a, b, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, a, b, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, b, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, a, b, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(a, b, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, b, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, b, loc=0, scale=1)\n        Median of the distribution.\n    mean(a, b, loc=0, scale=1)\n        Mean of the distribution.\n    var(a, b, loc=0, scale=1)\n        Variance of the distribution.\n    std(a, b, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, a, b, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `beta` is:\n\n    .. math::\n\n        f(x, a, b) = \\frac{\\Gamma(a+b) x^{a-1} (1-x)^{b-1}}\n                          {\\Gamma(a) \\Gamma(b)}\n\n    for :math:`0 <= x <= 1`, :math:`a > 0`, :math:`b > 0`, where\n    :math:`\\Gamma` is the gamma function (`scipy.special.gamma`).\n\n    `beta` takes :math:`a` and :math:`b` as shape parameters.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``beta.pdf(x, a, b, loc, scale)`` is identically\n    equivalent to ``beta.pdf(y, a, b) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import beta\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a, b = 2.31, 0.627\n    >>> mean, var, skew, kurt = beta.stats(a, b, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(beta.ppf(0.01, a, b),\n    ...                 beta.ppf(0.99, a, b), 100)\n    >>> ax.plot(x, beta.pdf(x, a, b),\n    ...        'r-', lw=5, alpha=0.6, label='beta pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = beta(a, b)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = beta.ppf([0.001, 0.5, 0.999], a, b)\n    >>> np.allclose([0.001, 0.5, 0.999], beta.cdf(vals, a, b))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = beta.rvs(a, b, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.betabinom": "A beta-binomial discrete random variable.\n\n    As an instance of the `rv_discrete` class, `betabinom` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(n, a, b, loc=0, size=1, random_state=None)\n        Random variates.\n    pmf(k, n, a, b, loc=0)\n        Probability mass function.\n    logpmf(k, n, a, b, loc=0)\n        Log of the probability mass function.\n    cdf(k, n, a, b, loc=0)\n        Cumulative distribution function.\n    logcdf(k, n, a, b, loc=0)\n        Log of the cumulative distribution function.\n    sf(k, n, a, b, loc=0)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(k, n, a, b, loc=0)\n        Log of the survival function.\n    ppf(q, n, a, b, loc=0)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, n, a, b, loc=0)\n        Inverse survival function (inverse of ``sf``).\n    stats(n, a, b, loc=0, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(n, a, b, loc=0)\n        (Differential) entropy of the RV.\n    expect(func, args=(n, a, b), loc=0, lb=None, ub=None, conditional=False)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(n, a, b, loc=0)\n        Median of the distribution.\n    mean(n, a, b, loc=0)\n        Mean of the distribution.\n    var(n, a, b, loc=0)\n        Variance of the distribution.\n    std(n, a, b, loc=0)\n        Standard deviation of the distribution.\n    interval(confidence, n, a, b, loc=0)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The beta-binomial distribution is a binomial distribution with a\n    probability of success `p` that follows a beta distribution.\n\n    The probability mass function for `betabinom` is:\n\n    .. math::\n\n       f(k) = \\binom{n}{k} \\frac{B(k + a, n - k + b)}{B(a, b)}\n\n    for :math:`k \\in \\{0, 1, \\dots, n\\}`, :math:`n \\geq 0`, :math:`a > 0`,\n    :math:`b > 0`, where :math:`B(a, b)` is the beta function.\n\n    `betabinom` takes :math:`n`, :math:`a`, and :math:`b` as shape parameters.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Beta-binomial_distribution\n\n    The probability mass function above is defined in the \"standardized\" form.\n    To shift distribution use the ``loc`` parameter.\n    Specifically, ``betabinom.pmf(k, n, a, b, loc)`` is identically\n    equivalent to ``betabinom.pmf(k - loc, n, a, b)``.\n\n    .. versionadded:: 1.4.0\n\n    See Also\n    --------\n    beta, binom\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import betabinom\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> n, a, b = 5, 2.3, 0.63\n    >>> mean, var, skew, kurt = betabinom.stats(n, a, b, moments='mvsk')\n    \n    Display the probability mass function (``pmf``):\n    \n    >>> x = np.arange(betabinom.ppf(0.01, n, a, b),\n    ...               betabinom.ppf(0.99, n, a, b))\n    >>> ax.plot(x, betabinom.pmf(x, n, a, b), 'bo', ms=8, label='betabinom pmf')\n    >>> ax.vlines(x, 0, betabinom.pmf(x, n, a, b), colors='b', lw=5, alpha=0.5)\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape and location. This returns a \"frozen\" RV object holding\n    the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pmf``:\n    \n    >>> rv = betabinom(n, a, b)\n    >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n    ...         label='frozen pmf')\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> prob = betabinom.cdf(x, n, a, b)\n    >>> np.allclose(x, betabinom.ppf(prob, n, a, b))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = betabinom.rvs(n, a, b, size=1000)\n\n    ",
    "scipy.stats.betanbinom": "A beta-negative-binomial discrete random variable.\n\n    As an instance of the `rv_discrete` class, `betanbinom` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(n, a, b, loc=0, size=1, random_state=None)\n        Random variates.\n    pmf(k, n, a, b, loc=0)\n        Probability mass function.\n    logpmf(k, n, a, b, loc=0)\n        Log of the probability mass function.\n    cdf(k, n, a, b, loc=0)\n        Cumulative distribution function.\n    logcdf(k, n, a, b, loc=0)\n        Log of the cumulative distribution function.\n    sf(k, n, a, b, loc=0)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(k, n, a, b, loc=0)\n        Log of the survival function.\n    ppf(q, n, a, b, loc=0)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, n, a, b, loc=0)\n        Inverse survival function (inverse of ``sf``).\n    stats(n, a, b, loc=0, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(n, a, b, loc=0)\n        (Differential) entropy of the RV.\n    expect(func, args=(n, a, b), loc=0, lb=None, ub=None, conditional=False)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(n, a, b, loc=0)\n        Median of the distribution.\n    mean(n, a, b, loc=0)\n        Mean of the distribution.\n    var(n, a, b, loc=0)\n        Variance of the distribution.\n    std(n, a, b, loc=0)\n        Standard deviation of the distribution.\n    interval(confidence, n, a, b, loc=0)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The beta-negative-binomial distribution is a negative binomial\n    distribution with a probability of success `p` that follows a\n    beta distribution.\n\n    The probability mass function for `betanbinom` is:\n\n    .. math::\n\n       f(k) = \\binom{n + k - 1}{k} \\frac{B(a + n, b + k)}{B(a, b)}\n\n    for :math:`k \\ge 0`, :math:`n \\geq 0`, :math:`a > 0`,\n    :math:`b > 0`, where :math:`B(a, b)` is the beta function.\n\n    `betanbinom` takes :math:`n`, :math:`a`, and :math:`b` as shape parameters.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Beta_negative_binomial_distribution\n\n    The probability mass function above is defined in the \"standardized\" form.\n    To shift distribution use the ``loc`` parameter.\n    Specifically, ``betanbinom.pmf(k, n, a, b, loc)`` is identically\n    equivalent to ``betanbinom.pmf(k - loc, n, a, b)``.\n\n    .. versionadded:: 1.12.0\n\n    See Also\n    --------\n    betabinom : Beta binomial distribution\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import betanbinom\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> n, a, b = 5, 9.3, 1\n    >>> mean, var, skew, kurt = betanbinom.stats(n, a, b, moments='mvsk')\n    \n    Display the probability mass function (``pmf``):\n    \n    >>> x = np.arange(betanbinom.ppf(0.01, n, a, b),\n    ...               betanbinom.ppf(0.99, n, a, b))\n    >>> ax.plot(x, betanbinom.pmf(x, n, a, b), 'bo', ms=8, label='betanbinom pmf')\n    >>> ax.vlines(x, 0, betanbinom.pmf(x, n, a, b), colors='b', lw=5, alpha=0.5)\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape and location. This returns a \"frozen\" RV object holding\n    the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pmf``:\n    \n    >>> rv = betanbinom(n, a, b)\n    >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n    ...         label='frozen pmf')\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> prob = betanbinom.cdf(x, n, a, b)\n    >>> np.allclose(x, betanbinom.ppf(prob, n, a, b))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = betanbinom.rvs(n, a, b, size=1000)\n\n    ",
    "scipy.stats.betaprime": "A beta prime continuous random variable.\n\n    As an instance of the `rv_continuous` class, `betaprime` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, a, b, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, a, b, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, a, b, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, a, b, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, a, b, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, a, b, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, a, b, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, b, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, a, b, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(a, b, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, b, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, b, loc=0, scale=1)\n        Median of the distribution.\n    mean(a, b, loc=0, scale=1)\n        Mean of the distribution.\n    var(a, b, loc=0, scale=1)\n        Variance of the distribution.\n    std(a, b, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, a, b, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `betaprime` is:\n\n    .. math::\n\n        f(x, a, b) = \\frac{x^{a-1} (1+x)^{-a-b}}{\\beta(a, b)}\n\n    for :math:`x >= 0`, :math:`a > 0`, :math:`b > 0`, where\n    :math:`\\beta(a, b)` is the beta function (see `scipy.special.beta`).\n\n    `betaprime` takes ``a`` and ``b`` as shape parameters.\n\n    The distribution is related to the `beta` distribution as follows:\n    If :math:`X` follows a beta distribution with parameters :math:`a, b`,\n    then :math:`Y = X/(1-X)` has a beta prime distribution with\n    parameters :math:`a, b` ([1]_).\n\n    The beta prime distribution is a reparametrized version of the\n    F distribution.  The beta prime distribution with shape parameters\n    ``a`` and ``b`` and ``scale = s`` is equivalent to the F distribution\n    with parameters ``d1 = 2*a``, ``d2 = 2*b`` and ``scale = (a/b)*s``.\n    For example,\n\n    >>> from scipy.stats import betaprime, f\n    >>> x = [1, 2, 5, 10]\n    >>> a = 12\n    >>> b = 5\n    >>> betaprime.pdf(x, a, b, scale=2)\n    array([0.00541179, 0.08331299, 0.14669185, 0.03150079])\n    >>> f.pdf(x, 2*a, 2*b, scale=(a/b)*2)\n    array([0.00541179, 0.08331299, 0.14669185, 0.03150079])\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``betaprime.pdf(x, a, b, loc, scale)`` is identically\n    equivalent to ``betaprime.pdf(y, a, b) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] Beta prime distribution, Wikipedia,\n           https://en.wikipedia.org/wiki/Beta_prime_distribution\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import betaprime\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a, b = 5, 6\n    >>> mean, var, skew, kurt = betaprime.stats(a, b, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(betaprime.ppf(0.01, a, b),\n    ...                 betaprime.ppf(0.99, a, b), 100)\n    >>> ax.plot(x, betaprime.pdf(x, a, b),\n    ...        'r-', lw=5, alpha=0.6, label='betaprime pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = betaprime(a, b)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = betaprime.ppf([0.001, 0.5, 0.999], a, b)\n    >>> np.allclose([0.001, 0.5, 0.999], betaprime.cdf(vals, a, b))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = betaprime.rvs(a, b, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.binned_statistic": "\n    Compute a binned statistic for one or more sets of data.\n\n    This is a generalization of a histogram function.  A histogram divides\n    the space into bins, and returns the count of the number of points in\n    each bin.  This function allows the computation of the sum, mean, median,\n    or other statistic of the values (or set of values) within each bin.\n\n    Parameters\n    ----------\n    x : (N,) array_like\n        A sequence of values to be binned.\n    values : (N,) array_like or list of (N,) array_like\n        The data on which the statistic will be computed.  This must be\n        the same shape as `x`, or a set of sequences - each the same shape as\n        `x`.  If `values` is a set of sequences, the statistic will be computed\n        on each independently.\n    statistic : string or callable, optional\n        The statistic to compute (default is 'mean').\n        The following statistics are available:\n\n          * 'mean' : compute the mean of values for points within each bin.\n            Empty bins will be represented by NaN.\n          * 'std' : compute the standard deviation within each bin. This\n            is implicitly calculated with ddof=0.\n          * 'median' : compute the median of values for points within each\n            bin. Empty bins will be represented by NaN.\n          * 'count' : compute the count of points within each bin.  This is\n            identical to an unweighted histogram.  `values` array is not\n            referenced.\n          * 'sum' : compute the sum of values for points within each bin.\n            This is identical to a weighted histogram.\n          * 'min' : compute the minimum of values for points within each bin.\n            Empty bins will be represented by NaN.\n          * 'max' : compute the maximum of values for point within each bin.\n            Empty bins will be represented by NaN.\n          * function : a user-defined function which takes a 1D array of\n            values, and outputs a single numerical statistic. This function\n            will be called on the values in each bin.  Empty bins will be\n            represented by function([]), or NaN if this returns an error.\n\n    bins : int or sequence of scalars, optional\n        If `bins` is an int, it defines the number of equal-width bins in the\n        given range (10 by default).  If `bins` is a sequence, it defines the\n        bin edges, including the rightmost edge, allowing for non-uniform bin\n        widths.  Values in `x` that are smaller than lowest bin edge are\n        assigned to bin number 0, values beyond the highest bin are assigned to\n        ``bins[-1]``.  If the bin edges are specified, the number of bins will\n        be, (nx = len(bins)-1).\n    range : (float, float) or [(float, float)], optional\n        The lower and upper range of the bins.  If not provided, range\n        is simply ``(x.min(), x.max())``.  Values outside the range are\n        ignored.\n\n    Returns\n    -------\n    statistic : array\n        The values of the selected statistic in each bin.\n    bin_edges : array of dtype float\n        Return the bin edges ``(length(statistic)+1)``.\n    binnumber: 1-D ndarray of ints\n        Indices of the bins (corresponding to `bin_edges`) in which each value\n        of `x` belongs.  Same length as `values`.  A binnumber of `i` means the\n        corresponding value is between (bin_edges[i-1], bin_edges[i]).\n\n    See Also\n    --------\n    numpy.digitize, numpy.histogram, binned_statistic_2d, binned_statistic_dd\n\n    Notes\n    -----\n    All but the last (righthand-most) bin is half-open.  In other words, if\n    `bins` is ``[1, 2, 3, 4]``, then the first bin is ``[1, 2)`` (including 1,\n    but excluding 2) and the second ``[2, 3)``.  The last bin, however, is\n    ``[3, 4]``, which *includes* 4.\n\n    .. versionadded:: 0.11.0\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n\n    First some basic examples:\n\n    Create two evenly spaced bins in the range of the given sample, and sum the\n    corresponding values in each of those bins:\n\n    >>> values = [1.0, 1.0, 2.0, 1.5, 3.0]\n    >>> stats.binned_statistic([1, 1, 2, 5, 7], values, 'sum', bins=2)\n    BinnedStatisticResult(statistic=array([4. , 4.5]),\n            bin_edges=array([1., 4., 7.]), binnumber=array([1, 1, 1, 2, 2]))\n\n    Multiple arrays of values can also be passed.  The statistic is calculated\n    on each set independently:\n\n    >>> values = [[1.0, 1.0, 2.0, 1.5, 3.0], [2.0, 2.0, 4.0, 3.0, 6.0]]\n    >>> stats.binned_statistic([1, 1, 2, 5, 7], values, 'sum', bins=2)\n    BinnedStatisticResult(statistic=array([[4. , 4.5],\n           [8. , 9. ]]), bin_edges=array([1., 4., 7.]),\n           binnumber=array([1, 1, 1, 2, 2]))\n\n    >>> stats.binned_statistic([1, 2, 1, 2, 4], np.arange(5), statistic='mean',\n    ...                        bins=3)\n    BinnedStatisticResult(statistic=array([1., 2., 4.]),\n            bin_edges=array([1., 2., 3., 4.]),\n            binnumber=array([1, 2, 1, 2, 3]))\n\n    As a second example, we now generate some random data of sailing boat speed\n    as a function of wind speed, and then determine how fast our boat is for\n    certain wind speeds:\n\n    >>> rng = np.random.default_rng()\n    >>> windspeed = 8 * rng.random(500)\n    >>> boatspeed = .3 * windspeed**.5 + .2 * rng.random(500)\n    >>> bin_means, bin_edges, binnumber = stats.binned_statistic(windspeed,\n    ...                 boatspeed, statistic='median', bins=[1,2,3,4,5,6,7])\n    >>> plt.figure()\n    >>> plt.plot(windspeed, boatspeed, 'b.', label='raw data')\n    >>> plt.hlines(bin_means, bin_edges[:-1], bin_edges[1:], colors='g', lw=5,\n    ...            label='binned statistic of data')\n    >>> plt.legend()\n\n    Now we can use ``binnumber`` to select all datapoints with a windspeed\n    below 1:\n\n    >>> low_boatspeed = boatspeed[binnumber == 0]\n\n    As a final example, we will use ``bin_edges`` and ``binnumber`` to make a\n    plot of a distribution that shows the mean and distribution around that\n    mean per bin, on top of a regular histogram and the probability\n    distribution function:\n\n    >>> x = np.linspace(0, 5, num=500)\n    >>> x_pdf = stats.maxwell.pdf(x)\n    >>> samples = stats.maxwell.rvs(size=10000)\n\n    >>> bin_means, bin_edges, binnumber = stats.binned_statistic(x, x_pdf,\n    ...         statistic='mean', bins=25)\n    >>> bin_width = (bin_edges[1] - bin_edges[0])\n    >>> bin_centers = bin_edges[1:] - bin_width/2\n\n    >>> plt.figure()\n    >>> plt.hist(samples, bins=50, density=True, histtype='stepfilled',\n    ...          alpha=0.2, label='histogram of data')\n    >>> plt.plot(x, x_pdf, 'r-', label='analytical pdf')\n    >>> plt.hlines(bin_means, bin_edges[:-1], bin_edges[1:], colors='g', lw=2,\n    ...            label='binned statistic of data')\n    >>> plt.plot((binnumber - 0.5) * bin_width, x_pdf, 'g.', alpha=0.5)\n    >>> plt.legend(fontsize=10)\n    >>> plt.show()\n\n    ",
    "scipy.stats.binned_statistic_2d": "\n    Compute a bidimensional binned statistic for one or more sets of data.\n\n    This is a generalization of a histogram2d function.  A histogram divides\n    the space into bins, and returns the count of the number of points in\n    each bin.  This function allows the computation of the sum, mean, median,\n    or other statistic of the values (or set of values) within each bin.\n\n    Parameters\n    ----------\n    x : (N,) array_like\n        A sequence of values to be binned along the first dimension.\n    y : (N,) array_like\n        A sequence of values to be binned along the second dimension.\n    values : (N,) array_like or list of (N,) array_like\n        The data on which the statistic will be computed.  This must be\n        the same shape as `x`, or a list of sequences - each with the same\n        shape as `x`.  If `values` is such a list, the statistic will be\n        computed on each independently.\n    statistic : string or callable, optional\n        The statistic to compute (default is 'mean').\n        The following statistics are available:\n\n          * 'mean' : compute the mean of values for points within each bin.\n            Empty bins will be represented by NaN.\n          * 'std' : compute the standard deviation within each bin. This\n            is implicitly calculated with ddof=0.\n          * 'median' : compute the median of values for points within each\n            bin. Empty bins will be represented by NaN.\n          * 'count' : compute the count of points within each bin.  This is\n            identical to an unweighted histogram.  `values` array is not\n            referenced.\n          * 'sum' : compute the sum of values for points within each bin.\n            This is identical to a weighted histogram.\n          * 'min' : compute the minimum of values for points within each bin.\n            Empty bins will be represented by NaN.\n          * 'max' : compute the maximum of values for point within each bin.\n            Empty bins will be represented by NaN.\n          * function : a user-defined function which takes a 1D array of\n            values, and outputs a single numerical statistic. This function\n            will be called on the values in each bin.  Empty bins will be\n            represented by function([]), or NaN if this returns an error.\n\n    bins : int or [int, int] or array_like or [array, array], optional\n        The bin specification:\n\n          * the number of bins for the two dimensions (nx = ny = bins),\n          * the number of bins in each dimension (nx, ny = bins),\n          * the bin edges for the two dimensions (x_edge = y_edge = bins),\n          * the bin edges in each dimension (x_edge, y_edge = bins).\n\n        If the bin edges are specified, the number of bins will be,\n        (nx = len(x_edge)-1, ny = len(y_edge)-1).\n\n    range : (2,2) array_like, optional\n        The leftmost and rightmost edges of the bins along each dimension\n        (if not specified explicitly in the `bins` parameters):\n        [[xmin, xmax], [ymin, ymax]]. All values outside of this range will be\n        considered outliers and not tallied in the histogram.\n    expand_binnumbers : bool, optional\n        'False' (default): the returned `binnumber` is a shape (N,) array of\n        linearized bin indices.\n        'True': the returned `binnumber` is 'unraveled' into a shape (2,N)\n        ndarray, where each row gives the bin numbers in the corresponding\n        dimension.\n        See the `binnumber` returned value, and the `Examples` section.\n\n        .. versionadded:: 0.17.0\n\n    Returns\n    -------\n    statistic : (nx, ny) ndarray\n        The values of the selected statistic in each two-dimensional bin.\n    x_edge : (nx + 1) ndarray\n        The bin edges along the first dimension.\n    y_edge : (ny + 1) ndarray\n        The bin edges along the second dimension.\n    binnumber : (N,) array of ints or (2,N) ndarray of ints\n        This assigns to each element of `sample` an integer that represents the\n        bin in which this observation falls.  The representation depends on the\n        `expand_binnumbers` argument.  See `Notes` for details.\n\n\n    See Also\n    --------\n    numpy.digitize, numpy.histogram2d, binned_statistic, binned_statistic_dd\n\n    Notes\n    -----\n    Binedges:\n    All but the last (righthand-most) bin is half-open.  In other words, if\n    `bins` is ``[1, 2, 3, 4]``, then the first bin is ``[1, 2)`` (including 1,\n    but excluding 2) and the second ``[2, 3)``.  The last bin, however, is\n    ``[3, 4]``, which *includes* 4.\n\n    `binnumber`:\n    This returned argument assigns to each element of `sample` an integer that\n    represents the bin in which it belongs.  The representation depends on the\n    `expand_binnumbers` argument. If 'False' (default): The returned\n    `binnumber` is a shape (N,) array of linearized indices mapping each\n    element of `sample` to its corresponding bin (using row-major ordering).\n    Note that the returned linearized bin indices are used for an array with\n    extra bins on the outer binedges to capture values outside of the defined\n    bin bounds.\n    If 'True': The returned `binnumber` is a shape (2,N) ndarray where\n    each row indicates bin placements for each dimension respectively.  In each\n    dimension, a binnumber of `i` means the corresponding value is between\n    (D_edge[i-1], D_edge[i]), where 'D' is either 'x' or 'y'.\n\n    .. versionadded:: 0.11.0\n\n    Examples\n    --------\n    >>> from scipy import stats\n\n    Calculate the counts with explicit bin-edges:\n\n    >>> x = [0.1, 0.1, 0.1, 0.6]\n    >>> y = [2.1, 2.6, 2.1, 2.1]\n    >>> binx = [0.0, 0.5, 1.0]\n    >>> biny = [2.0, 2.5, 3.0]\n    >>> ret = stats.binned_statistic_2d(x, y, None, 'count', bins=[binx, biny])\n    >>> ret.statistic\n    array([[2., 1.],\n           [1., 0.]])\n\n    The bin in which each sample is placed is given by the `binnumber`\n    returned parameter.  By default, these are the linearized bin indices:\n\n    >>> ret.binnumber\n    array([5, 6, 5, 9])\n\n    The bin indices can also be expanded into separate entries for each\n    dimension using the `expand_binnumbers` parameter:\n\n    >>> ret = stats.binned_statistic_2d(x, y, None, 'count', bins=[binx, biny],\n    ...                                 expand_binnumbers=True)\n    >>> ret.binnumber\n    array([[1, 1, 1, 2],\n           [1, 2, 1, 1]])\n\n    Which shows that the first three elements belong in the xbin 1, and the\n    fourth into xbin 2; and so on for y.\n\n    ",
    "scipy.stats.binned_statistic_dd": "\n    Compute a multidimensional binned statistic for a set of data.\n\n    This is a generalization of a histogramdd function.  A histogram divides\n    the space into bins, and returns the count of the number of points in\n    each bin.  This function allows the computation of the sum, mean, median,\n    or other statistic of the values within each bin.\n\n    Parameters\n    ----------\n    sample : array_like\n        Data to histogram passed as a sequence of N arrays of length D, or\n        as an (N,D) array.\n    values : (N,) array_like or list of (N,) array_like\n        The data on which the statistic will be computed.  This must be\n        the same shape as `sample`, or a list of sequences - each with the\n        same shape as `sample`.  If `values` is such a list, the statistic\n        will be computed on each independently.\n    statistic : string or callable, optional\n        The statistic to compute (default is 'mean').\n        The following statistics are available:\n\n          * 'mean' : compute the mean of values for points within each bin.\n            Empty bins will be represented by NaN.\n          * 'median' : compute the median of values for points within each\n            bin. Empty bins will be represented by NaN.\n          * 'count' : compute the count of points within each bin.  This is\n            identical to an unweighted histogram.  `values` array is not\n            referenced.\n          * 'sum' : compute the sum of values for points within each bin.\n            This is identical to a weighted histogram.\n          * 'std' : compute the standard deviation within each bin. This\n            is implicitly calculated with ddof=0. If the number of values\n            within a given bin is 0 or 1, the computed standard deviation value\n            will be 0 for the bin.\n          * 'min' : compute the minimum of values for points within each bin.\n            Empty bins will be represented by NaN.\n          * 'max' : compute the maximum of values for point within each bin.\n            Empty bins will be represented by NaN.\n          * function : a user-defined function which takes a 1D array of\n            values, and outputs a single numerical statistic. This function\n            will be called on the values in each bin.  Empty bins will be\n            represented by function([]), or NaN if this returns an error.\n\n    bins : sequence or positive int, optional\n        The bin specification must be in one of the following forms:\n\n          * A sequence of arrays describing the bin edges along each dimension.\n          * The number of bins for each dimension (nx, ny, ... = bins).\n          * The number of bins for all dimensions (nx = ny = ... = bins).\n    range : sequence, optional\n        A sequence of lower and upper bin edges to be used if the edges are\n        not given explicitly in `bins`. Defaults to the minimum and maximum\n        values along each dimension.\n    expand_binnumbers : bool, optional\n        'False' (default): the returned `binnumber` is a shape (N,) array of\n        linearized bin indices.\n        'True': the returned `binnumber` is 'unraveled' into a shape (D,N)\n        ndarray, where each row gives the bin numbers in the corresponding\n        dimension.\n        See the `binnumber` returned value, and the `Examples` section of\n        `binned_statistic_2d`.\n    binned_statistic_result : binnedStatisticddResult\n        Result of a previous call to the function in order to reuse bin edges\n        and bin numbers with new values and/or a different statistic.\n        To reuse bin numbers, `expand_binnumbers` must have been set to False\n        (the default)\n\n        .. versionadded:: 0.17.0\n\n    Returns\n    -------\n    statistic : ndarray, shape(nx1, nx2, nx3,...)\n        The values of the selected statistic in each two-dimensional bin.\n    bin_edges : list of ndarrays\n        A list of D arrays describing the (nxi + 1) bin edges for each\n        dimension.\n    binnumber : (N,) array of ints or (D,N) ndarray of ints\n        This assigns to each element of `sample` an integer that represents the\n        bin in which this observation falls.  The representation depends on the\n        `expand_binnumbers` argument.  See `Notes` for details.\n\n\n    See Also\n    --------\n    numpy.digitize, numpy.histogramdd, binned_statistic, binned_statistic_2d\n\n    Notes\n    -----\n    Binedges:\n    All but the last (righthand-most) bin is half-open in each dimension.  In\n    other words, if `bins` is ``[1, 2, 3, 4]``, then the first bin is\n    ``[1, 2)`` (including 1, but excluding 2) and the second ``[2, 3)``.  The\n    last bin, however, is ``[3, 4]``, which *includes* 4.\n\n    `binnumber`:\n    This returned argument assigns to each element of `sample` an integer that\n    represents the bin in which it belongs.  The representation depends on the\n    `expand_binnumbers` argument. If 'False' (default): The returned\n    `binnumber` is a shape (N,) array of linearized indices mapping each\n    element of `sample` to its corresponding bin (using row-major ordering).\n    If 'True': The returned `binnumber` is a shape (D,N) ndarray where\n    each row indicates bin placements for each dimension respectively.  In each\n    dimension, a binnumber of `i` means the corresponding value is between\n    (bin_edges[D][i-1], bin_edges[D][i]), for each dimension 'D'.\n\n    .. versionadded:: 0.11.0\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n    >>> from mpl_toolkits.mplot3d import Axes3D\n\n    Take an array of 600 (x, y) coordinates as an example.\n    `binned_statistic_dd` can handle arrays of higher dimension `D`. But a plot\n    of dimension `D+1` is required.\n\n    >>> mu = np.array([0., 1.])\n    >>> sigma = np.array([[1., -0.5],[-0.5, 1.5]])\n    >>> multinormal = stats.multivariate_normal(mu, sigma)\n    >>> data = multinormal.rvs(size=600, random_state=235412)\n    >>> data.shape\n    (600, 2)\n\n    Create bins and count how many arrays fall in each bin:\n\n    >>> N = 60\n    >>> x = np.linspace(-3, 3, N)\n    >>> y = np.linspace(-3, 4, N)\n    >>> ret = stats.binned_statistic_dd(data, np.arange(600), bins=[x, y],\n    ...                                 statistic='count')\n    >>> bincounts = ret.statistic\n\n    Set the volume and the location of bars:\n\n    >>> dx = x[1] - x[0]\n    >>> dy = y[1] - y[0]\n    >>> x, y = np.meshgrid(x[:-1]+dx/2, y[:-1]+dy/2)\n    >>> z = 0\n\n    >>> bincounts = bincounts.ravel()\n    >>> x = x.ravel()\n    >>> y = y.ravel()\n\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111, projection='3d')\n    >>> with np.errstate(divide='ignore'):   # silence random axes3d warning\n    ...     ax.bar3d(x, y, z, dx, dy, bincounts)\n\n    Reuse bin numbers and bin edges with new values:\n\n    >>> ret2 = stats.binned_statistic_dd(data, -np.arange(600),\n    ...                                  binned_statistic_result=ret,\n    ...                                  statistic='mean')\n    ",
    "scipy.stats.binom": "A binomial discrete random variable.\n\n    As an instance of the `rv_discrete` class, `binom` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(n, p, loc=0, size=1, random_state=None)\n        Random variates.\n    pmf(k, n, p, loc=0)\n        Probability mass function.\n    logpmf(k, n, p, loc=0)\n        Log of the probability mass function.\n    cdf(k, n, p, loc=0)\n        Cumulative distribution function.\n    logcdf(k, n, p, loc=0)\n        Log of the cumulative distribution function.\n    sf(k, n, p, loc=0)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(k, n, p, loc=0)\n        Log of the survival function.\n    ppf(q, n, p, loc=0)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, n, p, loc=0)\n        Inverse survival function (inverse of ``sf``).\n    stats(n, p, loc=0, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(n, p, loc=0)\n        (Differential) entropy of the RV.\n    expect(func, args=(n, p), loc=0, lb=None, ub=None, conditional=False)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(n, p, loc=0)\n        Median of the distribution.\n    mean(n, p, loc=0)\n        Mean of the distribution.\n    var(n, p, loc=0)\n        Variance of the distribution.\n    std(n, p, loc=0)\n        Standard deviation of the distribution.\n    interval(confidence, n, p, loc=0)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability mass function for `binom` is:\n\n    .. math::\n\n       f(k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\n    for :math:`k \\in \\{0, 1, \\dots, n\\}`, :math:`0 \\leq p \\leq 1`\n\n    `binom` takes :math:`n` and :math:`p` as shape parameters,\n    where :math:`p` is the probability of a single success\n    and :math:`1-p` is the probability of a single failure.\n\n    The probability mass function above is defined in the \"standardized\" form.\n    To shift distribution use the ``loc`` parameter.\n    Specifically, ``binom.pmf(k, n, p, loc)`` is identically\n    equivalent to ``binom.pmf(k - loc, n, p)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import binom\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> n, p = 5, 0.4\n    >>> mean, var, skew, kurt = binom.stats(n, p, moments='mvsk')\n    \n    Display the probability mass function (``pmf``):\n    \n    >>> x = np.arange(binom.ppf(0.01, n, p),\n    ...               binom.ppf(0.99, n, p))\n    >>> ax.plot(x, binom.pmf(x, n, p), 'bo', ms=8, label='binom pmf')\n    >>> ax.vlines(x, 0, binom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape and location. This returns a \"frozen\" RV object holding\n    the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pmf``:\n    \n    >>> rv = binom(n, p)\n    >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n    ...         label='frozen pmf')\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> prob = binom.cdf(x, n, p)\n    >>> np.allclose(x, binom.ppf(prob, n, p))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = binom.rvs(n, p, size=1000)\n\n    See Also\n    --------\n    hypergeom, nbinom, nhypergeom\n\n    ",
    "scipy.stats.binomtest": "\n    Perform a test that the probability of success is p.\n\n    The binomial test [1]_ is a test of the null hypothesis that the\n    probability of success in a Bernoulli experiment is `p`.\n\n    Details of the test can be found in many texts on statistics, such\n    as section 24.5 of [2]_.\n\n    Parameters\n    ----------\n    k : int\n        The number of successes.\n    n : int\n        The number of trials.\n    p : float, optional\n        The hypothesized probability of success, i.e. the expected\n        proportion of successes.  The value must be in the interval\n        ``0 <= p <= 1``. The default value is ``p = 0.5``.\n    alternative : {'two-sided', 'greater', 'less'}, optional\n        Indicates the alternative hypothesis. The default value is\n        'two-sided'.\n\n    Returns\n    -------\n    result : `~scipy.stats._result_classes.BinomTestResult` instance\n        The return value is an object with the following attributes:\n\n        k : int\n            The number of successes (copied from `binomtest` input).\n        n : int\n            The number of trials (copied from `binomtest` input).\n        alternative : str\n            Indicates the alternative hypothesis specified in the input\n            to `binomtest`.  It will be one of ``'two-sided'``, ``'greater'``,\n            or ``'less'``.\n        statistic : float\n            The estimate of the proportion of successes.\n        pvalue : float\n            The p-value of the hypothesis test.\n\n        The object has the following methods:\n\n        proportion_ci(confidence_level=0.95, method='exact') :\n            Compute the confidence interval for ``statistic``.\n\n    Notes\n    -----\n    .. versionadded:: 1.7.0\n\n    References\n    ----------\n    .. [1] Binomial test, https://en.wikipedia.org/wiki/Binomial_test\n    .. [2] Jerrold H. Zar, Biostatistical Analysis (fifth edition),\n           Prentice Hall, Upper Saddle River, New Jersey USA (2010)\n\n    Examples\n    --------\n    >>> from scipy.stats import binomtest\n\n    A car manufacturer claims that no more than 10% of their cars are unsafe.\n    15 cars are inspected for safety, 3 were found to be unsafe. Test the\n    manufacturer's claim:\n\n    >>> result = binomtest(3, n=15, p=0.1, alternative='greater')\n    >>> result.pvalue\n    0.18406106910639114\n\n    The null hypothesis cannot be rejected at the 5% level of significance\n    because the returned p-value is greater than the critical value of 5%.\n\n    The test statistic is equal to the estimated proportion, which is simply\n    ``3/15``:\n\n    >>> result.statistic\n    0.2\n\n    We can use the `proportion_ci()` method of the result to compute the\n    confidence interval of the estimate:\n\n    >>> result.proportion_ci(confidence_level=0.95)\n    ConfidenceInterval(low=0.05684686759024681, high=1.0)\n\n    ",
    "scipy.stats.boltzmann": "A Boltzmann (Truncated Discrete Exponential) random variable.\n\n    As an instance of the `rv_discrete` class, `boltzmann` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(lambda_, N, loc=0, size=1, random_state=None)\n        Random variates.\n    pmf(k, lambda_, N, loc=0)\n        Probability mass function.\n    logpmf(k, lambda_, N, loc=0)\n        Log of the probability mass function.\n    cdf(k, lambda_, N, loc=0)\n        Cumulative distribution function.\n    logcdf(k, lambda_, N, loc=0)\n        Log of the cumulative distribution function.\n    sf(k, lambda_, N, loc=0)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(k, lambda_, N, loc=0)\n        Log of the survival function.\n    ppf(q, lambda_, N, loc=0)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, lambda_, N, loc=0)\n        Inverse survival function (inverse of ``sf``).\n    stats(lambda_, N, loc=0, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(lambda_, N, loc=0)\n        (Differential) entropy of the RV.\n    expect(func, args=(lambda_, N), loc=0, lb=None, ub=None, conditional=False)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(lambda_, N, loc=0)\n        Median of the distribution.\n    mean(lambda_, N, loc=0)\n        Mean of the distribution.\n    var(lambda_, N, loc=0)\n        Variance of the distribution.\n    std(lambda_, N, loc=0)\n        Standard deviation of the distribution.\n    interval(confidence, lambda_, N, loc=0)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability mass function for `boltzmann` is:\n\n    .. math::\n\n        f(k) = (1-\\exp(-\\lambda)) \\exp(-\\lambda k) / (1-\\exp(-\\lambda N))\n\n    for :math:`k = 0,..., N-1`.\n\n    `boltzmann` takes :math:`\\lambda > 0` and :math:`N > 0` as shape parameters.\n\n    The probability mass function above is defined in the \"standardized\" form.\n    To shift distribution use the ``loc`` parameter.\n    Specifically, ``boltzmann.pmf(k, lambda_, N, loc)`` is identically\n    equivalent to ``boltzmann.pmf(k - loc, lambda_, N)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import boltzmann\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> lambda_, N = 1.4, 19\n    >>> mean, var, skew, kurt = boltzmann.stats(lambda_, N, moments='mvsk')\n    \n    Display the probability mass function (``pmf``):\n    \n    >>> x = np.arange(boltzmann.ppf(0.01, lambda_, N),\n    ...               boltzmann.ppf(0.99, lambda_, N))\n    >>> ax.plot(x, boltzmann.pmf(x, lambda_, N), 'bo', ms=8, label='boltzmann pmf')\n    >>> ax.vlines(x, 0, boltzmann.pmf(x, lambda_, N), colors='b', lw=5, alpha=0.5)\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape and location. This returns a \"frozen\" RV object holding\n    the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pmf``:\n    \n    >>> rv = boltzmann(lambda_, N)\n    >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n    ...         label='frozen pmf')\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> prob = boltzmann.cdf(x, lambda_, N)\n    >>> np.allclose(x, boltzmann.ppf(prob, lambda_, N))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = boltzmann.rvs(lambda_, N, size=1000)\n\n    ",
    "scipy.stats.bootstrap": "\n    Compute a two-sided bootstrap confidence interval of a statistic.\n\n    When `method` is ``'percentile'`` and `alternative` is ``'two-sided'``,\n    a bootstrap confidence interval is computed according to the following\n    procedure.\n\n    1. Resample the data: for each sample in `data` and for each of\n       `n_resamples`, take a random sample of the original sample\n       (with replacement) of the same size as the original sample.\n\n    2. Compute the bootstrap distribution of the statistic: for each set of\n       resamples, compute the test statistic.\n\n    3. Determine the confidence interval: find the interval of the bootstrap\n       distribution that is\n\n       - symmetric about the median and\n       - contains `confidence_level` of the resampled statistic values.\n\n    While the ``'percentile'`` method is the most intuitive, it is rarely\n    used in practice. Two more common methods are available, ``'basic'``\n    ('reverse percentile') and ``'BCa'`` ('bias-corrected and accelerated');\n    they differ in how step 3 is performed.\n\n    If the samples in `data` are  taken at random from their respective\n    distributions :math:`n` times, the confidence interval returned by\n    `bootstrap` will contain the true value of the statistic for those\n    distributions approximately `confidence_level`:math:`\\, \\times \\, n` times.\n\n    Parameters\n    ----------\n    data : sequence of array-like\n         Each element of `data` is a sample containing scalar observations from an\n         underlying distribution. Elements of `data` must be broadcastable to the\n         same shape (with the possible exception of the dimension specified by `axis`).\n\n         .. versionchanged:: 1.14.0\n             `bootstrap` will now emit a ``FutureWarning`` if the shapes of the\n             elements of `data` are not the same (with the exception of the dimension\n             specified by `axis`).\n             Beginning in SciPy 1.16.0, `bootstrap` will explicitly broadcast the\n             elements to the same shape (except along `axis`) before performing\n             the calculation.\n\n    statistic : callable\n        Statistic for which the confidence interval is to be calculated.\n        `statistic` must be a callable that accepts ``len(data)`` samples\n        as separate arguments and returns the resulting statistic.\n        If `vectorized` is set ``True``,\n        `statistic` must also accept a keyword argument `axis` and be\n        vectorized to compute the statistic along the provided `axis`.\n    n_resamples : int, default: ``9999``\n        The number of resamples performed to form the bootstrap distribution\n        of the statistic.\n    batch : int, optional\n        The number of resamples to process in each vectorized call to\n        `statistic`. Memory usage is O( `batch` * ``n`` ), where ``n`` is the\n        sample size. Default is ``None``, in which case ``batch = n_resamples``\n        (or ``batch = max(n_resamples, n)`` for ``method='BCa'``).\n    vectorized : bool, optional\n        If `vectorized` is set ``False``, `statistic` will not be passed\n        keyword argument `axis` and is expected to calculate the statistic\n        only for 1D samples. If ``True``, `statistic` will be passed keyword\n        argument `axis` and is expected to calculate the statistic along `axis`\n        when passed an ND sample array. If ``None`` (default), `vectorized`\n        will be set ``True`` if ``axis`` is a parameter of `statistic`. Use of\n        a vectorized statistic typically reduces computation time.\n    paired : bool, default: ``False``\n        Whether the statistic treats corresponding elements of the samples\n        in `data` as paired.\n    axis : int, default: ``0``\n        The axis of the samples in `data` along which the `statistic` is\n        calculated.\n    confidence_level : float, default: ``0.95``\n        The confidence level of the confidence interval.\n    alternative : {'two-sided', 'less', 'greater'}, default: ``'two-sided'``\n        Choose ``'two-sided'`` (default) for a two-sided confidence interval,\n        ``'less'`` for a one-sided confidence interval with the lower bound\n        at ``-np.inf``, and ``'greater'`` for a one-sided confidence interval\n        with the upper bound at ``np.inf``. The other bound of the one-sided\n        confidence intervals is the same as that of a two-sided confidence\n        interval with `confidence_level` twice as far from 1.0; e.g. the upper\n        bound of a 95% ``'less'``  confidence interval is the same as the upper\n        bound of a 90% ``'two-sided'`` confidence interval.\n    method : {'percentile', 'basic', 'bca'}, default: ``'BCa'``\n        Whether to return the 'percentile' bootstrap confidence interval\n        (``'percentile'``), the 'basic' (AKA 'reverse') bootstrap confidence\n        interval (``'basic'``), or the bias-corrected and accelerated bootstrap\n        confidence interval (``'BCa'``).\n    bootstrap_result : BootstrapResult, optional\n        Provide the result object returned by a previous call to `bootstrap`\n        to include the previous bootstrap distribution in the new bootstrap\n        distribution. This can be used, for example, to change\n        `confidence_level`, change `method`, or see the effect of performing\n        additional resampling without repeating computations.\n    random_state : {None, int, `numpy.random.Generator`,\n                    `numpy.random.RandomState`}, optional\n\n        Pseudorandom number generator state used to generate resamples.\n\n        If `random_state` is ``None`` (or `np.random`), the\n        `numpy.random.RandomState` singleton is used.\n        If `random_state` is an int, a new ``RandomState`` instance is used,\n        seeded with `random_state`.\n        If `random_state` is already a ``Generator`` or ``RandomState``\n        instance then that instance is used.\n\n    Returns\n    -------\n    res : BootstrapResult\n        An object with attributes:\n\n        confidence_interval : ConfidenceInterval\n            The bootstrap confidence interval as an instance of\n            `collections.namedtuple` with attributes `low` and `high`.\n        bootstrap_distribution : ndarray\n            The bootstrap distribution, that is, the value of `statistic` for\n            each resample. The last dimension corresponds with the resamples\n            (e.g. ``res.bootstrap_distribution.shape[-1] == n_resamples``).\n        standard_error : float or ndarray\n            The bootstrap standard error, that is, the sample standard\n            deviation of the bootstrap distribution.\n\n    Warns\n    -----\n    `~scipy.stats.DegenerateDataWarning`\n        Generated when ``method='BCa'`` and the bootstrap distribution is\n        degenerate (e.g. all elements are identical).\n\n    Notes\n    -----\n    Elements of the confidence interval may be NaN for ``method='BCa'`` if\n    the bootstrap distribution is degenerate (e.g. all elements are identical).\n    In this case, consider using another `method` or inspecting `data` for\n    indications that other analysis may be more appropriate (e.g. all\n    observations are identical).\n\n    References\n    ----------\n    .. [1] B. Efron and R. J. Tibshirani, An Introduction to the Bootstrap,\n       Chapman & Hall/CRC, Boca Raton, FL, USA (1993)\n    .. [2] Nathaniel E. Helwig, \"Bootstrap Confidence Intervals\",\n       http://users.stat.umn.edu/~helwig/notes/bootci-Notes.pdf\n    .. [3] Bootstrapping (statistics), Wikipedia,\n       https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29\n\n    Examples\n    --------\n    Suppose we have sampled data from an unknown distribution.\n\n    >>> import numpy as np\n    >>> rng = np.random.default_rng()\n    >>> from scipy.stats import norm\n    >>> dist = norm(loc=2, scale=4)  # our \"unknown\" distribution\n    >>> data = dist.rvs(size=100, random_state=rng)\n\n    We are interested in the standard deviation of the distribution.\n\n    >>> std_true = dist.std()      # the true value of the statistic\n    >>> print(std_true)\n    4.0\n    >>> std_sample = np.std(data)  # the sample statistic\n    >>> print(std_sample)\n    3.9460644295563863\n\n    The bootstrap is used to approximate the variability we would expect if we\n    were to repeatedly sample from the unknown distribution and calculate the\n    statistic of the sample each time. It does this by repeatedly resampling\n    values *from the original sample* with replacement and calculating the\n    statistic of each resample. This results in a \"bootstrap distribution\" of\n    the statistic.\n\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy.stats import bootstrap\n    >>> data = (data,)  # samples must be in a sequence\n    >>> res = bootstrap(data, np.std, confidence_level=0.9,\n    ...                 random_state=rng)\n    >>> fig, ax = plt.subplots()\n    >>> ax.hist(res.bootstrap_distribution, bins=25)\n    >>> ax.set_title('Bootstrap Distribution')\n    >>> ax.set_xlabel('statistic value')\n    >>> ax.set_ylabel('frequency')\n    >>> plt.show()\n\n    The standard error quantifies this variability. It is calculated as the\n    standard deviation of the bootstrap distribution.\n\n    >>> res.standard_error\n    0.24427002125829136\n    >>> res.standard_error == np.std(res.bootstrap_distribution, ddof=1)\n    True\n\n    The bootstrap distribution of the statistic is often approximately normal\n    with scale equal to the standard error.\n\n    >>> x = np.linspace(3, 5)\n    >>> pdf = norm.pdf(x, loc=std_sample, scale=res.standard_error)\n    >>> fig, ax = plt.subplots()\n    >>> ax.hist(res.bootstrap_distribution, bins=25, density=True)\n    >>> ax.plot(x, pdf)\n    >>> ax.set_title('Normal Approximation of the Bootstrap Distribution')\n    >>> ax.set_xlabel('statistic value')\n    >>> ax.set_ylabel('pdf')\n    >>> plt.show()\n\n    This suggests that we could construct a 90% confidence interval on the\n    statistic based on quantiles of this normal distribution.\n\n    >>> norm.interval(0.9, loc=std_sample, scale=res.standard_error)\n    (3.5442759991341726, 4.3478528599786)\n\n    Due to central limit theorem, this normal approximation is accurate for a\n    variety of statistics and distributions underlying the samples; however,\n    the approximation is not reliable in all cases. Because `bootstrap` is\n    designed to work with arbitrary underlying distributions and statistics,\n    it uses more advanced techniques to generate an accurate confidence\n    interval.\n\n    >>> print(res.confidence_interval)\n    ConfidenceInterval(low=3.57655333533867, high=4.382043696342881)\n\n    If we sample from the original distribution 100 times and form a bootstrap\n    confidence interval for each sample, the confidence interval\n    contains the true value of the statistic approximately 90% of the time.\n\n    >>> n_trials = 100\n    >>> ci_contains_true_std = 0\n    >>> for i in range(n_trials):\n    ...    data = (dist.rvs(size=100, random_state=rng),)\n    ...    res = bootstrap(data, np.std, confidence_level=0.9,\n    ...                    n_resamples=999, random_state=rng)\n    ...    ci = res.confidence_interval\n    ...    if ci[0] < std_true < ci[1]:\n    ...        ci_contains_true_std += 1\n    >>> print(ci_contains_true_std)\n    88\n\n    Rather than writing a loop, we can also determine the confidence intervals\n    for all 100 samples at once.\n\n    >>> data = (dist.rvs(size=(n_trials, 100), random_state=rng),)\n    >>> res = bootstrap(data, np.std, axis=-1, confidence_level=0.9,\n    ...                 n_resamples=999, random_state=rng)\n    >>> ci_l, ci_u = res.confidence_interval\n\n    Here, `ci_l` and `ci_u` contain the confidence interval for each of the\n    ``n_trials = 100`` samples.\n\n    >>> print(ci_l[:5])\n    [3.86401283 3.33304394 3.52474647 3.54160981 3.80569252]\n    >>> print(ci_u[:5])\n    [4.80217409 4.18143252 4.39734707 4.37549713 4.72843584]\n\n    And again, approximately 90% contain the true value, ``std_true = 4``.\n\n    >>> print(np.sum((ci_l < std_true) & (std_true < ci_u)))\n    93\n\n    `bootstrap` can also be used to estimate confidence intervals of\n    multi-sample statistics. For example, to get a confidence interval\n    for the difference between means, we write a function that accepts\n    two sample arguments and returns only the statistic. The use of the\n    ``axis`` argument ensures that all mean calculations are perform in\n    a single vectorized call, which is faster than looping over pairs\n    of resamples in Python.\n\n    >>> def my_statistic(sample1, sample2, axis=-1):\n    ...     mean1 = np.mean(sample1, axis=axis)\n    ...     mean2 = np.mean(sample2, axis=axis)\n    ...     return mean1 - mean2\n\n    Here, we use the 'percentile' method with the default 95% confidence level.\n\n    >>> sample1 = norm.rvs(scale=1, size=100, random_state=rng)\n    >>> sample2 = norm.rvs(scale=2, size=100, random_state=rng)\n    >>> data = (sample1, sample2)\n    >>> res = bootstrap(data, my_statistic, method='basic', random_state=rng)\n    >>> print(my_statistic(sample1, sample2))\n    0.16661030792089523\n    >>> print(res.confidence_interval)\n    ConfidenceInterval(low=-0.29087973240818693, high=0.6371338699912273)\n\n    The bootstrap estimate of the standard error is also available.\n\n    >>> print(res.standard_error)\n    0.238323948262459\n\n    Paired-sample statistics work, too. For example, consider the Pearson\n    correlation coefficient.\n\n    >>> from scipy.stats import pearsonr\n    >>> n = 100\n    >>> x = np.linspace(0, 10, n)\n    >>> y = x + rng.uniform(size=n)\n    >>> print(pearsonr(x, y)[0])  # element 0 is the statistic\n    0.9954306665125647\n\n    We wrap `pearsonr` so that it returns only the statistic, ensuring\n    that we use the `axis` argument because it is available.\n\n    >>> def my_statistic(x, y, axis=-1):\n    ...     return pearsonr(x, y, axis=axis)[0]\n\n    We call `bootstrap` using ``paired=True``.\n\n    >>> res = bootstrap((x, y), my_statistic, paired=True, random_state=rng)\n    >>> print(res.confidence_interval)\n    ConfidenceInterval(low=0.9941504301315878, high=0.996377412215445)\n\n    The result object can be passed back into `bootstrap` to perform additional\n    resampling:\n\n    >>> len(res.bootstrap_distribution)\n    9999\n    >>> res = bootstrap((x, y), my_statistic, paired=True,\n    ...                 n_resamples=1000, random_state=rng,\n    ...                 bootstrap_result=res)\n    >>> len(res.bootstrap_distribution)\n    10999\n\n    or to change the confidence interval options:\n\n    >>> res2 = bootstrap((x, y), my_statistic, paired=True,\n    ...                  n_resamples=0, random_state=rng, bootstrap_result=res,\n    ...                  method='percentile', confidence_level=0.9)\n    >>> np.testing.assert_equal(res2.bootstrap_distribution,\n    ...                         res.bootstrap_distribution)\n    >>> res.confidence_interval\n    ConfidenceInterval(low=0.9941574828235082, high=0.9963781698210212)\n\n    without repeating computation of the original bootstrap distribution.\n\n    ",
    "scipy.stats.boschloo_exact": "Perform Boschloo's exact test on a 2x2 contingency table.\n\n    Parameters\n    ----------\n    table : array_like of ints\n        A 2x2 contingency table.  Elements should be non-negative integers.\n\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the null and alternative hypotheses. Default is 'two-sided'.\n        Please see explanations in the Notes section below.\n\n    n : int, optional\n        Number of sampling points used in the construction of the sampling\n        method. Note that this argument will automatically be converted to\n        the next higher power of 2 since `scipy.stats.qmc.Sobol` is used to\n        select sample points. Default is 32. Must be positive. In most cases,\n        32 points is enough to reach good precision. More points comes at\n        performance cost.\n\n    Returns\n    -------\n    ber : BoschlooExactResult\n        A result object with the following attributes.\n\n        statistic : float\n            The statistic used in Boschloo's test; that is, the p-value\n            from Fisher's exact test.\n\n        pvalue : float\n            P-value, the probability of obtaining a distribution at least as\n            extreme as the one that was actually observed, assuming that the\n            null hypothesis is true.\n\n    See Also\n    --------\n    chi2_contingency : Chi-square test of independence of variables in a\n        contingency table.\n    fisher_exact : Fisher exact test on a 2x2 contingency table.\n    barnard_exact : Barnard's exact test, which is a more powerful alternative\n        than Fisher's exact test for 2x2 contingency tables.\n\n    Notes\n    -----\n    Boschloo's test is an exact test used in the analysis of contingency\n    tables. It examines the association of two categorical variables, and\n    is a uniformly more powerful alternative to Fisher's exact test\n    for 2x2 contingency tables.\n\n    Boschloo's exact test uses the p-value of Fisher's exact test as a\n    statistic, and Boschloo's p-value is the probability under the null\n    hypothesis of observing such an extreme value of this statistic.\n\n    Let's define :math:`X_0` a 2x2 matrix representing the observed sample,\n    where each column stores the binomial experiment, as in the example\n    below. Let's also define :math:`p_1, p_2` the theoretical binomial\n    probabilities for  :math:`x_{11}` and :math:`x_{12}`. When using\n    Boschloo exact test, we can assert three different alternative hypotheses:\n\n    - :math:`H_0 : p_1=p_2` versus :math:`H_1 : p_1 < p_2`,\n      with `alternative` = \"less\"\n\n    - :math:`H_0 : p_1=p_2` versus :math:`H_1 : p_1 > p_2`,\n      with `alternative` = \"greater\"\n\n    - :math:`H_0 : p_1=p_2` versus :math:`H_1 : p_1 \\neq p_2`,\n      with `alternative` = \"two-sided\" (default)\n\n    There are multiple conventions for computing a two-sided p-value when the\n    null distribution is asymmetric. Here, we apply the convention that the\n    p-value of a two-sided test is twice the minimum of the p-values of the\n    one-sided tests (clipped to 1.0). Note that `fisher_exact` follows a\n    different convention, so for a given `table`, the statistic reported by\n    `boschloo_exact` may differ from the p-value reported by `fisher_exact`\n    when ``alternative='two-sided'``.\n\n    .. versionadded:: 1.7.0\n\n    References\n    ----------\n    .. [1] R.D. Boschloo. \"Raised conditional level of significance for the\n       2 x 2-table when testing the equality of two probabilities\",\n       Statistica Neerlandica, 24(1), 1970\n\n    .. [2] \"Boschloo's test\", Wikipedia,\n       https://en.wikipedia.org/wiki/Boschloo%27s_test\n\n    .. [3] Lise M. Saari et al. \"Employee attitudes and job satisfaction\",\n       Human Resource Management, 43(4), 395-407, 2004,\n       :doi:`10.1002/hrm.20032`.\n\n    Examples\n    --------\n    In the following example, we consider the article \"Employee\n    attitudes and job satisfaction\" [3]_\n    which reports the results of a survey from 63 scientists and 117 college\n    professors. Of the 63 scientists, 31 said they were very satisfied with\n    their jobs, whereas 74 of the college professors were very satisfied\n    with their work. Is this significant evidence that college\n    professors are happier with their work than scientists?\n    The following table summarizes the data mentioned above::\n\n                         college professors   scientists\n        Very Satisfied   74                     31\n        Dissatisfied     43                     32\n\n    When working with statistical hypothesis testing, we usually use a\n    threshold probability or significance level upon which we decide\n    to reject the null hypothesis :math:`H_0`. Suppose we choose the common\n    significance level of 5%.\n\n    Our alternative hypothesis is that college professors are truly more\n    satisfied with their work than scientists. Therefore, we expect\n    :math:`p_1` the proportion of very satisfied college professors to be\n    greater than :math:`p_2`, the proportion of very satisfied scientists.\n    We thus call `boschloo_exact` with the ``alternative=\"greater\"`` option:\n\n    >>> import scipy.stats as stats\n    >>> res = stats.boschloo_exact([[74, 31], [43, 32]], alternative=\"greater\")\n    >>> res.statistic\n    0.0483\n    >>> res.pvalue\n    0.0355\n\n    Under the null hypothesis that scientists are happier in their work than\n    college professors, the probability of obtaining test\n    results at least as extreme as the observed data is approximately 3.55%.\n    Since this p-value is less than our chosen significance level, we have\n    evidence to reject :math:`H_0` in favor of the alternative hypothesis.\n\n    ",
    "scipy.stats.boxcox": "Return a dataset transformed by a Box-Cox power transformation.\n\n    Parameters\n    ----------\n    x : ndarray\n        Input array to be transformed.\n\n        If `lmbda` is not None, this is an alias of\n        `scipy.special.boxcox`.\n        Returns nan if ``x < 0``; returns -inf if ``x == 0 and lmbda < 0``.\n\n        If `lmbda` is None, array must be positive, 1-dimensional, and\n        non-constant.\n\n    lmbda : scalar, optional\n        If `lmbda` is None (default), find the value of `lmbda` that maximizes\n        the log-likelihood function and return it as the second output\n        argument.\n\n        If `lmbda` is not None, do the transformation for that value.\n\n    alpha : float, optional\n        If `lmbda` is None and `alpha` is not None (default), return the\n        ``100 * (1-alpha)%`` confidence  interval for `lmbda` as the third\n        output argument. Must be between 0.0 and 1.0.\n\n        If `lmbda` is not None, `alpha` is ignored.\n    optimizer : callable, optional\n        If `lmbda` is None, `optimizer` is the scalar optimizer used to find\n        the value of `lmbda` that minimizes the negative log-likelihood\n        function. `optimizer` is a callable that accepts one argument:\n\n        fun : callable\n            The objective function, which evaluates the negative\n            log-likelihood function at a provided value of `lmbda`\n\n        and returns an object, such as an instance of\n        `scipy.optimize.OptimizeResult`, which holds the optimal value of\n        `lmbda` in an attribute `x`.\n\n        See the example in `boxcox_normmax` or the documentation of\n        `scipy.optimize.minimize_scalar` for more information.\n\n        If `lmbda` is not None, `optimizer` is ignored.\n\n    Returns\n    -------\n    boxcox : ndarray\n        Box-Cox power transformed array.\n    maxlog : float, optional\n        If the `lmbda` parameter is None, the second returned argument is\n        the `lmbda` that maximizes the log-likelihood function.\n    (min_ci, max_ci) : tuple of float, optional\n        If `lmbda` parameter is None and `alpha` is not None, this returned\n        tuple of floats represents the minimum and maximum confidence limits\n        given `alpha`.\n\n    See Also\n    --------\n    probplot, boxcox_normplot, boxcox_normmax, boxcox_llf\n\n    Notes\n    -----\n    The Box-Cox transform is given by::\n\n        y = (x**lmbda - 1) / lmbda,  for lmbda != 0\n            log(x),                  for lmbda = 0\n\n    `boxcox` requires the input data to be positive.  Sometimes a Box-Cox\n    transformation provides a shift parameter to achieve this; `boxcox` does\n    not.  Such a shift parameter is equivalent to adding a positive constant to\n    `x` before calling `boxcox`.\n\n    The confidence limits returned when `alpha` is provided give the interval\n    where:\n\n    .. math::\n\n        llf(\\hat{\\lambda}) - llf(\\lambda) < \\frac{1}{2}\\chi^2(1 - \\alpha, 1),\n\n    with ``llf`` the log-likelihood function and :math:`\\chi^2` the chi-squared\n    function.\n\n    References\n    ----------\n    G.E.P. Box and D.R. Cox, \"An Analysis of Transformations\", Journal of the\n    Royal Statistical Society B, 26, 211-252 (1964).\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n\n    We generate some random variates from a non-normal distribution and make a\n    probability plot for it, to show it is non-normal in the tails:\n\n    >>> fig = plt.figure()\n    >>> ax1 = fig.add_subplot(211)\n    >>> x = stats.loggamma.rvs(5, size=500) + 5\n    >>> prob = stats.probplot(x, dist=stats.norm, plot=ax1)\n    >>> ax1.set_xlabel('')\n    >>> ax1.set_title('Probplot against normal distribution')\n\n    We now use `boxcox` to transform the data so it's closest to normal:\n\n    >>> ax2 = fig.add_subplot(212)\n    >>> xt, _ = stats.boxcox(x)\n    >>> prob = stats.probplot(xt, dist=stats.norm, plot=ax2)\n    >>> ax2.set_title('Probplot after Box-Cox transformation')\n\n    >>> plt.show()\n\n    ",
    "scipy.stats.boxcox_llf": "The boxcox log-likelihood function.\n\n    Parameters\n    ----------\n    lmb : scalar\n        Parameter for Box-Cox transformation.  See `boxcox` for details.\n    data : array_like\n        Data to calculate Box-Cox log-likelihood for.  If `data` is\n        multi-dimensional, the log-likelihood is calculated along the first\n        axis.\n\n    Returns\n    -------\n    llf : float or ndarray\n        Box-Cox log-likelihood of `data` given `lmb`.  A float for 1-D `data`,\n        an array otherwise.\n\n    See Also\n    --------\n    boxcox, probplot, boxcox_normplot, boxcox_normmax\n\n    Notes\n    -----\n    The Box-Cox log-likelihood function is defined here as\n\n    .. math::\n\n        llf = (\\lambda - 1) \\sum_i(\\log(x_i)) -\n              N/2 \\log(\\sum_i (y_i - \\bar{y})^2 / N),\n\n    where ``y`` is the Box-Cox transformed input data ``x``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n    >>> from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\n    Generate some random variates and calculate Box-Cox log-likelihood values\n    for them for a range of ``lmbda`` values:\n\n    >>> rng = np.random.default_rng()\n    >>> x = stats.loggamma.rvs(5, loc=10, size=1000, random_state=rng)\n    >>> lmbdas = np.linspace(-2, 10)\n    >>> llf = np.zeros(lmbdas.shape, dtype=float)\n    >>> for ii, lmbda in enumerate(lmbdas):\n    ...     llf[ii] = stats.boxcox_llf(lmbda, x)\n\n    Also find the optimal lmbda value with `boxcox`:\n\n    >>> x_most_normal, lmbda_optimal = stats.boxcox(x)\n\n    Plot the log-likelihood as function of lmbda.  Add the optimal lmbda as a\n    horizontal line to check that that's really the optimum:\n\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> ax.plot(lmbdas, llf, 'b.-')\n    >>> ax.axhline(stats.boxcox_llf(lmbda_optimal, x), color='r')\n    >>> ax.set_xlabel('lmbda parameter')\n    >>> ax.set_ylabel('Box-Cox log-likelihood')\n\n    Now add some probability plots to show that where the log-likelihood is\n    maximized the data transformed with `boxcox` looks closest to normal:\n\n    >>> locs = [3, 10, 4]  # 'lower left', 'center', 'lower right'\n    >>> for lmbda, loc in zip([-1, lmbda_optimal, 9], locs):\n    ...     xt = stats.boxcox(x, lmbda=lmbda)\n    ...     (osm, osr), (slope, intercept, r_sq) = stats.probplot(xt)\n    ...     ax_inset = inset_axes(ax, width=\"20%\", height=\"20%\", loc=loc)\n    ...     ax_inset.plot(osm, osr, 'c.', osm, slope*osm + intercept, 'k-')\n    ...     ax_inset.set_xticklabels([])\n    ...     ax_inset.set_yticklabels([])\n    ...     ax_inset.set_title(r'$\\lambda=%1.2f$' % lmbda)\n\n    >>> plt.show()\n\n    ",
    "scipy.stats.boxcox_normmax": "Compute optimal Box-Cox transform parameter for input data.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array. All entries must be positive, finite, real numbers.\n    brack : 2-tuple, optional, default (-2.0, 2.0)\n         The starting interval for a downhill bracket search for the default\n         `optimize.brent` solver. Note that this is in most cases not\n         critical; the final result is allowed to be outside this bracket.\n         If `optimizer` is passed, `brack` must be None.\n    method : str, optional\n        The method to determine the optimal transform parameter (`boxcox`\n        ``lmbda`` parameter). Options are:\n\n        'pearsonr'  (default)\n            Maximizes the Pearson correlation coefficient between\n            ``y = boxcox(x)`` and the expected values for ``y`` if `x` would be\n            normally-distributed.\n\n        'mle'\n            Maximizes the log-likelihood `boxcox_llf`.  This is the method used\n            in `boxcox`.\n\n        'all'\n            Use all optimization methods available, and return all results.\n            Useful to compare different methods.\n    optimizer : callable, optional\n        `optimizer` is a callable that accepts one argument:\n\n        fun : callable\n            The objective function to be minimized. `fun` accepts one argument,\n            the Box-Cox transform parameter `lmbda`, and returns the value of\n            the function (e.g., the negative log-likelihood) at the provided\n            argument. The job of `optimizer` is to find the value of `lmbda`\n            that *minimizes* `fun`.\n\n        and returns an object, such as an instance of\n        `scipy.optimize.OptimizeResult`, which holds the optimal value of\n        `lmbda` in an attribute `x`.\n\n        See the example below or the documentation of\n        `scipy.optimize.minimize_scalar` for more information.\n    ymax : float, optional\n        The unconstrained optimal transform parameter may cause Box-Cox\n        transformed data to have extreme magnitude or even overflow.\n        This parameter constrains MLE optimization such that the magnitude\n        of the transformed `x` does not exceed `ymax`. The default is\n        the maximum value of the input dtype. If set to infinity,\n        `boxcox_normmax` returns the unconstrained optimal lambda.\n        Ignored when ``method='pearsonr'``.\n\n    Returns\n    -------\n    maxlog : float or ndarray\n        The optimal transform parameter found.  An array instead of a scalar\n        for ``method='all'``.\n\n    See Also\n    --------\n    boxcox, boxcox_llf, boxcox_normplot, scipy.optimize.minimize_scalar\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n\n    We can generate some data and determine the optimal ``lmbda`` in various\n    ways:\n\n    >>> rng = np.random.default_rng()\n    >>> x = stats.loggamma.rvs(5, size=30, random_state=rng) + 5\n    >>> y, lmax_mle = stats.boxcox(x)\n    >>> lmax_pearsonr = stats.boxcox_normmax(x)\n\n    >>> lmax_mle\n    2.217563431465757\n    >>> lmax_pearsonr\n    2.238318660200961\n    >>> stats.boxcox_normmax(x, method='all')\n    array([2.23831866, 2.21756343])\n\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> prob = stats.boxcox_normplot(x, -10, 10, plot=ax)\n    >>> ax.axvline(lmax_mle, color='r')\n    >>> ax.axvline(lmax_pearsonr, color='g', ls='--')\n\n    >>> plt.show()\n\n    Alternatively, we can define our own `optimizer` function. Suppose we\n    are only interested in values of `lmbda` on the interval [6, 7], we\n    want to use `scipy.optimize.minimize_scalar` with ``method='bounded'``,\n    and we want to use tighter tolerances when optimizing the log-likelihood\n    function. To do this, we define a function that accepts positional argument\n    `fun` and uses `scipy.optimize.minimize_scalar` to minimize `fun` subject\n    to the provided bounds and tolerances:\n\n    >>> from scipy import optimize\n    >>> options = {'xatol': 1e-12}  # absolute tolerance on `x`\n    >>> def optimizer(fun):\n    ...     return optimize.minimize_scalar(fun, bounds=(6, 7),\n    ...                                     method=\"bounded\", options=options)\n    >>> stats.boxcox_normmax(x, optimizer=optimizer)\n    6.000000000\n    ",
    "scipy.stats.boxcox_normplot": "Compute parameters for a Box-Cox normality plot, optionally show it.\n\n    A Box-Cox normality plot shows graphically what the best transformation\n    parameter is to use in `boxcox` to obtain a distribution that is close\n    to normal.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array.\n    la, lb : scalar\n        The lower and upper bounds for the ``lmbda`` values to pass to `boxcox`\n        for Box-Cox transformations.  These are also the limits of the\n        horizontal axis of the plot if that is generated.\n    plot : object, optional\n        If given, plots the quantiles and least squares fit.\n        `plot` is an object that has to have methods \"plot\" and \"text\".\n        The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n        or a custom object with the same methods.\n        Default is None, which means that no plot is created.\n    N : int, optional\n        Number of points on the horizontal axis (equally distributed from\n        `la` to `lb`).\n\n    Returns\n    -------\n    lmbdas : ndarray\n        The ``lmbda`` values for which a Box-Cox transform was done.\n    ppcc : ndarray\n        Probability Plot Correlelation Coefficient, as obtained from `probplot`\n        when fitting the Box-Cox transformed input `x` against a normal\n        distribution.\n\n    See Also\n    --------\n    probplot, boxcox, boxcox_normmax, boxcox_llf, ppcc_max\n\n    Notes\n    -----\n    Even if `plot` is given, the figure is not shown or saved by\n    `boxcox_normplot`; ``plt.show()`` or ``plt.savefig('figname.png')``\n    should be used after calling `probplot`.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n\n    Generate some non-normally distributed data, and create a Box-Cox plot:\n\n    >>> x = stats.loggamma.rvs(5, size=500) + 5\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> prob = stats.boxcox_normplot(x, -20, 20, plot=ax)\n\n    Determine and plot the optimal ``lmbda`` to transform ``x`` and plot it in\n    the same plot:\n\n    >>> _, maxlog = stats.boxcox(x)\n    >>> ax.axvline(maxlog, color='r')\n\n    >>> plt.show()\n\n    ",
    "scipy.stats.bradford": "A Bradford continuous random variable.\n\n    As an instance of the `rv_continuous` class, `bradford` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `bradford` is:\n\n    .. math::\n\n        f(x, c) = \\frac{c}{\\log(1+c) (1+cx)}\n\n    for :math:`0 <= x <= 1` and :math:`c > 0`.\n\n    `bradford` takes ``c`` as a shape parameter for :math:`c`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``bradford.pdf(x, c, loc, scale)`` is identically\n    equivalent to ``bradford.pdf(y, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import bradford\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c = 0.299\n    >>> mean, var, skew, kurt = bradford.stats(c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(bradford.ppf(0.01, c),\n    ...                 bradford.ppf(0.99, c), 100)\n    >>> ax.plot(x, bradford.pdf(x, c),\n    ...        'r-', lw=5, alpha=0.6, label='bradford pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = bradford(c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = bradford.ppf([0.001, 0.5, 0.999], c)\n    >>> np.allclose([0.001, 0.5, 0.999], bradford.cdf(vals, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = bradford.rvs(c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.brunnermunzel": "    \n\n\nCompute the Brunner-Munzel test on samples x and y.\n\nThe Brunner-Munzel test is a nonparametric test of the null hypothesis that\nwhen values are taken one by one from each group, the probabilities of\ngetting large values in both groups are equal.\nUnlike the Wilcoxon-Mann-Whitney's U test, this does not require the\nassumption of equivariance of two groups. Note that this does not assume\nthe distributions are same. This test works on two independent samples,\nwhich may have different sizes.\n\nParameters\n----------\nx, y : array_like\n    Array of samples, should be one-dimensional.\nalternative : {'two-sided', 'less', 'greater'}, optional\n    Defines the alternative hypothesis.\n    The following options are available (default is 'two-sided'):\n    \n      * 'two-sided'\n      * 'less': one-sided\n      * 'greater': one-sided\ndistribution : {'t', 'normal'}, optional\n    Defines how to get the p-value.\n    The following options are available (default is 't'):\n    \n      * 't': get the p-value by t-distribution\n      * 'normal': get the p-value by standard normal distribution.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nstatistic : float\n    The Brunner-Munzer W statistic.\npvalue : float\n    p-value assuming an t distribution. One-sided or\n    two-sided, depending on the choice of `alternative` and `distribution`.\n\nSee Also\n--------\n\n:func:`mannwhitneyu`\n    Mann-Whitney rank test on two samples.\n\n\nNotes\n-----\nBrunner and Munzel recommended to estimate the p-value by t-distribution\nwhen the size of data is 50 or less. If the size is lower than 10, it would\nbe better to use permuted Brunner Munzel test (see [2]_).\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] Brunner, E. and Munzel, U. \"The nonparametric Benhrens-Fisher\n       problem: Asymptotic theory and a small-sample approximation\".\n       Biometrical Journal. Vol. 42(2000): 17-25.\n.. [2] Neubert, K. and Brunner, E. \"A studentized permutation test for the\n       non-parametric Behrens-Fisher problem\". Computational Statistics and\n       Data Analysis. Vol. 51(2007): 5192-5204.\n\nExamples\n--------\n>>> from scipy import stats\n>>> x1 = [1,2,1,1,1,1,1,1,1,1,2,4,1,1]\n>>> x2 = [3,3,4,3,1,2,3,1,1,5,4]\n>>> w, p_value = stats.brunnermunzel(x1, x2)\n>>> w\n3.1374674823029505\n>>> p_value\n0.0057862086661515377\n",
    "scipy.stats.burr": "A Burr (Type III) continuous random variable.\n\n    As an instance of the `rv_continuous` class, `burr` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, d, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, d, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, d, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, d, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, d, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, d, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, d, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, d, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, d, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, d, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, d, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, d, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c, d), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, d, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, d, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, d, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, d, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, d, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    fisk : a special case of either `burr` or `burr12` with ``d=1``\n    burr12 : Burr Type XII distribution\n    mielke : Mielke Beta-Kappa / Dagum distribution\n\n    Notes\n    -----\n    The probability density function for `burr` is:\n\n    .. math::\n\n        f(x; c, d) = c d \\frac{x^{-c - 1}}\n                              {{(1 + x^{-c})}^{d + 1}}\n\n    for :math:`x >= 0` and :math:`c, d > 0`.\n\n    `burr` takes ``c`` and ``d`` as shape parameters for :math:`c` and\n    :math:`d`.\n\n    This is the PDF corresponding to the third CDF given in Burr's list;\n    specifically, it is equation (11) in Burr's paper [1]_. The distribution\n    is also commonly referred to as the Dagum distribution [2]_. If the\n    parameter :math:`c < 1` then the mean of the distribution does not\n    exist and if :math:`c < 2` the variance does not exist [2]_.\n    The PDF is finite at the left endpoint :math:`x = 0` if :math:`c * d >= 1`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``burr.pdf(x, c, d, loc, scale)`` is identically\n    equivalent to ``burr.pdf(y, c, d) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] Burr, I. W. \"Cumulative frequency functions\", Annals of\n       Mathematical Statistics, 13(2), pp 215-232 (1942).\n    .. [2] https://en.wikipedia.org/wiki/Dagum_distribution\n    .. [3] Kleiber, Christian. \"A guide to the Dagum distributions.\"\n       Modeling Income Distributions and Lorenz Curves  pp 97-117 (2008).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import burr\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c, d = 10.5, 4.3\n    >>> mean, var, skew, kurt = burr.stats(c, d, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(burr.ppf(0.01, c, d),\n    ...                 burr.ppf(0.99, c, d), 100)\n    >>> ax.plot(x, burr.pdf(x, c, d),\n    ...        'r-', lw=5, alpha=0.6, label='burr pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = burr(c, d)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = burr.ppf([0.001, 0.5, 0.999], c, d)\n    >>> np.allclose([0.001, 0.5, 0.999], burr.cdf(vals, c, d))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = burr.rvs(c, d, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.burr12": "A Burr (Type XII) continuous random variable.\n\n    As an instance of the `rv_continuous` class, `burr12` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, d, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, d, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, d, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, d, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, d, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, d, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, d, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, d, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, d, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, d, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, d, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, d, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c, d), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, d, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, d, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, d, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, d, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, d, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    fisk : a special case of either `burr` or `burr12` with ``d=1``\n    burr : Burr Type III distribution\n\n    Notes\n    -----\n    The probability density function for `burr12` is:\n\n    .. math::\n\n        f(x; c, d) = c d \\frac{x^{c-1}}\n                              {(1 + x^c)^{d + 1}}\n\n    for :math:`x >= 0` and :math:`c, d > 0`.\n\n    `burr12` takes ``c`` and ``d`` as shape parameters for :math:`c`\n    and :math:`d`.\n\n    This is the PDF corresponding to the twelfth CDF given in Burr's list;\n    specifically, it is equation (20) in Burr's paper [1]_.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``burr12.pdf(x, c, d, loc, scale)`` is identically\n    equivalent to ``burr12.pdf(y, c, d) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    The Burr type 12 distribution is also sometimes referred to as\n    the Singh-Maddala distribution from NIST [2]_.\n\n    References\n    ----------\n    .. [1] Burr, I. W. \"Cumulative frequency functions\", Annals of\n       Mathematical Statistics, 13(2), pp 215-232 (1942).\n\n    .. [2] https://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/b12pdf.htm\n\n    .. [3] \"Burr distribution\",\n       https://en.wikipedia.org/wiki/Burr_distribution\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import burr12\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c, d = 10, 4\n    >>> mean, var, skew, kurt = burr12.stats(c, d, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(burr12.ppf(0.01, c, d),\n    ...                 burr12.ppf(0.99, c, d), 100)\n    >>> ax.plot(x, burr12.pdf(x, c, d),\n    ...        'r-', lw=5, alpha=0.6, label='burr12 pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = burr12(c, d)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = burr12.ppf([0.001, 0.5, 0.999], c, d)\n    >>> np.allclose([0.001, 0.5, 0.999], burr12.cdf(vals, c, d))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = burr12.rvs(c, d, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.bws_test": "Perform the Baumgartner-Weiss-Schindler test on two independent samples.\n\n    The Baumgartner-Weiss-Schindler (BWS) test is a nonparametric test of \n    the null hypothesis that the distribution underlying sample `x` \n    is the same as the distribution underlying sample `y`. Unlike \n    the Kolmogorov-Smirnov, Wilcoxon, and Cramer-Von Mises tests, \n    the BWS test weights the integral by the variance of the difference\n    in cumulative distribution functions (CDFs), emphasizing the tails of the\n    distributions, which increases the power of the test in many applications.\n\n    Parameters\n    ----------\n    x, y : array-like\n        1-d arrays of samples.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n        Let *F(u)* and *G(u)* be the cumulative distribution functions of the\n        distributions underlying `x` and `y`, respectively. Then the following\n        alternative hypotheses are available:\n\n        * 'two-sided': the distributions are not equal, i.e. *F(u) \u2260 G(u)* for\n          at least one *u*.\n        * 'less': the distribution underlying `x` is stochastically less than\n          the distribution underlying `y`, i.e. *F(u) >= G(u)* for all *u*.\n        * 'greater': the distribution underlying `x` is stochastically greater\n          than the distribution underlying `y`, i.e. *F(u) <= G(u)* for all\n          *u*.\n\n        Under a more restrictive set of assumptions, the alternative hypotheses\n        can be expressed in terms of the locations of the distributions;\n        see [2] section 5.1.\n    method : PermutationMethod, optional\n        Configures the method used to compute the p-value. The default is\n        the default `PermutationMethod` object.\n\n    Returns\n    -------\n    res : PermutationTestResult\n    An object with attributes:\n\n    statistic : float\n        The observed test statistic of the data.\n    pvalue : float\n        The p-value for the given alternative.\n    null_distribution : ndarray\n        The values of the test statistic generated under the null hypothesis.\n\n    See also\n    --------\n    scipy.stats.wilcoxon, scipy.stats.mannwhitneyu, scipy.stats.ttest_ind\n\n    Notes\n    -----\n    When ``alternative=='two-sided'``, the statistic is defined by the\n    equations given in [1]_ Section 2. This statistic is not appropriate for\n    one-sided alternatives; in that case, the statistic is the *negative* of\n    that given by the equations in [1]_ Section 2. Consequently, when the\n    distribution of the first sample is stochastically greater than that of the\n    second sample, the statistic will tend to be positive.\n\n    References\n    ----------\n    .. [1] Neuh\u00e4user, M. (2005). Exact Tests Based on the\n           Baumgartner-Weiss-Schindler Statistic: A Survey. Statistical Papers,\n           46(1), 1-29.\n    .. [2] Fay, M. P., & Proschan, M. A. (2010). Wilcoxon-Mann-Whitney or t-test?\n           On assumptions for hypothesis tests and multiple interpretations of \n           decision rules. Statistics surveys, 4, 1.\n\n    Examples\n    --------\n    We follow the example of table 3 in [1]_: Fourteen children were divided\n    randomly into two groups. Their ranks at performing a specific tests are\n    as follows.\n\n    >>> import numpy as np\n    >>> x = [1, 2, 3, 4, 6, 7, 8]\n    >>> y = [5, 9, 10, 11, 12, 13, 14]\n\n    We use the BWS test to assess whether there is a statistically significant\n    difference between the two groups.\n    The null hypothesis is that there is no difference in the distributions of\n    performance between the two groups. We decide that a significance level of\n    1% is required to reject the null hypothesis in favor of the alternative\n    that the distributions are different.\n    Since the number of samples is very small, we can compare the observed test\n    statistic against the *exact* distribution of the test statistic under the\n    null hypothesis.\n\n    >>> from scipy.stats import bws_test\n    >>> res = bws_test(x, y)\n    >>> print(res.statistic)\n    5.132167152575315\n\n    This agrees with :math:`B = 5.132` reported in [1]_. The *p*-value produced\n    by `bws_test` also agrees with :math:`p = 0.0029` reported in [1]_.\n\n    >>> print(res.pvalue)\n    0.002913752913752914\n\n    Because the p-value is below our threshold of 1%, we take this as evidence\n    against the null hypothesis in favor of the alternative that there is a\n    difference in performance between the two groups.\n    ",
    "scipy.stats.cauchy": "A Cauchy continuous random variable.\n\n    As an instance of the `rv_continuous` class, `cauchy` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `cauchy` is\n\n    .. math::\n\n        f(x) = \\frac{1}{\\pi (1 + x^2)}\n\n    for a real number :math:`x`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``cauchy.pdf(x, loc, scale)`` is identically\n    equivalent to ``cauchy.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import cauchy\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    \n    >>> mean, var, skew, kurt = cauchy.stats(moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(cauchy.ppf(0.01),\n    ...                 cauchy.ppf(0.99), 100)\n    >>> ax.plot(x, cauchy.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='cauchy pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = cauchy()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = cauchy.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], cauchy.cdf(vals))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = cauchy.rvs(size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.chi": "A chi continuous random variable.\n\n    As an instance of the `rv_continuous` class, `chi` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(df, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, df, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, df, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, df, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, df, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, df, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, df, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, df, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, df, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, df, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(df, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(df, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(df,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(df, loc=0, scale=1)\n        Median of the distribution.\n    mean(df, loc=0, scale=1)\n        Mean of the distribution.\n    var(df, loc=0, scale=1)\n        Variance of the distribution.\n    std(df, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, df, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `chi` is:\n\n    .. math::\n\n        f(x, k) = \\frac{1}{2^{k/2-1} \\Gamma \\left( k/2 \\right)}\n                   x^{k-1} \\exp \\left( -x^2/2 \\right)\n\n    for :math:`x >= 0` and :math:`k > 0` (degrees of freedom, denoted ``df``\n    in the implementation). :math:`\\Gamma` is the gamma function\n    (`scipy.special.gamma`).\n\n    Special cases of `chi` are:\n\n        - ``chi(1, loc, scale)`` is equivalent to `halfnorm`\n        - ``chi(2, 0, scale)`` is equivalent to `rayleigh`\n        - ``chi(3, 0, scale)`` is equivalent to `maxwell`\n\n    `chi` takes ``df`` as a shape parameter.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``chi.pdf(x, df, loc, scale)`` is identically\n    equivalent to ``chi.pdf(y, df) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import chi\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> df = 78\n    >>> mean, var, skew, kurt = chi.stats(df, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(chi.ppf(0.01, df),\n    ...                 chi.ppf(0.99, df), 100)\n    >>> ax.plot(x, chi.pdf(x, df),\n    ...        'r-', lw=5, alpha=0.6, label='chi pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = chi(df)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = chi.ppf([0.001, 0.5, 0.999], df)\n    >>> np.allclose([0.001, 0.5, 0.999], chi.cdf(vals, df))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = chi.rvs(df, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.chi2": "A chi-squared continuous random variable.\n\n    For the noncentral chi-square distribution, see `ncx2`.\n\n    As an instance of the `rv_continuous` class, `chi2` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(df, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, df, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, df, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, df, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, df, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, df, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, df, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, df, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, df, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, df, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(df, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(df, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(df,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(df, loc=0, scale=1)\n        Median of the distribution.\n    mean(df, loc=0, scale=1)\n        Mean of the distribution.\n    var(df, loc=0, scale=1)\n        Variance of the distribution.\n    std(df, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, df, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    ncx2\n\n    Notes\n    -----\n    The probability density function for `chi2` is:\n\n    .. math::\n\n        f(x, k) = \\frac{1}{2^{k/2} \\Gamma \\left( k/2 \\right)}\n                   x^{k/2-1} \\exp \\left( -x/2 \\right)\n\n    for :math:`x > 0`  and :math:`k > 0` (degrees of freedom, denoted ``df``\n    in the implementation).\n\n    `chi2` takes ``df`` as a shape parameter.\n\n    The chi-squared distribution is a special case of the gamma\n    distribution, with gamma parameters ``a = df/2``, ``loc = 0`` and\n    ``scale = 2``.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``chi2.pdf(x, df, loc, scale)`` is identically\n    equivalent to ``chi2.pdf(y, df) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import chi2\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> df = 55\n    >>> mean, var, skew, kurt = chi2.stats(df, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(chi2.ppf(0.01, df),\n    ...                 chi2.ppf(0.99, df), 100)\n    >>> ax.plot(x, chi2.pdf(x, df),\n    ...        'r-', lw=5, alpha=0.6, label='chi2 pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = chi2(df)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = chi2.ppf([0.001, 0.5, 0.999], df)\n    >>> np.allclose([0.001, 0.5, 0.999], chi2.cdf(vals, df))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = chi2.rvs(df, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.chi2_contingency": "Chi-square test of independence of variables in a contingency table.\n\n    This function computes the chi-square statistic and p-value for the\n    hypothesis test of independence of the observed frequencies in the\n    contingency table [1]_ `observed`.  The expected frequencies are computed\n    based on the marginal sums under the assumption of independence; see\n    `scipy.stats.contingency.expected_freq`.  The number of degrees of\n    freedom is (expressed using numpy functions and attributes)::\n\n        dof = observed.size - sum(observed.shape) + observed.ndim - 1\n\n\n    Parameters\n    ----------\n    observed : array_like\n        The contingency table. The table contains the observed frequencies\n        (i.e. number of occurrences) in each category.  In the two-dimensional\n        case, the table is often described as an \"R x C table\".\n    correction : bool, optional\n        If True, *and* the degrees of freedom is 1, apply Yates' correction\n        for continuity.  The effect of the correction is to adjust each\n        observed value by 0.5 towards the corresponding expected value.\n    lambda_ : float or str, optional\n        By default, the statistic computed in this test is Pearson's\n        chi-squared statistic [2]_.  `lambda_` allows a statistic from the\n        Cressie-Read power divergence family [3]_ to be used instead.  See\n        `scipy.stats.power_divergence` for details.\n\n    Returns\n    -------\n    res : Chi2ContingencyResult\n        An object containing attributes:\n\n        statistic : float\n            The test statistic.\n        pvalue : float\n            The p-value of the test.\n        dof : int\n            The degrees of freedom.\n        expected_freq : ndarray, same shape as `observed`\n            The expected frequencies, based on the marginal sums of the table.\n\n    See Also\n    --------\n    scipy.stats.contingency.expected_freq\n    scipy.stats.fisher_exact\n    scipy.stats.chisquare\n    scipy.stats.power_divergence\n    scipy.stats.barnard_exact\n    scipy.stats.boschloo_exact\n\n    Notes\n    -----\n    An often quoted guideline for the validity of this calculation is that\n    the test should be used only if the observed and expected frequencies\n    in each cell are at least 5.\n\n    This is a test for the independence of different categories of a\n    population. The test is only meaningful when the dimension of\n    `observed` is two or more.  Applying the test to a one-dimensional\n    table will always result in `expected` equal to `observed` and a\n    chi-square statistic equal to 0.\n\n    This function does not handle masked arrays, because the calculation\n    does not make sense with missing values.\n\n    Like `scipy.stats.chisquare`, this function computes a chi-square\n    statistic; the convenience this function provides is to figure out the\n    expected frequencies and degrees of freedom from the given contingency\n    table. If these were already known, and if the Yates' correction was not\n    required, one could use `scipy.stats.chisquare`.  That is, if one calls::\n\n        res = chi2_contingency(obs, correction=False)\n\n    then the following is true::\n\n        (res.statistic, res.pvalue) == stats.chisquare(obs.ravel(),\n                                                       f_exp=ex.ravel(),\n                                                       ddof=obs.size - 1 - dof)\n\n    The `lambda_` argument was added in version 0.13.0 of scipy.\n\n    References\n    ----------\n    .. [1] \"Contingency table\",\n           https://en.wikipedia.org/wiki/Contingency_table\n    .. [2] \"Pearson's chi-squared test\",\n           https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test\n    .. [3] Cressie, N. and Read, T. R. C., \"Multinomial Goodness-of-Fit\n           Tests\", J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),\n           pp. 440-464.\n    .. [4] Berger, Jeffrey S. et al. \"Aspirin for the Primary Prevention of\n           Cardiovascular Events in Women and Men: A Sex-Specific\n           Meta-analysis of Randomized Controlled Trials.\"\n           JAMA, 295(3):306-313, :doi:`10.1001/jama.295.3.306`, 2006.\n\n    Examples\n    --------\n    In [4]_, the use of aspirin to prevent cardiovascular events in women\n    and men was investigated. The study notably concluded:\n\n        ...aspirin therapy reduced the risk of a composite of\n        cardiovascular events due to its effect on reducing the risk of\n        ischemic stroke in women [...]\n\n    The article lists studies of various cardiovascular events. Let's\n    focus on the ischemic stoke in women.\n\n    The following table summarizes the results of the experiment in which\n    participants took aspirin or a placebo on a regular basis for several\n    years. Cases of ischemic stroke were recorded::\n\n                          Aspirin   Control/Placebo\n        Ischemic stroke     176           230\n        No stroke         21035         21018\n\n    Is there evidence that the aspirin reduces the risk of ischemic stroke?\n    We begin by formulating a null hypothesis :math:`H_0`:\n\n        The effect of aspirin is equivalent to that of placebo.\n\n    Let's assess the plausibility of this hypothesis with\n    a chi-square test.\n\n    >>> import numpy as np\n    >>> from scipy.stats import chi2_contingency\n    >>> table = np.array([[176, 230], [21035, 21018]])\n    >>> res = chi2_contingency(table)\n    >>> res.statistic\n    6.892569132546561\n    >>> res.pvalue\n    0.008655478161175739\n\n    Using a significance level of 5%, we would reject the null hypothesis in\n    favor of the alternative hypothesis: \"the effect of aspirin\n    is not equivalent to the effect of placebo\".\n    Because `scipy.stats.contingency.chi2_contingency` performs a two-sided\n    test, the alternative hypothesis does not indicate the direction of the\n    effect. We can use `stats.contingency.odds_ratio` to support the\n    conclusion that aspirin *reduces* the risk of ischemic stroke.\n\n    Below are further examples showing how larger contingency tables can be\n    tested.\n\n    A two-way example (2 x 3):\n\n    >>> obs = np.array([[10, 10, 20], [20, 20, 20]])\n    >>> res = chi2_contingency(obs)\n    >>> res.statistic\n    2.7777777777777777\n    >>> res.pvalue\n    0.24935220877729619\n    >>> res.dof\n    2\n    >>> res.expected_freq\n    array([[ 12.,  12.,  16.],\n           [ 18.,  18.,  24.]])\n\n    Perform the test using the log-likelihood ratio (i.e. the \"G-test\")\n    instead of Pearson's chi-squared statistic.\n\n    >>> res = chi2_contingency(obs, lambda_=\"log-likelihood\")\n    >>> res.statistic\n    2.7688587616781319\n    >>> res.pvalue\n    0.25046668010954165\n\n    A four-way example (2 x 2 x 2 x 2):\n\n    >>> obs = np.array(\n    ...     [[[[12, 17],\n    ...        [11, 16]],\n    ...       [[11, 12],\n    ...        [15, 16]]],\n    ...      [[[23, 15],\n    ...        [30, 22]],\n    ...       [[14, 17],\n    ...        [15, 16]]]])\n    >>> res = chi2_contingency(obs)\n    >>> res.statistic\n    8.7584514426741897\n    >>> res.pvalue\n    0.64417725029295503\n    ",
    "scipy.stats.chisquare": "Calculate a one-way chi-square test.\n\n    The chi-square test tests the null hypothesis that the categorical data\n    has the given frequencies.\n\n    Parameters\n    ----------\n    f_obs : array_like\n        Observed frequencies in each category.\n    f_exp : array_like, optional\n        Expected frequencies in each category.  By default the categories are\n        assumed to be equally likely.\n    ddof : int, optional\n        \"Delta degrees of freedom\": adjustment to the degrees of freedom\n        for the p-value.  The p-value is computed using a chi-squared\n        distribution with ``k - 1 - ddof`` degrees of freedom, where `k`\n        is the number of observed frequencies.  The default value of `ddof`\n        is 0.\n    axis : int or None, optional\n        The axis of the broadcast result of `f_obs` and `f_exp` along which to\n        apply the test.  If axis is None, all values in `f_obs` are treated\n        as a single data set.  Default is 0.\n\n    Returns\n    -------\n    res: Power_divergenceResult\n        An object containing attributes:\n\n        statistic : float or ndarray\n            The chi-squared test statistic.  The value is a float if `axis` is\n            None or `f_obs` and `f_exp` are 1-D.\n        pvalue : float or ndarray\n            The p-value of the test.  The value is a float if `ddof` and the\n            result attribute `statistic` are scalars.\n\n    See Also\n    --------\n    scipy.stats.power_divergence\n    scipy.stats.fisher_exact : Fisher exact test on a 2x2 contingency table.\n    scipy.stats.barnard_exact : An unconditional exact test. An alternative\n        to chi-squared test for small sample sizes.\n\n    Notes\n    -----\n    This test is invalid when the observed or expected frequencies in each\n    category are too small.  A typical rule is that all of the observed\n    and expected frequencies should be at least 5. According to [3]_, the\n    total number of samples is recommended to be greater than 13,\n    otherwise exact tests (such as Barnard's Exact test) should be used\n    because they do not overreject.\n\n    Also, the sum of the observed and expected frequencies must be the same\n    for the test to be valid; `chisquare` raises an error if the sums do not\n    agree within a relative tolerance of ``1e-8``.\n\n    The default degrees of freedom, k-1, are for the case when no parameters\n    of the distribution are estimated. If p parameters are estimated by\n    efficient maximum likelihood then the correct degrees of freedom are\n    k-1-p. If the parameters are estimated in a different way, then the\n    dof can be between k-1-p and k-1. However, it is also possible that\n    the asymptotic distribution is not chi-square, in which case this test\n    is not appropriate.\n\n    References\n    ----------\n    .. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n           Statistics\". Chapter 8.\n           https://web.archive.org/web/20171022032306/http://vassarstats.net:80/textbook/ch8pt1.html\n    .. [2] \"Chi-squared test\", https://en.wikipedia.org/wiki/Chi-squared_test\n    .. [3] Pearson, Karl. \"On the criterion that a given system of deviations from the probable\n           in the case of a correlated system of variables is such that it can be reasonably\n           supposed to have arisen from random sampling\", Philosophical Magazine. Series 5. 50\n           (1900), pp. 157-175.\n    .. [4] Mannan, R. William and E. Charles. Meslow. \"Bird populations and\n           vegetation characteristics in managed and old-growth forests,\n           northeastern Oregon.\" Journal of Wildlife Management\n           48, 1219-1238, :doi:`10.2307/3801783`, 1984.\n\n    Examples\n    --------\n    In [4]_, bird foraging behavior was investigated in an old-growth forest\n    of Oregon.\n    In the forest, 44% of the canopy volume was Douglas fir,\n    24% was ponderosa pine, 29% was grand fir, and 3% was western larch.\n    The authors observed the behavior of several species of birds, one of\n    which was the red-breasted nuthatch. They made 189 observations of this\n    species foraging, recording 43 (\"23%\") of observations in Douglas fir,\n    52 (\"28%\") in ponderosa pine, 54 (\"29%\") in grand fir, and 40 (\"21%\") in\n    western larch.\n\n    Using a chi-square test, we can test the null hypothesis that the\n    proportions of foraging events are equal to the proportions of canopy\n    volume. The authors of the paper considered a p-value less than 1% to be\n    significant.\n\n    Using the above proportions of canopy volume and observed events, we can\n    infer expected frequencies.\n\n    >>> import numpy as np\n    >>> f_exp = np.array([44, 24, 29, 3]) / 100 * 189\n\n    The observed frequencies of foraging were:\n\n    >>> f_obs = np.array([43, 52, 54, 40])\n\n    We can now compare the observed frequencies with the expected frequencies.\n\n    >>> from scipy.stats import chisquare\n    >>> chisquare(f_obs=f_obs, f_exp=f_exp)\n    Power_divergenceResult(statistic=228.23515947653874, pvalue=3.3295585338846486e-49)\n\n    The p-value is well below the chosen significance level. Hence, the\n    authors considered the difference to be significant and concluded\n    that the relative proportions of foraging events were not the same\n    as the relative proportions of tree canopy volume.\n\n    Following are other generic examples to demonstrate how the other\n    parameters can be used.\n\n    When just `f_obs` is given, it is assumed that the expected frequencies\n    are uniform and given by the mean of the observed frequencies.\n\n    >>> chisquare([16, 18, 16, 14, 12, 12])\n    Power_divergenceResult(statistic=2.0, pvalue=0.84914503608460956)\n\n    With `f_exp` the expected frequencies can be given.\n\n    >>> chisquare([16, 18, 16, 14, 12, 12], f_exp=[16, 16, 16, 16, 16, 8])\n    Power_divergenceResult(statistic=3.5, pvalue=0.62338762774958223)\n\n    When `f_obs` is 2-D, by default the test is applied to each column.\n\n    >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n    >>> obs.shape\n    (6, 2)\n    >>> chisquare(obs)\n    Power_divergenceResult(statistic=array([2.        , 6.66666667]), pvalue=array([0.84914504, 0.24663415]))\n\n    By setting ``axis=None``, the test is applied to all data in the array,\n    which is equivalent to applying the test to the flattened array.\n\n    >>> chisquare(obs, axis=None)\n    Power_divergenceResult(statistic=23.31034482758621, pvalue=0.015975692534127565)\n    >>> chisquare(obs.ravel())\n    Power_divergenceResult(statistic=23.310344827586206, pvalue=0.01597569253412758)\n\n    `ddof` is the change to make to the default degrees of freedom.\n\n    >>> chisquare([16, 18, 16, 14, 12, 12], ddof=1)\n    Power_divergenceResult(statistic=2.0, pvalue=0.7357588823428847)\n\n    The calculation of the p-values is done by broadcasting the\n    chi-squared statistic with `ddof`.\n\n    >>> chisquare([16, 18, 16, 14, 12, 12], ddof=[0,1,2])\n    Power_divergenceResult(statistic=2.0, pvalue=array([0.84914504, 0.73575888, 0.5724067 ]))\n\n    `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n    shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n    `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n    statistics, we use ``axis=1``:\n\n    >>> chisquare([16, 18, 16, 14, 12, 12],\n    ...           f_exp=[[16, 16, 16, 16, 16, 8], [8, 20, 20, 16, 12, 12]],\n    ...           axis=1)\n    Power_divergenceResult(statistic=array([3.5 , 9.25]), pvalue=array([0.62338763, 0.09949846]))\n\n    ",
    "scipy.stats.circmean": "    \n\n\nCompute the circular mean of a sample of angle observations.\n\nGiven :math:`n` angle observations :math:`x_1, \\cdots, x_n` measured in\nradians, their `circular mean` is defined by ([1]_, Eq. 2.2.4)\n\n.. math::\n\n   \\mathrm{Arg} \\left( \\frac{1}{n} \\sum_{k=1}^n e^{i x_k} \\right)\n\nwhere :math:`i` is the imaginary unit and :math:`\\mathop{\\mathrm{Arg}} z`\ngives the principal value of the argument of complex number :math:`z`,\nrestricted to the range :math:`[0,2\\pi]` by default.  :math:`z` in the\nabove expression is known as the `mean resultant vector`.\n\nParameters\n----------\nsamples : array_like\n    Input array of angle observations.  The value of a full angle is\n    equal to ``(high - low)``.\nhigh : float, optional\n    Upper boundary of the principal value of an angle.  Default is ``2*pi``.\nlow : float, optional\n    Lower boundary of the principal value of an angle.  Default is ``0``.\naxis : int or None, default: None\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\ncircmean : float\n    Circular mean, restricted to the range ``[low, high]``.\n    \n    If the mean resultant vector is zero, an input-dependent,\n    implementation-defined number between ``[low, high]`` is returned.\n    If the input array is empty, ``np.nan`` is returned.\n\nSee Also\n--------\n\n:func:`circstd`\n    Circular standard deviation.\n:func:`circvar`\n    Circular variance.\n\n\nNotes\n-----\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] Mardia, K. V. and Jupp, P. E. *Directional Statistics*.\n       John Wiley & Sons, 1999.\n\nExamples\n--------\nFor readability, all angles are printed out in degrees.\n\n>>> import numpy as np\n>>> from scipy.stats import circmean\n>>> import matplotlib.pyplot as plt\n>>> angles = np.deg2rad(np.array([20, 30, 330]))\n>>> circmean = circmean(angles)\n>>> np.rad2deg(circmean)\n7.294976657784009\n\n>>> mean = angles.mean()\n>>> np.rad2deg(mean)\n126.66666666666666\n\nPlot and compare the circular mean against the arithmetic mean.\n\n>>> plt.plot(np.cos(np.linspace(0, 2*np.pi, 500)),\n...          np.sin(np.linspace(0, 2*np.pi, 500)),\n...          c='k')\n>>> plt.scatter(np.cos(angles), np.sin(angles), c='k')\n>>> plt.scatter(np.cos(circmean), np.sin(circmean), c='b',\n...             label='circmean')\n>>> plt.scatter(np.cos(mean), np.sin(mean), c='r', label='mean')\n>>> plt.legend()\n>>> plt.axis('equal')\n>>> plt.show()\n",
    "scipy.stats.circstd": "    \n\n\nCompute the circular standard deviation of a sample of angle observations.\n\nGiven :math:`n` angle observations :math:`x_1, \\cdots, x_n` measured in\nradians, their `circular standard deviation` is defined by\n([2]_, Eq. 2.3.11)\n\n.. math::\n\n   \\sqrt{ -2 \\log \\left| \\frac{1}{n} \\sum_{k=1}^n e^{i x_k} \\right| }\n\nwhere :math:`i` is the imaginary unit and :math:`|z|` gives the length\nof the complex number :math:`z`.  :math:`|z|` in the above expression\nis known as the `mean resultant length`.\n\nParameters\n----------\nsamples : array_like\n    Input array of angle observations.  The value of a full angle is\n    equal to ``(high - low)``.\nhigh : float, optional\n    Upper boundary of the principal value of an angle.  Default is ``2*pi``.\nlow : float, optional\n    Lower boundary of the principal value of an angle.  Default is ``0``.\nnormalize : boolean, optional\n    If ``False`` (the default), the return value is computed from the\n    above formula with the input scaled by ``(2*pi)/(high-low)`` and\n    the output scaled (back) by ``(high-low)/(2*pi)``.  If ``True``,\n    the output is not scaled and is returned directly.\naxis : int or None, default: None\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\ncircstd : float\n    Circular standard deviation, optionally normalized.\n    \n    If the input array is empty, ``np.nan`` is returned.\n\nSee Also\n--------\n\n:func:`circmean`\n    Circular mean.\n:func:`circvar`\n    Circular variance.\n\n\nNotes\n-----\nIn the limit of small angles, the circular standard deviation is close\nto the 'linear' standard deviation if ``normalize`` is ``False``.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] Mardia, K. V. (1972). 2. In *Statistics of Directional Data*\n   (pp. 18-24). Academic Press. :doi:`10.1016/C2013-0-07425-7`.\n.. [2] Mardia, K. V. and Jupp, P. E. *Directional Statistics*.\n       John Wiley & Sons, 1999.\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy.stats import circstd\n>>> import matplotlib.pyplot as plt\n>>> samples_1 = np.array([0.072, -0.158, 0.077, 0.108, 0.286,\n...                       0.133, -0.473, -0.001, -0.348, 0.131])\n>>> samples_2 = np.array([0.111, -0.879, 0.078, 0.733, 0.421,\n...                       0.104, -0.136, -0.867,  0.012,  0.105])\n>>> circstd_1 = circstd(samples_1)\n>>> circstd_2 = circstd(samples_2)\n\nPlot the samples.\n\n>>> fig, (left, right) = plt.subplots(ncols=2)\n>>> for image in (left, right):\n...     image.plot(np.cos(np.linspace(0, 2*np.pi, 500)),\n...                np.sin(np.linspace(0, 2*np.pi, 500)),\n...                c='k')\n...     image.axis('equal')\n...     image.axis('off')\n>>> left.scatter(np.cos(samples_1), np.sin(samples_1), c='k', s=15)\n>>> left.set_title(f\"circular std: {np.round(circstd_1, 2)!r}\")\n>>> right.plot(np.cos(np.linspace(0, 2*np.pi, 500)),\n...            np.sin(np.linspace(0, 2*np.pi, 500)),\n...            c='k')\n>>> right.scatter(np.cos(samples_2), np.sin(samples_2), c='k', s=15)\n>>> right.set_title(f\"circular std: {np.round(circstd_2, 2)!r}\")\n>>> plt.show()\n",
    "scipy.stats.circvar": "    \n\n\nCompute the circular variance of a sample of angle observations.\n\nGiven :math:`n` angle observations :math:`x_1, \\cdots, x_n` measured in\nradians, their `circular variance` is defined by ([2]_, Eq. 2.3.3)\n\n.. math::\n\n   1 - \\left| \\frac{1}{n} \\sum_{k=1}^n e^{i x_k} \\right|\n\nwhere :math:`i` is the imaginary unit and :math:`|z|` gives the length\nof the complex number :math:`z`.  :math:`|z|` in the above expression\nis known as the `mean resultant length`.\n\nParameters\n----------\nsamples : array_like\n    Input array of angle observations.  The value of a full angle is\n    equal to ``(high - low)``.\nhigh : float, optional\n    Upper boundary of the principal value of an angle.  Default is ``2*pi``.\nlow : float, optional\n    Lower boundary of the principal value of an angle.  Default is ``0``.\naxis : int or None, default: None\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\ncircvar : float\n    Circular variance.  The returned value is in the range ``[0, 1]``,\n    where ``0`` indicates no variance and ``1`` indicates large variance.\n    \n    If the input array is empty, ``np.nan`` is returned.\n\nSee Also\n--------\n\n:func:`circmean`\n    Circular mean.\n:func:`circstd`\n    Circular standard deviation.\n\n\nNotes\n-----\nIn the limit of small angles, the circular variance is close to\nhalf the 'linear' variance if measured in radians.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] Fisher, N.I. *Statistical analysis of circular data*. Cambridge\n       University Press, 1993.\n.. [2] Mardia, K. V. and Jupp, P. E. *Directional Statistics*.\n       John Wiley & Sons, 1999.\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy.stats import circvar\n>>> import matplotlib.pyplot as plt\n>>> samples_1 = np.array([0.072, -0.158, 0.077, 0.108, 0.286,\n...                       0.133, -0.473, -0.001, -0.348, 0.131])\n>>> samples_2 = np.array([0.111, -0.879, 0.078, 0.733, 0.421,\n...                       0.104, -0.136, -0.867,  0.012,  0.105])\n>>> circvar_1 = circvar(samples_1)\n>>> circvar_2 = circvar(samples_2)\n\nPlot the samples.\n\n>>> fig, (left, right) = plt.subplots(ncols=2)\n>>> for image in (left, right):\n...     image.plot(np.cos(np.linspace(0, 2*np.pi, 500)),\n...                np.sin(np.linspace(0, 2*np.pi, 500)),\n...                c='k')\n...     image.axis('equal')\n...     image.axis('off')\n>>> left.scatter(np.cos(samples_1), np.sin(samples_1), c='k', s=15)\n>>> left.set_title(f\"circular variance: {np.round(circvar_1, 2)!r}\")\n>>> right.scatter(np.cos(samples_2), np.sin(samples_2), c='k', s=15)\n>>> right.set_title(f\"circular variance: {np.round(circvar_2, 2)!r}\")\n>>> plt.show()\n",
    "scipy.stats.combine_pvalues": "    \n\n\nCombine p-values from independent tests that bear upon the same hypothesis.\n\nThese methods are intended only for combining p-values from hypothesis\ntests based upon continuous distributions.\n\nEach method assumes that under the null hypothesis, the p-values are\nsampled independently and uniformly from the interval [0, 1]. A test\nstatistic (different for each method) is computed and a combined\np-value is calculated based upon the distribution of this test statistic\nunder the null hypothesis.\n\nParameters\n----------\npvalues : array_like\n    Array of p-values assumed to come from independent tests based on\n    continuous distributions.\nmethod : {'fisher', 'pearson', 'tippett', 'stouffer', 'mudholkar_george'}\n    Name of method to use to combine p-values.\n    \n    The available methods are (see Notes for details):\n    \n    * 'fisher': Fisher's method (Fisher's combined probability test)\n    * 'pearson': Pearson's method\n    * 'mudholkar_george': Mudholkar's and George's method\n    * 'tippett': Tippett's method\n    * 'stouffer': Stouffer's Z-score method\nweights : array_like, optional\n    Optional array of weights used only for Stouffer's Z-score method.\n    Ignored by other methods.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nres : SignificanceResult\n    An object containing attributes:\n    \n    statistic : float\n        The statistic calculated by the specified method.\n    pvalue : float\n        The combined p-value.\n\nNotes\n-----\nIf this function is applied to tests with a discrete statistics such as\nany rank test or contingency-table test, it will yield systematically\nwrong results, e.g. Fisher's method will systematically overestimate the\np-value [1]_. This problem becomes less severe for large sample sizes\nwhen the discrete distributions become approximately continuous.\n\nThe differences between the methods can be best illustrated by their\nstatistics and what aspects of a combination of p-values they emphasise\nwhen considering significance [2]_. For example, methods emphasising large\np-values are more sensitive to strong false and true negatives; conversely\nmethods focussing on small p-values are sensitive to positives.\n\n* The statistics of Fisher's method (also known as Fisher's combined\n  probability test) [3]_ is :math:`-2\\sum_i \\log(p_i)`, which is\n  equivalent (as a test statistics) to the product of individual p-values:\n  :math:`\\prod_i p_i`. Under the null hypothesis, this statistics follows\n  a :math:`\\chi^2` distribution. This method emphasises small p-values.\n* Pearson's method uses :math:`-2\\sum_i\\log(1-p_i)`, which is equivalent\n  to :math:`\\prod_i \\frac{1}{1-p_i}` [2]_.\n  It thus emphasises large p-values.\n* Mudholkar and George compromise between Fisher's and Pearson's method by\n  averaging their statistics [4]_. Their method emphasises extreme\n  p-values, both close to 1 and 0.\n* Stouffer's method [5]_ uses Z-scores and the statistic:\n  :math:`\\sum_i \\Phi^{-1} (p_i)`, where :math:`\\Phi` is the CDF of the\n  standard normal distribution. The advantage of this method is that it is\n  straightforward to introduce weights, which can make Stouffer's method\n  more powerful than Fisher's method when the p-values are from studies\n  of different size [6]_ [7]_.\n* Tippett's method uses the smallest p-value as a statistic.\n  (Mind that this minimum is not the combined p-value.)\n\nFisher's method may be extended to combine p-values from dependent tests\n[8]_. Extensions such as Brown's method and Kost's method are not currently\nimplemented.\n\n.. versionadded:: 0.15.0\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] Kincaid, W. M., \"The Combination of Tests Based on Discrete\n       Distributions.\" Journal of the American Statistical Association 57,\n       no. 297 (1962), 10-19.\n.. [2] Heard, N. and Rubin-Delanchey, P. \"Choosing between methods of\n       combining p-values.\"  Biometrika 105.1 (2018): 239-246.\n.. [3] https://en.wikipedia.org/wiki/Fisher%27s_method\n.. [4] George, E. O., and G. S. Mudholkar. \"On the convolution of logistic\n       random variables.\" Metrika 30.1 (1983): 1-13.\n.. [5] https://en.wikipedia.org/wiki/Fisher%27s_method#Relation_to_Stouffer.27s_Z-score_method\n.. [6] Whitlock, M. C. \"Combining probability from independent tests: the\n       weighted Z-method is superior to Fisher's approach.\" Journal of\n       Evolutionary Biology 18, no. 5 (2005): 1368-1373.\n.. [7] Zaykin, Dmitri V. \"Optimally weighted Z-test is a powerful method\n       for combining probabilities in meta-analysis.\" Journal of\n       Evolutionary Biology 24, no. 8 (2011): 1836-1841.\n.. [8] https://en.wikipedia.org/wiki/Extensions_of_Fisher%27s_method\n\nExamples\n--------\nSuppose we wish to combine p-values from four independent tests\nof the same null hypothesis using Fisher's method (default).\n\n>>> from scipy.stats import combine_pvalues\n>>> pvalues = [0.1, 0.05, 0.02, 0.3]\n>>> combine_pvalues(pvalues)\nSignificanceResult(statistic=20.828626352604235, pvalue=0.007616871850449092)\n\nWhen the individual p-values carry different weights, consider Stouffer's\nmethod.\n\n>>> weights = [1, 2, 3, 4]\n>>> res = combine_pvalues(pvalues, method='stouffer', weights=weights)\n>>> res.pvalue\n0.009578891494533616\n",
    "scipy.stats.cosine": "A cosine continuous random variable.\n\n    As an instance of the `rv_continuous` class, `cosine` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The cosine distribution is an approximation to the normal distribution.\n    The probability density function for `cosine` is:\n\n    .. math::\n\n        f(x) = \\frac{1}{2\\pi} (1+\\cos(x))\n\n    for :math:`-\\pi \\le x \\le \\pi`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``cosine.pdf(x, loc, scale)`` is identically\n    equivalent to ``cosine.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import cosine\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    \n    >>> mean, var, skew, kurt = cosine.stats(moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(cosine.ppf(0.01),\n    ...                 cosine.ppf(0.99), 100)\n    >>> ax.plot(x, cosine.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='cosine pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = cosine()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = cosine.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], cosine.cdf(vals))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = cosine.rvs(size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.cramervonmises": "    \n\n\nPerform the one-sample Cram\u00e9r-von Mises test for goodness of fit.\n\nThis performs a test of the goodness of fit of a cumulative distribution\nfunction (cdf) :math:`F` compared to the empirical distribution function\n:math:`F_n` of observed random variates :math:`X_1, ..., X_n` that are\nassumed to be independent and identically distributed ([1]_).\nThe null hypothesis is that the :math:`X_i` have cumulative distribution\n:math:`F`.\n\nParameters\n----------\nrvs : array_like\n    A 1-D array of observed values of the random variables :math:`X_i`.\n    The sample must contain at least two observations.\ncdf : str or callable\n    The cumulative distribution function :math:`F` to test the\n    observations against. If a string, it should be the name of a\n    distribution in `scipy.stats`. If a callable, that callable is used\n    to calculate the cdf: ``cdf(x, *args) -> float``.\nargs : tuple, optional\n    Distribution parameters. These are assumed to be known; see Notes.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nres : object with attributes\n    statistic : float\n        Cram\u00e9r-von Mises statistic.\n    pvalue : float\n        The p-value.\n\nSee Also\n--------\n\n:func:`kstest`, :func:`cramervonmises_2samp`\n    ..\n\nNotes\n-----\n.. versionadded:: 1.6.0\n\nThe p-value relies on the approximation given by equation 1.8 in [2]_.\nIt is important to keep in mind that the p-value is only accurate if\none tests a simple hypothesis, i.e. the parameters of the reference\ndistribution are known. If the parameters are estimated from the data\n(composite hypothesis), the computed p-value is not reliable.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] Cram\u00e9r-von Mises criterion, Wikipedia,\n       https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93von_Mises_criterion\n.. [2] Cs\u00f6rg\u0151, S. and Faraway, J. (1996). The Exact and Asymptotic\n       Distribution of Cram\u00e9r-von Mises Statistics. Journal of the\n       Royal Statistical Society, pp. 221-234.\n\nExamples\n--------\nSuppose we wish to test whether data generated by ``scipy.stats.norm.rvs``\nwere, in fact, drawn from the standard normal distribution. We choose a\nsignificance level of ``alpha=0.05``.\n\n>>> import numpy as np\n>>> from scipy import stats\n>>> rng = np.random.default_rng(165417232101553420507139617764912913465)\n>>> x = stats.norm.rvs(size=500, random_state=rng)\n>>> res = stats.cramervonmises(x, 'norm')\n>>> res.statistic, res.pvalue\n(0.1072085112565724, 0.5508482238203407)\n\nThe p-value exceeds our chosen significance level, so we do not\nreject the null hypothesis that the observed sample is drawn from the\nstandard normal distribution.\n\nNow suppose we wish to check whether the same samples shifted by 2.1 is\nconsistent with being drawn from a normal distribution with a mean of 2.\n\n>>> y = x + 2.1\n>>> res = stats.cramervonmises(y, 'norm', args=(2,))\n>>> res.statistic, res.pvalue\n(0.8364446265294695, 0.00596286797008283)\n\nHere we have used the `args` keyword to specify the mean (``loc``)\nof the normal distribution to test the data against. This is equivalent\nto the following, in which we create a frozen normal distribution with\nmean 2.1, then pass its ``cdf`` method as an argument.\n\n>>> frozen_dist = stats.norm(loc=2)\n>>> res = stats.cramervonmises(y, frozen_dist.cdf)\n>>> res.statistic, res.pvalue\n(0.8364446265294695, 0.00596286797008283)\n\nIn either case, we would reject the null hypothesis that the observed\nsample is drawn from a normal distribution with a mean of 2 (and default\nvariance of 1) because the p-value is less than our chosen\nsignificance level.\n",
    "scipy.stats.cramervonmises_2samp": "    \n\n\nPerform the two-sample Cram\u00e9r-von Mises test for goodness of fit.\n\nThis is the two-sample version of the Cram\u00e9r-von Mises test ([1]_):\nfor two independent samples :math:`X_1, ..., X_n` and\n:math:`Y_1, ..., Y_m`, the null hypothesis is that the samples\ncome from the same (unspecified) continuous distribution.\n\nParameters\n----------\nx : array_like\n    A 1-D array of observed values of the random variables :math:`X_i`.\n    Must contain at least two observations.\ny : array_like\n    A 1-D array of observed values of the random variables :math:`Y_i`.\n    Must contain at least two observations.\nmethod : {'auto', 'asymptotic', 'exact'}, optional\n    The method used to compute the p-value, see Notes for details.\n    The default is 'auto'.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nres : object with attributes\n    statistic : float\n        Cram\u00e9r-von Mises statistic.\n    pvalue : float\n        The p-value.\n\nSee Also\n--------\n\n:func:`cramervonmises`, :func:`anderson_ksamp`, :func:`epps_singleton_2samp`, :func:`ks_2samp`\n    ..\n\nNotes\n-----\n.. versionadded:: 1.7.0\n\nThe statistic is computed according to equation 9 in [2]_. The\ncalculation of the p-value depends on the keyword `method`:\n\n- ``asymptotic``: The p-value is approximated by using the limiting\n  distribution of the test statistic.\n- ``exact``: The exact p-value is computed by enumerating all\n  possible combinations of the test statistic, see [2]_.\n\nIf ``method='auto'``, the exact approach is used\nif both samples contain equal to or less than 20 observations,\notherwise the asymptotic distribution is used.\n\nIf the underlying distribution is not continuous, the p-value is likely to\nbe conservative (Section 6.2 in [3]_). When ranking the data to compute\nthe test statistic, midranks are used if there are ties.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] https://en.wikipedia.org/wiki/Cramer-von_Mises_criterion\n.. [2] Anderson, T.W. (1962). On the distribution of the two-sample\n       Cramer-von-Mises criterion. The Annals of Mathematical\n       Statistics, pp. 1148-1159.\n.. [3] Conover, W.J., Practical Nonparametric Statistics, 1971.\n\nExamples\n--------\nSuppose we wish to test whether two samples generated by\n``scipy.stats.norm.rvs`` have the same distribution. We choose a\nsignificance level of alpha=0.05.\n\n>>> import numpy as np\n>>> from scipy import stats\n>>> rng = np.random.default_rng()\n>>> x = stats.norm.rvs(size=100, random_state=rng)\n>>> y = stats.norm.rvs(size=70, random_state=rng)\n>>> res = stats.cramervonmises_2samp(x, y)\n>>> res.statistic, res.pvalue\n(0.29376470588235293, 0.1412873014573014)\n\nThe p-value exceeds our chosen significance level, so we do not\nreject the null hypothesis that the observed samples are drawn from the\nsame distribution.\n\nFor small sample sizes, one can compute the exact p-values:\n\n>>> x = stats.norm.rvs(size=7, random_state=rng)\n>>> y = stats.t.rvs(df=2, size=6, random_state=rng)\n>>> res = stats.cramervonmises_2samp(x, y, method='exact')\n>>> res.statistic, res.pvalue\n(0.197802197802198, 0.31643356643356646)\n\nThe p-value based on the asymptotic distribution is a good approximation\neven though the sample size is small.\n\n>>> res = stats.cramervonmises_2samp(x, y, method='asymptotic')\n>>> res.statistic, res.pvalue\n(0.197802197802198, 0.2966041181527128)\n\nIndependent of the method, one would not reject the null hypothesis at the\nchosen significance level in this example.\n",
    "scipy.stats.crystalball": "\n    Crystalball distribution\n\n    As an instance of the `rv_continuous` class, `crystalball` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(beta, m, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, beta, m, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, beta, m, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, beta, m, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, beta, m, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, beta, m, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, beta, m, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, beta, m, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, beta, m, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, beta, m, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(beta, m, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(beta, m, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(beta, m), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(beta, m, loc=0, scale=1)\n        Median of the distribution.\n    mean(beta, m, loc=0, scale=1)\n        Mean of the distribution.\n    var(beta, m, loc=0, scale=1)\n        Variance of the distribution.\n    std(beta, m, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, beta, m, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `crystalball` is:\n\n    .. math::\n\n        f(x, \\beta, m) =  \\begin{cases}\n                            N \\exp(-x^2 / 2),  &\\text{for } x > -\\beta\\\\\n                            N A (B - x)^{-m}  &\\text{for } x \\le -\\beta\n                          \\end{cases}\n\n    where :math:`A = (m / |\\beta|)^m  \\exp(-\\beta^2 / 2)`,\n    :math:`B = m/|\\beta| - |\\beta|` and :math:`N` is a normalisation constant.\n\n    `crystalball` takes :math:`\\beta > 0` and :math:`m > 1` as shape\n    parameters.  :math:`\\beta` defines the point where the pdf changes\n    from a power-law to a Gaussian distribution.  :math:`m` is the power\n    of the power-law tail.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``crystalball.pdf(x, beta, m, loc, scale)`` is identically\n    equivalent to ``crystalball.pdf(y, beta, m) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    .. versionadded:: 0.19.0\n\n    References\n    ----------\n    .. [1] \"Crystal Ball Function\",\n           https://en.wikipedia.org/wiki/Crystal_Ball_function\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import crystalball\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> beta, m = 2, 3\n    >>> mean, var, skew, kurt = crystalball.stats(beta, m, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(crystalball.ppf(0.01, beta, m),\n    ...                 crystalball.ppf(0.99, beta, m), 100)\n    >>> ax.plot(x, crystalball.pdf(x, beta, m),\n    ...        'r-', lw=5, alpha=0.6, label='crystalball pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = crystalball(beta, m)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = crystalball.ppf([0.001, 0.5, 0.999], beta, m)\n    >>> np.allclose([0.001, 0.5, 0.999], crystalball.cdf(vals, beta, m))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = crystalball.rvs(beta, m, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n    ",
    "scipy.stats.cumfreq": "Return a cumulative frequency histogram, using the histogram function.\n\n    A cumulative histogram is a mapping that counts the cumulative number of\n    observations in all of the bins up to the specified bin.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    numbins : int, optional\n        The number of bins to use for the histogram. Default is 10.\n    defaultreallimits : tuple (lower, upper), optional\n        The lower and upper values for the range of the histogram.\n        If no value is given, a range slightly larger than the range of the\n        values in `a` is used. Specifically ``(a.min() - s, a.max() + s)``,\n        where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.\n    weights : array_like, optional\n        The weights for each value in `a`. Default is None, which gives each\n        value a weight of 1.0\n\n    Returns\n    -------\n    cumcount : ndarray\n        Binned values of cumulative frequency.\n    lowerlimit : float\n        Lower real limit\n    binsize : float\n        Width of each bin.\n    extrapoints : int\n        Extra points.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> x = [1, 4, 2, 1, 3, 1]\n    >>> res = stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))\n    >>> res.cumcount\n    array([ 1.,  2.,  3.,  3.])\n    >>> res.extrapoints\n    3\n\n    Create a normal distribution with 1000 random values\n\n    >>> samples = stats.norm.rvs(size=1000, random_state=rng)\n\n    Calculate cumulative frequencies\n\n    >>> res = stats.cumfreq(samples, numbins=25)\n\n    Calculate space of values for x\n\n    >>> x = res.lowerlimit + np.linspace(0, res.binsize*res.cumcount.size,\n    ...                                  res.cumcount.size)\n\n    Plot histogram and cumulative histogram\n\n    >>> fig = plt.figure(figsize=(10, 4))\n    >>> ax1 = fig.add_subplot(1, 2, 1)\n    >>> ax2 = fig.add_subplot(1, 2, 2)\n    >>> ax1.hist(samples, bins=25)\n    >>> ax1.set_title('Histogram')\n    >>> ax2.bar(x, res.cumcount, width=res.binsize)\n    >>> ax2.set_title('Cumulative histogram')\n    >>> ax2.set_xlim([x.min(), x.max()])\n\n    >>> plt.show()\n\n    ",
    "scipy.stats.describe": "Compute several descriptive statistics of the passed array.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n    axis : int or None, optional\n        Axis along which statistics are calculated. Default is 0.\n        If None, compute over the whole array `a`.\n    ddof : int, optional\n        Delta degrees of freedom (only for variance).  Default is 1.\n    bias : bool, optional\n        If False, then the skewness and kurtosis calculations are corrected\n        for statistical bias.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    nobs : int or ndarray of ints\n        Number of observations (length of data along `axis`).\n        When 'omit' is chosen as nan_policy, the length along each axis\n        slice is counted separately.\n    minmax: tuple of ndarrays or floats\n        Minimum and maximum value of `a` along the given axis.\n    mean : ndarray or float\n        Arithmetic mean of `a` along the given axis.\n    variance : ndarray or float\n        Unbiased variance of `a` along the given axis; denominator is number\n        of observations minus one.\n    skewness : ndarray or float\n        Skewness of `a` along the given axis, based on moment calculations\n        with denominator equal to the number of observations, i.e. no degrees\n        of freedom correction.\n    kurtosis : ndarray or float\n        Kurtosis (Fisher) of `a` along the given axis.  The kurtosis is\n        normalized so that it is zero for the normal distribution.  No\n        degrees of freedom are used.\n\n    See Also\n    --------\n    skew, kurtosis\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> a = np.arange(10)\n    >>> stats.describe(a)\n    DescribeResult(nobs=10, minmax=(0, 9), mean=4.5,\n                   variance=9.166666666666666, skewness=0.0,\n                   kurtosis=-1.2242424242424244)\n    >>> b = [[1, 2], [3, 4]]\n    >>> stats.describe(b)\n    DescribeResult(nobs=2, minmax=(array([1, 2]), array([3, 4])),\n                   mean=array([2., 3.]), variance=array([2., 2.]),\n                   skewness=array([0., 0.]), kurtosis=array([-2., -2.]))\n\n    ",
    "scipy.stats.dgamma": "A double gamma continuous random variable.\n\n    The double gamma distribution is also known as the reflected gamma\n    distribution [1]_.\n\n    As an instance of the `rv_continuous` class, `dgamma` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, a, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, a, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, a, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, a, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, a, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, a, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, a, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, a, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(a, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, loc=0, scale=1)\n        Median of the distribution.\n    mean(a, loc=0, scale=1)\n        Mean of the distribution.\n    var(a, loc=0, scale=1)\n        Variance of the distribution.\n    std(a, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, a, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `dgamma` is:\n\n    .. math::\n\n        f(x, a) = \\frac{1}{2\\Gamma(a)} |x|^{a-1} \\exp(-|x|)\n\n    for a real number :math:`x` and :math:`a > 0`. :math:`\\Gamma` is the\n    gamma function (`scipy.special.gamma`).\n\n    `dgamma` takes ``a`` as a shape parameter for :math:`a`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``dgamma.pdf(x, a, loc, scale)`` is identically\n    equivalent to ``dgamma.pdf(y, a) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] Johnson, Kotz, and Balakrishnan, \"Continuous Univariate\n           Distributions, Volume 1\", Second Edition, John Wiley and Sons\n           (1994).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import dgamma\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a = 1.1\n    >>> mean, var, skew, kurt = dgamma.stats(a, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(dgamma.ppf(0.01, a),\n    ...                 dgamma.ppf(0.99, a), 100)\n    >>> ax.plot(x, dgamma.pdf(x, a),\n    ...        'r-', lw=5, alpha=0.6, label='dgamma pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = dgamma(a)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = dgamma.ppf([0.001, 0.5, 0.999], a)\n    >>> np.allclose([0.001, 0.5, 0.999], dgamma.cdf(vals, a))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = dgamma.rvs(a, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.differential_entropy": "    \n\n\nGiven a sample of a distribution, estimate the differential entropy.\n\nSeveral estimation methods are available using the `method` parameter. By\ndefault, a method is selected based the size of the sample.\n\nParameters\n----------\nvalues : sequence\n    Sample from a continuous distribution.\nwindow_length : int, optional\n    Window length for computing Vasicek estimate. Must be an integer\n    between 1 and half of the sample size. If ``None`` (the default), it\n    uses the heuristic value\n    \n    .. math::\n        \\left \\lfloor \\sqrt{n} + 0.5 \\right \\rfloor\n    \n    where :math:`n` is the sample size. This heuristic was originally\n    proposed in [2]_ and has become common in the literature.\nbase : float, optional\n    The logarithmic base to use, defaults to ``e`` (natural logarithm).\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nmethod : {'vasicek', 'van es', 'ebrahimi', 'correa', 'auto'}, optional\n    The method used to estimate the differential entropy from the sample.\n    Default is ``'auto'``.  See Notes for more information.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nentropy : float\n    The calculated differential entropy.\n\nNotes\n-----\nThis function will converge to the true differential entropy in the limit\n\n.. math::\n    n \\to \\infty, \\quad m \\to \\infty, \\quad \\frac{m}{n} \\to 0\n\nThe optimal choice of ``window_length`` for a given sample size depends on\nthe (unknown) distribution. Typically, the smoother the density of the\ndistribution, the larger the optimal value of ``window_length`` [1]_.\n\nThe following options are available for the `method` parameter.\n\n* ``'vasicek'`` uses the estimator presented in [1]_. This is\n  one of the first and most influential estimators of differential entropy.\n* ``'van es'`` uses the bias-corrected estimator presented in [3]_, which\n  is not only consistent but, under some conditions, asymptotically normal.\n* ``'ebrahimi'`` uses an estimator presented in [4]_, which was shown\n  in simulation to have smaller bias and mean squared error than\n  the Vasicek estimator.\n* ``'correa'`` uses the estimator presented in [5]_ based on local linear\n  regression. In a simulation study, it had consistently smaller mean\n  square error than the Vasiceck estimator, but it is more expensive to\n  compute.\n* ``'auto'`` selects the method automatically (default). Currently,\n  this selects ``'van es'`` for very small samples (<10), ``'ebrahimi'``\n  for moderate sample sizes (11-1000), and ``'vasicek'`` for larger\n  samples, but this behavior is subject to change in future versions.\n\nAll estimators are implemented as described in [6]_.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] Vasicek, O. (1976). A test for normality based on sample entropy.\n       Journal of the Royal Statistical Society:\n       Series B (Methodological), 38(1), 54-59.\n.. [2] Crzcgorzewski, P., & Wirczorkowski, R. (1999). Entropy-based\n       goodness-of-fit test for exponentiality. Communications in\n       Statistics-Theory and Methods, 28(5), 1183-1202.\n.. [3] Van Es, B. (1992). Estimating functionals related to a density by a\n       class of statistics based on spacings. Scandinavian Journal of\n       Statistics, 61-72.\n.. [4] Ebrahimi, N., Pflughoeft, K., & Soofi, E. S. (1994). Two measures\n       of sample entropy. Statistics & Probability Letters, 20(3), 225-234.\n.. [5] Correa, J. C. (1995). A new estimator of entropy. Communications\n       in Statistics-Theory and Methods, 24(10), 2439-2449.\n.. [6] Noughabi, H. A. (2015). Entropy Estimation Using Numerical Methods.\n       Annals of Data Science, 2(2), 231-241.\n       https://link.springer.com/article/10.1007/s40745-015-0045-9\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy.stats import differential_entropy, norm\n\nEntropy of a standard normal distribution:\n\n>>> rng = np.random.default_rng()\n>>> values = rng.standard_normal(100)\n>>> differential_entropy(values)\n1.3407817436640392\n\nCompare with the true entropy:\n\n>>> float(norm.entropy())\n1.4189385332046727\n\nFor several sample sizes between 5 and 1000, compare the accuracy of\nthe ``'vasicek'``, ``'van es'``, and ``'ebrahimi'`` methods. Specifically,\ncompare the root mean squared error (over 1000 trials) between the estimate\nand the true differential entropy of the distribution.\n\n>>> from scipy import stats\n>>> import matplotlib.pyplot as plt\n>>>\n>>>\n>>> def rmse(res, expected):\n...     '''Root mean squared error'''\n...     return np.sqrt(np.mean((res - expected)**2))\n>>>\n>>>\n>>> a, b = np.log10(5), np.log10(1000)\n>>> ns = np.round(np.logspace(a, b, 10)).astype(int)\n>>> reps = 1000  # number of repetitions for each sample size\n>>> expected = stats.expon.entropy()\n>>>\n>>> method_errors = {'vasicek': [], 'van es': [], 'ebrahimi': []}\n>>> for method in method_errors:\n...     for n in ns:\n...        rvs = stats.expon.rvs(size=(reps, n), random_state=rng)\n...        res = stats.differential_entropy(rvs, method=method, axis=-1)\n...        error = rmse(res, expected)\n...        method_errors[method].append(error)\n>>>\n>>> for method, errors in method_errors.items():\n...     plt.loglog(ns, errors, label=method)\n>>>\n>>> plt.legend()\n>>> plt.xlabel('sample size')\n>>> plt.ylabel('RMSE (1000 trials)')\n>>> plt.title('Entropy Estimator Error (Exponential Distribution)')\n",
    "scipy.stats.directional_stats": "\n    Computes sample statistics for directional data.\n\n    Computes the directional mean (also called the mean direction vector) and\n    mean resultant length of a sample of vectors.\n\n    The directional mean is a measure of \"preferred direction\" of vector data.\n    It is analogous to the sample mean, but it is for use when the length of\n    the data is irrelevant (e.g. unit vectors).\n\n    The mean resultant length is a value between 0 and 1 used to quantify the\n    dispersion of directional data: the smaller the mean resultant length, the\n    greater the dispersion. Several definitions of directional variance\n    involving the mean resultant length are given in [1]_ and [2]_.\n\n    Parameters\n    ----------\n    samples : array_like\n        Input array. Must be at least two-dimensional, and the last axis of the\n        input must correspond with the dimensionality of the vector space.\n        When the input is exactly two dimensional, this means that each row\n        of the data is a vector observation.\n    axis : int, default: 0\n        Axis along which the directional mean is computed.\n    normalize: boolean, default: True\n        If True, normalize the input to ensure that each observation is a\n        unit vector. It the observations are already unit vectors, consider\n        setting this to False to avoid unnecessary computation.\n\n    Returns\n    -------\n    res : DirectionalStats\n        An object containing attributes:\n\n        mean_direction : ndarray\n            Directional mean.\n        mean_resultant_length : ndarray\n            The mean resultant length [1]_.\n\n    See Also\n    --------\n    circmean: circular mean; i.e. directional mean for 2D *angles*\n    circvar: circular variance; i.e. directional variance for 2D *angles*\n\n    Notes\n    -----\n    This uses a definition of directional mean from [1]_.\n    Assuming the observations are unit vectors, the calculation is as follows.\n\n    .. code-block:: python\n\n        mean = samples.mean(axis=0)\n        mean_resultant_length = np.linalg.norm(mean)\n        mean_direction = mean / mean_resultant_length\n\n    This definition is appropriate for *directional* data (i.e. vector data\n    for which the magnitude of each observation is irrelevant) but not\n    for *axial* data (i.e. vector data for which the magnitude and *sign* of\n    each observation is irrelevant).\n\n    Several definitions of directional variance involving the mean resultant\n    length ``R`` have been proposed, including ``1 - R`` [1]_, ``1 - R**2``\n    [2]_, and ``2 * (1 - R)`` [2]_. Rather than choosing one, this function\n    returns ``R`` as attribute `mean_resultant_length` so the user can compute\n    their preferred measure of dispersion.\n\n    References\n    ----------\n    .. [1] Mardia, Jupp. (2000). *Directional Statistics*\n       (p. 163). Wiley.\n\n    .. [2] https://en.wikipedia.org/wiki/Directional_statistics\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import directional_stats\n    >>> data = np.array([[3, 4],    # first observation, 2D vector space\n    ...                  [6, -8]])  # second observation\n    >>> dirstats = directional_stats(data)\n    >>> dirstats.mean_direction\n    array([1., 0.])\n\n    In contrast, the regular sample mean of the vectors would be influenced\n    by the magnitude of each observation. Furthermore, the result would not be\n    a unit vector.\n\n    >>> data.mean(axis=0)\n    array([4.5, -2.])\n\n    An exemplary use case for `directional_stats` is to find a *meaningful*\n    center for a set of observations on a sphere, e.g. geographical locations.\n\n    >>> data = np.array([[0.8660254, 0.5, 0.],\n    ...                  [0.8660254, -0.5, 0.]])\n    >>> dirstats = directional_stats(data)\n    >>> dirstats.mean_direction\n    array([1., 0., 0.])\n\n    The regular sample mean on the other hand yields a result which does not\n    lie on the surface of the sphere.\n\n    >>> data.mean(axis=0)\n    array([0.8660254, 0., 0.])\n\n    The function also returns the mean resultant length, which\n    can be used to calculate a directional variance. For example, using the\n    definition ``Var(z) = 1 - R`` from [2]_ where ``R`` is the\n    mean resultant length, we can calculate the directional variance of the\n    vectors in the above example as:\n\n    >>> 1 - dirstats.mean_resultant_length\n    0.13397459716167093\n    ",
    "scipy.stats.dirichlet": "A Dirichlet random variable.\n\n    The ``alpha`` keyword specifies the concentration parameters of the\n    distribution.\n\n    .. versionadded:: 0.15.0\n\n    Methods\n    -------\n    pdf(x, alpha)\n        Probability density function.\n    logpdf(x, alpha)\n        Log of the probability density function.\n    rvs(alpha, size=1, random_state=None)\n        Draw random samples from a Dirichlet distribution.\n    mean(alpha)\n        The mean of the Dirichlet distribution\n    var(alpha)\n        The variance of the Dirichlet distribution\n    cov(alpha)\n        The covariance of the Dirichlet distribution\n    entropy(alpha)\n        Compute the differential entropy of the Dirichlet distribution.\n\n    Parameters\n    ----------\n    alpha : array_like\n        The concentration parameters. The number of entries determines the\n        dimensionality of the distribution.\n    seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n        Used for drawing random variates.\n        If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used, seeded\n        with seed.\n        If `seed` is already a ``RandomState`` or ``Generator`` instance,\n        then that object is used.\n        Default is `None`.\n\n    Notes\n    -----\n    Each :math:`\\alpha` entry must be positive. The distribution has only\n    support on the simplex defined by\n\n    .. math::\n        \\sum_{i=1}^{K} x_i = 1\n\n    where :math:`0 < x_i < 1`.\n\n    If the quantiles don't lie within the simplex, a ValueError is raised.\n\n    The probability density function for `dirichlet` is\n\n    .. math::\n\n        f(x) = \\frac{1}{\\mathrm{B}(\\boldsymbol\\alpha)} \\prod_{i=1}^K x_i^{\\alpha_i - 1}\n\n    where\n\n    .. math::\n\n        \\mathrm{B}(\\boldsymbol\\alpha) = \\frac{\\prod_{i=1}^K \\Gamma(\\alpha_i)}\n                                     {\\Gamma\\bigl(\\sum_{i=1}^K \\alpha_i\\bigr)}\n\n    and :math:`\\boldsymbol\\alpha=(\\alpha_1,\\ldots,\\alpha_K)`, the\n    concentration parameters and :math:`K` is the dimension of the space\n    where :math:`x` takes values.\n\n    Note that the `dirichlet` interface is somewhat inconsistent.\n    The array returned by the rvs function is transposed\n    with respect to the format expected by the pdf and logpdf.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import dirichlet\n\n    Generate a dirichlet random variable\n\n    >>> quantiles = np.array([0.2, 0.2, 0.6])  # specify quantiles\n    >>> alpha = np.array([0.4, 5, 15])  # specify concentration parameters\n    >>> dirichlet.pdf(quantiles, alpha)\n    0.2843831684937255\n\n    The same PDF but following a log scale\n\n    >>> dirichlet.logpdf(quantiles, alpha)\n    -1.2574327653159187\n\n    Once we specify the dirichlet distribution\n    we can then calculate quantities of interest\n\n    >>> dirichlet.mean(alpha)  # get the mean of the distribution\n    array([0.01960784, 0.24509804, 0.73529412])\n    >>> dirichlet.var(alpha) # get variance\n    array([0.00089829, 0.00864603, 0.00909517])\n    >>> dirichlet.entropy(alpha)  # calculate the differential entropy\n    -4.3280162474082715\n\n    We can also return random samples from the distribution\n\n    >>> dirichlet.rvs(alpha, size=1, random_state=1)\n    array([[0.00766178, 0.24670518, 0.74563305]])\n    >>> dirichlet.rvs(alpha, size=2, random_state=2)\n    array([[0.01639427, 0.1292273 , 0.85437844],\n           [0.00156917, 0.19033695, 0.80809388]])\n\n    Alternatively, the object may be called (as a function) to fix\n    concentration parameters, returning a \"frozen\" Dirichlet\n    random variable:\n\n    >>> rv = dirichlet(alpha)\n    >>> # Frozen object with the same methods but holding the given\n    >>> # concentration parameters fixed.\n\n    ",
    "scipy.stats.dirichlet_multinomial": "A Dirichlet multinomial random variable.\n\n    The Dirichlet multinomial distribution is a compound probability\n    distribution: it is the multinomial distribution with number of trials\n    `n` and class probabilities ``p`` randomly sampled from a Dirichlet\n    distribution with concentration parameters ``alpha``.\n\n    Methods\n    -------\n    logpmf(x, alpha, n):\n        Log of the probability mass function.\n    pmf(x, alpha, n):\n        Probability mass function.\n    mean(alpha, n):\n        Mean of the Dirichlet multinomial distribution.\n    var(alpha, n):\n        Variance of the Dirichlet multinomial distribution.\n    cov(alpha, n):\n        The covariance of the Dirichlet multinomial distribution.\n\n    Parameters\n    ----------\n    alpha : array_like\n        The concentration parameters. The number of entries along the last axis\n        determines the dimensionality of the distribution. Each entry must be\n        strictly positive.\n    n : int or array_like\n        The number of trials. Each element must be a strictly positive integer.\n    seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n        Used for drawing random variates.\n        If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used, seeded\n        with seed.\n        If `seed` is already a ``RandomState`` or ``Generator`` instance,\n        then that object is used.\n        Default is `None`.\n\n    See Also\n    --------\n    scipy.stats.dirichlet : The dirichlet distribution.\n    scipy.stats.multinomial : The multinomial distribution.\n\n    References\n    ----------\n    .. [1] Dirichlet-multinomial distribution, Wikipedia,\n           https://www.wikipedia.org/wiki/Dirichlet-multinomial_distribution\n\n    Examples\n    --------\n    >>> from scipy.stats import dirichlet_multinomial\n\n    Get the PMF\n\n    >>> n = 6  # number of trials\n    >>> alpha = [3, 4, 5]  # concentration parameters\n    >>> x = [1, 2, 3]  # counts\n    >>> dirichlet_multinomial.pmf(x, alpha, n)\n    0.08484162895927604\n\n    If the sum of category counts does not equal the number of trials,\n    the probability mass is zero.\n\n    >>> dirichlet_multinomial.pmf(x, alpha, n=7)\n    0.0\n\n    Get the log of the PMF\n\n    >>> dirichlet_multinomial.logpmf(x, alpha, n)\n    -2.4669689491013327\n\n    Get the mean\n\n    >>> dirichlet_multinomial.mean(alpha, n)\n    array([1.5, 2. , 2.5])\n\n    Get the variance\n\n    >>> dirichlet_multinomial.var(alpha, n)\n    array([1.55769231, 1.84615385, 2.01923077])\n\n    Get the covariance\n\n    >>> dirichlet_multinomial.cov(alpha, n)\n    array([[ 1.55769231, -0.69230769, -0.86538462],\n           [-0.69230769,  1.84615385, -1.15384615],\n           [-0.86538462, -1.15384615,  2.01923077]])\n\n    Alternatively, the object may be called (as a function) to fix the\n    `alpha` and `n` parameters, returning a \"frozen\" Dirichlet multinomial\n    random variable.\n\n    >>> dm = dirichlet_multinomial(alpha, n)\n    >>> dm.pmf(x)\n    0.08484162895927579\n\n    All methods are fully vectorized. Each element of `x` and `alpha` is\n    a vector (along the last axis), each element of `n` is an\n    integer (scalar), and the result is computed element-wise.\n\n    >>> x = [[1, 2, 3], [4, 5, 6]]\n    >>> alpha = [[1, 2, 3], [4, 5, 6]]\n    >>> n = [6, 15]\n    >>> dirichlet_multinomial.pmf(x, alpha, n)\n    array([0.06493506, 0.02626937])\n\n    >>> dirichlet_multinomial.cov(alpha, n).shape  # both covariance matrices\n    (2, 3, 3)\n\n    Broadcasting according to standard NumPy conventions is supported. Here,\n    we have four sets of concentration parameters (each a two element vector)\n    for each of three numbers of trials (each a scalar).\n\n    >>> alpha = [[3, 4], [4, 5], [5, 6], [6, 7]]\n    >>> n = [[6], [7], [8]]\n    >>> dirichlet_multinomial.mean(alpha, n).shape\n    (3, 4, 2)\n\n    ",
    "scipy.stats.dlaplace": "A  Laplacian discrete random variable.\n\n    As an instance of the `rv_discrete` class, `dlaplace` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, loc=0, size=1, random_state=None)\n        Random variates.\n    pmf(k, a, loc=0)\n        Probability mass function.\n    logpmf(k, a, loc=0)\n        Log of the probability mass function.\n    cdf(k, a, loc=0)\n        Cumulative distribution function.\n    logcdf(k, a, loc=0)\n        Log of the cumulative distribution function.\n    sf(k, a, loc=0)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(k, a, loc=0)\n        Log of the survival function.\n    ppf(q, a, loc=0)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, loc=0)\n        Inverse survival function (inverse of ``sf``).\n    stats(a, loc=0, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, loc=0)\n        (Differential) entropy of the RV.\n    expect(func, args=(a,), loc=0, lb=None, ub=None, conditional=False)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, loc=0)\n        Median of the distribution.\n    mean(a, loc=0)\n        Mean of the distribution.\n    var(a, loc=0)\n        Variance of the distribution.\n    std(a, loc=0)\n        Standard deviation of the distribution.\n    interval(confidence, a, loc=0)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability mass function for `dlaplace` is:\n\n    .. math::\n\n        f(k) = \\tanh(a/2) \\exp(-a |k|)\n\n    for integers :math:`k` and :math:`a > 0`.\n\n    `dlaplace` takes :math:`a` as shape parameter.\n\n    The probability mass function above is defined in the \"standardized\" form.\n    To shift distribution use the ``loc`` parameter.\n    Specifically, ``dlaplace.pmf(k, a, loc)`` is identically\n    equivalent to ``dlaplace.pmf(k - loc, a)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import dlaplace\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a = 0.8\n    >>> mean, var, skew, kurt = dlaplace.stats(a, moments='mvsk')\n    \n    Display the probability mass function (``pmf``):\n    \n    >>> x = np.arange(dlaplace.ppf(0.01, a),\n    ...               dlaplace.ppf(0.99, a))\n    >>> ax.plot(x, dlaplace.pmf(x, a), 'bo', ms=8, label='dlaplace pmf')\n    >>> ax.vlines(x, 0, dlaplace.pmf(x, a), colors='b', lw=5, alpha=0.5)\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape and location. This returns a \"frozen\" RV object holding\n    the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pmf``:\n    \n    >>> rv = dlaplace(a)\n    >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n    ...         label='frozen pmf')\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> prob = dlaplace.cdf(x, a)\n    >>> np.allclose(x, dlaplace.ppf(prob, a))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = dlaplace.rvs(a, size=1000)\n\n    ",
    "scipy.stats.dunnett": "Dunnett's test: multiple comparisons of means against a control group.\n\n    This is an implementation of Dunnett's original, single-step test as\n    described in [1]_.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : 1D array_like\n        The sample measurements for each experimental group.\n    control : 1D array_like\n        The sample measurements for the control group.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n\n        The null hypothesis is that the means of the distributions underlying\n        the samples and control are equal. The following alternative\n        hypotheses are available (default is 'two-sided'):\n\n        * 'two-sided': the means of the distributions underlying the samples\n          and control are unequal.\n        * 'less': the means of the distributions underlying the samples\n          are less than the mean of the distribution underlying the control.\n        * 'greater': the means of the distributions underlying the\n          samples are greater than the mean of the distribution underlying\n          the control.\n    random_state : {None, int, `numpy.random.Generator`}, optional\n        If `random_state` is an int or None, a new `numpy.random.Generator` is\n        created using ``np.random.default_rng(random_state)``.\n        If `random_state` is already a ``Generator`` instance, then the\n        provided instance is used.\n\n        The random number generator is used to control the randomized\n        Quasi-Monte Carlo integration of the multivariate-t distribution.\n\n    Returns\n    -------\n    res : `~scipy.stats._result_classes.DunnettResult`\n        An object containing attributes:\n\n        statistic : float ndarray\n            The computed statistic of the test for each comparison. The element\n            at index ``i`` is the statistic for the comparison between\n            groups ``i`` and the control.\n        pvalue : float ndarray\n            The computed p-value of the test for each comparison. The element\n            at index ``i`` is the p-value for the comparison between\n            group ``i`` and the control.\n\n        And the following method:\n\n        confidence_interval(confidence_level=0.95) :\n            Compute the difference in means of the groups\n            with the control +- the allowance.\n\n    See Also\n    --------\n    tukey_hsd : performs pairwise comparison of means.\n\n    Notes\n    -----\n    Like the independent-sample t-test, Dunnett's test [1]_ is used to make\n    inferences about the means of distributions from which samples were drawn.\n    However, when multiple t-tests are performed at a fixed significance level,\n    the \"family-wise error rate\" - the probability of incorrectly rejecting the\n    null hypothesis in at least one test - will exceed the significance level.\n    Dunnett's test is designed to perform multiple comparisons while\n    controlling the family-wise error rate.\n\n    Dunnett's test compares the means of multiple experimental groups\n    against a single control group. Tukey's Honestly Significant Difference Test\n    is another multiple-comparison test that controls the family-wise error\n    rate, but `tukey_hsd` performs *all* pairwise comparisons between groups.\n    When pairwise comparisons between experimental groups are not needed,\n    Dunnett's test is preferable due to its higher power.\n\n\n    The use of this test relies on several assumptions.\n\n    1. The observations are independent within and among groups.\n    2. The observations within each group are normally distributed.\n    3. The distributions from which the samples are drawn have the same finite\n       variance.\n\n    References\n    ----------\n    .. [1] Charles W. Dunnett. \"A Multiple Comparison Procedure for Comparing\n       Several Treatments with a Control.\"\n       Journal of the American Statistical Association, 50:272, 1096-1121,\n       :doi:`10.1080/01621459.1955.10501294`, 1955.\n\n    Examples\n    --------\n    In [1]_, the influence of drugs on blood count measurements on three groups\n    of animal is investigated.\n\n    The following table summarizes the results of the experiment in which\n    two groups received different drugs, and one group acted as a control.\n    Blood counts (in millions of cells per cubic millimeter) were recorded::\n\n    >>> import numpy as np\n    >>> control = np.array([7.40, 8.50, 7.20, 8.24, 9.84, 8.32])\n    >>> drug_a = np.array([9.76, 8.80, 7.68, 9.36])\n    >>> drug_b = np.array([12.80, 9.68, 12.16, 9.20, 10.55])\n\n    We would like to see if the means between any of the groups are\n    significantly different. First, visually examine a box and whisker plot.\n\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    >>> ax.boxplot([control, drug_a, drug_b])\n    >>> ax.set_xticklabels([\"Control\", \"Drug A\", \"Drug B\"])  # doctest: +SKIP\n    >>> ax.set_ylabel(\"mean\")  # doctest: +SKIP\n    >>> plt.show()\n\n    Note the overlapping interquartile ranges of the drug A group and control\n    group and the apparent separation between the drug B group and control\n    group.\n\n    Next, we will use Dunnett's test to assess whether the difference\n    between group means is significant while controlling the family-wise error\n    rate: the probability of making any false discoveries.\n    Let the null hypothesis be that the experimental groups have the same\n    mean as the control and the alternative be that an experimental group does\n    not have the same mean as the control. We will consider a 5% family-wise\n    error rate to be acceptable, and therefore we choose 0.05 as the threshold\n    for significance.\n\n    >>> from scipy.stats import dunnett\n    >>> res = dunnett(drug_a, drug_b, control=control)\n    >>> res.pvalue\n    array([0.62004941, 0.0059035 ])  # may vary\n\n    The p-value corresponding with the comparison between group A and control\n    exceeds 0.05, so we do not reject the null hypothesis for that comparison.\n    However, the p-value corresponding with the comparison between group B\n    and control is less than 0.05, so we consider the experimental results\n    to be evidence against the null hypothesis in favor of the alternative:\n    group B has a different mean than the control group.\n\n    ",
    "scipy.stats.dweibull": "A double Weibull continuous random variable.\n\n    As an instance of the `rv_continuous` class, `dweibull` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `dweibull` is given by\n\n    .. math::\n\n        f(x, c) = c / 2 |x|^{c-1} \\exp(-|x|^c)\n\n    for a real number :math:`x` and :math:`c > 0`.\n\n    `dweibull` takes ``c`` as a shape parameter for :math:`c`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``dweibull.pdf(x, c, loc, scale)`` is identically\n    equivalent to ``dweibull.pdf(y, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import dweibull\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c = 2.07\n    >>> mean, var, skew, kurt = dweibull.stats(c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(dweibull.ppf(0.01, c),\n    ...                 dweibull.ppf(0.99, c), 100)\n    >>> ax.plot(x, dweibull.pdf(x, c),\n    ...        'r-', lw=5, alpha=0.6, label='dweibull pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = dweibull(c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = dweibull.ppf([0.001, 0.5, 0.999], c)\n    >>> np.allclose([0.001, 0.5, 0.999], dweibull.cdf(vals, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = dweibull.rvs(c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.ecdf": "Empirical cumulative distribution function of a sample.\n\n    The empirical cumulative distribution function (ECDF) is a step function\n    estimate of the CDF of the distribution underlying a sample. This function\n    returns objects representing both the empirical distribution function and\n    its complement, the empirical survival function.\n\n    Parameters\n    ----------\n    sample : 1D array_like or `scipy.stats.CensoredData`\n        Besides array_like, instances of `scipy.stats.CensoredData` containing\n        uncensored and right-censored observations are supported. Currently,\n        other instances of `scipy.stats.CensoredData` will result in a\n        ``NotImplementedError``.\n\n    Returns\n    -------\n    res : `~scipy.stats._result_classes.ECDFResult`\n        An object with the following attributes.\n\n        cdf : `~scipy.stats._result_classes.EmpiricalDistributionFunction`\n            An object representing the empirical cumulative distribution\n            function.\n        sf : `~scipy.stats._result_classes.EmpiricalDistributionFunction`\n            An object representing the empirical survival function.\n\n        The `cdf` and `sf` attributes themselves have the following attributes.\n\n        quantiles : ndarray\n            The unique values in the sample that defines the empirical CDF/SF.\n        probabilities : ndarray\n            The point estimates of the probabilities corresponding with\n            `quantiles`.\n\n        And the following methods:\n\n        evaluate(x) :\n            Evaluate the CDF/SF at the argument.\n\n        plot(ax) :\n            Plot the CDF/SF on the provided axes.\n\n        confidence_interval(confidence_level=0.95) :\n            Compute the confidence interval around the CDF/SF at the values in\n            `quantiles`.\n\n    Notes\n    -----\n    When each observation of the sample is a precise measurement, the ECDF\n    steps up by ``1/len(sample)`` at each of the observations [1]_.\n\n    When observations are lower bounds, upper bounds, or both upper and lower\n    bounds, the data is said to be \"censored\", and `sample` may be provided as\n    an instance of `scipy.stats.CensoredData`.\n\n    For right-censored data, the ECDF is given by the Kaplan-Meier estimator\n    [2]_; other forms of censoring are not supported at this time.\n\n    Confidence intervals are computed according to the Greenwood formula or the\n    more recent \"Exponential Greenwood\" formula as described in [4]_.\n\n    References\n    ----------\n    .. [1] Conover, William Jay. Practical nonparametric statistics. Vol. 350.\n           John Wiley & Sons, 1999.\n\n    .. [2] Kaplan, Edward L., and Paul Meier. \"Nonparametric estimation from\n           incomplete observations.\" Journal of the American statistical\n           association 53.282 (1958): 457-481.\n\n    .. [3] Goel, Manish Kumar, Pardeep Khanna, and Jugal Kishore.\n           \"Understanding survival analysis: Kaplan-Meier estimate.\"\n           International journal of Ayurveda research 1.4 (2010): 274.\n\n    .. [4] Sawyer, Stanley. \"The Greenwood and Exponential Greenwood Confidence\n           Intervals in Survival Analysis.\"\n           https://www.math.wustl.edu/~sawyer/handouts/greenwood.pdf\n\n    Examples\n    --------\n    **Uncensored Data**\n\n    As in the example from [1]_ page 79, five boys were selected at random from\n    those in a single high school. Their one-mile run times were recorded as\n    follows.\n\n    >>> sample = [6.23, 5.58, 7.06, 6.42, 5.20]  # one-mile run times (minutes)\n\n    The empirical distribution function, which approximates the distribution\n    function of one-mile run times of the population from which the boys were\n    sampled, is calculated as follows.\n\n    >>> from scipy import stats\n    >>> res = stats.ecdf(sample)\n    >>> res.cdf.quantiles\n    array([5.2 , 5.58, 6.23, 6.42, 7.06])\n    >>> res.cdf.probabilities\n    array([0.2, 0.4, 0.6, 0.8, 1. ])\n\n    To plot the result as a step function:\n\n    >>> import matplotlib.pyplot as plt\n    >>> ax = plt.subplot()\n    >>> res.cdf.plot(ax)\n    >>> ax.set_xlabel('One-Mile Run Time (minutes)')\n    >>> ax.set_ylabel('Empirical CDF')\n    >>> plt.show()\n\n    **Right-censored Data**\n\n    As in the example from [1]_ page 91, the lives of ten car fanbelts were\n    tested. Five tests concluded because the fanbelt being tested broke, but\n    the remaining tests concluded for other reasons (e.g. the study ran out of\n    funding, but the fanbelt was still functional). The mileage driven\n    with the fanbelts were recorded as follows.\n\n    >>> broken = [77, 47, 81, 56, 80]  # in thousands of miles driven\n    >>> unbroken = [62, 60, 43, 71, 37]\n\n    Precise survival times of the fanbelts that were still functional at the\n    end of the tests are unknown, but they are known to exceed the values\n    recorded in ``unbroken``. Therefore, these observations are said to be\n    \"right-censored\", and the data is represented using\n    `scipy.stats.CensoredData`.\n\n    >>> sample = stats.CensoredData(uncensored=broken, right=unbroken)\n\n    The empirical survival function is calculated as follows.\n\n    >>> res = stats.ecdf(sample)\n    >>> res.sf.quantiles\n    array([37., 43., 47., 56., 60., 62., 71., 77., 80., 81.])\n    >>> res.sf.probabilities\n    array([1.   , 1.   , 0.875, 0.75 , 0.75 , 0.75 , 0.75 , 0.5  , 0.25 , 0.   ])\n\n    To plot the result as a step function:\n\n    >>> ax = plt.subplot()\n    >>> res.cdf.plot(ax)\n    >>> ax.set_xlabel('Fanbelt Survival Time (thousands of miles)')\n    >>> ax.set_ylabel('Empirical SF')\n    >>> plt.show()\n\n    ",
    "scipy.stats.energy_distance": "Compute the energy distance between two 1D distributions.\n\n    .. versionadded:: 1.0.0\n\n    Parameters\n    ----------\n    u_values, v_values : array_like\n        Values observed in the (empirical) distribution.\n    u_weights, v_weights : array_like, optional\n        Weight for each value. If unspecified, each value is assigned the same\n        weight.\n        `u_weights` (resp. `v_weights`) must have the same length as\n        `u_values` (resp. `v_values`). If the weight sum differs from 1, it\n        must still be positive and finite so that the weights can be normalized\n        to sum to 1.\n\n    Returns\n    -------\n    distance : float\n        The computed distance between the distributions.\n\n    Notes\n    -----\n    The energy distance between two distributions :math:`u` and :math:`v`, whose\n    respective CDFs are :math:`U` and :math:`V`, equals to:\n\n    .. math::\n\n        D(u, v) = \\left( 2\\mathbb E|X - Y| - \\mathbb E|X - X'| -\n        \\mathbb E|Y - Y'| \\right)^{1/2}\n\n    where :math:`X` and :math:`X'` (resp. :math:`Y` and :math:`Y'`) are\n    independent random variables whose probability distribution is :math:`u`\n    (resp. :math:`v`).\n\n    Sometimes the square of this quantity is referred to as the \"energy\n    distance\" (e.g. in [2]_, [4]_), but as noted in [1]_ and [3]_, only the\n    definition above satisfies the axioms of a distance function (metric).\n\n    As shown in [2]_, for one-dimensional real-valued variables, the energy\n    distance is linked to the non-distribution-free version of the Cram\u00e9r-von\n    Mises distance:\n\n    .. math::\n\n        D(u, v) = \\sqrt{2} l_2(u, v) = \\left( 2 \\int_{-\\infty}^{+\\infty} (U-V)^2\n        \\right)^{1/2}\n\n    Note that the common Cram\u00e9r-von Mises criterion uses the distribution-free\n    version of the distance. See [2]_ (section 2), for more details about both\n    versions of the distance.\n\n    The input distributions can be empirical, therefore coming from samples\n    whose values are effectively inputs of the function, or they can be seen as\n    generalized functions, in which case they are weighted sums of Dirac delta\n    functions located at the specified values.\n\n    References\n    ----------\n    .. [1] Rizzo, Szekely \"Energy distance.\" Wiley Interdisciplinary Reviews:\n           Computational Statistics, 8(1):27-38 (2015).\n    .. [2] Szekely \"E-statistics: The energy of statistical samples.\" Bowling\n           Green State University, Department of Mathematics and Statistics,\n           Technical Report 02-16 (2002).\n    .. [3] \"Energy distance\", https://en.wikipedia.org/wiki/Energy_distance\n    .. [4] Bellemare, Danihelka, Dabney, Mohamed, Lakshminarayanan, Hoyer,\n           Munos \"The Cramer Distance as a Solution to Biased Wasserstein\n           Gradients\" (2017). :arXiv:`1705.10743`.\n\n    Examples\n    --------\n    >>> from scipy.stats import energy_distance\n    >>> energy_distance([0], [2])\n    2.0000000000000004\n    >>> energy_distance([0, 8], [0, 8], [3, 1], [2, 2])\n    1.0000000000000002\n    >>> energy_distance([0.7, 7.4, 2.4, 6.8], [1.4, 8. ],\n    ...                 [2.1, 4.2, 7.4, 8. ], [7.6, 8.8])\n    0.88003340976158217\n\n    ",
    "scipy.stats.entropy": "    \n\n\nCalculate the Shannon entropy/relative entropy of given distribution(s).\n\nIf only probabilities `pk` are given, the Shannon entropy is calculated as\n``H = -sum(pk * log(pk))``.\n\nIf `qk` is not None, then compute the relative entropy\n``D = sum(pk * log(pk / qk))``. This quantity is also known\nas the Kullback-Leibler divergence.\n\nThis routine will normalize `pk` and `qk` if they don't sum to 1.\n\nParameters\n----------\npk : array_like\n    Defines the (discrete) distribution. Along each axis-slice of ``pk``,\n    element ``i`` is the  (possibly unnormalized) probability of event\n    ``i``.\nqk : array_like, optional\n    Sequence against which the relative entropy is computed. Should be in\n    the same format as `pk`.\nbase : float, optional\n    The logarithmic base to use, defaults to ``e`` (natural logarithm).\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nS : {float, array_like}\n    The calculated entropy.\n\nNotes\n-----\nInformally, the Shannon entropy quantifies the expected uncertainty\ninherent in the possible outcomes of a discrete random variable.\nFor example,\nif messages consisting of sequences of symbols from a set are to be\nencoded and transmitted over a noiseless channel, then the Shannon entropy\n``H(pk)`` gives a tight lower bound for the average number of units of\ninformation needed per symbol if the symbols occur with frequencies\ngoverned by the discrete distribution `pk` [1]_. The choice of base\ndetermines the choice of units; e.g., ``e`` for nats, ``2`` for bits, etc.\n\nThe relative entropy, ``D(pk|qk)``, quantifies the increase in the average\nnumber of units of information needed per symbol if the encoding is\noptimized for the probability distribution `qk` instead of the true\ndistribution `pk`. Informally, the relative entropy quantifies the expected\nexcess in surprise experienced if one believes the true distribution is\n`qk` when it is actually `pk`.\n\nA related quantity, the cross entropy ``CE(pk, qk)``, satisfies the\nequation ``CE(pk, qk) = H(pk) + D(pk|qk)`` and can also be calculated with\nthe formula ``CE = -sum(pk * log(qk))``. It gives the average\nnumber of units of information needed per symbol if an encoding is\noptimized for the probability distribution `qk` when the true distribution\nis `pk`. It is not computed directly by `entropy`, but it can be computed\nusing two calls to the function (see Examples).\n\nSee [2]_ for more information.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] Shannon, C.E. (1948), A Mathematical Theory of Communication.\n       Bell System Technical Journal, 27: 379-423.\n       https://doi.org/10.1002/j.1538-7305.1948.tb01338.x\n.. [2] Thomas M. Cover and Joy A. Thomas. 2006. Elements of Information\n       Theory (Wiley Series in Telecommunications and Signal Processing).\n       Wiley-Interscience, USA.\n\nExamples\n--------\nThe outcome of a fair coin is the most uncertain:\n\n>>> import numpy as np\n>>> from scipy.stats import entropy\n>>> base = 2  # work in units of bits\n>>> pk = np.array([1/2, 1/2])  # fair coin\n>>> H = entropy(pk, base=base)\n>>> H\n1.0\n>>> H == -np.sum(pk * np.log(pk)) / np.log(base)\nTrue\n\nThe outcome of a biased coin is less uncertain:\n\n>>> qk = np.array([9/10, 1/10])  # biased coin\n>>> entropy(qk, base=base)\n0.46899559358928117\n\nThe relative entropy between the fair coin and biased coin is calculated\nas:\n\n>>> D = entropy(pk, qk, base=base)\n>>> D\n0.7369655941662062\n>>> D == np.sum(pk * np.log(pk/qk)) / np.log(base)\nTrue\n\nThe cross entropy can be calculated as the sum of the entropy and\nrelative entropy`:\n\n>>> CE = entropy(pk, base=base) + entropy(pk, qk, base=base)\n>>> CE\n1.736965594166206\n>>> CE == -np.sum(pk * np.log(qk)) / np.log(base)\nTrue\n",
    "scipy.stats.epps_singleton_2samp": "    \n\n\nCompute the Epps-Singleton (ES) test statistic.\n\nTest the null hypothesis that two samples have the same underlying\nprobability distribution.\n\nParameters\n----------\nx, y : array-like\n    The two samples of observations to be tested. Input must not have more\n    than one dimension. Samples can have different lengths, but both\n    must have at least five observations.\nt : array-like, optional\n    The points (t1, ..., tn) where the empirical characteristic function is\n    to be evaluated. It should be positive distinct numbers. The default\n    value (0.4, 0.8) is proposed in [1]_. Input must not have more than\n    one dimension.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nstatistic : float\n    The test statistic.\npvalue : float\n    The associated p-value based on the asymptotic chi2-distribution.\n\nSee Also\n--------\n\n:func:`ks_2samp`, :func:`anderson_ksamp`\n    ..\n\nNotes\n-----\nTesting whether two samples are generated by the same underlying\ndistribution is a classical question in statistics. A widely used test is\nthe Kolmogorov-Smirnov (KS) test which relies on the empirical\ndistribution function. Epps and Singleton introduce a test based on the\nempirical characteristic function in [1]_.\n\nOne advantage of the ES test compared to the KS test is that is does\nnot assume a continuous distribution. In [1]_, the authors conclude\nthat the test also has a higher power than the KS test in many\nexamples. They recommend the use of the ES test for discrete samples as\nwell as continuous samples with at least 25 observations each, whereas\n`anderson_ksamp` is recommended for smaller sample sizes in the\ncontinuous case.\n\nThe p-value is computed from the asymptotic distribution of the test\nstatistic which follows a `chi2` distribution. If the sample size of both\n`x` and `y` is below 25, the small sample correction proposed in [1]_ is\napplied to the test statistic.\n\nThe default values of `t` are determined in [1]_ by considering\nvarious distributions and finding good values that lead to a high power\nof the test in general. Table III in [1]_ gives the optimal values for\nthe distributions tested in that study. The values of `t` are scaled by\nthe semi-interquartile range in the implementation, see [1]_.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] T. W. Epps and K. J. Singleton, \"An omnibus test for the two-sample\n   problem using the empirical characteristic function\", Journal of\n   Statistical Computation and Simulation 26, p. 177--203, 1986.\n\n.. [2] S. J. Goerg and J. Kaiser, \"Nonparametric testing of distributions\n   - the Epps-Singleton two-sample test using the empirical characteristic\n   function\", The Stata Journal 9(3), p. 454--465, 2009.\n",
    "scipy.stats.erlang": "An Erlang continuous random variable.\n\n    As an instance of the `rv_continuous` class, `erlang` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, a, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, a, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, a, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, a, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, a, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, a, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, a, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, a, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(a, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, loc=0, scale=1)\n        Median of the distribution.\n    mean(a, loc=0, scale=1)\n        Mean of the distribution.\n    var(a, loc=0, scale=1)\n        Variance of the distribution.\n    std(a, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, a, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    gamma\n\n    Notes\n    -----\n    The Erlang distribution is a special case of the Gamma distribution, with\n    the shape parameter `a` an integer.  Note that this restriction is not\n    enforced by `erlang`. It will, however, generate a warning the first time\n    a non-integer value is used for the shape parameter.\n\n    Refer to `gamma` for examples.\n\n    ",
    "scipy.stats.expectile": "Compute the expectile at the specified level.\n\n    Expectiles are a generalization of the expectation in the same way as\n    quantiles are a generalization of the median. The expectile at level\n    `alpha = 0.5` is the mean (average). See Notes for more details.\n\n    Parameters\n    ----------\n    a : array_like\n        Array containing numbers whose expectile is desired.\n    alpha : float, default: 0.5\n        The level of the expectile; e.g., `alpha=0.5` gives the mean.\n    weights : array_like, optional\n        An array of weights associated with the values in `a`.\n        The `weights` must be broadcastable to the same shape as `a`.\n        Default is None, which gives each value a weight of 1.0.\n        An integer valued weight element acts like repeating the corresponding\n        observation in `a` that many times. See Notes for more details.\n\n    Returns\n    -------\n    expectile : ndarray\n        The empirical expectile at level `alpha`.\n\n    See Also\n    --------\n    numpy.mean : Arithmetic average\n    numpy.quantile : Quantile\n\n    Notes\n    -----\n    In general, the expectile at level :math:`\\alpha` of a random variable\n    :math:`X` with cumulative distribution function (CDF) :math:`F` is given\n    by the unique solution :math:`t` of:\n\n    .. math::\n\n        \\alpha E((X - t)_+) = (1 - \\alpha) E((t - X)_+) \\,.\n\n    Here, :math:`(x)_+ = \\max(0, x)` is the positive part of :math:`x`.\n    This equation can be equivalently written as:\n\n    .. math::\n\n        \\alpha \\int_t^\\infty (x - t)\\mathrm{d}F(x)\n        = (1 - \\alpha) \\int_{-\\infty}^t (t - x)\\mathrm{d}F(x) \\,.\n\n    The empirical expectile at level :math:`\\alpha` (`alpha`) of a sample\n    :math:`a_i` (the array `a`) is defined by plugging in the empirical CDF of\n    `a`. Given sample or case weights :math:`w` (the array `weights`), it\n    reads :math:`F_a(x) = \\frac{1}{\\sum_i w_i} \\sum_i w_i 1_{a_i \\leq x}`\n    with indicator function :math:`1_{A}`. This leads to the definition of the\n    empirical expectile at level `alpha` as the unique solution :math:`t` of:\n\n    .. math::\n\n        \\alpha \\sum_{i=1}^n w_i (a_i - t)_+ =\n            (1 - \\alpha) \\sum_{i=1}^n w_i (t - a_i)_+ \\,.\n\n    For :math:`\\alpha=0.5`, this simplifies to the weighted average.\n    Furthermore, the larger :math:`\\alpha`, the larger the value of the\n    expectile.\n\n    As a final remark, the expectile at level :math:`\\alpha` can also be\n    written as a minimization problem. One often used choice is\n\n    .. math::\n\n        \\operatorname{argmin}_t\n        E(\\lvert 1_{t\\geq X} - \\alpha\\rvert(t - X)^2) \\,.\n\n    References\n    ----------\n    .. [1] W. K. Newey and J. L. Powell (1987), \"Asymmetric Least Squares\n           Estimation and Testing,\" Econometrica, 55, 819-847.\n    .. [2] T. Gneiting (2009). \"Making and Evaluating Point Forecasts,\"\n           Journal of the American Statistical Association, 106, 746 - 762.\n           :doi:`10.48550/arXiv.0912.0902`\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import expectile\n    >>> a = [1, 4, 2, -1]\n    >>> expectile(a, alpha=0.5) == np.mean(a)\n    True\n    >>> expectile(a, alpha=0.2)\n    0.42857142857142855\n    >>> expectile(a, alpha=0.8)\n    2.5714285714285716\n    >>> weights = [1, 3, 1, 1]\n\n    ",
    "scipy.stats.expon": "An exponential continuous random variable.\n\n    As an instance of the `rv_continuous` class, `expon` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `expon` is:\n\n    .. math::\n\n        f(x) = \\exp(-x)\n\n    for :math:`x \\ge 0`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``expon.pdf(x, loc, scale)`` is identically\n    equivalent to ``expon.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    A common parameterization for `expon` is in terms of the rate parameter\n    ``lambda``, such that ``pdf = lambda * exp(-lambda * x)``. This\n    parameterization corresponds to using ``scale = 1 / lambda``.\n\n    The exponential distribution is a special case of the gamma\n    distributions, with gamma shape parameter ``a = 1``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import expon\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    \n    >>> mean, var, skew, kurt = expon.stats(moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(expon.ppf(0.01),\n    ...                 expon.ppf(0.99), 100)\n    >>> ax.plot(x, expon.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='expon pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = expon()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = expon.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], expon.cdf(vals))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = expon.rvs(size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.exponnorm": "An exponentially modified Normal continuous random variable.\n\n    Also known as the exponentially modified Gaussian distribution [1]_.\n\n    As an instance of the `rv_continuous` class, `exponnorm` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(K, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, K, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, K, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, K, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, K, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, K, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, K, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, K, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, K, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, K, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(K, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(K, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(K,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(K, loc=0, scale=1)\n        Median of the distribution.\n    mean(K, loc=0, scale=1)\n        Mean of the distribution.\n    var(K, loc=0, scale=1)\n        Variance of the distribution.\n    std(K, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, K, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `exponnorm` is:\n\n    .. math::\n\n        f(x, K) = \\frac{1}{2K} \\exp\\left(\\frac{1}{2 K^2} - x / K \\right)\n                  \\text{erfc}\\left(-\\frac{x - 1/K}{\\sqrt{2}}\\right)\n\n    where :math:`x` is a real number and :math:`K > 0`.\n\n    It can be thought of as the sum of a standard normal random variable\n    and an independent exponentially distributed random variable with rate\n    ``1/K``.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``exponnorm.pdf(x, K, loc, scale)`` is identically\n    equivalent to ``exponnorm.pdf(y, K) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    An alternative parameterization of this distribution (for example, in\n    the Wikipedia article [1]_) involves three parameters, :math:`\\mu`,\n    :math:`\\lambda` and :math:`\\sigma`.\n\n    In the present parameterization this corresponds to having ``loc`` and\n    ``scale`` equal to :math:`\\mu` and :math:`\\sigma`, respectively, and\n    shape parameter :math:`K = 1/(\\sigma\\lambda)`.\n\n    .. versionadded:: 0.16.0\n\n    References\n    ----------\n    .. [1] Exponentially modified Gaussian distribution, Wikipedia,\n           https://en.wikipedia.org/wiki/Exponentially_modified_Gaussian_distribution\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import exponnorm\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> K = 1.5\n    >>> mean, var, skew, kurt = exponnorm.stats(K, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(exponnorm.ppf(0.01, K),\n    ...                 exponnorm.ppf(0.99, K), 100)\n    >>> ax.plot(x, exponnorm.pdf(x, K),\n    ...        'r-', lw=5, alpha=0.6, label='exponnorm pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = exponnorm(K)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = exponnorm.ppf([0.001, 0.5, 0.999], K)\n    >>> np.allclose([0.001, 0.5, 0.999], exponnorm.cdf(vals, K))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = exponnorm.rvs(K, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.exponpow": "An exponential power continuous random variable.\n\n    As an instance of the `rv_continuous` class, `exponpow` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(b, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, b, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, b, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, b, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, b, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, b, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, b, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, b, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, b, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, b, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(b, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(b, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(b,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(b, loc=0, scale=1)\n        Median of the distribution.\n    mean(b, loc=0, scale=1)\n        Mean of the distribution.\n    var(b, loc=0, scale=1)\n        Variance of the distribution.\n    std(b, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, b, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `exponpow` is:\n\n    .. math::\n\n        f(x, b) = b x^{b-1} \\exp(1 + x^b - \\exp(x^b))\n\n    for :math:`x \\ge 0`, :math:`b > 0`.  Note that this is a different\n    distribution from the exponential power distribution that is also known\n    under the names \"generalized normal\" or \"generalized Gaussian\".\n\n    `exponpow` takes ``b`` as a shape parameter for :math:`b`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``exponpow.pdf(x, b, loc, scale)`` is identically\n    equivalent to ``exponpow.pdf(y, b) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    http://www.math.wm.edu/~leemis/chart/UDR/PDFs/Exponentialpower.pdf\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import exponpow\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> b = 2.7\n    >>> mean, var, skew, kurt = exponpow.stats(b, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(exponpow.ppf(0.01, b),\n    ...                 exponpow.ppf(0.99, b), 100)\n    >>> ax.plot(x, exponpow.pdf(x, b),\n    ...        'r-', lw=5, alpha=0.6, label='exponpow pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = exponpow(b)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = exponpow.ppf([0.001, 0.5, 0.999], b)\n    >>> np.allclose([0.001, 0.5, 0.999], exponpow.cdf(vals, b))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = exponpow.rvs(b, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.exponweib": "An exponentiated Weibull continuous random variable.\n\n    As an instance of the `rv_continuous` class, `exponweib` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, a, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, a, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, a, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, a, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, a, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, a, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, a, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, a, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(a, c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(a, c), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, c, loc=0, scale=1)\n        Median of the distribution.\n    mean(a, c, loc=0, scale=1)\n        Mean of the distribution.\n    var(a, c, loc=0, scale=1)\n        Variance of the distribution.\n    std(a, c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, a, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    weibull_min, numpy.random.Generator.weibull\n\n    Notes\n    -----\n    The probability density function for `exponweib` is:\n\n    .. math::\n\n        f(x, a, c) = a c [1-\\exp(-x^c)]^{a-1} \\exp(-x^c) x^{c-1}\n\n    and its cumulative distribution function is:\n\n    .. math::\n\n        F(x, a, c) = [1-\\exp(-x^c)]^a\n\n    for :math:`x > 0`, :math:`a > 0`, :math:`c > 0`.\n\n    `exponweib` takes :math:`a` and :math:`c` as shape parameters:\n\n    * :math:`a` is the exponentiation parameter,\n      with the special case :math:`a=1` corresponding to the\n      (non-exponentiated) Weibull distribution `weibull_min`.\n    * :math:`c` is the shape parameter of the non-exponentiated Weibull law.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``exponweib.pdf(x, a, c, loc, scale)`` is identically\n    equivalent to ``exponweib.pdf(y, a, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    https://en.wikipedia.org/wiki/Exponentiated_Weibull_distribution\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import exponweib\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a, c = 2.89, 1.95\n    >>> mean, var, skew, kurt = exponweib.stats(a, c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(exponweib.ppf(0.01, a, c),\n    ...                 exponweib.ppf(0.99, a, c), 100)\n    >>> ax.plot(x, exponweib.pdf(x, a, c),\n    ...        'r-', lw=5, alpha=0.6, label='exponweib pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = exponweib(a, c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = exponweib.ppf([0.001, 0.5, 0.999], a, c)\n    >>> np.allclose([0.001, 0.5, 0.999], exponweib.cdf(vals, a, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = exponweib.rvs(a, c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.f": "An F continuous random variable.\n\n    For the noncentral F distribution, see `ncf`.\n\n    As an instance of the `rv_continuous` class, `f` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(dfn, dfd, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, dfn, dfd, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, dfn, dfd, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, dfn, dfd, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, dfn, dfd, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, dfn, dfd, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, dfn, dfd, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, dfn, dfd, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, dfn, dfd, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, dfn, dfd, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(dfn, dfd, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(dfn, dfd, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(dfn, dfd), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(dfn, dfd, loc=0, scale=1)\n        Median of the distribution.\n    mean(dfn, dfd, loc=0, scale=1)\n        Mean of the distribution.\n    var(dfn, dfd, loc=0, scale=1)\n        Variance of the distribution.\n    std(dfn, dfd, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, dfn, dfd, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    ncf\n\n    Notes\n    -----\n    The F distribution with :math:`df_1 > 0` and :math:`df_2 > 0` degrees of freedom is\n    the distribution of the ratio of two independent chi-squared distributions with\n    :math:`df_1` and :math:`df_2` degrees of freedom, after rescaling by\n    :math:`df_2 / df_1`.\n\n    The probability density function for `f` is:\n\n    .. math::\n\n        f(x, df_1, df_2) = \\frac{df_2^{df_2/2} df_1^{df_1/2} x^{df_1 / 2-1}}\n                                {(df_2+df_1 x)^{(df_1+df_2)/2}\n                                 B(df_1/2, df_2/2)}\n\n    for :math:`x > 0`.\n\n    `f` accepts shape parameters ``dfn`` and ``dfd`` for :math:`df_1`, the degrees of\n    freedom of the chi-squared distribution in the numerator, and :math:`df_2`, the\n    degrees of freedom of the chi-squared distribution in the denominator, respectively.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``f.pdf(x, dfn, dfd, loc, scale)`` is identically\n    equivalent to ``f.pdf(y, dfn, dfd) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import f\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> dfn, dfd = 29, 18\n    >>> mean, var, skew, kurt = f.stats(dfn, dfd, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(f.ppf(0.01, dfn, dfd),\n    ...                 f.ppf(0.99, dfn, dfd), 100)\n    >>> ax.plot(x, f.pdf(x, dfn, dfd),\n    ...        'r-', lw=5, alpha=0.6, label='f pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = f(dfn, dfd)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = f.ppf([0.001, 0.5, 0.999], dfn, dfd)\n    >>> np.allclose([0.001, 0.5, 0.999], f.cdf(vals, dfn, dfd))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = f.rvs(dfn, dfd, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.f_oneway": "    \n\n\nPerform one-way ANOVA.\n\nThe one-way ANOVA tests the null hypothesis that two or more groups have\nthe same population mean.  The test is applied to samples from two or\nmore groups, possibly with differing sizes.\n\nParameters\n----------\nsample1, sample2, ... : array_like\n    The sample measurements for each group.  There must be at least\n    two arguments.  If the arrays are multidimensional, then all the\n    dimensions of the array must be the same except for `axis`.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nstatistic : float\n    The computed F statistic of the test.\npvalue : float\n    The associated p-value from the F distribution.\n\nWarns\n-----\n`~scipy.stats.ConstantInputWarning`\n    Emitted if all values within each of the input arrays are identical.\n    In this case the F statistic is either infinite or isn't defined,\n    so ``np.inf`` or ``np.nan`` is returned.\nRuntimeWarning\n    Emitted if the length of any input array is 0, or if all the input\n    arrays have length 1.  ``np.nan`` is returned for the F statistic\n    and the p-value in these cases.\n\nNotes\n-----\nThe ANOVA test has important assumptions that must be satisfied in order\nfor the associated p-value to be valid.\n\n1. The samples are independent.\n2. Each sample is from a normally distributed population.\n3. The population standard deviations of the groups are all equal.  This\n   property is known as homoscedasticity.\n\nIf these assumptions are not true for a given set of data, it may still\nbe possible to use the Kruskal-Wallis H-test (`scipy.stats.kruskal`) or\nthe Alexander-Govern test (`scipy.stats.alexandergovern`) although with\nsome loss of power.\n\nThe length of each group must be at least one, and there must be at\nleast one group with length greater than one.  If these conditions\nare not satisfied, a warning is generated and (``np.nan``, ``np.nan``)\nis returned.\n\nIf all values in each group are identical, and there exist at least two\ngroups with different values, the function generates a warning and\nreturns (``np.inf``, 0).\n\nIf all values in all groups are the same, function generates a warning\nand returns (``np.nan``, ``np.nan``).\n\nThe algorithm is from Heiman [2]_, pp.394-7.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] R. Lowry, \"Concepts and Applications of Inferential Statistics\",\n       Chapter 14, 2014, http://vassarstats.net/textbook/\n\n.. [2] G.W. Heiman, \"Understanding research methods and statistics: An\n       integrated introduction for psychology\", Houghton, Mifflin and\n       Company, 2001.\n\n.. [3] G.H. McDonald, \"Handbook of Biological Statistics\", One-way ANOVA.\n       http://www.biostathandbook.com/onewayanova.html\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy.stats import f_oneway\n\nHere are some data [3]_ on a shell measurement (the length of the anterior\nadductor muscle scar, standardized by dividing by length) in the mussel\nMytilus trossulus from five locations: Tillamook, Oregon; Newport, Oregon;\nPetersburg, Alaska; Magadan, Russia; and Tvarminne, Finland, taken from a\nmuch larger data set used in McDonald et al. (1991).\n\n>>> tillamook = [0.0571, 0.0813, 0.0831, 0.0976, 0.0817, 0.0859, 0.0735,\n...              0.0659, 0.0923, 0.0836]\n>>> newport = [0.0873, 0.0662, 0.0672, 0.0819, 0.0749, 0.0649, 0.0835,\n...            0.0725]\n>>> petersburg = [0.0974, 0.1352, 0.0817, 0.1016, 0.0968, 0.1064, 0.105]\n>>> magadan = [0.1033, 0.0915, 0.0781, 0.0685, 0.0677, 0.0697, 0.0764,\n...            0.0689]\n>>> tvarminne = [0.0703, 0.1026, 0.0956, 0.0973, 0.1039, 0.1045]\n>>> f_oneway(tillamook, newport, petersburg, magadan, tvarminne)\nF_onewayResult(statistic=7.121019471642447, pvalue=0.0002812242314534544)\n\n`f_oneway` accepts multidimensional input arrays.  When the inputs\nare multidimensional and `axis` is not given, the test is performed\nalong the first axis of the input arrays.  For the following data, the\ntest is performed three times, once for each column.\n\n>>> a = np.array([[9.87, 9.03, 6.81],\n...               [7.18, 8.35, 7.00],\n...               [8.39, 7.58, 7.68],\n...               [7.45, 6.33, 9.35],\n...               [6.41, 7.10, 9.33],\n...               [8.00, 8.24, 8.44]])\n>>> b = np.array([[6.35, 7.30, 7.16],\n...               [6.65, 6.68, 7.63],\n...               [5.72, 7.73, 6.72],\n...               [7.01, 9.19, 7.41],\n...               [7.75, 7.87, 8.30],\n...               [6.90, 7.97, 6.97]])\n>>> c = np.array([[3.31, 8.77, 1.01],\n...               [8.25, 3.24, 3.62],\n...               [6.32, 8.81, 5.19],\n...               [7.48, 8.83, 8.91],\n...               [8.59, 6.01, 6.07],\n...               [3.07, 9.72, 7.48]])\n>>> F, p = f_oneway(a, b, c)\n>>> F\narray([1.75676344, 0.03701228, 3.76439349])\n>>> p\narray([0.20630784, 0.96375203, 0.04733157])\n",
    "scipy.stats.false_discovery_control": "Adjust p-values to control the false discovery rate.\n\n    The false discovery rate (FDR) is the expected proportion of rejected null\n    hypotheses that are actually true.\n    If the null hypothesis is rejected when the *adjusted* p-value falls below\n    a specified level, the false discovery rate is controlled at that level.\n\n    Parameters\n    ----------\n    ps : 1D array_like\n        The p-values to adjust. Elements must be real numbers between 0 and 1.\n    axis : int\n        The axis along which to perform the adjustment. The adjustment is\n        performed independently along each axis-slice. If `axis` is None, `ps`\n        is raveled before performing the adjustment.\n    method : {'bh', 'by'}\n        The false discovery rate control procedure to apply: ``'bh'`` is for\n        Benjamini-Hochberg [1]_ (Eq. 1), ``'by'`` is for Benjaminini-Yekutieli\n        [2]_ (Theorem 1.3). The latter is more conservative, but it is\n        guaranteed to control the FDR even when the p-values are not from\n        independent tests.\n\n    Returns\n    -------\n    ps_adusted : array_like\n        The adjusted p-values. If the null hypothesis is rejected where these\n        fall below a specified level, the false discovery rate is controlled\n        at that level.\n\n    See Also\n    --------\n    combine_pvalues\n    statsmodels.stats.multitest.multipletests\n\n    Notes\n    -----\n    In multiple hypothesis testing, false discovery control procedures tend to\n    offer higher power than familywise error rate control procedures (e.g.\n    Bonferroni correction [1]_).\n\n    If the p-values correspond with independent tests (or tests with\n    \"positive regression dependencies\" [2]_), rejecting null hypotheses\n    corresponding with Benjamini-Hochberg-adjusted p-values below :math:`q`\n    controls the false discovery rate at a level less than or equal to\n    :math:`q m_0 / m`, where :math:`m_0` is the number of true null hypotheses\n    and :math:`m` is the total number of null hypotheses tested. The same is\n    true even for dependent tests when the p-values are adjusted accorded to\n    the more conservative Benjaminini-Yekutieli procedure.\n\n    The adjusted p-values produced by this function are comparable to those\n    produced by the R function ``p.adjust`` and the statsmodels function\n    `statsmodels.stats.multitest.multipletests`. Please consider the latter\n    for more advanced methods of multiple comparison correction.\n\n    References\n    ----------\n    .. [1] Benjamini, Yoav, and Yosef Hochberg. \"Controlling the false\n           discovery rate: a practical and powerful approach to multiple\n           testing.\" Journal of the Royal statistical society: series B\n           (Methodological) 57.1 (1995): 289-300.\n\n    .. [2] Benjamini, Yoav, and Daniel Yekutieli. \"The control of the false\n           discovery rate in multiple testing under dependency.\" Annals of\n           statistics (2001): 1165-1188.\n\n    .. [3] TileStats. FDR - Benjamini-Hochberg explained - Youtube.\n           https://www.youtube.com/watch?v=rZKa4tW2NKs.\n\n    .. [4] Neuhaus, Karl-Ludwig, et al. \"Improved thrombolysis in acute\n           myocardial infarction with front-loaded administration of alteplase:\n           results of the rt-PA-APSAC patency study (TAPS).\" Journal of the\n           American College of Cardiology 19.5 (1992): 885-891.\n\n    Examples\n    --------\n    We follow the example from [1]_.\n\n        Thrombolysis with recombinant tissue-type plasminogen activator (rt-PA)\n        and anisoylated plasminogen streptokinase activator (APSAC) in\n        myocardial infarction has been proved to reduce mortality. [4]_\n        investigated the effects of a new front-loaded administration of rt-PA\n        versus those obtained with a standard regimen of APSAC, in a randomized\n        multicentre trial in 421 patients with acute myocardial infarction.\n\n    There were four families of hypotheses tested in the study, the last of\n    which was \"cardiac and other events after the start of thrombolitic\n    treatment\". FDR control may be desired in this family of hypotheses\n    because it would not be appropriate to conclude that the front-loaded\n    treatment is better if it is merely equivalent to the previous treatment.\n\n    The p-values corresponding with the 15 hypotheses in this family were\n\n    >>> ps = [0.0001, 0.0004, 0.0019, 0.0095, 0.0201, 0.0278, 0.0298, 0.0344,\n    ...       0.0459, 0.3240, 0.4262, 0.5719, 0.6528, 0.7590, 1.000]\n\n    If the chosen significance level is 0.05, we may be tempted to reject the\n    null hypotheses for the tests corresponding with the first nine p-values,\n    as the first nine p-values fall below the chosen significance level.\n    However, this would ignore the problem of \"multiplicity\": if we fail to\n    correct for the fact that multiple comparisons are being performed, we\n    are more likely to incorrectly reject true null hypotheses.\n\n    One approach to the multiplicity problem is to control the family-wise\n    error rate (FWER), that is, the rate at which the null hypothesis is\n    rejected when it is actually true. A common procedure of this kind is the\n    Bonferroni correction [1]_.  We begin by multiplying the p-values by the\n    number of hypotheses tested.\n\n    >>> import numpy as np\n    >>> np.array(ps) * len(ps)\n    array([1.5000e-03, 6.0000e-03, 2.8500e-02, 1.4250e-01, 3.0150e-01,\n           4.1700e-01, 4.4700e-01, 5.1600e-01, 6.8850e-01, 4.8600e+00,\n           6.3930e+00, 8.5785e+00, 9.7920e+00, 1.1385e+01, 1.5000e+01])\n\n    To control the FWER at 5%, we reject only the hypotheses corresponding\n    with adjusted p-values less than 0.05. In this case, only the hypotheses\n    corresponding with the first three p-values can be rejected. According to\n    [1]_, these three hypotheses concerned \"allergic reaction\" and \"two\n    different aspects of bleeding.\"\n\n    An alternative approach is to control the false discovery rate: the\n    expected fraction of rejected null hypotheses that are actually true. The\n    advantage of this approach is that it typically affords greater power: an\n    increased rate of rejecting the null hypothesis when it is indeed false. To\n    control the false discovery rate at 5%, we apply the Benjamini-Hochberg\n    p-value adjustment.\n\n    >>> from scipy import stats\n    >>> stats.false_discovery_control(ps)\n    array([0.0015    , 0.003     , 0.0095    , 0.035625  , 0.0603    ,\n           0.06385714, 0.06385714, 0.0645    , 0.0765    , 0.486     ,\n           0.58118182, 0.714875  , 0.75323077, 0.81321429, 1.        ])\n\n    Now, the first *four* adjusted p-values fall below 0.05, so we would reject\n    the null hypotheses corresponding with these *four* p-values. Rejection\n    of the fourth null hypothesis was particularly important to the original\n    study as it led to the conclusion that the new treatment had a\n    \"substantially lower in-hospital mortality rate.\"\n\n    ",
    "scipy.stats.fatiguelife": "A fatigue-life (Birnbaum-Saunders) continuous random variable.\n\n    As an instance of the `rv_continuous` class, `fatiguelife` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `fatiguelife` is:\n\n    .. math::\n\n        f(x, c) = \\frac{x+1}{2c\\sqrt{2\\pi x^3}} \\exp(-\\frac{(x-1)^2}{2x c^2})\n\n    for :math:`x >= 0` and :math:`c > 0`.\n\n    `fatiguelife` takes ``c`` as a shape parameter for :math:`c`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``fatiguelife.pdf(x, c, loc, scale)`` is identically\n    equivalent to ``fatiguelife.pdf(y, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] \"Birnbaum-Saunders distribution\",\n           https://en.wikipedia.org/wiki/Birnbaum-Saunders_distribution\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import fatiguelife\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c = 29\n    >>> mean, var, skew, kurt = fatiguelife.stats(c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(fatiguelife.ppf(0.01, c),\n    ...                 fatiguelife.ppf(0.99, c), 100)\n    >>> ax.plot(x, fatiguelife.pdf(x, c),\n    ...        'r-', lw=5, alpha=0.6, label='fatiguelife pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = fatiguelife(c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = fatiguelife.ppf([0.001, 0.5, 0.999], c)\n    >>> np.allclose([0.001, 0.5, 0.999], fatiguelife.cdf(vals, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = fatiguelife.rvs(c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.find_repeats": "Find repeats and repeat counts.\n\n    Parameters\n    ----------\n    arr : array_like\n        Input array. This is cast to float64.\n\n    Returns\n    -------\n    values : ndarray\n        The unique values from the (flattened) input that are repeated.\n\n    counts : ndarray\n        Number of times the corresponding 'value' is repeated.\n\n    Notes\n    -----\n    In numpy >= 1.9 `numpy.unique` provides similar functionality. The main\n    difference is that `find_repeats` only returns repeated values.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> stats.find_repeats([2, 1, 2, 3, 2, 2, 5])\n    RepeatedResults(values=array([2.]), counts=array([4]))\n\n    >>> stats.find_repeats([[10, 20, 1, 2], [5, 5, 4, 4]])\n    RepeatedResults(values=array([4.,  5.]), counts=array([2, 2]))\n\n    ",
    "scipy.stats.fisher_exact": "Perform a Fisher exact test on a 2x2 contingency table.\n\n    The null hypothesis is that the true odds ratio of the populations\n    underlying the observations is one, and the observations were sampled\n    from these populations under a condition: the marginals of the\n    resulting table must equal those of the observed table. The statistic\n    returned is the unconditional maximum likelihood estimate of the odds\n    ratio, and the p-value is the probability under the null hypothesis of\n    obtaining a table at least as extreme as the one that was actually\n    observed. There are other possible choices of statistic and two-sided\n    p-value definition associated with Fisher's exact test; please see the\n    Notes for more information.\n\n    Parameters\n    ----------\n    table : array_like of ints\n        A 2x2 contingency table.  Elements must be non-negative integers.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n        The following options are available (default is 'two-sided'):\n\n        * 'two-sided': the odds ratio of the underlying population is not one\n        * 'less': the odds ratio of the underlying population is less than one\n        * 'greater': the odds ratio of the underlying population is greater\n          than one\n\n        See the Notes for more details.\n\n    Returns\n    -------\n    res : SignificanceResult\n        An object containing attributes:\n\n        statistic : float\n            This is the prior odds ratio, not a posterior estimate.\n        pvalue : float\n            The probability under the null hypothesis of obtaining a\n            table at least as extreme as the one that was actually observed.\n\n    See Also\n    --------\n    chi2_contingency : Chi-square test of independence of variables in a\n        contingency table.  This can be used as an alternative to\n        `fisher_exact` when the numbers in the table are large.\n    contingency.odds_ratio : Compute the odds ratio (sample or conditional\n        MLE) for a 2x2 contingency table.\n    barnard_exact : Barnard's exact test, which is a more powerful alternative\n        than Fisher's exact test for 2x2 contingency tables.\n    boschloo_exact : Boschloo's exact test, which is a more powerful\n        alternative than Fisher's exact test for 2x2 contingency tables.\n\n    Notes\n    -----\n    *Null hypothesis and p-values*\n\n    The null hypothesis is that the true odds ratio of the populations\n    underlying the observations is one, and the observations were sampled at\n    random from these populations under a condition: the marginals of the\n    resulting table must equal those of the observed table. Equivalently,\n    the null hypothesis is that the input table is from the hypergeometric\n    distribution with parameters (as used in `hypergeom`)\n    ``M = a + b + c + d``, ``n = a + b`` and ``N = a + c``, where the\n    input table is ``[[a, b], [c, d]]``.  This distribution has support\n    ``max(0, N + n - M) <= x <= min(N, n)``, or, in terms of the values\n    in the input table, ``min(0, a - d) <= x <= a + min(b, c)``.  ``x``\n    can be interpreted as the upper-left element of a 2x2 table, so the\n    tables in the distribution have form::\n\n        [  x           n - x     ]\n        [N - x    M - (n + N) + x]\n\n    For example, if::\n\n        table = [6  2]\n                [1  4]\n\n    then the support is ``2 <= x <= 7``, and the tables in the distribution\n    are::\n\n        [2 6]   [3 5]   [4 4]   [5 3]   [6 2]  [7 1]\n        [5 0]   [4 1]   [3 2]   [2 3]   [1 4]  [0 5]\n\n    The probability of each table is given by the hypergeometric distribution\n    ``hypergeom.pmf(x, M, n, N)``.  For this example, these are (rounded to\n    three significant digits)::\n\n        x       2      3      4      5       6        7\n        p  0.0163  0.163  0.408  0.326  0.0816  0.00466\n\n    These can be computed with::\n\n        >>> import numpy as np\n        >>> from scipy.stats import hypergeom\n        >>> table = np.array([[6, 2], [1, 4]])\n        >>> M = table.sum()\n        >>> n = table[0].sum()\n        >>> N = table[:, 0].sum()\n        >>> start, end = hypergeom.support(M, n, N)\n        >>> hypergeom.pmf(np.arange(start, end+1), M, n, N)\n        array([0.01631702, 0.16317016, 0.40792541, 0.32634033, 0.08158508,\n               0.004662  ])\n\n    The two-sided p-value is the probability that, under the null hypothesis,\n    a random table would have a probability equal to or less than the\n    probability of the input table.  For our example, the probability of\n    the input table (where ``x = 6``) is 0.0816.  The x values where the\n    probability does not exceed this are 2, 6 and 7, so the two-sided p-value\n    is ``0.0163 + 0.0816 + 0.00466 ~= 0.10256``::\n\n        >>> from scipy.stats import fisher_exact\n        >>> res = fisher_exact(table, alternative='two-sided')\n        >>> res.pvalue\n        0.10256410256410257\n\n    The one-sided p-value for ``alternative='greater'`` is the probability\n    that a random table has ``x >= a``, which in our example is ``x >= 6``,\n    or ``0.0816 + 0.00466 ~= 0.08626``::\n\n        >>> res = fisher_exact(table, alternative='greater')\n        >>> res.pvalue\n        0.08624708624708627\n\n    This is equivalent to computing the survival function of the\n    distribution at ``x = 5`` (one less than ``x`` from the input table,\n    because we want to include the probability of ``x = 6`` in the sum)::\n\n        >>> hypergeom.sf(5, M, n, N)\n        0.08624708624708627\n\n    For ``alternative='less'``, the one-sided p-value is the probability\n    that a random table has ``x <= a``, (i.e. ``x <= 6`` in our example),\n    or ``0.0163 + 0.163 + 0.408 + 0.326 + 0.0816 ~= 0.9949``::\n\n        >>> res = fisher_exact(table, alternative='less')\n        >>> res.pvalue\n        0.9953379953379957\n\n    This is equivalent to computing the cumulative distribution function\n    of the distribution at ``x = 6``:\n\n        >>> hypergeom.cdf(6, M, n, N)\n        0.9953379953379957\n\n    *Odds ratio*\n\n    The calculated odds ratio is different from the value computed by the\n    R function ``fisher.test``.  This implementation returns the \"sample\"\n    or \"unconditional\" maximum likelihood estimate, while ``fisher.test``\n    in R uses the conditional maximum likelihood estimate.  To compute the\n    conditional maximum likelihood estimate of the odds ratio, use\n    `scipy.stats.contingency.odds_ratio`.\n\n    References\n    ----------\n    .. [1] Fisher, Sir Ronald A, \"The Design of Experiments:\n           Mathematics of a Lady Tasting Tea.\" ISBN 978-0-486-41151-4, 1935.\n    .. [2] \"Fisher's exact test\",\n           https://en.wikipedia.org/wiki/Fisher's_exact_test\n    .. [3] Emma V. Low et al. \"Identifying the lowest effective dose of\n           acetazolamide for the prophylaxis of acute mountain sickness:\n           systematic review and meta-analysis.\"\n           BMJ, 345, :doi:`10.1136/bmj.e6779`, 2012.\n\n    Examples\n    --------\n    In [3]_, the effective dose of acetazolamide for the prophylaxis of acute\n    mountain sickness was investigated. The study notably concluded:\n\n        Acetazolamide 250 mg, 500 mg, and 750 mg daily were all efficacious for\n        preventing acute mountain sickness. Acetazolamide 250 mg was the lowest\n        effective dose with available evidence for this indication.\n\n    The following table summarizes the results of the experiment in which\n    some participants took a daily dose of acetazolamide 250 mg while others\n    took a placebo.\n    Cases of acute mountain sickness were recorded::\n\n                                    Acetazolamide   Control/Placebo\n        Acute mountain sickness            7           17\n        No                                15            5\n\n\n    Is there evidence that the acetazolamide 250 mg reduces the risk of\n    acute mountain sickness?\n    We begin by formulating a null hypothesis :math:`H_0`:\n\n        The odds of experiencing acute mountain sickness are the same with\n        the acetazolamide treatment as they are with placebo.\n\n    Let's assess the plausibility of this hypothesis with\n    Fisher's test.\n\n    >>> from scipy.stats import fisher_exact\n    >>> res = fisher_exact([[7, 17], [15, 5]], alternative='less')\n    >>> res.statistic\n    0.13725490196078433\n    >>> res.pvalue\n    0.0028841933752349743\n\n    Using a significance level of 5%, we would reject the null hypothesis in\n    favor of the alternative hypothesis: \"The odds of experiencing acute\n    mountain sickness with acetazolamide treatment are less than the odds of\n    experiencing acute mountain sickness with placebo.\"\n\n    .. note::\n\n        Because the null distribution of Fisher's exact test is formed under\n        the assumption that both row and column sums are fixed, the result of\n        the test are conservative when applied to an experiment in which the\n        row sums are not fixed.\n\n        In this case, the column sums are fixed; there are 22 subjects in each\n        group. But the number of cases of acute mountain sickness is not\n        (and cannot be) fixed before conducting the experiment. It is a\n        consequence.\n\n        Boschloo's test does not depend on the assumption that the row sums\n        are fixed, and consequently, it provides a more powerful test in this\n        situation.\n\n        >>> from scipy.stats import boschloo_exact\n        >>> res = boschloo_exact([[7, 17], [15, 5]], alternative='less')\n        >>> res.statistic\n        0.0028841933752349743\n        >>> res.pvalue\n        0.0015141406667567101\n\n        We verify that the p-value is less than with `fisher_exact`.\n\n    ",
    "scipy.stats.fisk": "A Fisk continuous random variable.\n\n    The Fisk distribution is also known as the log-logistic distribution.\n\n    As an instance of the `rv_continuous` class, `fisk` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    burr\n\n    Notes\n    -----\n    The probability density function for `fisk` is:\n\n    .. math::\n\n        f(x, c) = \\frac{c x^{c-1}}\n                       {(1 + x^c)^2}\n\n    for :math:`x >= 0` and :math:`c > 0`.\n\n    Please note that the above expression can be transformed into the following\n    one, which is also commonly used:\n\n    .. math::\n\n        f(x, c) = \\frac{c x^{-c-1}}\n                       {(1 + x^{-c})^2}\n\n    `fisk` takes ``c`` as a shape parameter for :math:`c`.\n\n    `fisk` is a special case of `burr` or `burr12` with ``d=1``.\n\n    Suppose ``X`` is a logistic random variable with location ``l``\n    and scale ``s``. Then ``Y = exp(X)`` is a Fisk (log-logistic)\n    random variable with ``scale = exp(l)`` and shape ``c = 1/s``.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``fisk.pdf(x, c, loc, scale)`` is identically\n    equivalent to ``fisk.pdf(y, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import fisk\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c = 3.09\n    >>> mean, var, skew, kurt = fisk.stats(c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(fisk.ppf(0.01, c),\n    ...                 fisk.ppf(0.99, c), 100)\n    >>> ax.plot(x, fisk.pdf(x, c),\n    ...        'r-', lw=5, alpha=0.6, label='fisk pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = fisk(c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = fisk.ppf([0.001, 0.5, 0.999], c)\n    >>> np.allclose([0.001, 0.5, 0.999], fisk.cdf(vals, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = fisk.rvs(c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.fit": "Fit a discrete or continuous distribution to data\n\n    Given a distribution, data, and bounds on the parameters of the\n    distribution, return maximum likelihood estimates of the parameters.\n\n    Parameters\n    ----------\n    dist : `scipy.stats.rv_continuous` or `scipy.stats.rv_discrete`\n        The object representing the distribution to be fit to the data.\n    data : 1D array_like\n        The data to which the distribution is to be fit. If the data contain\n        any of ``np.nan``, ``np.inf``, or -``np.inf``, the fit method will\n        raise a ``ValueError``.\n    bounds : dict or sequence of tuples, optional\n        If a dictionary, each key is the name of a parameter of the\n        distribution, and the corresponding value is a tuple containing the\n        lower and upper bound on that parameter.  If the distribution is\n        defined only for a finite range of values of that parameter, no entry\n        for that parameter is required; e.g., some distributions have\n        parameters which must be on the interval [0, 1]. Bounds for parameters\n        location (``loc``) and scale (``scale``) are optional; by default,\n        they are fixed to 0 and 1, respectively.\n\n        If a sequence, element *i* is a tuple containing the lower and upper\n        bound on the *i*\\ th parameter of the distribution. In this case,\n        bounds for *all* distribution shape parameters must be provided.\n        Optionally, bounds for location and scale may follow the\n        distribution shape parameters.\n\n        If a shape is to be held fixed (e.g. if it is known), the\n        lower and upper bounds may be equal. If a user-provided lower or upper\n        bound is beyond a bound of the domain for which the distribution is\n        defined, the bound of the distribution's domain will replace the\n        user-provided value. Similarly, parameters which must be integral\n        will be constrained to integral values within the user-provided bounds.\n    guess : dict or array_like, optional\n        If a dictionary, each key is the name of a parameter of the\n        distribution, and the corresponding value is a guess for the value\n        of the parameter.\n\n        If a sequence, element *i* is a guess for the *i*\\ th parameter of the\n        distribution. In this case, guesses for *all* distribution shape\n        parameters must be provided.\n\n        If `guess` is not provided, guesses for the decision variables will\n        not be passed to the optimizer. If `guess` is provided, guesses for\n        any missing parameters will be set at the mean of the lower and\n        upper bounds. Guesses for parameters which must be integral will be\n        rounded to integral values, and guesses that lie outside the\n        intersection of the user-provided bounds and the domain of the\n        distribution will be clipped.\n    method : {'mle', 'mse'}\n        With ``method=\"mle\"`` (default), the fit is computed by minimizing\n        the negative log-likelihood function. A large, finite penalty\n        (rather than infinite negative log-likelihood) is applied for\n        observations beyond the support of the distribution.\n        With ``method=\"mse\"``, the fit is computed by minimizing\n        the negative log-product spacing function. The same penalty is applied\n        for observations beyond the support. We follow the approach of [1]_,\n        which is generalized for samples with repeated observations.\n    optimizer : callable, optional\n        `optimizer` is a callable that accepts the following positional\n        argument.\n\n        fun : callable\n            The objective function to be optimized. `fun` accepts one argument\n            ``x``, candidate shape parameters of the distribution, and returns\n            the objective function value given ``x``, `dist`, and the provided\n            `data`.\n            The job of `optimizer` is to find values of the decision variables\n            that minimizes `fun`.\n\n        `optimizer` must also accept the following keyword argument.\n\n        bounds : sequence of tuples\n            The bounds on values of the decision variables; each element will\n            be a tuple containing the lower and upper bound on a decision\n            variable.\n\n        If `guess` is provided, `optimizer` must also accept the following\n        keyword argument.\n\n        x0 : array_like\n            The guesses for each decision variable.\n\n        If the distribution has any shape parameters that must be integral or\n        if the distribution is discrete and the location parameter is not\n        fixed, `optimizer` must also accept the following keyword argument.\n\n        integrality : array_like of bools\n            For each decision variable, True if the decision variable\n            must be constrained to integer values and False if the decision\n            variable is continuous.\n\n        `optimizer` must return an object, such as an instance of\n        `scipy.optimize.OptimizeResult`, which holds the optimal values of\n        the decision variables in an attribute ``x``. If attributes\n        ``fun``, ``status``, or ``message`` are provided, they will be\n        included in the result object returned by `fit`.\n\n    Returns\n    -------\n    result : `~scipy.stats._result_classes.FitResult`\n        An object with the following fields.\n\n        params : namedtuple\n            A namedtuple containing the maximum likelihood estimates of the\n            shape parameters, location, and (if applicable) scale of the\n            distribution.\n        success : bool or None\n            Whether the optimizer considered the optimization to terminate\n            successfully or not.\n        message : str or None\n            Any status message provided by the optimizer.\n\n        The object has the following method:\n\n        nllf(params=None, data=None)\n            By default, the negative log-likehood function at the fitted\n            `params` for the given `data`. Accepts a tuple containing\n            alternative shapes, location, and scale of the distribution and\n            an array of alternative data.\n\n        plot(ax=None)\n            Superposes the PDF/PMF of the fitted distribution over a normalized\n            histogram of the data.\n\n    See Also\n    --------\n    rv_continuous,  rv_discrete\n\n    Notes\n    -----\n    Optimization is more likely to converge to the maximum likelihood estimate\n    when the user provides tight bounds containing the maximum likelihood\n    estimate. For example, when fitting a binomial distribution to data, the\n    number of experiments underlying each sample may be known, in which case\n    the corresponding shape parameter ``n`` can be fixed.\n\n    References\n    ----------\n    .. [1] Shao, Yongzhao, and Marjorie G. Hahn. \"Maximum product of spacings\n           method: a unified formulation with illustration of strong\n           consistency.\" Illinois Journal of Mathematics 43.3 (1999): 489-499.\n\n    Examples\n    --------\n    Suppose we wish to fit a distribution to the following data.\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> dist = stats.nbinom\n    >>> shapes = (5, 0.5)\n    >>> data = dist.rvs(*shapes, size=1000, random_state=rng)\n\n    Suppose we do not know how the data were generated, but we suspect that\n    it follows a negative binomial distribution with parameters *n* and *p*\\.\n    (See `scipy.stats.nbinom`.) We believe that the parameter *n* was fewer\n    than 30, and we know that the parameter *p* must lie on the interval\n    [0, 1]. We record this information in a variable `bounds` and pass\n    this information to `fit`.\n\n    >>> bounds = [(0, 30), (0, 1)]\n    >>> res = stats.fit(dist, data, bounds)\n\n    `fit` searches within the user-specified `bounds` for the\n    values that best match the data (in the sense of maximum likelihood\n    estimation). In this case, it found shape values similar to those\n    from which the data were actually generated.\n\n    >>> res.params\n    FitParams(n=5.0, p=0.5028157644634368, loc=0.0)  # may vary\n\n    We can visualize the results by superposing the probability mass function\n    of the distribution (with the shapes fit to the data) over a normalized\n    histogram of the data.\n\n    >>> import matplotlib.pyplot as plt  # matplotlib must be installed to plot\n    >>> res.plot()\n    >>> plt.show()\n\n    Note that the estimate for *n* was exactly integral; this is because\n    the domain of the `nbinom` PMF includes only integral *n*, and the `nbinom`\n    object \"knows\" that. `nbinom` also knows that the shape *p* must be a\n    value between 0 and 1. In such a case - when the domain of the distribution\n    with respect to a parameter is finite - we are not required to specify\n    bounds for the parameter.\n\n    >>> bounds = {'n': (0, 30)}  # omit parameter p using a `dict`\n    >>> res2 = stats.fit(dist, data, bounds)\n    >>> res2.params\n    FitParams(n=5.0, p=0.5016492009232932, loc=0.0)  # may vary\n\n    If we wish to force the distribution to be fit with *n* fixed at 6, we can\n    set both the lower and upper bounds on *n* to 6. Note, however, that the\n    value of the objective function being optimized is typically worse (higher)\n    in this case.\n\n    >>> bounds = {'n': (6, 6)}  # fix parameter `n`\n    >>> res3 = stats.fit(dist, data, bounds)\n    >>> res3.params\n    FitParams(n=6.0, p=0.5486556076755706, loc=0.0)  # may vary\n    >>> res3.nllf() > res.nllf()\n    True  # may vary\n\n    Note that the numerical results of the previous examples are typical, but\n    they may vary because the default optimizer used by `fit`,\n    `scipy.optimize.differential_evolution`, is stochastic. However, we can\n    customize the settings used by the optimizer to ensure reproducibility -\n    or even use a different optimizer entirely - using the `optimizer`\n    parameter.\n\n    >>> from scipy.optimize import differential_evolution\n    >>> rng = np.random.default_rng(767585560716548)\n    >>> def optimizer(fun, bounds, *, integrality):\n    ...     return differential_evolution(fun, bounds, strategy='best2bin',\n    ...                                   seed=rng, integrality=integrality)\n    >>> bounds = [(0, 30), (0, 1)]\n    >>> res4 = stats.fit(dist, data, bounds, optimizer=optimizer)\n    >>> res4.params\n    FitParams(n=5.0, p=0.5015183149259951, loc=0.0)\n\n    ",
    "scipy.stats.fligner": "    \n\n\nPerform Fligner-Killeen test for equality of variance.\n\nFligner's test tests the null hypothesis that all input samples\nare from populations with equal variances.  Fligner-Killeen's test is\ndistribution free when populations are identical [2]_.\n\nParameters\n----------\nsample1, sample2, ... : array_like\n    Arrays of sample data.  Need not be the same length.\ncenter : {'mean', 'median', 'trimmed'}, optional\n    Keyword argument controlling which function of the data is used in\n    computing the test statistic.  The default is 'median'.\nproportiontocut : float, optional\n    When `center` is 'trimmed', this gives the proportion of data points\n    to cut from each end. (See `scipy.stats.trim_mean`.)\n    Default is 0.05.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nstatistic : float\n    The test statistic.\npvalue : float\n    The p-value for the hypothesis test.\n\nSee Also\n--------\n\n:func:`bartlett`\n    A parametric test for equality of k variances in normal samples\n:func:`levene`\n    A robust parametric test for equality of k variances\n\n\nNotes\n-----\nAs with Levene's test there are three variants of Fligner's test that\ndiffer by the measure of central tendency used in the test.  See `levene`\nfor more information.\n\nConover et al. (1981) examine many of the existing parametric and\nnonparametric tests by extensive simulations and they conclude that the\ntests proposed by Fligner and Killeen (1976) and Levene (1960) appear to be\nsuperior in terms of robustness of departures from normality and power\n[3]_.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] Park, C. and Lindsay, B. G. (1999). Robust Scale Estimation and\n       Hypothesis Testing based on Quadratic Inference Function. Technical\n       Report #99-03, Center for Likelihood Studies, Pennsylvania State\n       University.\n       https://cecas.clemson.edu/~cspark/cv/paper/qif/draftqif2.pdf\n.. [2] Fligner, M.A. and Killeen, T.J. (1976). Distribution-free two-sample\n       tests for scale. 'Journal of the American Statistical Association.'\n       71(353), 210-213.\n.. [3] Park, C. and Lindsay, B. G. (1999). Robust Scale Estimation and\n       Hypothesis Testing based on Quadratic Inference Function. Technical\n       Report #99-03, Center for Likelihood Studies, Pennsylvania State\n       University.\n.. [4] Conover, W. J., Johnson, M. E. and Johnson M. M. (1981). A\n       comparative study of tests for homogeneity of variances, with\n       applications to the outer continental shelf bidding data.\n       Technometrics, 23(4), 351-361.\n.. [5] C.I. BLISS (1952), The Statistics of Bioassay: With Special\n       Reference to the Vitamins, pp 499-503,\n       :doi:`10.1016/C2013-0-12584-6`.\n.. [6] B. Phipson and G. K. Smyth. \"Permutation P-values Should Never Be\n       Zero: Calculating Exact P-values When Permutations Are Randomly\n       Drawn.\" Statistical Applications in Genetics and Molecular Biology\n       9.1 (2010).\n.. [7] Ludbrook, J., & Dudley, H. (1998). Why permutation tests are\n       superior to t and F tests in biomedical research. The American\n       Statistician, 52(2), 127-132.\n\nExamples\n--------\nIn [5]_, the influence of vitamin C on the tooth growth of guinea pigs\nwas investigated. In a control study, 60 subjects were divided into\nsmall dose, medium dose, and large dose groups that received\ndaily doses of 0.5, 1.0 and 2.0 mg of vitamin C, respectively.\nAfter 42 days, the tooth growth was measured.\n\nThe ``small_dose``, ``medium_dose``, and ``large_dose`` arrays below record\ntooth growth measurements of the three groups in microns.\n\n>>> import numpy as np\n>>> small_dose = np.array([\n...     4.2, 11.5, 7.3, 5.8, 6.4, 10, 11.2, 11.2, 5.2, 7,\n...     15.2, 21.5, 17.6, 9.7, 14.5, 10, 8.2, 9.4, 16.5, 9.7\n... ])\n>>> medium_dose = np.array([\n...     16.5, 16.5, 15.2, 17.3, 22.5, 17.3, 13.6, 14.5, 18.8, 15.5,\n...     19.7, 23.3, 23.6, 26.4, 20, 25.2, 25.8, 21.2, 14.5, 27.3\n... ])\n>>> large_dose = np.array([\n...     23.6, 18.5, 33.9, 25.5, 26.4, 32.5, 26.7, 21.5, 23.3, 29.5,\n...     25.5, 26.4, 22.4, 24.5, 24.8, 30.9, 26.4, 27.3, 29.4, 23\n... ])\n\nThe `fligner` statistic is sensitive to differences in variances\nbetween the samples.\n\n>>> from scipy import stats\n>>> res = stats.fligner(small_dose, medium_dose, large_dose)\n>>> res.statistic\n1.3878943408857916\n\nThe value of the statistic tends to be high when there is a large\ndifference in variances.\n\nWe can test for inequality of variance among the groups by comparing the\nobserved value of the statistic against the null distribution: the\ndistribution of statistic values derived under the null hypothesis that\nthe population variances of the three groups are equal.\n\nFor this test, the null distribution follows the chi-square distribution\nas shown below.\n\n>>> import matplotlib.pyplot as plt\n>>> k = 3  # number of samples\n>>> dist = stats.chi2(df=k-1)\n>>> val = np.linspace(0, 8, 100)\n>>> pdf = dist.pdf(val)\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> def plot(ax):  # we'll reuse this\n...     ax.plot(val, pdf, color='C0')\n...     ax.set_title(\"Fligner Test Null Distribution\")\n...     ax.set_xlabel(\"statistic\")\n...     ax.set_ylabel(\"probability density\")\n...     ax.set_xlim(0, 8)\n...     ax.set_ylim(0, 0.5)\n>>> plot(ax)\n>>> plt.show()\n\nThe comparison is quantified by the p-value: the proportion of values in\nthe null distribution greater than or equal to the observed value of the\nstatistic.\n\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> plot(ax)\n>>> pvalue = dist.sf(res.statistic)\n>>> annotation = (f'p-value={pvalue:.4f}\\n(shaded area)')\n>>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n>>> _ = ax.annotate(annotation, (1.5, 0.22), (2.25, 0.3), arrowprops=props)\n>>> i = val >= res.statistic\n>>> ax.fill_between(val[i], y1=0, y2=pdf[i], color='C0')\n>>> plt.show()\n\n>>> res.pvalue\n0.49960016501182125\n\nIf the p-value is \"small\" - that is, if there is a low probability of\nsampling data from distributions with identical variances that produces\nsuch an extreme value of the statistic - this may be taken as evidence\nagainst the null hypothesis in favor of the alternative: the variances of\nthe groups are not equal. Note that:\n\n- The inverse is not true; that is, the test is not used to provide\n  evidence for the null hypothesis.\n- The threshold for values that will be considered \"small\" is a choice that\n  should be made before the data is analyzed [6]_ with consideration of the\n  risks of both false positives (incorrectly rejecting the null hypothesis)\n  and false negatives (failure to reject a false null hypothesis).\n- Small p-values are not evidence for a *large* effect; rather, they can\n  only provide evidence for a \"significant\" effect, meaning that they are\n  unlikely to have occurred under the null hypothesis.\n\nNote that the chi-square distribution provides an asymptotic approximation\nof the null distribution.\nFor small samples, it may be more appropriate to perform a\npermutation test: Under the null hypothesis that all three samples were\ndrawn from the same population, each of the measurements is equally likely\nto have been observed in any of the three samples. Therefore, we can form\na randomized null distribution by calculating the statistic under many\nrandomly-generated partitionings of the observations into the three\nsamples.\n\n>>> def statistic(*samples):\n...     return stats.fligner(*samples).statistic\n>>> ref = stats.permutation_test(\n...     (small_dose, medium_dose, large_dose), statistic,\n...     permutation_type='independent', alternative='greater'\n... )\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> plot(ax)\n>>> bins = np.linspace(0, 8, 25)\n>>> ax.hist(\n...     ref.null_distribution, bins=bins, density=True, facecolor=\"C1\"\n... )\n>>> ax.legend(['aymptotic approximation\\n(many observations)',\n...            'randomized null distribution'])\n>>> plot(ax)\n>>> plt.show()\n\n>>> ref.pvalue  # randomized test p-value\n0.4332  # may vary\n\nNote that there is significant disagreement between the p-value calculated\nhere and the asymptotic approximation returned by `fligner` above.\nThe statistical inferences that can be drawn rigorously from a permutation\ntest are limited; nonetheless, they may be the preferred approach in many\ncircumstances [7]_.\n\nFollowing is another generic example where the null hypothesis would be\nrejected.\n\nTest whether the lists `a`, `b` and `c` come from populations\nwith equal variances.\n\n>>> a = [8.88, 9.12, 9.04, 8.98, 9.00, 9.08, 9.01, 8.85, 9.06, 8.99]\n>>> b = [8.88, 8.95, 9.29, 9.44, 9.15, 9.58, 8.36, 9.18, 8.67, 9.05]\n>>> c = [8.95, 9.12, 8.95, 8.85, 9.03, 8.84, 9.07, 8.98, 8.86, 8.98]\n>>> stat, p = stats.fligner(a, b, c)\n>>> p\n0.00450826080004775\n\nThe small p-value suggests that the populations do not have equal\nvariances.\n\nThis is not surprising, given that the sample variance of `b` is much\nlarger than that of `a` and `c`:\n\n>>> [np.var(x, ddof=1) for x in [a, b, c]]\n[0.007054444444444413, 0.13073888888888888, 0.008890000000000002]\n",
    "scipy.stats.foldcauchy": "A folded Cauchy continuous random variable.\n\n    As an instance of the `rv_continuous` class, `foldcauchy` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `foldcauchy` is:\n\n    .. math::\n\n        f(x, c) = \\frac{1}{\\pi (1+(x-c)^2)} + \\frac{1}{\\pi (1+(x+c)^2)}\n\n    for :math:`x \\ge 0` and :math:`c \\ge 0`.\n\n    `foldcauchy` takes ``c`` as a shape parameter for :math:`c`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import foldcauchy\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c = 4.72\n    >>> mean, var, skew, kurt = foldcauchy.stats(c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(foldcauchy.ppf(0.01, c),\n    ...                 foldcauchy.ppf(0.99, c), 100)\n    >>> ax.plot(x, foldcauchy.pdf(x, c),\n    ...        'r-', lw=5, alpha=0.6, label='foldcauchy pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = foldcauchy(c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = foldcauchy.ppf([0.001, 0.5, 0.999], c)\n    >>> np.allclose([0.001, 0.5, 0.999], foldcauchy.cdf(vals, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = foldcauchy.rvs(c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.foldnorm": "A folded normal continuous random variable.\n\n    As an instance of the `rv_continuous` class, `foldnorm` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `foldnorm` is:\n\n    .. math::\n\n        f(x, c) = \\sqrt{2/\\pi} cosh(c x) \\exp(-\\frac{x^2+c^2}{2})\n\n    for :math:`x \\ge 0` and :math:`c \\ge 0`.\n\n    `foldnorm` takes ``c`` as a shape parameter for :math:`c`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``foldnorm.pdf(x, c, loc, scale)`` is identically\n    equivalent to ``foldnorm.pdf(y, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import foldnorm\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c = 1.95\n    >>> mean, var, skew, kurt = foldnorm.stats(c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(foldnorm.ppf(0.01, c),\n    ...                 foldnorm.ppf(0.99, c), 100)\n    >>> ax.plot(x, foldnorm.pdf(x, c),\n    ...        'r-', lw=5, alpha=0.6, label='foldnorm pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = foldnorm(c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = foldnorm.ppf([0.001, 0.5, 0.999], c)\n    >>> np.allclose([0.001, 0.5, 0.999], foldnorm.cdf(vals, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = foldnorm.rvs(c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.friedmanchisquare": "    \n\n\nCompute the Friedman test for repeated samples.\n\nThe Friedman test tests the null hypothesis that repeated samples of\nthe same individuals have the same distribution.  It is often used\nto test for consistency among samples obtained in different ways.\nFor example, if two sampling techniques are used on the same set of\nindividuals, the Friedman test can be used to determine if the two\nsampling techniques are consistent.\n\nParameters\n----------\nsample1, sample2, sample3... : array_like\n    Arrays of observations.  All of the arrays must have the same number\n    of elements.  At least three samples must be given.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nstatistic : float\n    The test statistic, correcting for ties.\npvalue : float\n    The associated p-value assuming that the test statistic has a chi\n    squared distribution.\n\nNotes\n-----\nDue to the assumption that the test statistic has a chi squared\ndistribution, the p-value is only reliable for n > 10 and more than\n6 repeated samples.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] https://en.wikipedia.org/wiki/Friedman_test\n.. [2] P. Sprent and N.C. Smeeton, \"Applied Nonparametric Statistical\n       Methods, Third Edition\". Chapter 6, Section 6.3.2.\n\nExamples\n--------\nIn [2]_, the pulse rate (per minute) of a group of seven students was\nmeasured before exercise, immediately after exercise and 5 minutes\nafter exercise. Is there evidence to suggest that the pulse rates on\nthese three occasions are similar?\n\nWe begin by formulating a null hypothesis :math:`H_0`:\n\n    The pulse rates are identical on these three occasions.\n\nLet's assess the plausibility of this hypothesis with a Friedman test.\n\n>>> from scipy.stats import friedmanchisquare\n>>> before = [72, 96, 88, 92, 74, 76, 82]\n>>> immediately_after = [120, 120, 132, 120, 101, 96, 112]\n>>> five_min_after = [76, 95, 104, 96, 84, 72, 76]\n>>> res = friedmanchisquare(before, immediately_after, five_min_after)\n>>> res.statistic\n10.57142857142857\n>>> res.pvalue\n0.005063414171757498\n\nUsing a significance level of 5%, we would reject the null hypothesis in\nfavor of the alternative hypothesis: \"the pulse rates are different on\nthese three occasions\".\n",
    "scipy.stats.gamma": "A gamma continuous random variable.\n\n    As an instance of the `rv_continuous` class, `gamma` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, a, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, a, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, a, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, a, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, a, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, a, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, a, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, a, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(a, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, loc=0, scale=1)\n        Median of the distribution.\n    mean(a, loc=0, scale=1)\n        Mean of the distribution.\n    var(a, loc=0, scale=1)\n        Variance of the distribution.\n    std(a, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, a, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    erlang, expon\n\n    Notes\n    -----\n    The probability density function for `gamma` is:\n\n    .. math::\n\n        f(x, a) = \\frac{x^{a-1} e^{-x}}{\\Gamma(a)}\n\n    for :math:`x \\ge 0`, :math:`a > 0`. Here :math:`\\Gamma(a)` refers to the\n    gamma function.\n\n    `gamma` takes ``a`` as a shape parameter for :math:`a`.\n\n    When :math:`a` is an integer, `gamma` reduces to the Erlang\n    distribution, and when :math:`a=1` to the exponential distribution.\n\n    Gamma distributions are sometimes parameterized with two variables,\n    with a probability density function of:\n\n    .. math::\n\n        f(x, \\alpha, \\beta) =\n        \\frac{\\beta^\\alpha x^{\\alpha - 1} e^{-\\beta x }}{\\Gamma(\\alpha)}\n\n    Note that this parameterization is equivalent to the above, with\n    ``scale = 1 / beta``.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``gamma.pdf(x, a, loc, scale)`` is identically\n    equivalent to ``gamma.pdf(y, a) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import gamma\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a = 1.99\n    >>> mean, var, skew, kurt = gamma.stats(a, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(gamma.ppf(0.01, a),\n    ...                 gamma.ppf(0.99, a), 100)\n    >>> ax.plot(x, gamma.pdf(x, a),\n    ...        'r-', lw=5, alpha=0.6, label='gamma pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = gamma(a)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = gamma.ppf([0.001, 0.5, 0.999], a)\n    >>> np.allclose([0.001, 0.5, 0.999], gamma.cdf(vals, a))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = gamma.rvs(a, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.gausshyper": "A Gauss hypergeometric continuous random variable.\n\n    As an instance of the `rv_continuous` class, `gausshyper` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, b, c, z, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, a, b, c, z, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, a, b, c, z, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, a, b, c, z, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, a, b, c, z, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, a, b, c, z, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, a, b, c, z, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, a, b, c, z, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, b, c, z, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, a, b, c, z, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(a, b, c, z, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, b, c, z, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(a, b, c, z), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, b, c, z, loc=0, scale=1)\n        Median of the distribution.\n    mean(a, b, c, z, loc=0, scale=1)\n        Mean of the distribution.\n    var(a, b, c, z, loc=0, scale=1)\n        Variance of the distribution.\n    std(a, b, c, z, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, a, b, c, z, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `gausshyper` is:\n\n    .. math::\n\n        f(x, a, b, c, z) = C x^{a-1} (1-x)^{b-1} (1+zx)^{-c}\n\n    for :math:`0 \\le x \\le 1`, :math:`a,b > 0`, :math:`c` a real number,\n    :math:`z > -1`, and :math:`C = \\frac{1}{B(a, b) F[2, 1](c, a; a+b; -z)}`.\n    :math:`F[2, 1]` is the Gauss hypergeometric function\n    `scipy.special.hyp2f1`.\n\n    `gausshyper` takes :math:`a`, :math:`b`, :math:`c` and :math:`z` as shape\n    parameters.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``gausshyper.pdf(x, a, b, c, z, loc, scale)`` is identically\n    equivalent to ``gausshyper.pdf(y, a, b, c, z) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] Armero, C., and M. J. Bayarri. \"Prior Assessments for Prediction in\n           Queues.\" *Journal of the Royal Statistical Society*. Series D (The\n           Statistician) 43, no. 1 (1994): 139-53. doi:10.2307/2348939\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import gausshyper\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a, b, c, z = 13.8, 3.12, 2.51, 5.18\n    >>> mean, var, skew, kurt = gausshyper.stats(a, b, c, z, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(gausshyper.ppf(0.01, a, b, c, z),\n    ...                 gausshyper.ppf(0.99, a, b, c, z), 100)\n    >>> ax.plot(x, gausshyper.pdf(x, a, b, c, z),\n    ...        'r-', lw=5, alpha=0.6, label='gausshyper pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = gausshyper(a, b, c, z)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = gausshyper.ppf([0.001, 0.5, 0.999], a, b, c, z)\n    >>> np.allclose([0.001, 0.5, 0.999], gausshyper.cdf(vals, a, b, c, z))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = gausshyper.rvs(a, b, c, z, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.gaussian_kde": "Representation of a kernel-density estimate using Gaussian kernels.\n\n    Kernel density estimation is a way to estimate the probability density\n    function (PDF) of a random variable in a non-parametric way.\n    `gaussian_kde` works for both uni-variate and multi-variate data.   It\n    includes automatic bandwidth determination.  The estimation works best for\n    a unimodal distribution; bimodal or multi-modal distributions tend to be\n    oversmoothed.\n\n    Parameters\n    ----------\n    dataset : array_like\n        Datapoints to estimate from. In case of univariate data this is a 1-D\n        array, otherwise a 2-D array with shape (# of dims, # of data).\n    bw_method : str, scalar or callable, optional\n        The method used to calculate the estimator bandwidth.  This can be\n        'scott', 'silverman', a scalar constant or a callable.  If a scalar,\n        this will be used directly as `kde.factor`.  If a callable, it should\n        take a `gaussian_kde` instance as only parameter and return a scalar.\n        If None (default), 'scott' is used.  See Notes for more details.\n    weights : array_like, optional\n        weights of datapoints. This must be the same shape as dataset.\n        If None (default), the samples are assumed to be equally weighted\n\n    Attributes\n    ----------\n    dataset : ndarray\n        The dataset with which `gaussian_kde` was initialized.\n    d : int\n        Number of dimensions.\n    n : int\n        Number of datapoints.\n    neff : int\n        Effective number of datapoints.\n\n        .. versionadded:: 1.2.0\n    factor : float\n        The bandwidth factor, obtained from `kde.covariance_factor`. The square\n        of `kde.factor` multiplies the covariance matrix of the data in the kde\n        estimation.\n    covariance : ndarray\n        The covariance matrix of `dataset`, scaled by the calculated bandwidth\n        (`kde.factor`).\n    inv_cov : ndarray\n        The inverse of `covariance`.\n\n    Methods\n    -------\n    evaluate\n    __call__\n    integrate_gaussian\n    integrate_box_1d\n    integrate_box\n    integrate_kde\n    pdf\n    logpdf\n    resample\n    set_bandwidth\n    covariance_factor\n\n    Notes\n    -----\n    Bandwidth selection strongly influences the estimate obtained from the KDE\n    (much more so than the actual shape of the kernel).  Bandwidth selection\n    can be done by a \"rule of thumb\", by cross-validation, by \"plug-in\n    methods\" or by other means; see [3]_, [4]_ for reviews.  `gaussian_kde`\n    uses a rule of thumb, the default is Scott's Rule.\n\n    Scott's Rule [1]_, implemented as `scotts_factor`, is::\n\n        n**(-1./(d+4)),\n\n    with ``n`` the number of data points and ``d`` the number of dimensions.\n    In the case of unequally weighted points, `scotts_factor` becomes::\n\n        neff**(-1./(d+4)),\n\n    with ``neff`` the effective number of datapoints.\n    Silverman's Rule [2]_, implemented as `silverman_factor`, is::\n\n        (n * (d + 2) / 4.)**(-1. / (d + 4)).\n\n    or in the case of unequally weighted points::\n\n        (neff * (d + 2) / 4.)**(-1. / (d + 4)).\n\n    Good general descriptions of kernel density estimation can be found in [1]_\n    and [2]_, the mathematics for this multi-dimensional implementation can be\n    found in [1]_.\n\n    With a set of weighted samples, the effective number of datapoints ``neff``\n    is defined by::\n\n        neff = sum(weights)^2 / sum(weights^2)\n\n    as detailed in [5]_.\n\n    `gaussian_kde` does not currently support data that lies in a\n    lower-dimensional subspace of the space in which it is expressed. For such\n    data, consider performing principle component analysis / dimensionality\n    reduction and using `gaussian_kde` with the transformed data.\n\n    References\n    ----------\n    .. [1] D.W. Scott, \"Multivariate Density Estimation: Theory, Practice, and\n           Visualization\", John Wiley & Sons, New York, Chicester, 1992.\n    .. [2] B.W. Silverman, \"Density Estimation for Statistics and Data\n           Analysis\", Vol. 26, Monographs on Statistics and Applied Probability,\n           Chapman and Hall, London, 1986.\n    .. [3] B.A. Turlach, \"Bandwidth Selection in Kernel Density Estimation: A\n           Review\", CORE and Institut de Statistique, Vol. 19, pp. 1-33, 1993.\n    .. [4] D.M. Bashtannyk and R.J. Hyndman, \"Bandwidth selection for kernel\n           conditional density estimation\", Computational Statistics & Data\n           Analysis, Vol. 36, pp. 279-298, 2001.\n    .. [5] Gray P. G., 1969, Journal of the Royal Statistical Society.\n           Series A (General), 132, 272\n\n    Examples\n    --------\n    Generate some random two-dimensional data:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> def measure(n):\n    ...     \"Measurement model, return two coupled measurements.\"\n    ...     m1 = np.random.normal(size=n)\n    ...     m2 = np.random.normal(scale=0.5, size=n)\n    ...     return m1+m2, m1-m2\n\n    >>> m1, m2 = measure(2000)\n    >>> xmin = m1.min()\n    >>> xmax = m1.max()\n    >>> ymin = m2.min()\n    >>> ymax = m2.max()\n\n    Perform a kernel density estimate on the data:\n\n    >>> X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n    >>> positions = np.vstack([X.ravel(), Y.ravel()])\n    >>> values = np.vstack([m1, m2])\n    >>> kernel = stats.gaussian_kde(values)\n    >>> Z = np.reshape(kernel(positions).T, X.shape)\n\n    Plot the results:\n\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots()\n    >>> ax.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,\n    ...           extent=[xmin, xmax, ymin, ymax])\n    >>> ax.plot(m1, m2, 'k.', markersize=2)\n    >>> ax.set_xlim([xmin, xmax])\n    >>> ax.set_ylim([ymin, ymax])\n    >>> plt.show()\n\n    ",
    "scipy.stats.genexpon": "A generalized exponential continuous random variable.\n\n    As an instance of the `rv_continuous` class, `genexpon` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, b, c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, a, b, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, a, b, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, a, b, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, a, b, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, a, b, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, a, b, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, a, b, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, b, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, a, b, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(a, b, c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, b, c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(a, b, c), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, b, c, loc=0, scale=1)\n        Median of the distribution.\n    mean(a, b, c, loc=0, scale=1)\n        Mean of the distribution.\n    var(a, b, c, loc=0, scale=1)\n        Variance of the distribution.\n    std(a, b, c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, a, b, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `genexpon` is:\n\n    .. math::\n\n        f(x, a, b, c) = (a + b (1 - \\exp(-c x)))\n                        \\exp(-a x - b x + \\frac{b}{c}  (1-\\exp(-c x)))\n\n    for :math:`x \\ge 0`, :math:`a, b, c > 0`.\n\n    `genexpon` takes :math:`a`, :math:`b` and :math:`c` as shape parameters.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``genexpon.pdf(x, a, b, c, loc, scale)`` is identically\n    equivalent to ``genexpon.pdf(y, a, b, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    H.K. Ryu, \"An Extension of Marshall and Olkin's Bivariate Exponential\n    Distribution\", Journal of the American Statistical Association, 1993.\n\n    N. Balakrishnan, Asit P. Basu (editors), *The Exponential Distribution:\n    Theory, Methods and Applications*, Gordon and Breach, 1995.\n    ISBN 10: 2884491929\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import genexpon\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a, b, c = 9.13, 16.2, 3.28\n    >>> mean, var, skew, kurt = genexpon.stats(a, b, c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(genexpon.ppf(0.01, a, b, c),\n    ...                 genexpon.ppf(0.99, a, b, c), 100)\n    >>> ax.plot(x, genexpon.pdf(x, a, b, c),\n    ...        'r-', lw=5, alpha=0.6, label='genexpon pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = genexpon(a, b, c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = genexpon.ppf([0.001, 0.5, 0.999], a, b, c)\n    >>> np.allclose([0.001, 0.5, 0.999], genexpon.cdf(vals, a, b, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = genexpon.rvs(a, b, c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.genextreme": "A generalized extreme value continuous random variable.\n\n    As an instance of the `rv_continuous` class, `genextreme` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    gumbel_r\n\n    Notes\n    -----\n    For :math:`c=0`, `genextreme` is equal to `gumbel_r` with\n    probability density function\n\n    .. math::\n\n        f(x) = \\exp(-\\exp(-x)) \\exp(-x),\n\n    where :math:`-\\infty < x < \\infty`.\n\n    For :math:`c \\ne 0`, the probability density function for `genextreme` is:\n\n    .. math::\n\n        f(x, c) = \\exp(-(1-c x)^{1/c}) (1-c x)^{1/c-1},\n\n    where :math:`-\\infty < x \\le 1/c` if :math:`c > 0` and\n    :math:`1/c \\le x < \\infty` if :math:`c < 0`.\n\n    Note that several sources and software packages use the opposite\n    convention for the sign of the shape parameter :math:`c`.\n\n    `genextreme` takes ``c`` as a shape parameter for :math:`c`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``genextreme.pdf(x, c, loc, scale)`` is identically\n    equivalent to ``genextreme.pdf(y, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import genextreme\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c = -0.1\n    >>> mean, var, skew, kurt = genextreme.stats(c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(genextreme.ppf(0.01, c),\n    ...                 genextreme.ppf(0.99, c), 100)\n    >>> ax.plot(x, genextreme.pdf(x, c),\n    ...        'r-', lw=5, alpha=0.6, label='genextreme pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = genextreme(c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = genextreme.ppf([0.001, 0.5, 0.999], c)\n    >>> np.allclose([0.001, 0.5, 0.999], genextreme.cdf(vals, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = genextreme.rvs(c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.gengamma": "A generalized gamma continuous random variable.\n\n    As an instance of the `rv_continuous` class, `gengamma` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, a, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, a, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, a, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, a, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, a, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, a, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, a, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, a, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(a, c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(a, c), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, c, loc=0, scale=1)\n        Median of the distribution.\n    mean(a, c, loc=0, scale=1)\n        Mean of the distribution.\n    var(a, c, loc=0, scale=1)\n        Variance of the distribution.\n    std(a, c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, a, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    gamma, invgamma, weibull_min\n\n    Notes\n    -----\n    The probability density function for `gengamma` is ([1]_):\n\n    .. math::\n\n        f(x, a, c) = \\frac{|c| x^{c a-1} \\exp(-x^c)}{\\Gamma(a)}\n\n    for :math:`x \\ge 0`, :math:`a > 0`, and :math:`c \\ne 0`.\n    :math:`\\Gamma` is the gamma function (`scipy.special.gamma`).\n\n    `gengamma` takes :math:`a` and :math:`c` as shape parameters.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``gengamma.pdf(x, a, c, loc, scale)`` is identically\n    equivalent to ``gengamma.pdf(y, a, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] E.W. Stacy, \"A Generalization of the Gamma Distribution\",\n       Annals of Mathematical Statistics, Vol 33(3), pp. 1187--1192.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import gengamma\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a, c = 4.42, -3.12\n    >>> mean, var, skew, kurt = gengamma.stats(a, c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(gengamma.ppf(0.01, a, c),\n    ...                 gengamma.ppf(0.99, a, c), 100)\n    >>> ax.plot(x, gengamma.pdf(x, a, c),\n    ...        'r-', lw=5, alpha=0.6, label='gengamma pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = gengamma(a, c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = gengamma.ppf([0.001, 0.5, 0.999], a, c)\n    >>> np.allclose([0.001, 0.5, 0.999], gengamma.cdf(vals, a, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = gengamma.rvs(a, c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.genhalflogistic": "A generalized half-logistic continuous random variable.\n\n    As an instance of the `rv_continuous` class, `genhalflogistic` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `genhalflogistic` is:\n\n    .. math::\n\n        f(x, c) = \\frac{2 (1 - c x)^{1/(c-1)}}{[1 + (1 - c x)^{1/c}]^2}\n\n    for :math:`0 \\le x \\le 1/c`, and :math:`c > 0`.\n\n    `genhalflogistic` takes ``c`` as a shape parameter for :math:`c`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``genhalflogistic.pdf(x, c, loc, scale)`` is identically\n    equivalent to ``genhalflogistic.pdf(y, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import genhalflogistic\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c = 0.773\n    >>> mean, var, skew, kurt = genhalflogistic.stats(c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(genhalflogistic.ppf(0.01, c),\n    ...                 genhalflogistic.ppf(0.99, c), 100)\n    >>> ax.plot(x, genhalflogistic.pdf(x, c),\n    ...        'r-', lw=5, alpha=0.6, label='genhalflogistic pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = genhalflogistic(c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = genhalflogistic.ppf([0.001, 0.5, 0.999], c)\n    >>> np.allclose([0.001, 0.5, 0.999], genhalflogistic.cdf(vals, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = genhalflogistic.rvs(c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.genhyperbolic": "A generalized hyperbolic continuous random variable.\n\n    As an instance of the `rv_continuous` class, `genhyperbolic` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(p, a, b, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, p, a, b, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, p, a, b, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, p, a, b, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, p, a, b, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, p, a, b, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, p, a, b, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, p, a, b, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, p, a, b, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, p, a, b, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(p, a, b, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(p, a, b, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(p, a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(p, a, b, loc=0, scale=1)\n        Median of the distribution.\n    mean(p, a, b, loc=0, scale=1)\n        Mean of the distribution.\n    var(p, a, b, loc=0, scale=1)\n        Variance of the distribution.\n    std(p, a, b, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, p, a, b, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    t, norminvgauss, geninvgauss, laplace, cauchy\n\n    Notes\n    -----\n    The probability density function for `genhyperbolic` is:\n\n    .. math::\n\n        f(x, p, a, b) =\n            \\frac{(a^2 - b^2)^{p/2}}\n            {\\sqrt{2\\pi}a^{p-1/2}\n            K_p\\Big(\\sqrt{a^2 - b^2}\\Big)}\n            e^{bx} \\times \\frac{K_{p - 1/2}\n            (a \\sqrt{1 + x^2})}\n            {(\\sqrt{1 + x^2})^{1/2 - p}}\n\n    for :math:`x, p \\in ( - \\infty; \\infty)`,\n    :math:`|b| < a` if :math:`p \\ge 0`,\n    :math:`|b| \\le a` if :math:`p < 0`.\n    :math:`K_{p}(.)` denotes the modified Bessel function of the second\n    kind and order :math:`p` (`scipy.special.kv`)\n\n    `genhyperbolic` takes ``p`` as a tail parameter,\n    ``a`` as a shape parameter,\n    ``b`` as a skewness parameter.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``genhyperbolic.pdf(x, p, a, b, loc, scale)`` is identically\n    equivalent to ``genhyperbolic.pdf(y, p, a, b) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    The original parameterization of the Generalized Hyperbolic Distribution\n    is found in [1]_ as follows\n\n    .. math::\n\n        f(x, \\lambda, \\alpha, \\beta, \\delta, \\mu) =\n           \\frac{(\\gamma/\\delta)^\\lambda}{\\sqrt{2\\pi}K_\\lambda(\\delta \\gamma)}\n           e^{\\beta (x - \\mu)} \\times \\frac{K_{\\lambda - 1/2}\n           (\\alpha \\sqrt{\\delta^2 + (x - \\mu)^2})}\n           {(\\sqrt{\\delta^2 + (x - \\mu)^2} / \\alpha)^{1/2 - \\lambda}}\n\n    for :math:`x \\in ( - \\infty; \\infty)`,\n    :math:`\\gamma := \\sqrt{\\alpha^2 - \\beta^2}`,\n    :math:`\\lambda, \\mu \\in ( - \\infty; \\infty)`,\n    :math:`\\delta \\ge 0, |\\beta| < \\alpha` if :math:`\\lambda \\ge 0`,\n    :math:`\\delta > 0, |\\beta| \\le \\alpha` if :math:`\\lambda < 0`.\n\n    The location-scale-based parameterization implemented in\n    SciPy is based on [2]_, where :math:`a = \\alpha\\delta`,\n    :math:`b = \\beta\\delta`, :math:`p = \\lambda`,\n    :math:`scale=\\delta` and :math:`loc=\\mu`\n\n    Moments are implemented based on [3]_ and [4]_.\n\n    For the distributions that are a special case such as Student's t,\n    it is not recommended to rely on the implementation of genhyperbolic.\n    To avoid potential numerical problems and for performance reasons,\n    the methods of the specific distributions should be used.\n\n    References\n    ----------\n    .. [1] O. Barndorff-Nielsen, \"Hyperbolic Distributions and Distributions\n       on Hyperbolae\", Scandinavian Journal of Statistics, Vol. 5(3),\n       pp. 151-157, 1978. https://www.jstor.org/stable/4615705\n\n    .. [2] Eberlein E., Prause K. (2002) The Generalized Hyperbolic Model:\n        Financial Derivatives and Risk Measures. In: Geman H., Madan D.,\n        Pliska S.R., Vorst T. (eds) Mathematical Finance - Bachelier\n        Congress 2000. Springer Finance. Springer, Berlin, Heidelberg.\n        :doi:`10.1007/978-3-662-12429-1_12`\n\n    .. [3] Scott, David J, W\u00fcrtz, Diethelm, Dong, Christine and Tran,\n       Thanh Tam, (2009), Moments of the generalized hyperbolic\n       distribution, MPRA Paper, University Library of Munich, Germany,\n       https://EconPapers.repec.org/RePEc:pra:mprapa:19081.\n\n    .. [4] E. Eberlein and E. A. von Hammerstein. Generalized hyperbolic\n       and inverse Gaussian distributions: Limiting cases and approximation\n       of processes. FDM Preprint 80, April 2003. University of Freiburg.\n       https://freidok.uni-freiburg.de/fedora/objects/freidok:7974/datastreams/FILE1/content\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import genhyperbolic\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> p, a, b = 0.5, 1.5, -0.5\n    >>> mean, var, skew, kurt = genhyperbolic.stats(p, a, b, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(genhyperbolic.ppf(0.01, p, a, b),\n    ...                 genhyperbolic.ppf(0.99, p, a, b), 100)\n    >>> ax.plot(x, genhyperbolic.pdf(x, p, a, b),\n    ...        'r-', lw=5, alpha=0.6, label='genhyperbolic pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = genhyperbolic(p, a, b)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = genhyperbolic.ppf([0.001, 0.5, 0.999], p, a, b)\n    >>> np.allclose([0.001, 0.5, 0.999], genhyperbolic.cdf(vals, p, a, b))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = genhyperbolic.rvs(p, a, b, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.geninvgauss": "A Generalized Inverse Gaussian continuous random variable.\n\n    As an instance of the `rv_continuous` class, `geninvgauss` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(p, b, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, p, b, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, p, b, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, p, b, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, p, b, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, p, b, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, p, b, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, p, b, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, p, b, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, p, b, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(p, b, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(p, b, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(p, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(p, b, loc=0, scale=1)\n        Median of the distribution.\n    mean(p, b, loc=0, scale=1)\n        Mean of the distribution.\n    var(p, b, loc=0, scale=1)\n        Variance of the distribution.\n    std(p, b, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, p, b, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `geninvgauss` is:\n\n    .. math::\n\n        f(x, p, b) = x^{p-1} \\exp(-b (x + 1/x) / 2) / (2 K_p(b))\n\n    where `x > 0`, `p` is a real number and `b > 0`\\([1]_).\n    :math:`K_p` is the modified Bessel function of second kind of order `p`\n    (`scipy.special.kv`).\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``geninvgauss.pdf(x, p, b, loc, scale)`` is identically\n    equivalent to ``geninvgauss.pdf(y, p, b) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    The inverse Gaussian distribution `stats.invgauss(mu)` is a special case of\n    `geninvgauss` with `p = -1/2`, `b = 1 / mu` and `scale = mu`.\n\n    Generating random variates is challenging for this distribution. The\n    implementation is based on [2]_.\n\n    References\n    ----------\n    .. [1] O. Barndorff-Nielsen, P. Blaesild, C. Halgreen, \"First hitting time\n       models for the generalized inverse gaussian distribution\",\n       Stochastic Processes and their Applications 7, pp. 49--54, 1978.\n\n    .. [2] W. Hoermann and J. Leydold, \"Generating generalized inverse Gaussian\n       random variates\", Statistics and Computing, 24(4), p. 547--557, 2014.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import geninvgauss\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> p, b = 2.3, 1.5\n    >>> mean, var, skew, kurt = geninvgauss.stats(p, b, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(geninvgauss.ppf(0.01, p, b),\n    ...                 geninvgauss.ppf(0.99, p, b), 100)\n    >>> ax.plot(x, geninvgauss.pdf(x, p, b),\n    ...        'r-', lw=5, alpha=0.6, label='geninvgauss pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = geninvgauss(p, b)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = geninvgauss.ppf([0.001, 0.5, 0.999], p, b)\n    >>> np.allclose([0.001, 0.5, 0.999], geninvgauss.cdf(vals, p, b))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = geninvgauss.rvs(p, b, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.genlogistic": "A generalized logistic continuous random variable.\n\n    As an instance of the `rv_continuous` class, `genlogistic` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `genlogistic` is:\n\n    .. math::\n\n        f(x, c) = c \\frac{\\exp(-x)}\n                         {(1 + \\exp(-x))^{c+1}}\n\n    for real :math:`x` and :math:`c > 0`. In literature, different\n    generalizations of the logistic distribution can be found. This is the type 1\n    generalized logistic distribution according to [1]_. It is also referred to\n    as the skew-logistic distribution [2]_.\n\n    `genlogistic` takes ``c`` as a shape parameter for :math:`c`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``genlogistic.pdf(x, c, loc, scale)`` is identically\n    equivalent to ``genlogistic.pdf(y, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] Johnson et al. \"Continuous Univariate Distributions\", Volume 2,\n           Wiley. 1995.\n    .. [2] \"Generalized Logistic Distribution\", Wikipedia,\n           https://en.wikipedia.org/wiki/Generalized_logistic_distribution\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import genlogistic\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c = 0.412\n    >>> mean, var, skew, kurt = genlogistic.stats(c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(genlogistic.ppf(0.01, c),\n    ...                 genlogistic.ppf(0.99, c), 100)\n    >>> ax.plot(x, genlogistic.pdf(x, c),\n    ...        'r-', lw=5, alpha=0.6, label='genlogistic pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = genlogistic(c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = genlogistic.ppf([0.001, 0.5, 0.999], c)\n    >>> np.allclose([0.001, 0.5, 0.999], genlogistic.cdf(vals, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = genlogistic.rvs(c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.gennorm": "A generalized normal continuous random variable.\n\n    As an instance of the `rv_continuous` class, `gennorm` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(beta, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, beta, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, beta, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, beta, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, beta, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, beta, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, beta, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, beta, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, beta, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, beta, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(beta, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(beta, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(beta,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(beta, loc=0, scale=1)\n        Median of the distribution.\n    mean(beta, loc=0, scale=1)\n        Mean of the distribution.\n    var(beta, loc=0, scale=1)\n        Variance of the distribution.\n    std(beta, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, beta, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    laplace : Laplace distribution\n    norm : normal distribution\n\n    Notes\n    -----\n    The probability density function for `gennorm` is [1]_:\n\n    .. math::\n\n        f(x, \\beta) = \\frac{\\beta}{2 \\Gamma(1/\\beta)} \\exp(-|x|^\\beta),\n\n    where :math:`x` is a real number, :math:`\\beta > 0` and\n    :math:`\\Gamma` is the gamma function (`scipy.special.gamma`).\n\n    `gennorm` takes ``beta`` as a shape parameter for :math:`\\beta`.\n    For :math:`\\beta = 1`, it is identical to a Laplace distribution.\n    For :math:`\\beta = 2`, it is identical to a normal distribution\n    (with ``scale=1/sqrt(2)``).\n\n    References\n    ----------\n\n    .. [1] \"Generalized normal distribution, Version 1\",\n           https://en.wikipedia.org/wiki/Generalized_normal_distribution#Version_1\n\n    .. [2] Nardon, Martina, and Paolo Pianca. \"Simulation techniques for\n           generalized Gaussian densities.\" Journal of Statistical\n           Computation and Simulation 79.11 (2009): 1317-1329\n\n    .. [3] Wicklin, Rick. \"Simulate data from a generalized Gaussian\n           distribution\" in The DO Loop blog, September 21, 2016,\n           https://blogs.sas.com/content/iml/2016/09/21/simulate-generalized-gaussian-sas.html\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import gennorm\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> beta = 1.3\n    >>> mean, var, skew, kurt = gennorm.stats(beta, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(gennorm.ppf(0.01, beta),\n    ...                 gennorm.ppf(0.99, beta), 100)\n    >>> ax.plot(x, gennorm.pdf(x, beta),\n    ...        'r-', lw=5, alpha=0.6, label='gennorm pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = gennorm(beta)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = gennorm.ppf([0.001, 0.5, 0.999], beta)\n    >>> np.allclose([0.001, 0.5, 0.999], gennorm.cdf(vals, beta))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = gennorm.rvs(beta, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.genpareto": "A generalized Pareto continuous random variable.\n\n    As an instance of the `rv_continuous` class, `genpareto` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `genpareto` is:\n\n    .. math::\n\n        f(x, c) = (1 + c x)^{-1 - 1/c}\n\n    defined for :math:`x \\ge 0` if :math:`c \\ge 0`, and for\n    :math:`0 \\le x \\le -1/c` if :math:`c < 0`.\n\n    `genpareto` takes ``c`` as a shape parameter for :math:`c`.\n\n    For :math:`c=0`, `genpareto` reduces to the exponential\n    distribution, `expon`:\n\n    .. math::\n\n        f(x, 0) = \\exp(-x)\n\n    For :math:`c=-1`, `genpareto` is uniform on ``[0, 1]``:\n\n    .. math::\n\n        f(x, -1) = 1\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``genpareto.pdf(x, c, loc, scale)`` is identically\n    equivalent to ``genpareto.pdf(y, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import genpareto\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c = 0.1\n    >>> mean, var, skew, kurt = genpareto.stats(c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(genpareto.ppf(0.01, c),\n    ...                 genpareto.ppf(0.99, c), 100)\n    >>> ax.plot(x, genpareto.pdf(x, c),\n    ...        'r-', lw=5, alpha=0.6, label='genpareto pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = genpareto(c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = genpareto.ppf([0.001, 0.5, 0.999], c)\n    >>> np.allclose([0.001, 0.5, 0.999], genpareto.cdf(vals, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = genpareto.rvs(c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.geom": "A geometric discrete random variable.\n\n    As an instance of the `rv_discrete` class, `geom` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(p, loc=0, size=1, random_state=None)\n        Random variates.\n    pmf(k, p, loc=0)\n        Probability mass function.\n    logpmf(k, p, loc=0)\n        Log of the probability mass function.\n    cdf(k, p, loc=0)\n        Cumulative distribution function.\n    logcdf(k, p, loc=0)\n        Log of the cumulative distribution function.\n    sf(k, p, loc=0)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(k, p, loc=0)\n        Log of the survival function.\n    ppf(q, p, loc=0)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, p, loc=0)\n        Inverse survival function (inverse of ``sf``).\n    stats(p, loc=0, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(p, loc=0)\n        (Differential) entropy of the RV.\n    expect(func, args=(p,), loc=0, lb=None, ub=None, conditional=False)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(p, loc=0)\n        Median of the distribution.\n    mean(p, loc=0)\n        Mean of the distribution.\n    var(p, loc=0)\n        Variance of the distribution.\n    std(p, loc=0)\n        Standard deviation of the distribution.\n    interval(confidence, p, loc=0)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability mass function for `geom` is:\n\n    .. math::\n\n        f(k) = (1-p)^{k-1} p\n\n    for :math:`k \\ge 1`, :math:`0 < p \\leq 1`\n\n    `geom` takes :math:`p` as shape parameter,\n    where :math:`p` is the probability of a single success\n    and :math:`1-p` is the probability of a single failure.\n\n    The probability mass function above is defined in the \"standardized\" form.\n    To shift distribution use the ``loc`` parameter.\n    Specifically, ``geom.pmf(k, p, loc)`` is identically\n    equivalent to ``geom.pmf(k - loc, p)``.\n\n    See Also\n    --------\n    planck\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import geom\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> p = 0.5\n    >>> mean, var, skew, kurt = geom.stats(p, moments='mvsk')\n    \n    Display the probability mass function (``pmf``):\n    \n    >>> x = np.arange(geom.ppf(0.01, p),\n    ...               geom.ppf(0.99, p))\n    >>> ax.plot(x, geom.pmf(x, p), 'bo', ms=8, label='geom pmf')\n    >>> ax.vlines(x, 0, geom.pmf(x, p), colors='b', lw=5, alpha=0.5)\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape and location. This returns a \"frozen\" RV object holding\n    the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pmf``:\n    \n    >>> rv = geom(p)\n    >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n    ...         label='frozen pmf')\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> prob = geom.cdf(x, p)\n    >>> np.allclose(x, geom.ppf(prob, p))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = geom.rvs(p, size=1000)\n\n    ",
    "scipy.stats.gibrat": "A Gibrat continuous random variable.\n\n    As an instance of the `rv_continuous` class, `gibrat` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `gibrat` is:\n\n    .. math::\n\n        f(x) = \\frac{1}{x \\sqrt{2\\pi}} \\exp(-\\frac{1}{2} (\\log(x))^2)\n\n    `gibrat` is a special case of `lognorm` with ``s=1``.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``gibrat.pdf(x, loc, scale)`` is identically\n    equivalent to ``gibrat.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import gibrat\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    \n    >>> mean, var, skew, kurt = gibrat.stats(moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(gibrat.ppf(0.01),\n    ...                 gibrat.ppf(0.99), 100)\n    >>> ax.plot(x, gibrat.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='gibrat pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = gibrat()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = gibrat.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], gibrat.cdf(vals))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = gibrat.rvs(size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.gmean": "    \n\n\nCompute the weighted geometric mean along the specified axis.\n\nThe weighted geometric mean of the array :math:`a_i` associated to weights\n:math:`w_i` is:\n\n.. math::\n\n    \\exp \\left( \\frac{ \\sum_{i=1}^n w_i \\ln a_i }{ \\sum_{i=1}^n w_i }\n               \\right) \\, ,\n\nand, with equal weights, it gives:\n\n.. math::\n\n    \\sqrt[n]{ \\prod_{i=1}^n a_i } \\, .\n\nParameters\n----------\na : array_like\n    Input array or object that can be converted to an array.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\ndtype : dtype, optional\n    Type to which the input arrays are cast before the calculation is\n    performed.\nweights : array_like, optional\n    The `weights` array must be broadcastable to the same shape as `a`.\n    Default is None, which gives each value a weight of 1.0.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\ngmean : ndarray\n    See `dtype` parameter above.\n\nSee Also\n--------\n\n:func:`numpy.mean`\n    Arithmetic average\n:func:`numpy.average`\n    Weighted average\n:func:`hmean`\n    Harmonic mean\n\n\nNotes\n-----\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] \"Weighted Geometric Mean\", *Wikipedia*,\n       https://en.wikipedia.org/wiki/Weighted_geometric_mean.\n.. [2] Grossman, J., Grossman, M., Katz, R., \"Averages: A New Approach\",\n       Archimedes Foundation, 1983\n\nExamples\n--------\n>>> from scipy.stats import gmean\n>>> gmean([1, 4])\n2.0\n>>> gmean([1, 2, 3, 4, 5, 6, 7])\n3.3800151591412964\n>>> gmean([1, 4, 7], weights=[3, 1, 3])\n2.80668351922014\n",
    "scipy.stats.gompertz": "A Gompertz (or truncated Gumbel) continuous random variable.\n\n    As an instance of the `rv_continuous` class, `gompertz` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `gompertz` is:\n\n    .. math::\n\n        f(x, c) = c \\exp(x) \\exp(-c (e^x-1))\n\n    for :math:`x \\ge 0`, :math:`c > 0`.\n\n    `gompertz` takes ``c`` as a shape parameter for :math:`c`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``gompertz.pdf(x, c, loc, scale)`` is identically\n    equivalent to ``gompertz.pdf(y, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import gompertz\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c = 0.947\n    >>> mean, var, skew, kurt = gompertz.stats(c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(gompertz.ppf(0.01, c),\n    ...                 gompertz.ppf(0.99, c), 100)\n    >>> ax.plot(x, gompertz.pdf(x, c),\n    ...        'r-', lw=5, alpha=0.6, label='gompertz pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = gompertz(c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = gompertz.ppf([0.001, 0.5, 0.999], c)\n    >>> np.allclose([0.001, 0.5, 0.999], gompertz.cdf(vals, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = gompertz.rvs(c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.goodness_of_fit": "\n    Perform a goodness of fit test comparing data to a distribution family.\n\n    Given a distribution family and data, perform a test of the null hypothesis\n    that the data were drawn from a distribution in that family. Any known\n    parameters of the distribution may be specified. Remaining parameters of\n    the distribution will be fit to the data, and the p-value of the test\n    is computed accordingly. Several statistics for comparing the distribution\n    to data are available.\n\n    Parameters\n    ----------\n    dist : `scipy.stats.rv_continuous`\n        The object representing the distribution family under the null\n        hypothesis.\n    data : 1D array_like\n        Finite, uncensored data to be tested.\n    known_params : dict, optional\n        A dictionary containing name-value pairs of known distribution\n        parameters. Monte Carlo samples are randomly drawn from the\n        null-hypothesized distribution with these values of the parameters.\n        Before the statistic is evaluated for each Monte Carlo sample, only\n        remaining unknown parameters of the null-hypothesized distribution\n        family are fit to the samples; the known parameters are held fixed.\n        If all parameters of the distribution family are known, then the step\n        of fitting the distribution family to each sample is omitted.\n    fit_params : dict, optional\n        A dictionary containing name-value pairs of distribution parameters\n        that have already been fit to the data, e.g. using `scipy.stats.fit`\n        or the ``fit`` method of `dist`. Monte Carlo samples are drawn from the\n        null-hypothesized distribution with these specified values of the\n        parameter. On those Monte Carlo samples, however, these and all other\n        unknown parameters of the null-hypothesized distribution family are\n        fit before the statistic is evaluated.\n    guessed_params : dict, optional\n        A dictionary containing name-value pairs of distribution parameters\n        which have been guessed. These parameters are always considered as\n        free parameters and are fit both to the provided `data` as well as\n        to the Monte Carlo samples drawn from the null-hypothesized\n        distribution. The purpose of these `guessed_params` is to be used as\n        initial values for the numerical fitting procedure.\n    statistic : {\"ad\", \"ks\", \"cvm\", \"filliben\"} or callable, optional\n        The statistic used to compare data to a distribution after fitting\n        unknown parameters of the distribution family to the data. The\n        Anderson-Darling (\"ad\") [1]_, Kolmogorov-Smirnov (\"ks\") [1]_,\n        Cramer-von Mises (\"cvm\") [1]_, and Filliben (\"filliben\") [7]_\n        statistics are available.  Alternatively, a callable with signature\n        ``(dist, data, axis)`` may be supplied to compute the statistic. Here\n        ``dist`` is a frozen distribution object (potentially with array\n        parameters), ``data`` is an array of Monte Carlo samples (of\n        compatible shape), and ``axis`` is the axis of ``data`` along which\n        the statistic must be computed.\n    n_mc_samples : int, default: 9999\n        The number of Monte Carlo samples drawn from the null hypothesized\n        distribution to form the null distribution of the statistic. The\n        sample size of each is the same as the given `data`.\n    random_state : {None, int, `numpy.random.Generator`,\n                    `numpy.random.RandomState`}, optional\n\n        Pseudorandom number generator state used to generate the Monte Carlo\n        samples.\n\n        If `random_state` is ``None`` (default), the\n        `numpy.random.RandomState` singleton is used.\n        If `random_state` is an int, a new ``RandomState`` instance is used,\n        seeded with `random_state`.\n        If `random_state` is already a ``Generator`` or ``RandomState``\n        instance, then the provided instance is used.\n\n    Returns\n    -------\n    res : GoodnessOfFitResult\n        An object with the following attributes.\n\n        fit_result : `~scipy.stats._result_classes.FitResult`\n            An object representing the fit of the provided `dist` to `data`.\n            This  object includes the values of distribution family parameters\n            that fully define the null-hypothesized distribution, that is,\n            the distribution from which Monte Carlo samples are drawn.\n        statistic : float\n            The value of the statistic comparing provided `data` to the\n            null-hypothesized distribution.\n        pvalue : float\n            The proportion of elements in the null distribution with\n            statistic values at least as extreme as the statistic value of the\n            provided `data`.\n        null_distribution : ndarray\n            The value of the statistic for each Monte Carlo sample\n            drawn from the null-hypothesized distribution.\n\n    Notes\n    -----\n    This is a generalized Monte Carlo goodness-of-fit procedure, special cases\n    of which correspond with various Anderson-Darling tests, Lilliefors' test,\n    etc. The test is described in [2]_, [3]_, and [4]_ as a parametric\n    bootstrap test. This is a Monte Carlo test in which parameters that\n    specify the distribution from which samples are drawn have been estimated\n    from the data. We describe the test using \"Monte Carlo\" rather than\n    \"parametric bootstrap\" throughout to avoid confusion with the more familiar\n    nonparametric bootstrap, and describe how the test is performed below.\n\n    *Traditional goodness of fit tests*\n\n    Traditionally, critical values corresponding with a fixed set of\n    significance levels are pre-calculated using Monte Carlo methods. Users\n    perform the test by calculating the value of the test statistic only for\n    their observed `data` and comparing this value to tabulated critical\n    values. This practice is not very flexible, as tables are not available for\n    all distributions and combinations of known and unknown parameter values.\n    Also, results can be inaccurate when critical values are interpolated from\n    limited tabulated data to correspond with the user's sample size and\n    fitted parameter values. To overcome these shortcomings, this function\n    allows the user to perform the Monte Carlo trials adapted to their\n    particular data.\n\n    *Algorithmic overview*\n\n    In brief, this routine executes the following steps:\n\n      1. Fit unknown parameters to the given `data`, thereby forming the\n         \"null-hypothesized\" distribution, and compute the statistic of\n         this pair of data and distribution.\n      2. Draw random samples from this null-hypothesized distribution.\n      3. Fit the unknown parameters to each random sample.\n      4. Calculate the statistic between each sample and the distribution that\n         has been fit to the sample.\n      5. Compare the value of the statistic corresponding with `data` from (1)\n         against the values of the statistic corresponding with the random\n         samples from (4). The p-value is the proportion of samples with a\n         statistic value greater than or equal to the statistic of the observed\n         data.\n\n    In more detail, the steps are as follows.\n\n    First, any unknown parameters of the distribution family specified by\n    `dist` are fit to the provided `data` using maximum likelihood estimation.\n    (One exception is the normal distribution with unknown location and scale:\n    we use the bias-corrected standard deviation ``np.std(data, ddof=1)`` for\n    the scale as recommended in [1]_.)\n    These values of the parameters specify a particular member of the\n    distribution family referred to as the \"null-hypothesized distribution\",\n    that is, the distribution from which the data were sampled under the null\n    hypothesis. The `statistic`, which compares data to a distribution, is\n    computed between `data` and the null-hypothesized distribution.\n\n    Next, many (specifically `n_mc_samples`) new samples, each containing the\n    same number of observations as `data`, are drawn from the\n    null-hypothesized distribution. All unknown parameters of the distribution\n    family `dist` are fit to *each resample*, and the `statistic` is computed\n    between each sample and its corresponding fitted distribution. These\n    values of the statistic form the Monte Carlo null distribution (not to be\n    confused with the \"null-hypothesized distribution\" above).\n\n    The p-value of the test is the proportion of statistic values in the Monte\n    Carlo null distribution that are at least as extreme as the statistic value\n    of the provided `data`. More precisely, the p-value is given by\n\n    .. math::\n\n        p = \\frac{b + 1}\n                 {m + 1}\n\n    where :math:`b` is the number of statistic values in the Monte Carlo null\n    distribution that are greater than or equal to the statistic value\n    calculated for `data`, and :math:`m` is the number of elements in the\n    Monte Carlo null distribution (`n_mc_samples`). The addition of :math:`1`\n    to the numerator and denominator can be thought of as including the\n    value of the statistic corresponding with `data` in the null distribution,\n    but a more formal explanation is given in [5]_.\n\n    *Limitations*\n\n    The test can be very slow for some distribution families because unknown\n    parameters of the distribution family must be fit to each of the Monte\n    Carlo samples, and for most distributions in SciPy, distribution fitting\n    performed via numerical optimization.\n\n    *Anti-Pattern*\n\n    For this reason, it may be tempting\n    to treat parameters of the distribution pre-fit to `data` (by the user)\n    as though they were `known_params`, as specification of all parameters of\n    the distribution precludes the need to fit the distribution to each Monte\n    Carlo sample. (This is essentially how the original Kilmogorov-Smirnov\n    test is performed.) Although such a test can provide evidence against the\n    null hypothesis, the test is conservative in the sense that small p-values\n    will tend to (greatly) *overestimate* the probability of making a type I\n    error (that is, rejecting the null hypothesis although it is true), and the\n    power of the test is low (that is, it is less likely to reject the null\n    hypothesis even when the null hypothesis is false).\n    This is because the Monte Carlo samples are less likely to agree with the\n    null-hypothesized distribution as well as `data`. This tends to increase\n    the values of the statistic recorded in the null distribution, so that a\n    larger number of them exceed the value of statistic for `data`, thereby\n    inflating the p-value.\n\n    References\n    ----------\n    .. [1] M. A. Stephens (1974). \"EDF Statistics for Goodness of Fit and\n           Some Comparisons.\" Journal of the American Statistical Association,\n           Vol. 69, pp. 730-737.\n    .. [2] W. Stute, W. G. Manteiga, and M. P. Quindimil (1993).\n           \"Bootstrap based goodness-of-fit-tests.\" Metrika 40.1: 243-256.\n    .. [3] C. Genest, & B R\u00e9millard. (2008). \"Validity of the parametric\n           bootstrap for goodness-of-fit testing in semiparametric models.\"\n           Annales de l'IHP Probabilit\u00e9s et statistiques. Vol. 44. No. 6.\n    .. [4] I. Kojadinovic and J. Yan (2012). \"Goodness-of-fit testing based on\n           a weighted bootstrap: A fast large-sample alternative to the\n           parametric bootstrap.\" Canadian Journal of Statistics 40.3: 480-500.\n    .. [5] B. Phipson and G. K. Smyth (2010). \"Permutation P-values Should\n           Never Be Zero: Calculating Exact P-values When Permutations Are\n           Randomly Drawn.\" Statistical Applications in Genetics and Molecular\n           Biology 9.1.\n    .. [6] H. W. Lilliefors (1967). \"On the Kolmogorov-Smirnov test for\n           normality with mean and variance unknown.\" Journal of the American\n           statistical Association 62.318: 399-402.\n    .. [7] Filliben, James J. \"The probability plot correlation coefficient\n           test for normality.\" Technometrics 17.1 (1975): 111-117.\n\n    Examples\n    --------\n    A well-known test of the null hypothesis that data were drawn from a\n    given distribution is the Kolmogorov-Smirnov (KS) test, available in SciPy\n    as `scipy.stats.ks_1samp`. Suppose we wish to test whether the following\n    data:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> x = stats.uniform.rvs(size=75, random_state=rng)\n\n    were sampled from a normal distribution. To perform a KS test, the\n    empirical distribution function of the observed data will be compared\n    against the (theoretical) cumulative distribution function of a normal\n    distribution. Of course, to do this, the normal distribution under the null\n    hypothesis must be fully specified. This is commonly done by first fitting\n    the ``loc`` and ``scale`` parameters of the distribution to the observed\n    data, then performing the test.\n\n    >>> loc, scale = np.mean(x), np.std(x, ddof=1)\n    >>> cdf = stats.norm(loc, scale).cdf\n    >>> stats.ks_1samp(x, cdf)\n    KstestResult(statistic=0.1119257570456813,\n                 pvalue=0.2827756409939257,\n                 statistic_location=0.7751845155861765,\n                 statistic_sign=-1)\n\n    An advantage of the KS-test is that the p-value - the probability of\n    obtaining a value of the test statistic under the null hypothesis as\n    extreme as the value obtained from the observed data - can be calculated\n    exactly and efficiently. `goodness_of_fit` can only approximate these\n    results.\n\n    >>> known_params = {'loc': loc, 'scale': scale}\n    >>> res = stats.goodness_of_fit(stats.norm, x, known_params=known_params,\n    ...                             statistic='ks', random_state=rng)\n    >>> res.statistic, res.pvalue\n    (0.1119257570456813, 0.2788)\n\n    The statistic matches exactly, but the p-value is estimated by forming\n    a \"Monte Carlo null distribution\", that is, by explicitly drawing random\n    samples from `scipy.stats.norm` with the provided parameters and\n    calculating the stastic for each. The fraction of these statistic values\n    at least as extreme as ``res.statistic`` approximates the exact p-value\n    calculated by `scipy.stats.ks_1samp`.\n\n    However, in many cases, we would prefer to test only that the data were\n    sampled from one of *any* member of the normal distribution family, not\n    specifically from the normal distribution with the location and scale\n    fitted to the observed sample. In this case, Lilliefors [6]_ argued that\n    the KS test is far too conservative (that is, the p-value overstates\n    the actual probability of rejecting a true null hypothesis) and thus lacks\n    power - the ability to reject the null hypothesis when the null hypothesis\n    is actually false.\n    Indeed, our p-value above is approximately 0.28, which is far too large\n    to reject the null hypothesis at any common significance level.\n\n    Consider why this might be. Note that in the KS test above, the statistic\n    always compares data against the CDF of a normal distribution fitted to the\n    *observed data*. This tends to reduce the value of the statistic for the\n    observed data, but it is \"unfair\" when computing the statistic for other\n    samples, such as those we randomly draw to form the Monte Carlo null\n    distribution. It is easy to correct for this: whenever we compute the KS\n    statistic of a sample, we use the CDF of a normal distribution fitted\n    to *that sample*. The null distribution in this case has not been\n    calculated exactly and is tyically approximated using Monte Carlo methods\n    as described above. This is where `goodness_of_fit` excels.\n\n    >>> res = stats.goodness_of_fit(stats.norm, x, statistic='ks',\n    ...                             random_state=rng)\n    >>> res.statistic, res.pvalue\n    (0.1119257570456813, 0.0196)\n\n    Indeed, this p-value is much smaller, and small enough to (correctly)\n    reject the null hypothesis at common significance levels, including 5% and\n    2.5%.\n\n    However, the KS statistic is not very sensitive to all deviations from\n    normality. The original advantage of the KS statistic was the ability\n    to compute the null distribution theoretically, but a more sensitive\n    statistic - resulting in a higher test power - can be used now that we can\n    approximate the null distribution\n    computationally. The Anderson-Darling statistic [1]_ tends to be more\n    sensitive, and critical values of the this statistic have been tabulated\n    for various significance levels and sample sizes using Monte Carlo methods.\n\n    >>> res = stats.anderson(x, 'norm')\n    >>> print(res.statistic)\n    1.2139573337497467\n    >>> print(res.critical_values)\n    [0.549 0.625 0.75  0.875 1.041]\n    >>> print(res.significance_level)\n    [15.  10.   5.   2.5  1. ]\n\n    Here, the observed value of the statistic exceeds the critical value\n    corresponding with a 1% significance level. This tells us that the p-value\n    of the observed data is less than 1%, but what is it? We could interpolate\n    from these (already-interpolated) values, but `goodness_of_fit` can\n    estimate it directly.\n\n    >>> res = stats.goodness_of_fit(stats.norm, x, statistic='ad',\n    ...                             random_state=rng)\n    >>> res.statistic, res.pvalue\n    (1.2139573337497467, 0.0034)\n\n    A further advantage is that use of `goodness_of_fit` is not limited to\n    a particular set of distributions or conditions on which parameters\n    are known versus which must be estimated from data. Instead,\n    `goodness_of_fit` can estimate p-values relatively quickly for any\n    distribution with a sufficiently fast and reliable ``fit`` method. For\n    instance, here we perform a goodness of fit test using the Cramer-von Mises\n    statistic against the Rayleigh distribution with known location and unknown\n    scale.\n\n    >>> rng = np.random.default_rng()\n    >>> x = stats.chi(df=2.2, loc=0, scale=2).rvs(size=1000, random_state=rng)\n    >>> res = stats.goodness_of_fit(stats.rayleigh, x, statistic='cvm',\n    ...                             known_params={'loc': 0}, random_state=rng)\n\n    This executes fairly quickly, but to check the reliability of the ``fit``\n    method, we should inspect the fit result.\n\n    >>> res.fit_result  # location is as specified, and scale is reasonable\n      params: FitParams(loc=0.0, scale=2.1026719844231243)\n     success: True\n     message: 'The fit was performed successfully.'\n    >>> import matplotlib.pyplot as plt  # matplotlib must be installed to plot\n    >>> res.fit_result.plot()\n    >>> plt.show()\n\n    If the distribution is not fit to the observed data as well as possible,\n    the test may not control the type I error rate, that is, the chance of\n    rejecting the null hypothesis even when it is true.\n\n    We should also look for extreme outliers in the null distribution that\n    may be caused by unreliable fitting. These do not necessarily invalidate\n    the result, but they tend to reduce the test's power.\n\n    >>> _, ax = plt.subplots()\n    >>> ax.hist(np.log10(res.null_distribution))\n    >>> ax.set_xlabel(\"log10 of CVM statistic under the null hypothesis\")\n    >>> ax.set_ylabel(\"Frequency\")\n    >>> ax.set_title(\"Histogram of the Monte Carlo null distribution\")\n    >>> plt.show()\n\n    This plot seems reassuring.\n\n    If ``fit`` method is working reliably, and if the distribution of the test\n    statistic is not particularly sensitive to the values of the fitted\n    parameters, then the p-value provided by `goodness_of_fit` is expected to\n    be a good approximation.\n\n    >>> res.statistic, res.pvalue\n    (0.2231991510248692, 0.0525)\n\n    ",
    "scipy.stats.gstd": "\n    Calculate the geometric standard deviation of an array.\n\n    The geometric standard deviation describes the spread of a set of numbers\n    where the geometric mean is preferred. It is a multiplicative factor, and\n    so a dimensionless quantity.\n\n    It is defined as the exponential of the standard deviation of the\n    natural logarithms of the observations.\n\n    Parameters\n    ----------\n    a : array_like\n        An array containing finite, strictly positive, real numbers.\n\n        .. deprecated:: 1.14.0\n            Support for masked array input was deprecated in\n            SciPy 1.14.0 and will be removed in version 1.16.0.\n\n    axis : int, tuple or None, optional\n        Axis along which to operate. Default is 0. If None, compute over\n        the whole array `a`.\n    ddof : int, optional\n        Degree of freedom correction in the calculation of the\n        geometric standard deviation. Default is 1.\n\n    Returns\n    -------\n    gstd : ndarray or float\n        An array of the geometric standard deviation. If `axis` is None or `a`\n        is a 1d array a float is returned.\n\n    See Also\n    --------\n    gmean : Geometric mean\n    numpy.std : Standard deviation\n    gzscore : Geometric standard score\n\n    Notes\n    -----\n    Mathematically, the sample geometric standard deviation :math:`s_G` can be\n    defined in terms of the natural logarithms of the observations\n    :math:`y_i = \\log(x_i)`:\n\n    .. math::\n\n        s_G = \\exp(s), \\quad s = \\sqrt{\\frac{1}{n - d} \\sum_{i=1}^n (y_i - \\bar y)^2}\n\n    where :math:`n` is the number of observations, :math:`d` is the adjustment `ddof`\n    to the degrees of freedom, and :math:`\\bar y` denotes the mean of the natural\n    logarithms of the observations. Note that the default ``ddof=1`` is different from\n    the default value used by similar functions, such as `numpy.std` and `numpy.var`.\n\n    When an observation is infinite, the geometric standard deviation is\n    NaN (undefined). Non-positive observations will also produce NaNs in the\n    output because the *natural* logarithm (as opposed to the *complex*\n    logarithm) is defined and finite only for positive reals.\n    The geometric standard deviation is sometimes confused with the exponential\n    of the standard deviation, ``exp(std(a))``. Instead, the geometric standard\n    deviation is ``exp(std(log(a)))``.\n\n    References\n    ----------\n    .. [1] \"Geometric standard deviation\", *Wikipedia*,\n           https://en.wikipedia.org/wiki/Geometric_standard_deviation.\n    .. [2] Kirkwood, T. B., \"Geometric means and measures of dispersion\",\n           Biometrics, vol. 35, pp. 908-909, 1979\n\n    Examples\n    --------\n    Find the geometric standard deviation of a log-normally distributed sample.\n    Note that the standard deviation of the distribution is one; on a\n    log scale this evaluates to approximately ``exp(1)``.\n\n    >>> import numpy as np\n    >>> from scipy.stats import gstd\n    >>> rng = np.random.default_rng()\n    >>> sample = rng.lognormal(mean=0, sigma=1, size=1000)\n    >>> gstd(sample)\n    2.810010162475324\n\n    Compute the geometric standard deviation of a multidimensional array and\n    of a given axis.\n\n    >>> a = np.arange(1, 25).reshape(2, 3, 4)\n    >>> gstd(a, axis=None)\n    2.2944076136018947\n    >>> gstd(a, axis=2)\n    array([[1.82424757, 1.22436866, 1.13183117],\n           [1.09348306, 1.07244798, 1.05914985]])\n    >>> gstd(a, axis=(1,2))\n    array([2.12939215, 1.22120169])\n\n    ",
    "scipy.stats.gumbel_l": "A left-skewed Gumbel continuous random variable.\n\n    As an instance of the `rv_continuous` class, `gumbel_l` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    gumbel_r, gompertz, genextreme\n\n    Notes\n    -----\n    The probability density function for `gumbel_l` is:\n\n    .. math::\n\n        f(x) = \\exp(x - e^x)\n\n    The Gumbel distribution is sometimes referred to as a type I Fisher-Tippett\n    distribution.  It is also related to the extreme value distribution,\n    log-Weibull and Gompertz distributions.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``gumbel_l.pdf(x, loc, scale)`` is identically\n    equivalent to ``gumbel_l.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import gumbel_l\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    \n    >>> mean, var, skew, kurt = gumbel_l.stats(moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(gumbel_l.ppf(0.01),\n    ...                 gumbel_l.ppf(0.99), 100)\n    >>> ax.plot(x, gumbel_l.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='gumbel_l pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = gumbel_l()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = gumbel_l.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], gumbel_l.cdf(vals))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = gumbel_l.rvs(size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.gumbel_r": "A right-skewed Gumbel continuous random variable.\n\n    As an instance of the `rv_continuous` class, `gumbel_r` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    gumbel_l, gompertz, genextreme\n\n    Notes\n    -----\n    The probability density function for `gumbel_r` is:\n\n    .. math::\n\n        f(x) = \\exp(-(x + e^{-x}))\n\n    The Gumbel distribution is sometimes referred to as a type I Fisher-Tippett\n    distribution.  It is also related to the extreme value distribution,\n    log-Weibull and Gompertz distributions.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``gumbel_r.pdf(x, loc, scale)`` is identically\n    equivalent to ``gumbel_r.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import gumbel_r\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    \n    >>> mean, var, skew, kurt = gumbel_r.stats(moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(gumbel_r.ppf(0.01),\n    ...                 gumbel_r.ppf(0.99), 100)\n    >>> ax.plot(x, gumbel_r.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='gumbel_r pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = gumbel_r()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = gumbel_r.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], gumbel_r.cdf(vals))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = gumbel_r.rvs(size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.gzscore": "\n    Compute the geometric standard score.\n\n    Compute the geometric z score of each strictly positive value in the\n    sample, relative to the geometric mean and standard deviation.\n    Mathematically the geometric z score can be evaluated as::\n\n        gzscore = log(a/gmu) / log(gsigma)\n\n    where ``gmu`` (resp. ``gsigma``) is the geometric mean (resp. standard\n    deviation).\n\n    Parameters\n    ----------\n    a : array_like\n        Sample data.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over\n        the whole array `a`.\n    ddof : int, optional\n        Degrees of freedom correction in the calculation of the\n        standard deviation. Default is 0.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan. 'propagate' returns nan,\n        'raise' throws an error, 'omit' performs the calculations ignoring nan\n        values. Default is 'propagate'.  Note that when the value is 'omit',\n        nans in the input also propagate to the output, but they do not affect\n        the geometric z scores computed for the non-nan values.\n\n    Returns\n    -------\n    gzscore : array_like\n        The geometric z scores, standardized by geometric mean and geometric\n        standard deviation of input array `a`.\n\n    See Also\n    --------\n    gmean : Geometric mean\n    gstd : Geometric standard deviation\n    zscore : Standard score\n\n    Notes\n    -----\n    This function preserves ndarray subclasses, and works also with\n    matrices and masked arrays (it uses ``asanyarray`` instead of\n    ``asarray`` for parameters).\n\n    .. versionadded:: 1.8\n\n    References\n    ----------\n    .. [1] \"Geometric standard score\", *Wikipedia*,\n           https://en.wikipedia.org/wiki/Geometric_standard_deviation#Geometric_standard_score.\n\n    Examples\n    --------\n    Draw samples from a log-normal distribution:\n\n    >>> import numpy as np\n    >>> from scipy.stats import zscore, gzscore\n    >>> import matplotlib.pyplot as plt\n\n    >>> rng = np.random.default_rng()\n    >>> mu, sigma = 3., 1.  # mean and standard deviation\n    >>> x = rng.lognormal(mu, sigma, size=500)\n\n    Display the histogram of the samples:\n\n    >>> fig, ax = plt.subplots()\n    >>> ax.hist(x, 50)\n    >>> plt.show()\n\n    Display the histogram of the samples standardized by the classical zscore.\n    Distribution is rescaled but its shape is unchanged.\n\n    >>> fig, ax = plt.subplots()\n    >>> ax.hist(zscore(x), 50)\n    >>> plt.show()\n\n    Demonstrate that the distribution of geometric zscores is rescaled and\n    quasinormal:\n\n    >>> fig, ax = plt.subplots()\n    >>> ax.hist(gzscore(x), 50)\n    >>> plt.show()\n\n    ",
    "scipy.stats.halfcauchy": "A Half-Cauchy continuous random variable.\n\n    As an instance of the `rv_continuous` class, `halfcauchy` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `halfcauchy` is:\n\n    .. math::\n\n        f(x) = \\frac{2}{\\pi (1 + x^2)}\n\n    for :math:`x \\ge 0`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``halfcauchy.pdf(x, loc, scale)`` is identically\n    equivalent to ``halfcauchy.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import halfcauchy\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    \n    >>> mean, var, skew, kurt = halfcauchy.stats(moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(halfcauchy.ppf(0.01),\n    ...                 halfcauchy.ppf(0.99), 100)\n    >>> ax.plot(x, halfcauchy.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='halfcauchy pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = halfcauchy()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = halfcauchy.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], halfcauchy.cdf(vals))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = halfcauchy.rvs(size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.halfgennorm": "The upper half of a generalized normal continuous random variable.\n\n    As an instance of the `rv_continuous` class, `halfgennorm` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(beta, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, beta, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, beta, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, beta, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, beta, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, beta, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, beta, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, beta, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, beta, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, beta, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(beta, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(beta, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(beta,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(beta, loc=0, scale=1)\n        Median of the distribution.\n    mean(beta, loc=0, scale=1)\n        Mean of the distribution.\n    var(beta, loc=0, scale=1)\n        Variance of the distribution.\n    std(beta, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, beta, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    gennorm : generalized normal distribution\n    expon : exponential distribution\n    halfnorm : half normal distribution\n\n    Notes\n    -----\n    The probability density function for `halfgennorm` is:\n\n    .. math::\n\n        f(x, \\beta) = \\frac{\\beta}{\\Gamma(1/\\beta)} \\exp(-|x|^\\beta)\n\n    for :math:`x, \\beta > 0`. :math:`\\Gamma` is the gamma function\n    (`scipy.special.gamma`).\n\n    `halfgennorm` takes ``beta`` as a shape parameter for :math:`\\beta`.\n    For :math:`\\beta = 1`, it is identical to an exponential distribution.\n    For :math:`\\beta = 2`, it is identical to a half normal distribution\n    (with ``scale=1/sqrt(2)``).\n\n    References\n    ----------\n\n    .. [1] \"Generalized normal distribution, Version 1\",\n           https://en.wikipedia.org/wiki/Generalized_normal_distribution#Version_1\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import halfgennorm\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> beta = 0.675\n    >>> mean, var, skew, kurt = halfgennorm.stats(beta, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(halfgennorm.ppf(0.01, beta),\n    ...                 halfgennorm.ppf(0.99, beta), 100)\n    >>> ax.plot(x, halfgennorm.pdf(x, beta),\n    ...        'r-', lw=5, alpha=0.6, label='halfgennorm pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = halfgennorm(beta)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = halfgennorm.ppf([0.001, 0.5, 0.999], beta)\n    >>> np.allclose([0.001, 0.5, 0.999], halfgennorm.cdf(vals, beta))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = halfgennorm.rvs(beta, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.halflogistic": "A half-logistic continuous random variable.\n\n    As an instance of the `rv_continuous` class, `halflogistic` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `halflogistic` is:\n\n    .. math::\n\n        f(x) = \\frac{ 2 e^{-x} }{ (1+e^{-x})^2 }\n             = \\frac{1}{2} \\text{sech}(x/2)^2\n\n    for :math:`x \\ge 0`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``halflogistic.pdf(x, loc, scale)`` is identically\n    equivalent to ``halflogistic.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] Asgharzadeh et al (2011). \"Comparisons of Methods of Estimation for the\n           Half-Logistic Distribution\". Selcuk J. Appl. Math. 93-108.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import halflogistic\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    \n    >>> mean, var, skew, kurt = halflogistic.stats(moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(halflogistic.ppf(0.01),\n    ...                 halflogistic.ppf(0.99), 100)\n    >>> ax.plot(x, halflogistic.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='halflogistic pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = halflogistic()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = halflogistic.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], halflogistic.cdf(vals))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = halflogistic.rvs(size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.halfnorm": "A half-normal continuous random variable.\n\n    As an instance of the `rv_continuous` class, `halfnorm` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `halfnorm` is:\n\n    .. math::\n\n        f(x) = \\sqrt{2/\\pi} \\exp(-x^2 / 2)\n\n    for :math:`x >= 0`.\n\n    `halfnorm` is a special case of `chi` with ``df=1``.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``halfnorm.pdf(x, loc, scale)`` is identically\n    equivalent to ``halfnorm.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import halfnorm\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    \n    >>> mean, var, skew, kurt = halfnorm.stats(moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(halfnorm.ppf(0.01),\n    ...                 halfnorm.ppf(0.99), 100)\n    >>> ax.plot(x, halfnorm.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='halfnorm pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = halfnorm()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = halfnorm.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], halfnorm.cdf(vals))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = halfnorm.rvs(size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.hmean": "    \n\n\nCalculate the weighted harmonic mean along the specified axis.\n\nThe weighted harmonic mean of the array :math:`a_i` associated to weights\n:math:`w_i` is:\n\n.. math::\n\n    \\frac{ \\sum_{i=1}^n w_i }{ \\sum_{i=1}^n \\frac{w_i}{a_i} } \\, ,\n\nand, with equal weights, it gives:\n\n.. math::\n\n    \\frac{ n }{ \\sum_{i=1}^n \\frac{1}{a_i} } \\, .\n\nParameters\n----------\na : array_like\n    Input array, masked array or object that can be converted to an array.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\ndtype : dtype, optional\n    Type of the returned array and of the accumulator in which the\n    elements are summed. If `dtype` is not specified, it defaults to the\n    dtype of `a`, unless `a` has an integer `dtype` with a precision less\n    than that of the default platform integer. In that case, the default\n    platform integer is used.\nweights : array_like, optional\n    The weights array can either be 1-D (in which case its length must be\n    the size of `a` along the given `axis`) or of the same shape as `a`.\n    Default is None, which gives each value a weight of 1.0.\n    \n    .. versionadded:: 1.9\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nhmean : ndarray\n    See `dtype` parameter above.\n\nSee Also\n--------\n\n:func:`numpy.mean`\n    Arithmetic average\n:func:`numpy.average`\n    Weighted average\n:func:`gmean`\n    Geometric mean\n\n\nNotes\n-----\nThe harmonic mean is computed over a single dimension of the input\narray, axis=0 by default, or all values in the array if axis=None.\nfloat64 intermediate and return values are used for integer inputs.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] \"Weighted Harmonic Mean\", *Wikipedia*,\n       https://en.wikipedia.org/wiki/Harmonic_mean#Weighted_harmonic_mean\n.. [2] Ferger, F., \"The nature and use of the harmonic mean\", Journal of\n       the American Statistical Association, vol. 26, pp. 36-40, 1931\n\nExamples\n--------\n>>> from scipy.stats import hmean\n>>> hmean([1, 4])\n1.6000000000000001\n>>> hmean([1, 2, 3, 4, 5, 6, 7])\n2.6997245179063363\n>>> hmean([1, 4, 7], weights=[3, 1, 3])\n1.9029126213592233\n",
    "scipy.stats.hypergeom": "A hypergeometric discrete random variable.\n\n    The hypergeometric distribution models drawing objects from a bin.\n    `M` is the total number of objects, `n` is total number of Type I objects.\n    The random variate represents the number of Type I objects in `N` drawn\n    without replacement from the total population.\n\n    As an instance of the `rv_discrete` class, `hypergeom` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(M, n, N, loc=0, size=1, random_state=None)\n        Random variates.\n    pmf(k, M, n, N, loc=0)\n        Probability mass function.\n    logpmf(k, M, n, N, loc=0)\n        Log of the probability mass function.\n    cdf(k, M, n, N, loc=0)\n        Cumulative distribution function.\n    logcdf(k, M, n, N, loc=0)\n        Log of the cumulative distribution function.\n    sf(k, M, n, N, loc=0)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(k, M, n, N, loc=0)\n        Log of the survival function.\n    ppf(q, M, n, N, loc=0)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, M, n, N, loc=0)\n        Inverse survival function (inverse of ``sf``).\n    stats(M, n, N, loc=0, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(M, n, N, loc=0)\n        (Differential) entropy of the RV.\n    expect(func, args=(M, n, N), loc=0, lb=None, ub=None, conditional=False)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(M, n, N, loc=0)\n        Median of the distribution.\n    mean(M, n, N, loc=0)\n        Mean of the distribution.\n    var(M, n, N, loc=0)\n        Variance of the distribution.\n    std(M, n, N, loc=0)\n        Standard deviation of the distribution.\n    interval(confidence, M, n, N, loc=0)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The symbols used to denote the shape parameters (`M`, `n`, and `N`) are not\n    universally accepted.  See the Examples for a clarification of the\n    definitions used here.\n\n    The probability mass function is defined as,\n\n    .. math:: p(k, M, n, N) = \\frac{\\binom{n}{k} \\binom{M - n}{N - k}}\n                                   {\\binom{M}{N}}\n\n    for :math:`k \\in [\\max(0, N - M + n), \\min(n, N)]`, where the binomial\n    coefficients are defined as,\n\n    .. math:: \\binom{n}{k} \\equiv \\frac{n!}{k! (n - k)!}.\n\n    The probability mass function above is defined in the \"standardized\" form.\n    To shift distribution use the ``loc`` parameter.\n    Specifically, ``hypergeom.pmf(k, M, n, N, loc)`` is identically\n    equivalent to ``hypergeom.pmf(k - loc, M, n, N)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import hypergeom\n    >>> import matplotlib.pyplot as plt\n\n    Suppose we have a collection of 20 animals, of which 7 are dogs.  Then if\n    we want to know the probability of finding a given number of dogs if we\n    choose at random 12 of the 20 animals, we can initialize a frozen\n    distribution and plot the probability mass function:\n\n    >>> [M, n, N] = [20, 7, 12]\n    >>> rv = hypergeom(M, n, N)\n    >>> x = np.arange(0, n+1)\n    >>> pmf_dogs = rv.pmf(x)\n\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> ax.plot(x, pmf_dogs, 'bo')\n    >>> ax.vlines(x, 0, pmf_dogs, lw=2)\n    >>> ax.set_xlabel('# of dogs in our group of chosen animals')\n    >>> ax.set_ylabel('hypergeom PMF')\n    >>> plt.show()\n\n    Instead of using a frozen distribution we can also use `hypergeom`\n    methods directly.  To for example obtain the cumulative distribution\n    function, use:\n\n    >>> prb = hypergeom.cdf(x, M, n, N)\n\n    And to generate random numbers:\n\n    >>> R = hypergeom.rvs(M, n, N, size=10)\n\n    See Also\n    --------\n    nhypergeom, binom, nbinom\n\n    ",
    "scipy.stats.hypsecant": "A hyperbolic secant continuous random variable.\n\n    As an instance of the `rv_continuous` class, `hypsecant` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `hypsecant` is:\n\n    .. math::\n\n        f(x) = \\frac{1}{\\pi} \\text{sech}(x)\n\n    for a real number :math:`x`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``hypsecant.pdf(x, loc, scale)`` is identically\n    equivalent to ``hypsecant.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import hypsecant\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    \n    >>> mean, var, skew, kurt = hypsecant.stats(moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(hypsecant.ppf(0.01),\n    ...                 hypsecant.ppf(0.99), 100)\n    >>> ax.plot(x, hypsecant.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='hypsecant pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = hypsecant()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = hypsecant.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], hypsecant.cdf(vals))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = hypsecant.rvs(size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.invgamma": "An inverted gamma continuous random variable.\n\n    As an instance of the `rv_continuous` class, `invgamma` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, a, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, a, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, a, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, a, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, a, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, a, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, a, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, a, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(a, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, loc=0, scale=1)\n        Median of the distribution.\n    mean(a, loc=0, scale=1)\n        Mean of the distribution.\n    var(a, loc=0, scale=1)\n        Variance of the distribution.\n    std(a, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, a, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `invgamma` is:\n\n    .. math::\n\n        f(x, a) = \\frac{x^{-a-1}}{\\Gamma(a)} \\exp(-\\frac{1}{x})\n\n    for :math:`x >= 0`, :math:`a > 0`. :math:`\\Gamma` is the gamma function\n    (`scipy.special.gamma`).\n\n    `invgamma` takes ``a`` as a shape parameter for :math:`a`.\n\n    `invgamma` is a special case of `gengamma` with ``c=-1``, and it is a\n    different parameterization of the scaled inverse chi-squared distribution.\n    Specifically, if the scaled inverse chi-squared distribution is\n    parameterized with degrees of freedom :math:`\\nu` and scaling parameter\n    :math:`\\tau^2`, then it can be modeled using `invgamma` with\n    ``a=`` :math:`\\nu/2` and ``scale=`` :math:`\\nu \\tau^2/2`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``invgamma.pdf(x, a, loc, scale)`` is identically\n    equivalent to ``invgamma.pdf(y, a) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import invgamma\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a = 4.07\n    >>> mean, var, skew, kurt = invgamma.stats(a, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(invgamma.ppf(0.01, a),\n    ...                 invgamma.ppf(0.99, a), 100)\n    >>> ax.plot(x, invgamma.pdf(x, a),\n    ...        'r-', lw=5, alpha=0.6, label='invgamma pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = invgamma(a)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = invgamma.ppf([0.001, 0.5, 0.999], a)\n    >>> np.allclose([0.001, 0.5, 0.999], invgamma.cdf(vals, a))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = invgamma.rvs(a, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.invgauss": "An inverse Gaussian continuous random variable.\n\n    As an instance of the `rv_continuous` class, `invgauss` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(mu, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, mu, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, mu, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, mu, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, mu, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, mu, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, mu, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, mu, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, mu, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, mu, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(mu, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(mu, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(mu,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(mu, loc=0, scale=1)\n        Median of the distribution.\n    mean(mu, loc=0, scale=1)\n        Mean of the distribution.\n    var(mu, loc=0, scale=1)\n        Variance of the distribution.\n    std(mu, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, mu, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `invgauss` is:\n\n    .. math::\n\n        f(x; \\mu) = \\frac{1}{\\sqrt{2 \\pi x^3}}\n                    \\exp\\left(-\\frac{(x-\\mu)^2}{2 \\mu^2 x}\\right)\n\n    for :math:`x \\ge 0` and :math:`\\mu > 0`.\n\n    `invgauss` takes ``mu`` as a shape parameter for :math:`\\mu`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``invgauss.pdf(x, mu, loc, scale)`` is identically\n    equivalent to ``invgauss.pdf(y, mu) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    A common shape-scale parameterization of the inverse Gaussian distribution\n    has density\n\n    .. math::\n\n        f(x; \\nu, \\lambda) = \\sqrt{\\frac{\\lambda}{2 \\pi x^3}}\n                    \\exp\\left( -\\frac{\\lambda(x-\\nu)^2}{2 \\nu^2 x}\\right)\n\n    Using ``nu`` for :math:`\\nu` and ``lam`` for :math:`\\lambda`, this\n    parameterization is equivalent to the one above with ``mu = nu/lam``,\n    ``loc = 0``, and ``scale = lam``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import invgauss\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> mu = 0.145\n    >>> mean, var, skew, kurt = invgauss.stats(mu, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(invgauss.ppf(0.01, mu),\n    ...                 invgauss.ppf(0.99, mu), 100)\n    >>> ax.plot(x, invgauss.pdf(x, mu),\n    ...        'r-', lw=5, alpha=0.6, label='invgauss pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = invgauss(mu)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = invgauss.ppf([0.001, 0.5, 0.999], mu)\n    >>> np.allclose([0.001, 0.5, 0.999], invgauss.cdf(vals, mu))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = invgauss.rvs(mu, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.invweibull": "An inverted Weibull continuous random variable.\n\n    This distribution is also known as the Fr\u00e9chet distribution or the\n    type II extreme value distribution.\n\n    As an instance of the `rv_continuous` class, `invweibull` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `invweibull` is:\n\n    .. math::\n\n        f(x, c) = c x^{-c-1} \\exp(-x^{-c})\n\n    for :math:`x > 0`, :math:`c > 0`.\n\n    `invweibull` takes ``c`` as a shape parameter for :math:`c`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``invweibull.pdf(x, c, loc, scale)`` is identically\n    equivalent to ``invweibull.pdf(y, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    F.R.S. de Gusmao, E.M.M Ortega and G.M. Cordeiro, \"The generalized inverse\n    Weibull distribution\", Stat. Papers, vol. 52, pp. 591-619, 2011.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import invweibull\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c = 10.6\n    >>> mean, var, skew, kurt = invweibull.stats(c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(invweibull.ppf(0.01, c),\n    ...                 invweibull.ppf(0.99, c), 100)\n    >>> ax.plot(x, invweibull.pdf(x, c),\n    ...        'r-', lw=5, alpha=0.6, label='invweibull pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = invweibull(c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = invweibull.ppf([0.001, 0.5, 0.999], c)\n    >>> np.allclose([0.001, 0.5, 0.999], invweibull.cdf(vals, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = invweibull.rvs(c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.invwishart": "An inverse Wishart random variable.\n\n    The `df` keyword specifies the degrees of freedom. The `scale` keyword\n    specifies the scale matrix, which must be symmetric and positive definite.\n    In this context, the scale matrix is often interpreted in terms of a\n    multivariate normal covariance matrix.\n\n    Methods\n    -------\n    pdf(x, df, scale)\n        Probability density function.\n    logpdf(x, df, scale)\n        Log of the probability density function.\n    rvs(df, scale, size=1, random_state=None)\n        Draw random samples from an inverse Wishart distribution.\n    entropy(df, scale)\n        Differential entropy of the distribution.\n\n    Parameters\n    ----------\n    df : int\n        Degrees of freedom, must be greater than or equal to dimension of the\n        scale matrix\n    scale : array_like\n        Symmetric positive definite scale matrix of the distribution\n    seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n        Used for drawing random variates.\n        If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used, seeded\n        with seed.\n        If `seed` is already a ``RandomState`` or ``Generator`` instance,\n        then that object is used.\n        Default is `None`.\n\n    Raises\n    ------\n    scipy.linalg.LinAlgError\n        If the scale matrix `scale` is not positive definite.\n\n    See Also\n    --------\n    wishart\n\n    Notes\n    -----\n    \n\n    The scale matrix `scale` must be a symmetric positive definite\n    matrix. Singular matrices, including the symmetric positive semi-definite\n    case, are not supported. Symmetry is not checked; only the lower triangular\n    portion is used.\n\n    The inverse Wishart distribution is often denoted\n\n    .. math::\n\n        W_p^{-1}(\\nu, \\Psi)\n\n    where :math:`\\nu` is the degrees of freedom and :math:`\\Psi` is the\n    :math:`p \\times p` scale matrix.\n\n    The probability density function for `invwishart` has support over positive\n    definite matrices :math:`S`; if :math:`S \\sim W^{-1}_p(\\nu, \\Sigma)`,\n    then its PDF is given by:\n\n    .. math::\n\n        f(S) = \\frac{|\\Sigma|^\\frac{\\nu}{2}}{2^{ \\frac{\\nu p}{2} }\n               |S|^{\\frac{\\nu + p + 1}{2}} \\Gamma_p \\left(\\frac{\\nu}{2} \\right)}\n               \\exp\\left( -tr(\\Sigma S^{-1}) / 2 \\right)\n\n    If :math:`S \\sim W_p^{-1}(\\nu, \\Psi)` (inverse Wishart) then\n    :math:`S^{-1} \\sim W_p(\\nu, \\Psi^{-1})` (Wishart).\n\n    If the scale matrix is 1-dimensional and equal to one, then the inverse\n    Wishart distribution :math:`W_1(\\nu, 1)` collapses to the\n    inverse Gamma distribution with parameters shape = :math:`\\frac{\\nu}{2}`\n    and scale = :math:`\\frac{1}{2}`.\n\n    Instead of inverting a randomly generated Wishart matrix as described in [2],\n    here the algorithm in [4] is used to directly generate a random inverse-Wishart\n    matrix without inversion.\n\n    .. versionadded:: 0.16.0\n\n    References\n    ----------\n    .. [1] M.L. Eaton, \"Multivariate Statistics: A Vector Space Approach\",\n           Wiley, 1983.\n    .. [2] M.C. Jones, \"Generating Inverse Wishart Matrices\", Communications\n           in Statistics - Simulation and Computation, vol. 14.2, pp.511-514,\n           1985.\n    .. [3] Gupta, M. and Srivastava, S. \"Parametric Bayesian Estimation of\n           Differential Entropy and Relative Entropy\". Entropy 12, 818 - 843.\n           2010.\n    .. [4] S.D. Axen, \"Efficiently generating inverse-Wishart matrices and\n           their Cholesky factors\", :arXiv:`2310.15884v1`. 2023.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy.stats import invwishart, invgamma\n    >>> x = np.linspace(0.01, 1, 100)\n    >>> iw = invwishart.pdf(x, df=6, scale=1)\n    >>> iw[:3]\n    array([  1.20546865e-15,   5.42497807e-06,   4.45813929e-03])\n    >>> ig = invgamma.pdf(x, 6/2., scale=1./2)\n    >>> ig[:3]\n    array([  1.20546865e-15,   5.42497807e-06,   4.45813929e-03])\n    >>> plt.plot(x, iw)\n    >>> plt.show()\n\n    The input quantiles can be any shape of array, as long as the last\n    axis labels the components.\n\n    Alternatively, the object may be called (as a function) to fix the degrees\n    of freedom and scale parameters, returning a \"frozen\" inverse Wishart\n    random variable:\n\n    >>> rv = invwishart(df=1, scale=1)\n    >>> # Frozen object with the same methods but holding the given\n    >>> # degrees of freedom and scale fixed.\n\n    ",
    "scipy.stats.iqr": "    \n\n\nCompute the interquartile range of the data along the specified axis.\n\nThe interquartile range (IQR) is the difference between the 75th and\n25th percentile of the data. It is a measure of the dispersion\nsimilar to standard deviation or variance, but is much more robust\nagainst outliers [2]_.\n\nThe ``rng`` parameter allows this function to compute other\npercentile ranges than the actual IQR. For example, setting\n``rng=(0, 100)`` is equivalent to `numpy.ptp`.\n\nThe IQR of an empty array is `np.nan`.\n\n.. versionadded:: 0.18.0\n\nParameters\n----------\nx : array_like\n    Input array or object that can be converted to an array.\naxis : int or None, default: None\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nrng : Two-element sequence containing floats in range of [0,100] optional\n    Percentiles over which to compute the range. Each must be\n    between 0 and 100, inclusive. The default is the true IQR:\n    ``(25, 75)``. The order of the elements is not important.\nscale : scalar or str or array_like of reals, optional\n    The numerical value of scale will be divided out of the final\n    result. The following string value is also recognized:\n    \n      * 'normal' : Scale by\n        :math:`2 \\sqrt{2} erf^{-1}(\\frac{1}{2}) \\approx 1.349`.\n    \n    The default is 1.0.\n    Array-like `scale` of real dtype is also allowed, as long\n    as it broadcasts correctly to the output such that\n    ``out / scale`` is a valid operation. The output dimensions\n    depend on the input array, `x`, the `axis` argument, and the\n    `keepdims` flag.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\ninterpolation : str, optional\n    Specifies the interpolation method to use when the percentile\n    boundaries lie between two data points ``i`` and ``j``.\n    The following options are available (default is 'linear'):\n    \n      * 'linear': ``i + (j - i)*fraction``, where ``fraction`` is the\n        fractional part of the index surrounded by ``i`` and ``j``.\n      * 'lower': ``i``.\n      * 'higher': ``j``.\n      * 'nearest': ``i`` or ``j`` whichever is nearest.\n      * 'midpoint': ``(i + j)/2``.\n    \n    For NumPy >= 1.22.0, the additional options provided by the ``method``\n    keyword of `numpy.percentile` are also valid.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\niqr : scalar or ndarray\n    If ``axis=None``, a scalar is returned. If the input contains\n    integers or floats of smaller precision than ``np.float64``, then the\n    output data-type is ``np.float64``. Otherwise, the output data-type is\n    the same as that of the input.\n\nSee Also\n--------\n\n:func:`numpy.std`, :func:`numpy.var`\n    ..\n\nNotes\n-----\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] \"Interquartile range\" https://en.wikipedia.org/wiki/Interquartile_range\n.. [2] \"Robust measures of scale\" https://en.wikipedia.org/wiki/Robust_measures_of_scale\n.. [3] \"Quantile\" https://en.wikipedia.org/wiki/Quantile\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy.stats import iqr\n>>> x = np.array([[10, 7, 4], [3, 2, 1]])\n>>> x\narray([[10,  7,  4],\n       [ 3,  2,  1]])\n>>> iqr(x)\n4.0\n>>> iqr(x, axis=0)\narray([ 3.5,  2.5,  1.5])\n>>> iqr(x, axis=1)\narray([ 3.,  1.])\n>>> iqr(x, axis=1, keepdims=True)\narray([[ 3.],\n       [ 1.]])\n",
    "scipy.stats.irwinhall": "An Irwin-Hall (Uniform Sum) continuous random variable.\n\n    An `Irwin-Hall <https://en.wikipedia.org/wiki/Irwin-Hall_distribution/>`_\n    continuous random variable is the sum of :math:`n` independent\n    standard uniform random variables [1]_ [2]_.\n\n    As an instance of the `rv_continuous` class, `irwinhall` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(n, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, n, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, n, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, n, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, n, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, n, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, n, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, n, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, n, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, n, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(n, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(n, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(n,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(n, loc=0, scale=1)\n        Median of the distribution.\n    mean(n, loc=0, scale=1)\n        Mean of the distribution.\n    var(n, loc=0, scale=1)\n        Variance of the distribution.\n    std(n, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, n, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    Applications include `Rao's Spacing Test\n    <https://jammalam.faculty.pstat.ucsb.edu/html/favorite/test.htm>`_,\n    a more powerful alternative to the Rayleigh test\n    when the data are not unimodal, and radar [3]_.\n\n    Conveniently, the pdf and cdf are the :math:`n`-fold convolution of\n    the ones for the standard uniform distribution, which is also the\n    definition of the cardinal B-splines of degree :math:`n-1`\n    having knots evenly spaced from :math:`1` to :math:`n` [4]_ [5]_.\n\n    The Bates distribution, which represents the *mean* of statistically\n    independent, uniformly distributed random variables, is simply the\n    Irwin-Hall distribution scaled by :math:`1/n`. For example, the frozen\n    distribution ``bates = irwinhall(10, scale=1/10)`` represents the\n    distribution of the mean of 10 uniformly distributed random variables.\n    \n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``irwinhall.pdf(x, n, loc, scale)`` is identically\n    equivalent to ``irwinhall.pdf(y, n) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] P. Hall, \"The distribution of means for samples of size N drawn\n            from a population in which the variate takes values between 0 and 1,\n            all such values being equally probable\",\n            Biometrika, Volume 19, Issue 3-4, December 1927, Pages 240-244,\n            :doi:`10.1093/biomet/19.3-4.240`.\n    .. [2] J. O. Irwin, \"On the frequency distribution of the means of samples\n            from a population having any law of frequency with finite moments,\n            with special reference to Pearson's Type II,\n            Biometrika, Volume 19, Issue 3-4, December 1927, Pages 225-239,\n            :doi:`0.1093/biomet/19.3-4.225`.\n    .. [3] K. Buchanan, T. Adeyemi, C. Flores-Molina, S. Wheeland and D. Overturf, \n            \"Sidelobe behavior and bandwidth characteristics\n            of distributed antenna arrays,\"\n            2018 United States National Committee of\n            URSI National Radio Science Meeting (USNC-URSI NRSM),\n            Boulder, CO, USA, 2018, pp. 1-2.\n            https://www.usnc-ursi-archive.org/nrsm/2018/papers/B15-9.pdf.\n    .. [4] Amos Ron, \"Lecture 1: Cardinal B-splines and convolution operators\", p. 1\n            https://pages.cs.wisc.edu/~deboor/887/lec1new.pdf.\n    .. [5] Trefethen, N. (2012, July). B-splines and convolution. Chebfun. \n            Retrieved April 30, 2024, from http://www.chebfun.org/examples/approx/BSplineConv.html.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import irwinhall\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> n = 10\n    >>> mean, var, skew, kurt = irwinhall.stats(n, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(irwinhall.ppf(0.01, n),\n    ...                 irwinhall.ppf(0.99, n), 100)\n    >>> ax.plot(x, irwinhall.pdf(x, n),\n    ...        'r-', lw=5, alpha=0.6, label='irwinhall pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = irwinhall(n)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = irwinhall.ppf([0.001, 0.5, 0.999], n)\n    >>> np.allclose([0.001, 0.5, 0.999], irwinhall.cdf(vals, n))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = irwinhall.rvs(n, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n    ",
    "scipy.stats.jarque_bera": "    \n\n\nPerform the Jarque-Bera goodness of fit test on sample data.\n\nThe Jarque-Bera test tests whether the sample data has the skewness and\nkurtosis matching a normal distribution.\n\nNote that this test only works for a large enough number of data samples\n(>2000) as the test statistic asymptotically has a Chi-squared distribution\nwith 2 degrees of freedom.\n\nParameters\n----------\nx : array_like\n    Observations of a random variable.\naxis : int or None, default: None\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nresult : SignificanceResult\n    An object with the following attributes:\n    \n    statistic : float\n        The test statistic.\n    pvalue : float\n        The p-value for the hypothesis test.\n\nNotes\n-----\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] Jarque, C. and Bera, A. (1980) \"Efficient tests for normality,\n       homoscedasticity and serial independence of regression residuals\",\n       6 Econometric Letters 255-259.\n.. [2] Shapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test\n       for normality (complete samples). Biometrika, 52(3/4), 591-611.\n.. [3] B. Phipson and G. K. Smyth. \"Permutation P-values Should Never Be\n       Zero: Calculating Exact P-values When Permutations Are Randomly\n       Drawn.\" Statistical Applications in Genetics and Molecular Biology\n       9.1 (2010).\n.. [4] Panagiotakos, D. B. (2008). The value of p-value in biomedical\n       research. The open cardiovascular medicine journal, 2, 97.\n\nExamples\n--------\nSuppose we wish to infer from measurements whether the weights of adult\nhuman males in a medical study are not normally distributed [2]_.\nThe weights (lbs) are recorded in the array ``x`` below.\n\n>>> import numpy as np\n>>> x = np.array([148, 154, 158, 160, 161, 162, 166, 170, 182, 195, 236])\n\nThe Jarque-Bera test begins by computing a statistic based on the sample\nskewness and kurtosis.\n\n>>> from scipy import stats\n>>> res = stats.jarque_bera(x)\n>>> res.statistic\n6.982848237344646\n\nBecause the normal distribution has zero skewness and zero\n(\"excess\" or \"Fisher\") kurtosis, the value of this statistic tends to be\nlow for samples drawn from a normal distribution.\n\nThe test is performed by comparing the observed value of the statistic\nagainst the null distribution: the distribution of statistic values derived\nunder the null hypothesis that the weights were drawn from a normal\ndistribution.\nFor the Jarque-Bera test, the null distribution for very large samples is\nthe chi-squared distribution with two degrees of freedom.\n\n>>> import matplotlib.pyplot as plt\n>>> dist = stats.chi2(df=2)\n>>> jb_val = np.linspace(0, 11, 100)\n>>> pdf = dist.pdf(jb_val)\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> def jb_plot(ax):  # we'll reuse this\n...     ax.plot(jb_val, pdf)\n...     ax.set_title(\"Jarque-Bera Null Distribution\")\n...     ax.set_xlabel(\"statistic\")\n...     ax.set_ylabel(\"probability density\")\n>>> jb_plot(ax)\n>>> plt.show()\n\nThe comparison is quantified by the p-value: the proportion of values in\nthe null distribution greater than or equal to the observed value of the\nstatistic.\n\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> jb_plot(ax)\n>>> pvalue = dist.sf(res.statistic)\n>>> annotation = (f'p-value={pvalue:.6f}\\n(shaded area)')\n>>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n>>> _ = ax.annotate(annotation, (7.5, 0.01), (8, 0.05), arrowprops=props)\n>>> i = jb_val >= res.statistic  # indices of more extreme statistic values\n>>> ax.fill_between(jb_val[i], y1=0, y2=pdf[i])\n>>> ax.set_xlim(0, 11)\n>>> ax.set_ylim(0, 0.3)\n>>> plt.show()\n>>> res.pvalue\n0.03045746622458189\n\nIf the p-value is \"small\" - that is, if there is a low probability of\nsampling data from a normally distributed population that produces such an\nextreme value of the statistic - this may be taken as evidence against\nthe null hypothesis in favor of the alternative: the weights were not\ndrawn from a normal distribution. Note that:\n\n- The inverse is not true; that is, the test is not used to provide\n  evidence for the null hypothesis.\n- The threshold for values that will be considered \"small\" is a choice that\n  should be made before the data is analyzed [3]_ with consideration of the\n  risks of both false positives (incorrectly rejecting the null hypothesis)\n  and false negatives (failure to reject a false null hypothesis).\n\nNote that the chi-squared distribution provides an asymptotic approximation\nof the null distribution; it is only accurate for samples with many\nobservations. For small samples like ours, `scipy.stats.monte_carlo_test`\nmay provide a more accurate, albeit stochastic, approximation of the\nexact p-value.\n\n>>> def statistic(x, axis):\n...     # underlying calculation of the Jarque Bera statistic\n...     s = stats.skew(x, axis=axis)\n...     k = stats.kurtosis(x, axis=axis)\n...     return x.shape[axis]/6 * (s**2 + k**2/4)\n>>> res = stats.monte_carlo_test(x, stats.norm.rvs, statistic,\n...                              alternative='greater')\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> jb_plot(ax)\n>>> ax.hist(res.null_distribution, np.linspace(0, 10, 50),\n...         density=True)\n>>> ax.legend(['aymptotic approximation (many observations)',\n...            'Monte Carlo approximation (11 observations)'])\n>>> plt.show()\n>>> res.pvalue\n0.0097  # may vary\n\nFurthermore, despite their stochastic nature, p-values computed in this way\ncan be used to exactly control the rate of false rejections of the null\nhypothesis [4]_.\n",
    "scipy.stats.jf_skew_t": "Jones and Faddy skew-t distribution.\n\n    As an instance of the `rv_continuous` class, `jf_skew_t` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, a, b, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, a, b, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, a, b, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, a, b, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, a, b, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, a, b, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, a, b, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, b, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, a, b, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(a, b, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, b, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, b, loc=0, scale=1)\n        Median of the distribution.\n    mean(a, b, loc=0, scale=1)\n        Mean of the distribution.\n    var(a, b, loc=0, scale=1)\n        Variance of the distribution.\n    std(a, b, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, a, b, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `jf_skew_t` is:\n\n    .. math::\n\n        f(x; a, b) = C_{a,b}^{-1}\n                    \\left(1+\\frac{x}{\\left(a+b+x^2\\right)^{1/2}}\\right)^{a+1/2}\n                    \\left(1-\\frac{x}{\\left(a+b+x^2\\right)^{1/2}}\\right)^{b+1/2}\n\n    for real numbers :math:`a>0` and :math:`b>0`, where\n    :math:`C_{a,b} = 2^{a+b-1}B(a,b)(a+b)^{1/2}`, and :math:`B` denotes the\n    beta function (`scipy.special.beta`).\n\n    When :math:`a<b`, the distribution is negatively skewed, and when\n    :math:`a>b`, the distribution is positively skewed. If :math:`a=b`, then\n    we recover the `t` distribution with :math:`2a` degrees of freedom.\n\n    `jf_skew_t` takes :math:`a` and :math:`b` as shape parameters.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``jf_skew_t.pdf(x, a, b, loc, scale)`` is identically\n    equivalent to ``jf_skew_t.pdf(y, a, b) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] M.C. Jones and M.J. Faddy. \"A skew extension of the t distribution,\n           with applications\" *Journal of the Royal Statistical Society*.\n           Series B (Statistical Methodology) 65, no. 1 (2003): 159-174.\n           :doi:`10.1111/1467-9868.00378`\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import jf_skew_t\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a, b = 8, 4\n    >>> mean, var, skew, kurt = jf_skew_t.stats(a, b, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(jf_skew_t.ppf(0.01, a, b),\n    ...                 jf_skew_t.ppf(0.99, a, b), 100)\n    >>> ax.plot(x, jf_skew_t.pdf(x, a, b),\n    ...        'r-', lw=5, alpha=0.6, label='jf_skew_t pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = jf_skew_t(a, b)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = jf_skew_t.ppf([0.001, 0.5, 0.999], a, b)\n    >>> np.allclose([0.001, 0.5, 0.999], jf_skew_t.cdf(vals, a, b))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = jf_skew_t.rvs(a, b, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.johnsonsb": "A Johnson SB continuous random variable.\n\n    As an instance of the `rv_continuous` class, `johnsonsb` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, a, b, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, a, b, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, a, b, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, a, b, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, a, b, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, a, b, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, a, b, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, b, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, a, b, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(a, b, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, b, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, b, loc=0, scale=1)\n        Median of the distribution.\n    mean(a, b, loc=0, scale=1)\n        Mean of the distribution.\n    var(a, b, loc=0, scale=1)\n        Variance of the distribution.\n    std(a, b, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, a, b, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    johnsonsu\n\n    Notes\n    -----\n    The probability density function for `johnsonsb` is:\n\n    .. math::\n\n        f(x, a, b) = \\frac{b}{x(1-x)}  \\phi(a + b \\log \\frac{x}{1-x} )\n\n    where :math:`x`, :math:`a`, and :math:`b` are real scalars; :math:`b > 0`\n    and :math:`x \\in [0,1]`.  :math:`\\phi` is the pdf of the normal\n    distribution.\n\n    `johnsonsb` takes :math:`a` and :math:`b` as shape parameters.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``johnsonsb.pdf(x, a, b, loc, scale)`` is identically\n    equivalent to ``johnsonsb.pdf(y, a, b) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import johnsonsb\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a, b = 4.32, 3.18\n    >>> mean, var, skew, kurt = johnsonsb.stats(a, b, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(johnsonsb.ppf(0.01, a, b),\n    ...                 johnsonsb.ppf(0.99, a, b), 100)\n    >>> ax.plot(x, johnsonsb.pdf(x, a, b),\n    ...        'r-', lw=5, alpha=0.6, label='johnsonsb pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = johnsonsb(a, b)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = johnsonsb.ppf([0.001, 0.5, 0.999], a, b)\n    >>> np.allclose([0.001, 0.5, 0.999], johnsonsb.cdf(vals, a, b))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = johnsonsb.rvs(a, b, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.johnsonsu": "A Johnson SU continuous random variable.\n\n    As an instance of the `rv_continuous` class, `johnsonsu` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, a, b, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, a, b, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, a, b, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, a, b, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, a, b, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, a, b, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, a, b, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, b, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, a, b, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(a, b, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, b, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, b, loc=0, scale=1)\n        Median of the distribution.\n    mean(a, b, loc=0, scale=1)\n        Mean of the distribution.\n    var(a, b, loc=0, scale=1)\n        Variance of the distribution.\n    std(a, b, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, a, b, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    johnsonsb\n\n    Notes\n    -----\n    The probability density function for `johnsonsu` is:\n\n    .. math::\n\n        f(x, a, b) = \\frac{b}{\\sqrt{x^2 + 1}}\n                     \\phi(a + b \\log(x + \\sqrt{x^2 + 1}))\n\n    where :math:`x`, :math:`a`, and :math:`b` are real scalars; :math:`b > 0`.\n    :math:`\\phi` is the pdf of the normal distribution.\n\n    `johnsonsu` takes :math:`a` and :math:`b` as shape parameters.\n\n    The first four central moments are calculated according to the formulas\n    in [1]_.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``johnsonsu.pdf(x, a, b, loc, scale)`` is identically\n    equivalent to ``johnsonsu.pdf(y, a, b) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] Taylor Enterprises. \"Johnson Family of Distributions\".\n       https://variation.com/wp-content/distribution_analyzer_help/hs126.htm\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import johnsonsu\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a, b = 2.55, 2.25\n    >>> mean, var, skew, kurt = johnsonsu.stats(a, b, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(johnsonsu.ppf(0.01, a, b),\n    ...                 johnsonsu.ppf(0.99, a, b), 100)\n    >>> ax.plot(x, johnsonsu.pdf(x, a, b),\n    ...        'r-', lw=5, alpha=0.6, label='johnsonsu pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = johnsonsu(a, b)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = johnsonsu.ppf([0.001, 0.5, 0.999], a, b)\n    >>> np.allclose([0.001, 0.5, 0.999], johnsonsu.cdf(vals, a, b))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = johnsonsu.rvs(a, b, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.kappa3": "Kappa 3 parameter distribution.\n\n    As an instance of the `rv_continuous` class, `kappa3` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, a, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, a, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, a, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, a, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, a, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, a, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, a, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, a, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(a, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, loc=0, scale=1)\n        Median of the distribution.\n    mean(a, loc=0, scale=1)\n        Mean of the distribution.\n    var(a, loc=0, scale=1)\n        Variance of the distribution.\n    std(a, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, a, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `kappa3` is:\n\n    .. math::\n\n        f(x, a) = a (a + x^a)^{-(a + 1)/a}\n\n    for :math:`x > 0` and :math:`a > 0`.\n\n    `kappa3` takes ``a`` as a shape parameter for :math:`a`.\n\n    References\n    ----------\n    P.W. Mielke and E.S. Johnson, \"Three-Parameter Kappa Distribution Maximum\n    Likelihood and Likelihood Ratio Tests\", Methods in Weather Research,\n    701-707, (September, 1973),\n    :doi:`10.1175/1520-0493(1973)101<0701:TKDMLE>2.3.CO;2`\n\n    B. Kumphon, \"Maximum Entropy and Maximum Likelihood Estimation for the\n    Three-Parameter Kappa Distribution\", Open Journal of Statistics, vol 2,\n    415-419 (2012), :doi:`10.4236/ojs.2012.24050`\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``kappa3.pdf(x, a, loc, scale)`` is identically\n    equivalent to ``kappa3.pdf(y, a) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import kappa3\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a = 1\n    >>> mean, var, skew, kurt = kappa3.stats(a, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(kappa3.ppf(0.01, a),\n    ...                 kappa3.ppf(0.99, a), 100)\n    >>> ax.plot(x, kappa3.pdf(x, a),\n    ...        'r-', lw=5, alpha=0.6, label='kappa3 pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = kappa3(a)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = kappa3.ppf([0.001, 0.5, 0.999], a)\n    >>> np.allclose([0.001, 0.5, 0.999], kappa3.cdf(vals, a))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = kappa3.rvs(a, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.kappa4": "Kappa 4 parameter distribution.\n\n    As an instance of the `rv_continuous` class, `kappa4` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(h, k, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, h, k, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, h, k, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, h, k, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, h, k, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, h, k, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, h, k, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, h, k, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, h, k, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, h, k, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(h, k, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(h, k, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(h, k), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(h, k, loc=0, scale=1)\n        Median of the distribution.\n    mean(h, k, loc=0, scale=1)\n        Mean of the distribution.\n    var(h, k, loc=0, scale=1)\n        Variance of the distribution.\n    std(h, k, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, h, k, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for kappa4 is:\n\n    .. math::\n\n        f(x, h, k) = (1 - k x)^{1/k - 1} (1 - h (1 - k x)^{1/k})^{1/h-1}\n\n    if :math:`h` and :math:`k` are not equal to 0.\n\n    If :math:`h` or :math:`k` are zero then the pdf can be simplified:\n\n    h = 0 and k != 0::\n\n        kappa4.pdf(x, h, k) = (1.0 - k*x)**(1.0/k - 1.0)*\n                              exp(-(1.0 - k*x)**(1.0/k))\n\n    h != 0 and k = 0::\n\n        kappa4.pdf(x, h, k) = exp(-x)*(1.0 - h*exp(-x))**(1.0/h - 1.0)\n\n    h = 0 and k = 0::\n\n        kappa4.pdf(x, h, k) = exp(-x)*exp(-exp(-x))\n\n    kappa4 takes :math:`h` and :math:`k` as shape parameters.\n\n    The kappa4 distribution returns other distributions when certain\n    :math:`h` and :math:`k` values are used.\n\n    +------+-------------+----------------+------------------+\n    | h    | k=0.0       | k=1.0          | -inf<=k<=inf     |\n    +======+=============+================+==================+\n    | -1.0 | Logistic    |                | Generalized      |\n    |      |             |                | Logistic(1)      |\n    |      |             |                |                  |\n    |      | logistic(x) |                |                  |\n    +------+-------------+----------------+------------------+\n    |  0.0 | Gumbel      | Reverse        | Generalized      |\n    |      |             | Exponential(2) | Extreme Value    |\n    |      |             |                |                  |\n    |      | gumbel_r(x) |                | genextreme(x, k) |\n    +------+-------------+----------------+------------------+\n    |  1.0 | Exponential | Uniform        | Generalized      |\n    |      |             |                | Pareto           |\n    |      |             |                |                  |\n    |      | expon(x)    | uniform(x)     | genpareto(x, -k) |\n    +------+-------------+----------------+------------------+\n\n    (1) There are at least five generalized logistic distributions.\n        Four are described here:\n        https://en.wikipedia.org/wiki/Generalized_logistic_distribution\n        The \"fifth\" one is the one kappa4 should match which currently\n        isn't implemented in scipy:\n        https://en.wikipedia.org/wiki/Talk:Generalized_logistic_distribution\n        https://www.mathwave.com/help/easyfit/html/analyses/distributions/gen_logistic.html\n    (2) This distribution is currently not in scipy.\n\n    References\n    ----------\n    J.C. Finney, \"Optimization of a Skewed Logistic Distribution With Respect\n    to the Kolmogorov-Smirnov Test\", A Dissertation Submitted to the Graduate\n    Faculty of the Louisiana State University and Agricultural and Mechanical\n    College, (August, 2004),\n    https://digitalcommons.lsu.edu/gradschool_dissertations/3672\n\n    J.R.M. Hosking, \"The four-parameter kappa distribution\". IBM J. Res.\n    Develop. 38 (3), 25 1-258 (1994).\n\n    B. Kumphon, A. Kaew-Man, P. Seenoi, \"A Rainfall Distribution for the Lampao\n    Site in the Chi River Basin, Thailand\", Journal of Water Resource and\n    Protection, vol. 4, 866-869, (2012).\n    :doi:`10.4236/jwarp.2012.410101`\n\n    C. Winchester, \"On Estimation of the Four-Parameter Kappa Distribution\", A\n    Thesis Submitted to Dalhousie University, Halifax, Nova Scotia, (March\n    2000).\n    http://www.nlc-bnc.ca/obj/s4/f2/dsk2/ftp01/MQ57336.pdf\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``kappa4.pdf(x, h, k, loc, scale)`` is identically\n    equivalent to ``kappa4.pdf(y, h, k) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import kappa4\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> h, k = 0.1, 0\n    >>> mean, var, skew, kurt = kappa4.stats(h, k, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(kappa4.ppf(0.01, h, k),\n    ...                 kappa4.ppf(0.99, h, k), 100)\n    >>> ax.plot(x, kappa4.pdf(x, h, k),\n    ...        'r-', lw=5, alpha=0.6, label='kappa4 pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = kappa4(h, k)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = kappa4.ppf([0.001, 0.5, 0.999], h, k)\n    >>> np.allclose([0.001, 0.5, 0.999], kappa4.cdf(vals, h, k))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = kappa4.rvs(h, k, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.kendalltau": "Calculate Kendall's tau, a correlation measure for ordinal data.\n\n    Kendall's tau is a measure of the correspondence between two rankings.\n    Values close to 1 indicate strong agreement, and values close to -1\n    indicate strong disagreement. This implements two variants of Kendall's\n    tau: tau-b (the default) and tau-c (also known as Stuart's tau-c). These\n    differ only in how they are normalized to lie within the range -1 to 1;\n    the hypothesis tests (their p-values) are identical. Kendall's original\n    tau-a is not implemented separately because both tau-b and tau-c reduce\n    to tau-a in the absence of ties.\n\n    Parameters\n    ----------\n    x, y : array_like\n        Arrays of rankings, of the same shape. If arrays are not 1-D, they\n        will be flattened to 1-D.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n          * 'propagate': returns nan\n          * 'raise': throws an error\n          * 'omit': performs the calculations ignoring nan values\n\n    method : {'auto', 'asymptotic', 'exact'}, optional\n        Defines which method is used to calculate the p-value [5]_.\n        The following options are available (default is 'auto'):\n\n          * 'auto': selects the appropriate method based on a trade-off\n            between speed and accuracy\n          * 'asymptotic': uses a normal approximation valid for large samples\n          * 'exact': computes the exact p-value, but can only be used if no ties\n            are present. As the sample size increases, the 'exact' computation\n            time may grow and the result may lose some precision.\n    variant : {'b', 'c'}, optional\n        Defines which variant of Kendall's tau is returned. Default is 'b'.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n        The following options are available:\n\n        * 'two-sided': the rank correlation is nonzero\n        * 'less': the rank correlation is negative (less than zero)\n        * 'greater':  the rank correlation is positive (greater than zero)\n\n    Returns\n    -------\n    res : SignificanceResult\n        An object containing attributes:\n\n        statistic : float\n           The tau statistic.\n        pvalue : float\n           The p-value for a hypothesis test whose null hypothesis is\n           an absence of association, tau = 0.\n\n    See Also\n    --------\n    spearmanr : Calculates a Spearman rank-order correlation coefficient.\n    theilslopes : Computes the Theil-Sen estimator for a set of points (x, y).\n    weightedtau : Computes a weighted version of Kendall's tau.\n\n    Notes\n    -----\n    The definition of Kendall's tau that is used is [2]_::\n\n      tau_b = (P - Q) / sqrt((P + Q + T) * (P + Q + U))\n\n      tau_c = 2 (P - Q) / (n**2 * (m - 1) / m)\n\n    where P is the number of concordant pairs, Q the number of discordant\n    pairs, T the number of ties only in `x`, and U the number of ties only in\n    `y`.  If a tie occurs for the same pair in both `x` and `y`, it is not\n    added to either T or U. n is the total number of samples, and m is the\n    number of unique values in either `x` or `y`, whichever is smaller.\n\n    References\n    ----------\n    .. [1] Maurice G. Kendall, \"A New Measure of Rank Correlation\", Biometrika\n           Vol. 30, No. 1/2, pp. 81-93, 1938.\n    .. [2] Maurice G. Kendall, \"The treatment of ties in ranking problems\",\n           Biometrika Vol. 33, No. 3, pp. 239-251. 1945.\n    .. [3] Gottfried E. Noether, \"Elements of Nonparametric Statistics\", John\n           Wiley & Sons, 1967.\n    .. [4] Peter M. Fenwick, \"A new data structure for cumulative frequency\n           tables\", Software: Practice and Experience, Vol. 24, No. 3,\n           pp. 327-336, 1994.\n    .. [5] Maurice G. Kendall, \"Rank Correlation Methods\" (4th Edition),\n           Charles Griffin & Co., 1970.\n    .. [6] Kershenobich, D., Fierro, F. J., & Rojkind, M. (1970). The\n           relationship between the free pool of proline and collagen content\n           in human liver cirrhosis. The Journal of Clinical Investigation,\n           49(12), 2246-2249.\n    .. [7] Hollander, M., Wolfe, D. A., & Chicken, E. (2013). Nonparametric\n           statistical methods. John Wiley & Sons.\n    .. [8] B. Phipson and G. K. Smyth. \"Permutation P-values Should Never Be\n           Zero: Calculating Exact P-values When Permutations Are Randomly\n           Drawn.\" Statistical Applications in Genetics and Molecular Biology\n           9.1 (2010).\n\n    Examples\n    --------\n    Consider the following data from [6]_, which studied the relationship\n    between free proline (an amino acid) and total collagen (a protein often\n    found in connective tissue) in unhealthy human livers.\n\n    The ``x`` and ``y`` arrays below record measurements of the two compounds.\n    The observations are paired: each free proline measurement was taken from\n    the same liver as the total collagen measurement at the same index.\n\n    >>> import numpy as np\n    >>> # total collagen (mg/g dry weight of liver)\n    >>> x = np.array([7.1, 7.1, 7.2, 8.3, 9.4, 10.5, 11.4])\n    >>> # free proline (\u03bc mole/g dry weight of liver)\n    >>> y = np.array([2.8, 2.9, 2.8, 2.6, 3.5, 4.6, 5.0])\n\n    These data were analyzed in [7]_ using Spearman's correlation coefficient,\n    a statistic similar to Kendall's tau in that it is also sensitive to\n    ordinal correlation between the samples. Let's perform an analogous study\n    using Kendall's tau.\n\n    >>> from scipy import stats\n    >>> res = stats.kendalltau(x, y)\n    >>> res.statistic\n    0.5499999999999999\n\n    The value of this statistic tends to be high (close to 1) for samples with\n    a strongly positive ordinal correlation, low (close to -1) for samples with\n    a strongly negative ordinal correlation, and small in magnitude (close to\n    zero) for samples with weak ordinal correlation.\n\n    The test is performed by comparing the observed value of the\n    statistic against the null distribution: the distribution of statistic\n    values derived under the null hypothesis that total collagen and free\n    proline measurements are independent.\n\n    For this test, the null distribution for large samples without ties is\n    approximated as the normal distribution with variance\n    ``(2*(2*n + 5))/(9*n*(n - 1))``, where ``n = len(x)``.\n\n    >>> import matplotlib.pyplot as plt\n    >>> n = len(x)  # len(x) == len(y)\n    >>> var = (2*(2*n + 5))/(9*n*(n - 1))\n    >>> dist = stats.norm(scale=np.sqrt(var))\n    >>> z_vals = np.linspace(-1.25, 1.25, 100)\n    >>> pdf = dist.pdf(z_vals)\n    >>> fig, ax = plt.subplots(figsize=(8, 5))\n    >>> def plot(ax):  # we'll reuse this\n    ...     ax.plot(z_vals, pdf)\n    ...     ax.set_title(\"Kendall Tau Test Null Distribution\")\n    ...     ax.set_xlabel(\"statistic\")\n    ...     ax.set_ylabel(\"probability density\")\n    >>> plot(ax)\n    >>> plt.show()\n\n    The comparison is quantified by the p-value: the proportion of values in\n    the null distribution as extreme or more extreme than the observed\n    value of the statistic. In a two-sided test in which the statistic is\n    positive, elements of the null distribution greater than the transformed\n    statistic and elements of the null distribution less than the negative of\n    the observed statistic are both considered \"more extreme\".\n\n    >>> fig, ax = plt.subplots(figsize=(8, 5))\n    >>> plot(ax)\n    >>> pvalue = dist.cdf(-res.statistic) + dist.sf(res.statistic)\n    >>> annotation = (f'p-value={pvalue:.4f}\\n(shaded area)')\n    >>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n    >>> _ = ax.annotate(annotation, (0.65, 0.15), (0.8, 0.3), arrowprops=props)\n    >>> i = z_vals >= res.statistic\n    >>> ax.fill_between(z_vals[i], y1=0, y2=pdf[i], color='C0')\n    >>> i = z_vals <= -res.statistic\n    >>> ax.fill_between(z_vals[i], y1=0, y2=pdf[i], color='C0')\n    >>> ax.set_xlim(-1.25, 1.25)\n    >>> ax.set_ylim(0, 0.5)\n    >>> plt.show()\n    >>> res.pvalue\n    0.09108705741631495  # approximate p-value\n\n    Note that there is slight disagreement between the shaded area of the curve\n    and the p-value returned by `kendalltau`. This is because our data has\n    ties, and we have neglected a tie correction to the null distribution\n    variance that `kendalltau` performs. For samples without ties, the shaded\n    areas of our plot and p-value returned by `kendalltau` would match exactly.\n\n    If the p-value is \"small\" - that is, if there is a low probability of\n    sampling data from independent distributions that produces such an extreme\n    value of the statistic - this may be taken as evidence against the null\n    hypothesis in favor of the alternative: the distribution of total collagen\n    and free proline are *not* independent. Note that:\n\n    - The inverse is not true; that is, the test is not used to provide\n      evidence for the null hypothesis.\n    - The threshold for values that will be considered \"small\" is a choice that\n      should be made before the data is analyzed [8]_ with consideration of the\n      risks of both false positives (incorrectly rejecting the null hypothesis)\n      and false negatives (failure to reject a false null hypothesis).\n    - Small p-values are not evidence for a *large* effect; rather, they can\n      only provide evidence for a \"significant\" effect, meaning that they are\n      unlikely to have occurred under the null hypothesis.\n\n    For samples without ties of moderate size, `kendalltau` can compute the\n    p-value exactly. However, in the presence of ties, `kendalltau` resorts\n    to an asymptotic approximation. Nonetheles, we can use a permutation test\n    to compute the null distribution exactly: Under the null hypothesis that\n    total collagen and free proline are independent, each of the free proline\n    measurements were equally likely to have been observed with any of the\n    total collagen measurements. Therefore, we can form an *exact* null\n    distribution by calculating the statistic under each possible pairing of\n    elements between ``x`` and ``y``.\n\n    >>> def statistic(x):  # explore all possible pairings by permuting `x`\n    ...     return stats.kendalltau(x, y).statistic  # ignore pvalue\n    >>> ref = stats.permutation_test((x,), statistic,\n    ...                              permutation_type='pairings')\n    >>> fig, ax = plt.subplots(figsize=(8, 5))\n    >>> plot(ax)\n    >>> bins = np.linspace(-1.25, 1.25, 25)\n    >>> ax.hist(ref.null_distribution, bins=bins, density=True)\n    >>> ax.legend(['aymptotic approximation\\n(many observations)',\n    ...            'exact null distribution'])\n    >>> plot(ax)\n    >>> plt.show()\n    >>> ref.pvalue\n    0.12222222222222222  # exact p-value\n\n    Note that there is significant disagreement between the exact p-value\n    calculated here and the approximation returned by `kendalltau` above. For\n    small samples with ties, consider performing a permutation test for more\n    accurate results.\n\n    ",
    "scipy.stats.kruskal": "    \n\n\nCompute the Kruskal-Wallis H-test for independent samples.\n\nThe Kruskal-Wallis H-test tests the null hypothesis that the population\nmedian of all of the groups are equal.  It is a non-parametric version of\nANOVA.  The test works on 2 or more independent samples, which may have\ndifferent sizes.  Note that rejecting the null hypothesis does not\nindicate which of the groups differs.  Post hoc comparisons between\ngroups are required to determine which groups are different.\n\nParameters\n----------\nsample1, sample2, ... : array_like\n    Two or more arrays with the sample measurements can be given as\n    arguments. Samples must be one-dimensional.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nstatistic : float\n    The Kruskal-Wallis H statistic, corrected for ties.\npvalue : float\n    The p-value for the test using the assumption that H has a chi\n    square distribution. The p-value returned is the survival function of\n    the chi square distribution evaluated at H.\n\nSee Also\n--------\n\n:func:`f_oneway`\n    1-way ANOVA.\n:func:`mannwhitneyu`\n    Mann-Whitney rank test on two samples.\n:func:`friedmanchisquare`\n    Friedman test for repeated measurements.\n\n\nNotes\n-----\nDue to the assumption that H has a chi square distribution, the number\nof samples in each group must not be too small.  A typical rule is\nthat each sample must have at least 5 measurements.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] W. H. Kruskal & W. W. Wallis, \"Use of Ranks in\n   One-Criterion Variance Analysis\", Journal of the American Statistical\n   Association, Vol. 47, Issue 260, pp. 583-621, 1952.\n.. [2] https://en.wikipedia.org/wiki/Kruskal-Wallis_one-way_analysis_of_variance\n\nExamples\n--------\n>>> from scipy import stats\n>>> x = [1, 3, 5, 7, 9]\n>>> y = [2, 4, 6, 8, 10]\n>>> stats.kruskal(x, y)\nKruskalResult(statistic=0.2727272727272734, pvalue=0.6015081344405895)\n\n>>> x = [1, 1, 1]\n>>> y = [2, 2, 2]\n>>> z = [2, 2]\n>>> stats.kruskal(x, y, z)\nKruskalResult(statistic=7.0, pvalue=0.0301973834223185)\n",
    "scipy.stats.ks_1samp": "    \n\n\nPerforms the one-sample Kolmogorov-Smirnov test for goodness of fit.\n\nThis test compares the underlying distribution F(x) of a sample\nagainst a given continuous distribution G(x). See Notes for a description\nof the available null and alternative hypotheses.\n\nParameters\n----------\nx : array_like\n    a 1-D array of observations of iid random variables.\ncdf : callable\n    callable used to calculate the cdf.\nargs : tuple, sequence, optional\n    Distribution parameters, used with `cdf`.\nalternative : {'two-sided', 'less', 'greater'}, optional\n    Defines the null and alternative hypotheses. Default is 'two-sided'.\n    Please see explanations in the Notes below.\nmethod : {'auto', 'exact', 'approx', 'asymp'}, optional\n    Defines the distribution used for calculating the p-value.\n    The following options are available (default is 'auto'):\n    \n      * 'auto' : selects one of the other options.\n      * 'exact' : uses the exact distribution of test statistic.\n      * 'approx' : approximates the two-sided probability with twice\n        the one-sided probability\n      * 'asymp': uses asymptotic distribution of test statistic\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nres: KstestResult\n    An object containing attributes:\n    \n    statistic : float\n        KS test statistic, either D+, D-, or D (the maximum of the two)\n    pvalue : float\n        One-tailed or two-tailed p-value.\n    statistic_location : float\n        Value of `x` corresponding with the KS statistic; i.e., the\n        distance between the empirical distribution function and the\n        hypothesized cumulative distribution function is measured at this\n        observation.\n    statistic_sign : int\n        +1 if the KS statistic is the maximum positive difference between\n        the empirical distribution function and the hypothesized cumulative\n        distribution function (D+); -1 if the KS statistic is the maximum\n        negative difference (D-).\n\nSee Also\n--------\n\n:func:`ks_2samp`, :func:`kstest`\n    ..\n\nNotes\n-----\nThere are three options for the null and corresponding alternative\nhypothesis that can be selected using the `alternative` parameter.\n\n- `two-sided`: The null hypothesis is that the two distributions are\n  identical, F(x)=G(x) for all x; the alternative is that they are not\n  identical.\n\n- `less`: The null hypothesis is that F(x) >= G(x) for all x; the\n  alternative is that F(x) < G(x) for at least one x.\n\n- `greater`: The null hypothesis is that F(x) <= G(x) for all x; the\n  alternative is that F(x) > G(x) for at least one x.\n\nNote that the alternative hypotheses describe the *CDFs* of the\nunderlying distributions, not the observed values. For example,\nsuppose x1 ~ F and x2 ~ G. If F(x) > G(x) for all x, the values in\nx1 tend to be less than those in x2.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nExamples\n--------\nSuppose we wish to test the null hypothesis that a sample is distributed\naccording to the standard normal.\nWe choose a confidence level of 95%; that is, we will reject the null\nhypothesis in favor of the alternative if the p-value is less than 0.05.\n\nWhen testing uniformly distributed data, we would expect the\nnull hypothesis to be rejected.\n\n>>> import numpy as np\n>>> from scipy import stats\n>>> rng = np.random.default_rng()\n>>> stats.ks_1samp(stats.uniform.rvs(size=100, random_state=rng),\n...                stats.norm.cdf)\nKstestResult(statistic=0.5001899973268688,\n             pvalue=1.1616392184763533e-23,\n             statistic_location=0.00047625268963724654,\n             statistic_sign=-1)\n\nIndeed, the p-value is lower than our threshold of 0.05, so we reject the\nnull hypothesis in favor of the default \"two-sided\" alternative: the data\nare *not* distributed according to the standard normal.\n\nWhen testing random variates from the standard normal distribution, we\nexpect the data to be consistent with the null hypothesis most of the time.\n\n>>> x = stats.norm.rvs(size=100, random_state=rng)\n>>> stats.ks_1samp(x, stats.norm.cdf)\nKstestResult(statistic=0.05345882212970396,\n             pvalue=0.9227159037744717,\n             statistic_location=-1.2451343873745018,\n             statistic_sign=1)\n\nAs expected, the p-value of 0.92 is not below our threshold of 0.05, so\nwe cannot reject the null hypothesis.\n\nSuppose, however, that the random variates are distributed according to\na normal distribution that is shifted toward greater values. In this case,\nthe cumulative density function (CDF) of the underlying distribution tends\nto be *less* than the CDF of the standard normal. Therefore, we would\nexpect the null hypothesis to be rejected with ``alternative='less'``:\n\n>>> x = stats.norm.rvs(size=100, loc=0.5, random_state=rng)\n>>> stats.ks_1samp(x, stats.norm.cdf, alternative='less')\nKstestResult(statistic=0.17482387821055168,\n             pvalue=0.001913921057766743,\n             statistic_location=0.3713830565352756,\n             statistic_sign=-1)\n\nand indeed, with p-value smaller than our threshold, we reject the null\nhypothesis in favor of the alternative.\n",
    "scipy.stats.ks_2samp": "    \n\n\nPerforms the two-sample Kolmogorov-Smirnov test for goodness of fit.\n\nThis test compares the underlying continuous distributions F(x) and G(x)\nof two independent samples.  See Notes for a description of the available\nnull and alternative hypotheses.\n\nParameters\n----------\ndata1, data2 : array_like, 1-Dimensional\n    Two arrays of sample observations assumed to be drawn from a continuous\n    distribution, sample sizes can be different.\nalternative : {'two-sided', 'less', 'greater'}, optional\n    Defines the null and alternative hypotheses. Default is 'two-sided'.\n    Please see explanations in the Notes below.\nmethod : {'auto', 'exact', 'asymp'}, optional\n    Defines the method used for calculating the p-value.\n    The following options are available (default is 'auto'):\n    \n      * 'auto' : use 'exact' for small size arrays, 'asymp' for large\n      * 'exact' : use exact distribution of test statistic\n      * 'asymp' : use asymptotic distribution of test statistic\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nres: KstestResult\n    An object containing attributes:\n    \n    statistic : float\n        KS test statistic.\n    pvalue : float\n        One-tailed or two-tailed p-value.\n    statistic_location : float\n        Value from `data1` or `data2` corresponding with the KS statistic;\n        i.e., the distance between the empirical distribution functions is\n        measured at this observation.\n    statistic_sign : int\n        +1 if the empirical distribution function of `data1` exceeds\n        the empirical distribution function of `data2` at\n        `statistic_location`, otherwise -1.\n\nSee Also\n--------\n\n:func:`kstest`, :func:`ks_1samp`, :func:`epps_singleton_2samp`, :func:`anderson_ksamp`\n    ..\n\nNotes\n-----\nThere are three options for the null and corresponding alternative\nhypothesis that can be selected using the `alternative` parameter.\n\n- `less`: The null hypothesis is that F(x) >= G(x) for all x; the\n  alternative is that F(x) < G(x) for at least one x. The statistic\n  is the magnitude of the minimum (most negative) difference between the\n  empirical distribution functions of the samples.\n\n- `greater`: The null hypothesis is that F(x) <= G(x) for all x; the\n  alternative is that F(x) > G(x) for at least one x. The statistic\n  is the maximum (most positive) difference between the empirical\n  distribution functions of the samples.\n\n- `two-sided`: The null hypothesis is that the two distributions are\n  identical, F(x)=G(x) for all x; the alternative is that they are not\n  identical. The statistic is the maximum absolute difference between the\n  empirical distribution functions of the samples.\n\nNote that the alternative hypotheses describe the *CDFs* of the\nunderlying distributions, not the observed values of the data. For example,\nsuppose x1 ~ F and x2 ~ G. If F(x) > G(x) for all x, the values in\nx1 tend to be less than those in x2.\n\nIf the KS statistic is large, then the p-value will be small, and this may\nbe taken as evidence against the null hypothesis in favor of the\nalternative.\n\nIf ``method='exact'``, `ks_2samp` attempts to compute an exact p-value,\nthat is, the probability under the null hypothesis of obtaining a test\nstatistic value as extreme as the value computed from the data.\nIf ``method='asymp'``, the asymptotic Kolmogorov-Smirnov distribution is\nused to compute an approximate p-value.\nIf ``method='auto'``, an exact p-value computation is attempted if both\nsample sizes are less than 10000; otherwise, the asymptotic method is used.\nIn any case, if an exact p-value calculation is attempted and fails, a\nwarning will be emitted, and the asymptotic p-value will be returned.\n\nThe 'two-sided' 'exact' computation computes the complementary probability\nand then subtracts from 1.  As such, the minimum probability it can return\nis about 1e-16.  While the algorithm itself is exact, numerical\nerrors may accumulate for large sample sizes.   It is most suited to\nsituations in which one of the sample sizes is only a few thousand.\n\nWe generally follow Hodges' treatment of Drion/Gnedenko/Korolyuk [1]_.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] Hodges, J.L. Jr.,  \"The Significance Probability of the Smirnov\n       Two-Sample Test,\" Arkiv fiur Matematik, 3, No. 43 (1958), 469-486.\n\nExamples\n--------\nSuppose we wish to test the null hypothesis that two samples were drawn\nfrom the same distribution.\nWe choose a confidence level of 95%; that is, we will reject the null\nhypothesis in favor of the alternative if the p-value is less than 0.05.\n\nIf the first sample were drawn from a uniform distribution and the second\nwere drawn from the standard normal, we would expect the null hypothesis\nto be rejected.\n\n>>> import numpy as np\n>>> from scipy import stats\n>>> rng = np.random.default_rng()\n>>> sample1 = stats.uniform.rvs(size=100, random_state=rng)\n>>> sample2 = stats.norm.rvs(size=110, random_state=rng)\n>>> stats.ks_2samp(sample1, sample2)\nKstestResult(statistic=0.5454545454545454,\n             pvalue=7.37417839555191e-15,\n             statistic_location=-0.014071496412861274,\n             statistic_sign=-1)\n\nIndeed, the p-value is lower than our threshold of 0.05, so we reject the\nnull hypothesis in favor of the default \"two-sided\" alternative: the data\nwere *not* drawn from the same distribution.\n\nWhen both samples are drawn from the same distribution, we expect the data\nto be consistent with the null hypothesis most of the time.\n\n>>> sample1 = stats.norm.rvs(size=105, random_state=rng)\n>>> sample2 = stats.norm.rvs(size=95, random_state=rng)\n>>> stats.ks_2samp(sample1, sample2)\nKstestResult(statistic=0.10927318295739348,\n             pvalue=0.5438289009927495,\n             statistic_location=-0.1670157701848795,\n             statistic_sign=-1)\n\nAs expected, the p-value of 0.54 is not below our threshold of 0.05, so\nwe cannot reject the null hypothesis.\n\nSuppose, however, that the first sample were drawn from\na normal distribution shifted toward greater values. In this case,\nthe cumulative density function (CDF) of the underlying distribution tends\nto be *less* than the CDF underlying the second sample. Therefore, we would\nexpect the null hypothesis to be rejected with ``alternative='less'``:\n\n>>> sample1 = stats.norm.rvs(size=105, loc=0.5, random_state=rng)\n>>> stats.ks_2samp(sample1, sample2, alternative='less')\nKstestResult(statistic=0.4055137844611529,\n             pvalue=3.5474563068855554e-08,\n             statistic_location=-0.13249370614972575,\n             statistic_sign=-1)\n\nand indeed, with p-value smaller than our threshold, we reject the null\nhypothesis in favor of the alternative.\n",
    "scipy.stats.ksone": "Kolmogorov-Smirnov one-sided test statistic distribution.\n\n    This is the distribution of the one-sided Kolmogorov-Smirnov (KS)\n    statistics :math:`D_n^+` and :math:`D_n^-`\n    for a finite sample size ``n >= 1`` (the shape parameter).\n\n    As an instance of the `rv_continuous` class, `ksone` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(n, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, n, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, n, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, n, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, n, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, n, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, n, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, n, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, n, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, n, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(n, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(n, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(n,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(n, loc=0, scale=1)\n        Median of the distribution.\n    mean(n, loc=0, scale=1)\n        Mean of the distribution.\n    var(n, loc=0, scale=1)\n        Variance of the distribution.\n    std(n, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, n, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    kstwobign, kstwo, kstest\n\n    Notes\n    -----\n    :math:`D_n^+` and :math:`D_n^-` are given by\n\n    .. math::\n\n        D_n^+ &= \\text{sup}_x (F_n(x) - F(x)),\\\\\n        D_n^- &= \\text{sup}_x (F(x) - F_n(x)),\\\\\n\n    where :math:`F` is a continuous CDF and :math:`F_n` is an empirical CDF.\n    `ksone` describes the distribution under the null hypothesis of the KS test\n    that the empirical CDF corresponds to :math:`n` i.i.d. random variates\n    with CDF :math:`F`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``ksone.pdf(x, n, loc, scale)`` is identically\n    equivalent to ``ksone.pdf(y, n) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] Birnbaum, Z. W. and Tingey, F.H. \"One-sided confidence contours\n       for probability distribution functions\", The Annals of Mathematical\n       Statistics, 22(4), pp 592-596 (1951).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import ksone\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n\n    Display the probability density function (``pdf``):\n\n    >>> n = 1e+03\n    >>> x = np.linspace(ksone.ppf(0.01, n),\n    ...                 ksone.ppf(0.99, n), 100)\n    >>> ax.plot(x, ksone.pdf(x, n),\n    ...         'r-', lw=5, alpha=0.6, label='ksone pdf')\n\n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n\n    Freeze the distribution and display the frozen ``pdf``:\n\n    >>> rv = ksone(n)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n\n    Check accuracy of ``cdf`` and ``ppf``:\n\n    >>> vals = ksone.ppf([0.001, 0.5, 0.999], n)\n    >>> np.allclose([0.001, 0.5, 0.999], ksone.cdf(vals, n))\n    True\n\n    ",
    "scipy.stats.kstat": "    \n\n\nReturn the `n` th k-statistic ( ``1<=n<=4`` so far).\n\nThe `n` th k-statistic ``k_n`` is the unique symmetric unbiased estimator of the\n`n` th cumulant :math:`\\kappa_n` [1]_ [2]_.\n\nParameters\n----------\ndata : array_like\n    Input array.\nn : int, {1, 2, 3, 4}, optional\n    Default is equal to 2.\naxis : int or None, default: None\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nkstat : float\n    The `n` th k-statistic.\n\nSee Also\n--------\n\n:func:`kstatvar`\n    Returns an unbiased estimator of the variance of the k-statistic\n:func:`moment`\n    Returns the n-th central moment about the mean for a sample.\n\n\nNotes\n-----\nFor a sample size :math:`n`, the first few k-statistics are given by\n\n.. math::\n\n    k_1 &= \\frac{S_1}{n}, \\\\\n    k_2 &= \\frac{nS_2 - S_1^2}{n(n-1)}, \\\\\n    k_3 &= \\frac{2S_1^3 - 3nS_1S_2 + n^2S_3}{n(n-1)(n-2)}, \\\\\n    k_4 &= \\frac{-6S_1^4 + 12nS_1^2S_2 - 3n(n-1)S_2^2 - 4n(n+1)S_1S_3\n    + n^2(n+1)S_4}{n (n-1)(n-2)(n-3)},\n\nwhere\n\n.. math::\n\n    S_r \\equiv \\sum_{i=1}^n X_i^r,\n\nand :math:`X_i` is the :math:`i` th data point.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] http://mathworld.wolfram.com/k-Statistic.html\n\n.. [2] http://mathworld.wolfram.com/Cumulant.html\n\nExamples\n--------\n>>> from scipy import stats\n>>> from numpy.random import default_rng\n>>> rng = default_rng()\n\nAs sample size increases, `n`-th moment and `n`-th k-statistic converge to the\nsame number (although they aren't identical). In the case of the normal\ndistribution, they converge to zero.\n\n>>> for i in range(2,8):\n...     x = rng.normal(size=10**i)\n...     m, k = stats.moment(x, 3), stats.kstat(x, 3)\n...     print(f\"{i=}: {m=:.3g}, {k=:.3g}, {(m-k)=:.3g}\")\ni=2: m=-0.631, k=-0.651, (m-k)=0.0194  # random\ni=3: m=0.0282, k=0.0283, (m-k)=-8.49e-05\ni=4: m=-0.0454, k=-0.0454, (m-k)=1.36e-05\ni=6: m=7.53e-05, k=7.53e-05, (m-k)=-2.26e-09\ni=7: m=0.00166, k=0.00166, (m-k)=-4.99e-09\ni=8: m=-2.88e-06 k=-2.88e-06, (m-k)=8.63e-13\n",
    "scipy.stats.kstatvar": "    \n\n\nReturn an unbiased estimator of the variance of the k-statistic.\n\nSee `kstat` and [1]_ for more details about the k-statistic.\n\nParameters\n----------\ndata : array_like\n    Input array.\nn : int, {1, 2}, optional\n    Default is equal to 2.\naxis : int or None, default: None\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nkstatvar : float\n    The `n` th k-statistic variance.\n\nSee Also\n--------\n\n:func:`kstat`\n    Returns the n-th k-statistic.\n:func:`moment`\n    Returns the n-th central moment about the mean for a sample.\n\n\nNotes\n-----\nUnbiased estimators of the variances of the first two k-statistics are given by\n\n.. math::\n\n    \\mathrm{var}(k_1) &= \\frac{k_2}{n}, \\\\\n    \\mathrm{var}(k_2) &= \\frac{2k_2^2n + (n-1)k_4}{n(n - 1)}.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] http://mathworld.wolfram.com/k-Statistic.html\n",
    "scipy.stats.kstest": "    \n\n\nPerforms the (one-sample or two-sample) Kolmogorov-Smirnov test for\ngoodness of fit.\n\nThe one-sample test compares the underlying distribution F(x) of a sample\nagainst a given distribution G(x). The two-sample test compares the\nunderlying distributions of two independent samples. Both tests are valid\nonly for continuous distributions.\n\nParameters\n----------\nrvs : str, array_like, or callable\n    If an array, it should be a 1-D array of observations of random\n    variables.\n    If a callable, it should be a function to generate random variables;\n    it is required to have a keyword argument `size`.\n    If a string, it should be the name of a distribution in `scipy.stats`,\n    which will be used to generate random variables.\ncdf : str, array_like or callable\n    If array_like, it should be a 1-D array of observations of random\n    variables, and the two-sample test is performed\n    (and rvs must be array_like).\n    If a callable, that callable is used to calculate the cdf.\n    If a string, it should be the name of a distribution in `scipy.stats`,\n    which will be used as the cdf function.\nargs : tuple, sequence, optional\n    Distribution parameters, used if `rvs` or `cdf` are strings or\n    callables.\nN : int, optional\n    Sample size if `rvs` is string or callable.  Default is 20.\nalternative : {'two-sided', 'less', 'greater'}, optional\n    Defines the null and alternative hypotheses. Default is 'two-sided'.\n    Please see explanations in the Notes below.\nmethod : {'auto', 'exact', 'approx', 'asymp'}, optional\n    Defines the distribution used for calculating the p-value.\n    The following options are available (default is 'auto'):\n    \n      * 'auto' : selects one of the other options.\n      * 'exact' : uses the exact distribution of test statistic.\n      * 'approx' : approximates the two-sided probability with twice the\n        one-sided probability\n      * 'asymp': uses asymptotic distribution of test statistic\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nres: KstestResult\n    An object containing attributes:\n    \n    statistic : float\n        KS test statistic, either D+, D-, or D (the maximum of the two)\n    pvalue : float\n        One-tailed or two-tailed p-value.\n    statistic_location : float\n        In a one-sample test, this is the value of `rvs`\n        corresponding with the KS statistic; i.e., the distance between\n        the empirical distribution function and the hypothesized cumulative\n        distribution function is measured at this observation.\n    \n        In a two-sample test, this is the value from `rvs` or `cdf`\n        corresponding with the KS statistic; i.e., the distance between\n        the empirical distribution functions is measured at this\n        observation.\n    statistic_sign : int\n        In a one-sample test, this is +1 if the KS statistic is the\n        maximum positive difference between the empirical distribution\n        function and the hypothesized cumulative distribution function\n        (D+); it is -1 if the KS statistic is the maximum negative\n        difference (D-).\n    \n        In a two-sample test, this is +1 if the empirical distribution\n        function of `rvs` exceeds the empirical distribution\n        function of `cdf` at `statistic_location`, otherwise -1.\n\nSee Also\n--------\n\n:func:`ks_1samp`, :func:`ks_2samp`\n    ..\n\nNotes\n-----\nThere are three options for the null and corresponding alternative\nhypothesis that can be selected using the `alternative` parameter.\n\n- `two-sided`: The null hypothesis is that the two distributions are\n  identical, F(x)=G(x) for all x; the alternative is that they are not\n  identical.\n\n- `less`: The null hypothesis is that F(x) >= G(x) for all x; the\n  alternative is that F(x) < G(x) for at least one x.\n\n- `greater`: The null hypothesis is that F(x) <= G(x) for all x; the\n  alternative is that F(x) > G(x) for at least one x.\n\nNote that the alternative hypotheses describe the *CDFs* of the\nunderlying distributions, not the observed values. For example,\nsuppose x1 ~ F and x2 ~ G. If F(x) > G(x) for all x, the values in\nx1 tend to be less than those in x2.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nExamples\n--------\nSuppose we wish to test the null hypothesis that a sample is distributed\naccording to the standard normal.\nWe choose a confidence level of 95%; that is, we will reject the null\nhypothesis in favor of the alternative if the p-value is less than 0.05.\n\nWhen testing uniformly distributed data, we would expect the\nnull hypothesis to be rejected.\n\n>>> import numpy as np\n>>> from scipy import stats\n>>> rng = np.random.default_rng()\n>>> stats.kstest(stats.uniform.rvs(size=100, random_state=rng),\n...              stats.norm.cdf)\nKstestResult(statistic=0.5001899973268688,\n             pvalue=1.1616392184763533e-23,\n             statistic_location=0.00047625268963724654,\n             statistic_sign=-1)\n\nIndeed, the p-value is lower than our threshold of 0.05, so we reject the\nnull hypothesis in favor of the default \"two-sided\" alternative: the data\nare *not* distributed according to the standard normal.\n\nWhen testing random variates from the standard normal distribution, we\nexpect the data to be consistent with the null hypothesis most of the time.\n\n>>> x = stats.norm.rvs(size=100, random_state=rng)\n>>> stats.kstest(x, stats.norm.cdf)\nKstestResult(statistic=0.05345882212970396,\n             pvalue=0.9227159037744717,\n             statistic_location=-1.2451343873745018,\n             statistic_sign=1)\n\nAs expected, the p-value of 0.92 is not below our threshold of 0.05, so\nwe cannot reject the null hypothesis.\n\nSuppose, however, that the random variates are distributed according to\na normal distribution that is shifted toward greater values. In this case,\nthe cumulative density function (CDF) of the underlying distribution tends\nto be *less* than the CDF of the standard normal. Therefore, we would\nexpect the null hypothesis to be rejected with ``alternative='less'``:\n\n>>> x = stats.norm.rvs(size=100, loc=0.5, random_state=rng)\n>>> stats.kstest(x, stats.norm.cdf, alternative='less')\nKstestResult(statistic=0.17482387821055168,\n             pvalue=0.001913921057766743,\n             statistic_location=0.3713830565352756,\n             statistic_sign=-1)\n\nand indeed, with p-value smaller than our threshold, we reject the null\nhypothesis in favor of the alternative.\n\nFor convenience, the previous test can be performed using the name of the\ndistribution as the second argument.\n\n>>> stats.kstest(x, \"norm\", alternative='less')\nKstestResult(statistic=0.17482387821055168,\n             pvalue=0.001913921057766743,\n             statistic_location=0.3713830565352756,\n             statistic_sign=-1)\n\nThe examples above have all been one-sample tests identical to those\nperformed by `ks_1samp`. Note that `kstest` can also perform two-sample\ntests identical to those performed by `ks_2samp`. For example, when two\nsamples are drawn from the same distribution, we expect the data to be\nconsistent with the null hypothesis most of the time.\n\n>>> sample1 = stats.laplace.rvs(size=105, random_state=rng)\n>>> sample2 = stats.laplace.rvs(size=95, random_state=rng)\n>>> stats.kstest(sample1, sample2)\nKstestResult(statistic=0.11779448621553884,\n             pvalue=0.4494256912629795,\n             statistic_location=0.6138814275424155,\n             statistic_sign=1)\n\nAs expected, the p-value of 0.45 is not below our threshold of 0.05, so\nwe cannot reject the null hypothesis.\n",
    "scipy.stats.kstwo": "Kolmogorov-Smirnov two-sided test statistic distribution.\n\n    This is the distribution of the two-sided Kolmogorov-Smirnov (KS)\n    statistic :math:`D_n` for a finite sample size ``n >= 1``\n    (the shape parameter).\n\n    As an instance of the `rv_continuous` class, `kstwo` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(n, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, n, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, n, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, n, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, n, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, n, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, n, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, n, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, n, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, n, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(n, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(n, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(n,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(n, loc=0, scale=1)\n        Median of the distribution.\n    mean(n, loc=0, scale=1)\n        Mean of the distribution.\n    var(n, loc=0, scale=1)\n        Variance of the distribution.\n    std(n, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, n, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    kstwobign, ksone, kstest\n\n    Notes\n    -----\n    :math:`D_n` is given by\n\n    .. math::\n\n        D_n = \\text{sup}_x |F_n(x) - F(x)|\n\n    where :math:`F` is a (continuous) CDF and :math:`F_n` is an empirical CDF.\n    `kstwo` describes the distribution under the null hypothesis of the KS test\n    that the empirical CDF corresponds to :math:`n` i.i.d. random variates\n    with CDF :math:`F`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``kstwo.pdf(x, n, loc, scale)`` is identically\n    equivalent to ``kstwo.pdf(y, n) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] Simard, R., L'Ecuyer, P. \"Computing the Two-Sided\n       Kolmogorov-Smirnov Distribution\",  Journal of Statistical Software,\n       Vol 39, 11, 1-18 (2011).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import kstwo\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n\n    Display the probability density function (``pdf``):\n\n    >>> n = 10\n    >>> x = np.linspace(kstwo.ppf(0.01, n),\n    ...                 kstwo.ppf(0.99, n), 100)\n    >>> ax.plot(x, kstwo.pdf(x, n),\n    ...         'r-', lw=5, alpha=0.6, label='kstwo pdf')\n\n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n\n    Freeze the distribution and display the frozen ``pdf``:\n\n    >>> rv = kstwo(n)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n\n    Check accuracy of ``cdf`` and ``ppf``:\n\n    >>> vals = kstwo.ppf([0.001, 0.5, 0.999], n)\n    >>> np.allclose([0.001, 0.5, 0.999], kstwo.cdf(vals, n))\n    True\n\n    ",
    "scipy.stats.kstwobign": "Limiting distribution of scaled Kolmogorov-Smirnov two-sided test statistic.\n\n    This is the asymptotic distribution of the two-sided Kolmogorov-Smirnov\n    statistic :math:`\\sqrt{n} D_n` that measures the maximum absolute\n    distance of the theoretical (continuous) CDF from the empirical CDF.\n    (see `kstest`).\n\n    As an instance of the `rv_continuous` class, `kstwobign` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    ksone, kstwo, kstest\n\n    Notes\n    -----\n    :math:`\\sqrt{n} D_n` is given by\n\n    .. math::\n\n        D_n = \\text{sup}_x |F_n(x) - F(x)|\n\n    where :math:`F` is a continuous CDF and :math:`F_n` is an empirical CDF.\n    `kstwobign`  describes the asymptotic distribution (i.e. the limit of\n    :math:`\\sqrt{n} D_n`) under the null hypothesis of the KS test that the\n    empirical CDF corresponds to i.i.d. random variates with CDF :math:`F`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``kstwobign.pdf(x, loc, scale)`` is identically\n    equivalent to ``kstwobign.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] Feller, W. \"On the Kolmogorov-Smirnov Limit Theorems for Empirical\n       Distributions\",  Ann. Math. Statist. Vol 19, 177-189 (1948).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import kstwobign\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    \n    >>> mean, var, skew, kurt = kstwobign.stats(moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(kstwobign.ppf(0.01),\n    ...                 kstwobign.ppf(0.99), 100)\n    >>> ax.plot(x, kstwobign.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='kstwobign pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = kstwobign()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = kstwobign.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], kstwobign.cdf(vals))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = kstwobign.rvs(size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.kurtosis": "    \n\n\nCompute the kurtosis (Fisher or Pearson) of a dataset.\n\nKurtosis is the fourth central moment divided by the square of the\nvariance. If Fisher's definition is used, then 3.0 is subtracted from\nthe result to give 0.0 for a normal distribution.\n\nIf bias is False then the kurtosis is calculated using k statistics to\neliminate bias coming from biased moment estimators\n\nUse `kurtosistest` to see if result is close enough to normal.\n\nParameters\n----------\na : array\n    Data for which the kurtosis is calculated.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nfisher : bool, optional\n    If True, Fisher's definition is used (normal ==> 0.0). If False,\n    Pearson's definition is used (normal ==> 3.0).\nbias : bool, optional\n    If False, then the calculations are corrected for statistical bias.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nkurtosis : array\n    The kurtosis of values along an axis, returning NaN where all values\n    are equal.\n\nNotes\n-----\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n   Probability and Statistics Tables and Formulae. Chapman & Hall: New\n   York. 2000.\n\nExamples\n--------\nIn Fisher's definition, the kurtosis of the normal distribution is zero.\nIn the following example, the kurtosis is close to zero, because it was\ncalculated from the dataset, not from the continuous distribution.\n\n>>> import numpy as np\n>>> from scipy.stats import norm, kurtosis\n>>> data = norm.rvs(size=1000, random_state=3)\n>>> kurtosis(data)\n-0.06928694200380558\n\nThe distribution with a higher kurtosis has a heavier tail.\nThe zero valued kurtosis of the normal distribution in Fisher's definition\ncan serve as a reference point.\n\n>>> import matplotlib.pyplot as plt\n>>> import scipy.stats as stats\n>>> from scipy.stats import kurtosis\n\n>>> x = np.linspace(-5, 5, 100)\n>>> ax = plt.subplot()\n>>> distnames = ['laplace', 'norm', 'uniform']\n\n>>> for distname in distnames:\n...     if distname == 'uniform':\n...         dist = getattr(stats, distname)(loc=-2, scale=4)\n...     else:\n...         dist = getattr(stats, distname)\n...     data = dist.rvs(size=1000)\n...     kur = kurtosis(data, fisher=True)\n...     y = dist.pdf(x)\n...     ax.plot(x, y, label=\"{}, {}\".format(distname, round(kur, 3)))\n...     ax.legend()\n\nThe Laplace distribution has a heavier tail than the normal distribution.\nThe uniform distribution (which has negative kurtosis) has the thinnest\ntail.\n",
    "scipy.stats.kurtosistest": "    \n\n\nTest whether a dataset has normal kurtosis.\n\nThis function tests the null hypothesis that the kurtosis\nof the population from which the sample was drawn is that\nof the normal distribution.\n\nParameters\n----------\na : array\n    Array of the sample data. Must contain at least five observations.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nalternative : {'two-sided', 'less', 'greater'}, optional\n    Defines the alternative hypothesis.\n    The following options are available (default is 'two-sided'):\n    \n    * 'two-sided': the kurtosis of the distribution underlying the sample\n      is different from that of the normal distribution\n    * 'less': the kurtosis of the distribution underlying the sample\n      is less than that of the normal distribution\n    * 'greater': the kurtosis of the distribution underlying the sample\n      is greater than that of the normal distribution\n    \n    .. versionadded:: 1.7.0\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nstatistic : float\n    The computed z-score for this test.\npvalue : float\n    The p-value for the hypothesis test.\n\nNotes\n-----\nValid only for n>20. This function uses the method described in [1]_.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] see e.g. F. J. Anscombe, W. J. Glynn, \"Distribution of the kurtosis\n   statistic b2 for normal samples\", Biometrika, vol. 70, pp. 227-234, 1983.\n.. [2] Shapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test\n       for normality (complete samples). Biometrika, 52(3/4), 591-611.\n.. [3] B. Phipson and G. K. Smyth. \"Permutation P-values Should Never Be\n       Zero: Calculating Exact P-values When Permutations Are Randomly\n       Drawn.\" Statistical Applications in Genetics and Molecular Biology\n       9.1 (2010).\n.. [4] Panagiotakos, D. B. (2008). The value of p-value in biomedical\n       research. The open cardiovascular medicine journal, 2, 97.\n\nExamples\n--------\nSuppose we wish to infer from measurements whether the weights of adult\nhuman males in a medical study are not normally distributed [2]_.\nThe weights (lbs) are recorded in the array ``x`` below.\n\n>>> import numpy as np\n>>> x = np.array([148, 154, 158, 160, 161, 162, 166, 170, 182, 195, 236])\n\nThe kurtosis test from [1]_ begins by computing a statistic based on the\nsample (excess/Fisher) kurtosis.\n\n>>> from scipy import stats\n>>> res = stats.kurtosistest(x)\n>>> res.statistic\n2.3048235214240873\n\n(The test warns that our sample has too few observations to perform the\ntest. We'll return to this at the end of the example.)\nBecause normal distributions have zero excess kurtosis (by definition),\nthe magnitude of this statistic tends to be low for samples drawn from a\nnormal distribution.\n\nThe test is performed by comparing the observed value of the\nstatistic against the null distribution: the distribution of statistic\nvalues derived under the null hypothesis that the weights were drawn from\na normal distribution.\n\nFor this test, the null distribution of the statistic for very large\nsamples is the standard normal distribution.\n\n>>> import matplotlib.pyplot as plt\n>>> dist = stats.norm()\n>>> kt_val = np.linspace(-5, 5, 100)\n>>> pdf = dist.pdf(kt_val)\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> def kt_plot(ax):  # we'll reuse this\n...     ax.plot(kt_val, pdf)\n...     ax.set_title(\"Kurtosis Test Null Distribution\")\n...     ax.set_xlabel(\"statistic\")\n...     ax.set_ylabel(\"probability density\")\n>>> kt_plot(ax)\n>>> plt.show()\n\nThe comparison is quantified by the p-value: the proportion of values in\nthe null distribution as extreme or more extreme than the observed\nvalue of the statistic. In a two-sided test in which the statistic is\npositive, elements of the null distribution greater than the observed\nstatistic and elements of the null distribution less than the negative of\nthe observed statistic are both considered \"more extreme\".\n\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> kt_plot(ax)\n>>> pvalue = dist.cdf(-res.statistic) + dist.sf(res.statistic)\n>>> annotation = (f'p-value={pvalue:.3f}\\n(shaded area)')\n>>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n>>> _ = ax.annotate(annotation, (3, 0.005), (3.25, 0.02), arrowprops=props)\n>>> i = kt_val >= res.statistic\n>>> ax.fill_between(kt_val[i], y1=0, y2=pdf[i], color='C0')\n>>> i = kt_val <= -res.statistic\n>>> ax.fill_between(kt_val[i], y1=0, y2=pdf[i], color='C0')\n>>> ax.set_xlim(-5, 5)\n>>> ax.set_ylim(0, 0.1)\n>>> plt.show()\n>>> res.pvalue\n0.0211764592113868\n\nIf the p-value is \"small\" - that is, if there is a low probability of\nsampling data from a normally distributed population that produces such an\nextreme value of the statistic - this may be taken as evidence against\nthe null hypothesis in favor of the alternative: the weights were not\ndrawn from a normal distribution. Note that:\n\n- The inverse is not true; that is, the test is not used to provide\n  evidence for the null hypothesis.\n- The threshold for values that will be considered \"small\" is a choice that\n  should be made before the data is analyzed [3]_ with consideration of the\n  risks of both false positives (incorrectly rejecting the null hypothesis)\n  and false negatives (failure to reject a false null hypothesis).\n\nNote that the standard normal distribution provides an asymptotic\napproximation of the null distribution; it is only accurate for samples\nwith many observations. This is the reason we received a warning at the\nbeginning of the example; our sample is quite small. In this case,\n`scipy.stats.monte_carlo_test` may provide a more accurate, albeit\nstochastic, approximation of the exact p-value.\n\n>>> def statistic(x, axis):\n...     # get just the skewtest statistic; ignore the p-value\n...     return stats.kurtosistest(x, axis=axis).statistic\n>>> res = stats.monte_carlo_test(x, stats.norm.rvs, statistic)\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> kt_plot(ax)\n>>> ax.hist(res.null_distribution, np.linspace(-5, 5, 50),\n...         density=True)\n>>> ax.legend(['aymptotic approximation\\n(many observations)',\n...            'Monte Carlo approximation\\n(11 observations)'])\n>>> plt.show()\n>>> res.pvalue\n0.0272  # may vary\n\nFurthermore, despite their stochastic nature, p-values computed in this way\ncan be used to exactly control the rate of false rejections of the null\nhypothesis [4]_.\n",
    "scipy.stats.laplace": "A Laplace continuous random variable.\n\n    As an instance of the `rv_continuous` class, `laplace` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `laplace` is\n\n    .. math::\n\n        f(x) = \\frac{1}{2} \\exp(-|x|)\n\n    for a real number :math:`x`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``laplace.pdf(x, loc, scale)`` is identically\n    equivalent to ``laplace.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import laplace\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    \n    >>> mean, var, skew, kurt = laplace.stats(moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(laplace.ppf(0.01),\n    ...                 laplace.ppf(0.99), 100)\n    >>> ax.plot(x, laplace.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='laplace pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = laplace()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = laplace.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], laplace.cdf(vals))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = laplace.rvs(size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.laplace_asymmetric": "An asymmetric Laplace continuous random variable.\n\n    As an instance of the `rv_continuous` class, `laplace_asymmetric` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(kappa, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, kappa, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, kappa, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, kappa, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, kappa, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, kappa, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, kappa, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, kappa, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, kappa, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, kappa, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(kappa, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(kappa, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(kappa,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(kappa, loc=0, scale=1)\n        Median of the distribution.\n    mean(kappa, loc=0, scale=1)\n        Mean of the distribution.\n    var(kappa, loc=0, scale=1)\n        Variance of the distribution.\n    std(kappa, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, kappa, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    laplace : Laplace distribution\n\n    Notes\n    -----\n    The probability density function for `laplace_asymmetric` is\n\n    .. math::\n\n       f(x, \\kappa) &= \\frac{1}{\\kappa+\\kappa^{-1}}\\exp(-x\\kappa),\\quad x\\ge0\\\\\n                    &= \\frac{1}{\\kappa+\\kappa^{-1}}\\exp(x/\\kappa),\\quad x<0\\\\\n\n    for :math:`-\\infty < x < \\infty`, :math:`\\kappa > 0`.\n\n    `laplace_asymmetric` takes ``kappa`` as a shape parameter for\n    :math:`\\kappa`. For :math:`\\kappa = 1`, it is identical to a\n    Laplace distribution.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``laplace_asymmetric.pdf(x, kappa, loc, scale)`` is identically\n    equivalent to ``laplace_asymmetric.pdf(y, kappa) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Note that the scale parameter of some references is the reciprocal of\n    SciPy's ``scale``. For example, :math:`\\lambda = 1/2` in the\n    parameterization of [1]_ is equivalent to ``scale = 2`` with\n    `laplace_asymmetric`.\n\n    References\n    ----------\n    .. [1] \"Asymmetric Laplace distribution\", Wikipedia\n            https://en.wikipedia.org/wiki/Asymmetric_Laplace_distribution\n\n    .. [2] Kozubowski TJ and Podg\u00f3rski K. A Multivariate and\n           Asymmetric Generalization of Laplace Distribution,\n           Computational Statistics 15, 531--540 (2000).\n           :doi:`10.1007/PL00022717`\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import laplace_asymmetric\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> kappa = 2\n    >>> mean, var, skew, kurt = laplace_asymmetric.stats(kappa, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(laplace_asymmetric.ppf(0.01, kappa),\n    ...                 laplace_asymmetric.ppf(0.99, kappa), 100)\n    >>> ax.plot(x, laplace_asymmetric.pdf(x, kappa),\n    ...        'r-', lw=5, alpha=0.6, label='laplace_asymmetric pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = laplace_asymmetric(kappa)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = laplace_asymmetric.ppf([0.001, 0.5, 0.999], kappa)\n    >>> np.allclose([0.001, 0.5, 0.999], laplace_asymmetric.cdf(vals, kappa))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = laplace_asymmetric.rvs(kappa, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.levene": "    \n\n\nPerform Levene test for equal variances.\n\nThe Levene test tests the null hypothesis that all input samples\nare from populations with equal variances.  Levene's test is an\nalternative to Bartlett's test `bartlett` in the case where\nthere are significant deviations from normality.\n\nParameters\n----------\nsample1, sample2, ... : array_like\n    The sample data, possibly with different lengths. Only one-dimensional\n    samples are accepted.\ncenter : {'mean', 'median', 'trimmed'}, optional\n    Which function of the data to use in the test.  The default\n    is 'median'.\nproportiontocut : float, optional\n    When `center` is 'trimmed', this gives the proportion of data points\n    to cut from each end. (See `scipy.stats.trim_mean`.)\n    Default is 0.05.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nstatistic : float\n    The test statistic.\npvalue : float\n    The p-value for the test.\n\nSee Also\n--------\n\n:func:`fligner`\n    A non-parametric test for the equality of k variances\n:func:`bartlett`\n    A parametric test for equality of k variances in normal samples\n\n\nNotes\n-----\nThree variations of Levene's test are possible.  The possibilities\nand their recommended usages are:\n\n  * 'median' : Recommended for skewed (non-normal) distributions>\n  * 'mean' : Recommended for symmetric, moderate-tailed distributions.\n  * 'trimmed' : Recommended for heavy-tailed distributions.\n\nThe test version using the mean was proposed in the original article\nof Levene ([2]_) while the median and trimmed mean have been studied by\nBrown and Forsythe ([3]_), sometimes also referred to as Brown-Forsythe\ntest.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] https://www.itl.nist.gov/div898/handbook/eda/section3/eda35a.htm\n.. [2] Levene, H. (1960). In Contributions to Probability and Statistics:\n       Essays in Honor of Harold Hotelling, I. Olkin et al. eds.,\n       Stanford University Press, pp. 278-292.\n.. [3] Brown, M. B. and Forsythe, A. B. (1974), Journal of the American\n       Statistical Association, 69, 364-367\n.. [4] C.I. BLISS (1952), The Statistics of Bioassay: With Special\n       Reference to the Vitamins, pp 499-503,\n       :doi:`10.1016/C2013-0-12584-6`.\n.. [5] B. Phipson and G. K. Smyth. \"Permutation P-values Should Never Be\n       Zero: Calculating Exact P-values When Permutations Are Randomly\n       Drawn.\" Statistical Applications in Genetics and Molecular Biology\n       9.1 (2010).\n.. [6] Ludbrook, J., & Dudley, H. (1998). Why permutation tests are\n       superior to t and F tests in biomedical research. The American\n       Statistician, 52(2), 127-132.\n\nExamples\n--------\nIn [4]_, the influence of vitamin C on the tooth growth of guinea pigs\nwas investigated. In a control study, 60 subjects were divided into\nsmall dose, medium dose, and large dose groups that received\ndaily doses of 0.5, 1.0 and 2.0 mg of vitamin C, respectively.\nAfter 42 days, the tooth growth was measured.\n\nThe ``small_dose``, ``medium_dose``, and ``large_dose`` arrays below record\ntooth growth measurements of the three groups in microns.\n\n>>> import numpy as np\n>>> small_dose = np.array([\n...     4.2, 11.5, 7.3, 5.8, 6.4, 10, 11.2, 11.2, 5.2, 7,\n...     15.2, 21.5, 17.6, 9.7, 14.5, 10, 8.2, 9.4, 16.5, 9.7\n... ])\n>>> medium_dose = np.array([\n...     16.5, 16.5, 15.2, 17.3, 22.5, 17.3, 13.6, 14.5, 18.8, 15.5,\n...     19.7, 23.3, 23.6, 26.4, 20, 25.2, 25.8, 21.2, 14.5, 27.3\n... ])\n>>> large_dose = np.array([\n...     23.6, 18.5, 33.9, 25.5, 26.4, 32.5, 26.7, 21.5, 23.3, 29.5,\n...     25.5, 26.4, 22.4, 24.5, 24.8, 30.9, 26.4, 27.3, 29.4, 23\n... ])\n\nThe `levene` statistic is sensitive to differences in variances\nbetween the samples.\n\n>>> from scipy import stats\n>>> res = stats.levene(small_dose, medium_dose, large_dose)\n>>> res.statistic\n0.6457341109631506\n\nThe value of the statistic tends to be high when there is a large\ndifference in variances.\n\nWe can test for inequality of variance among the groups by comparing the\nobserved value of the statistic against the null distribution: the\ndistribution of statistic values derived under the null hypothesis that\nthe population variances of the three groups are equal.\n\nFor this test, the null distribution follows the F distribution as shown\nbelow.\n\n>>> import matplotlib.pyplot as plt\n>>> k, n = 3, 60   # number of samples, total number of observations\n>>> dist = stats.f(dfn=k-1, dfd=n-k)\n>>> val = np.linspace(0, 5, 100)\n>>> pdf = dist.pdf(val)\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> def plot(ax):  # we'll reuse this\n...     ax.plot(val, pdf, color='C0')\n...     ax.set_title(\"Levene Test Null Distribution\")\n...     ax.set_xlabel(\"statistic\")\n...     ax.set_ylabel(\"probability density\")\n...     ax.set_xlim(0, 5)\n...     ax.set_ylim(0, 1)\n>>> plot(ax)\n>>> plt.show()\n\nThe comparison is quantified by the p-value: the proportion of values in\nthe null distribution greater than or equal to the observed value of the\nstatistic.\n\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> plot(ax)\n>>> pvalue = dist.sf(res.statistic)\n>>> annotation = (f'p-value={pvalue:.3f}\\n(shaded area)')\n>>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n>>> _ = ax.annotate(annotation, (1.5, 0.22), (2.25, 0.3), arrowprops=props)\n>>> i = val >= res.statistic\n>>> ax.fill_between(val[i], y1=0, y2=pdf[i], color='C0')\n>>> plt.show()\n\n>>> res.pvalue\n0.5280694573759905\n\nIf the p-value is \"small\" - that is, if there is a low probability of\nsampling data from distributions with identical variances that produces\nsuch an extreme value of the statistic - this may be taken as evidence\nagainst the null hypothesis in favor of the alternative: the variances of\nthe groups are not equal. Note that:\n\n- The inverse is not true; that is, the test is not used to provide\n  evidence for the null hypothesis.\n- The threshold for values that will be considered \"small\" is a choice that\n  should be made before the data is analyzed [5]_ with consideration of the\n  risks of both false positives (incorrectly rejecting the null hypothesis)\n  and false negatives (failure to reject a false null hypothesis).\n- Small p-values are not evidence for a *large* effect; rather, they can\n  only provide evidence for a \"significant\" effect, meaning that they are\n  unlikely to have occurred under the null hypothesis.\n\nNote that the F distribution provides an asymptotic approximation of the\nnull distribution.\nFor small samples, it may be more appropriate to perform a permutation\ntest: Under the null hypothesis that all three samples were drawn from\nthe same population, each of the measurements is equally likely to have\nbeen observed in any of the three samples. Therefore, we can form a\nrandomized null distribution by calculating the statistic under many\nrandomly-generated partitionings of the observations into the three\nsamples.\n\n>>> def statistic(*samples):\n...     return stats.levene(*samples).statistic\n>>> ref = stats.permutation_test(\n...     (small_dose, medium_dose, large_dose), statistic,\n...     permutation_type='independent', alternative='greater'\n... )\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> plot(ax)\n>>> bins = np.linspace(0, 5, 25)\n>>> ax.hist(\n...     ref.null_distribution, bins=bins, density=True, facecolor=\"C1\"\n... )\n>>> ax.legend(['aymptotic approximation\\n(many observations)',\n...            'randomized null distribution'])\n>>> plot(ax)\n>>> plt.show()\n\n>>> ref.pvalue  # randomized test p-value\n0.4559  # may vary\n\nNote that there is significant disagreement between the p-value calculated\nhere and the asymptotic approximation returned by `levene` above.\nThe statistical inferences that can be drawn rigorously from a permutation\ntest are limited; nonetheless, they may be the preferred approach in many\ncircumstances [6]_.\n\nFollowing is another generic example where the null hypothesis would be\nrejected.\n\nTest whether the lists `a`, `b` and `c` come from populations\nwith equal variances.\n\n>>> a = [8.88, 9.12, 9.04, 8.98, 9.00, 9.08, 9.01, 8.85, 9.06, 8.99]\n>>> b = [8.88, 8.95, 9.29, 9.44, 9.15, 9.58, 8.36, 9.18, 8.67, 9.05]\n>>> c = [8.95, 9.12, 8.95, 8.85, 9.03, 8.84, 9.07, 8.98, 8.86, 8.98]\n>>> stat, p = stats.levene(a, b, c)\n>>> p\n0.002431505967249681\n\nThe small p-value suggests that the populations do not have equal\nvariances.\n\nThis is not surprising, given that the sample variance of `b` is much\nlarger than that of `a` and `c`:\n\n>>> [np.var(x, ddof=1) for x in [a, b, c]]\n[0.007054444444444413, 0.13073888888888888, 0.008890000000000002]\n",
    "scipy.stats.levy": "A Levy continuous random variable.\n\n    As an instance of the `rv_continuous` class, `levy` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    levy_stable, levy_l\n\n    Notes\n    -----\n    The probability density function for `levy` is:\n\n    .. math::\n\n        f(x) = \\frac{1}{\\sqrt{2\\pi x^3}} \\exp\\left(-\\frac{1}{2x}\\right)\n\n    for :math:`x > 0`.\n\n    This is the same as the Levy-stable distribution with :math:`a=1/2` and\n    :math:`b=1`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``levy.pdf(x, loc, scale)`` is identically\n    equivalent to ``levy.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import levy\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n\n    Calculate the first four moments:\n\n    >>> mean, var, skew, kurt = levy.stats(moments='mvsk')\n\n    Display the probability density function (``pdf``):\n\n    >>> # `levy` is very heavy-tailed.\n    >>> # To show a nice plot, let's cut off the upper 40 percent.\n    >>> a, b = levy.ppf(0), levy.ppf(0.6)\n    >>> x = np.linspace(a, b, 100)\n    >>> ax.plot(x, levy.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='levy pdf')\n\n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n\n    Freeze the distribution and display the frozen ``pdf``:\n\n    >>> rv = levy()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n\n    Check accuracy of ``cdf`` and ``ppf``:\n\n    >>> vals = levy.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], levy.cdf(vals))\n    True\n\n    Generate random numbers:\n\n    >>> r = levy.rvs(size=1000)\n\n    And compare the histogram:\n\n    >>> # manual binning to ignore the tail\n    >>> bins = np.concatenate((np.linspace(a, b, 20), [np.max(r)]))\n    >>> ax.hist(r, bins=bins, density=True, histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n\n    ",
    "scipy.stats.levy_l": "A left-skewed Levy continuous random variable.\n\n    As an instance of the `rv_continuous` class, `levy_l` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    levy, levy_stable\n\n    Notes\n    -----\n    The probability density function for `levy_l` is:\n\n    .. math::\n        f(x) = \\frac{1}{|x| \\sqrt{2\\pi |x|}} \\exp{ \\left(-\\frac{1}{2|x|} \\right)}\n\n    for :math:`x < 0`.\n\n    This is the same as the Levy-stable distribution with :math:`a=1/2` and\n    :math:`b=-1`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``levy_l.pdf(x, loc, scale)`` is identically\n    equivalent to ``levy_l.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import levy_l\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n\n    Calculate the first four moments:\n\n    >>> mean, var, skew, kurt = levy_l.stats(moments='mvsk')\n\n    Display the probability density function (``pdf``):\n\n    >>> # `levy_l` is very heavy-tailed.\n    >>> # To show a nice plot, let's cut off the lower 40 percent.\n    >>> a, b = levy_l.ppf(0.4), levy_l.ppf(1)\n    >>> x = np.linspace(a, b, 100)\n    >>> ax.plot(x, levy_l.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='levy_l pdf')\n\n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n\n    Freeze the distribution and display the frozen ``pdf``:\n\n    >>> rv = levy_l()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n\n    Check accuracy of ``cdf`` and ``ppf``:\n\n    >>> vals = levy_l.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], levy_l.cdf(vals))\n    True\n\n    Generate random numbers:\n\n    >>> r = levy_l.rvs(size=1000)\n\n    And compare the histogram:\n\n    >>> # manual binning to ignore the tail\n    >>> bins = np.concatenate(([np.min(r)], np.linspace(a, b, 20)))\n    >>> ax.hist(r, bins=bins, density=True, histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n\n    ",
    "scipy.stats.levy_stable": "A Levy-stable continuous random variable.\n\n    As an instance of the `rv_continuous` class, `levy_stable` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(alpha, beta, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, alpha, beta, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, alpha, beta, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, alpha, beta, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, alpha, beta, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, alpha, beta, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, alpha, beta, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, alpha, beta, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, alpha, beta, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, alpha, beta, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(alpha, beta, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(alpha, beta, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(alpha, beta), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(alpha, beta, loc=0, scale=1)\n        Median of the distribution.\n    mean(alpha, beta, loc=0, scale=1)\n        Mean of the distribution.\n    var(alpha, beta, loc=0, scale=1)\n        Variance of the distribution.\n    std(alpha, beta, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, alpha, beta, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    levy, levy_l, cauchy, norm\n\n    Notes\n    -----\n    The distribution for `levy_stable` has characteristic function:\n\n    .. math::\n\n        \\varphi(t, \\alpha, \\beta, c, \\mu) =\n        e^{it\\mu -|ct|^{\\alpha}(1-i\\beta\\operatorname{sign}(t)\\Phi(\\alpha, t))}\n\n    where two different parameterizations are supported. The first :math:`S_1`:\n\n    .. math::\n\n        \\Phi = \\begin{cases}\n                \\tan \\left({\\frac {\\pi \\alpha }{2}}\\right)&\\alpha \\neq 1\\\\\n                -{\\frac {2}{\\pi }}\\log |t|&\\alpha =1\n                \\end{cases}\n\n    The second :math:`S_0`:\n\n    .. math::\n\n        \\Phi = \\begin{cases}\n                -\\tan \\left({\\frac {\\pi \\alpha }{2}}\\right)(|ct|^{1-\\alpha}-1)\n                &\\alpha \\neq 1\\\\\n                -{\\frac {2}{\\pi }}\\log |ct|&\\alpha =1\n                \\end{cases}\n\n\n    The probability density function for `levy_stable` is:\n\n    .. math::\n\n        f(x) = \\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\varphi(t)e^{-ixt}\\,dt\n\n    where :math:`-\\infty < t < \\infty`. This integral does not have a known\n    closed form.\n\n    `levy_stable` generalizes several distributions.  Where possible, they\n    should be used instead.  Specifically, when the shape parameters\n    assume the values in the table below, the corresponding equivalent\n    distribution should be used.\n\n    =========  ========  ===========\n    ``alpha``  ``beta``   Equivalent\n    =========  ========  ===========\n     1/2       -1        `levy_l`\n     1/2       1         `levy`\n     1         0         `cauchy`\n     2         any       `norm` (with ``scale=sqrt(2)``)\n    =========  ========  ===========\n\n    Evaluation of the pdf uses Nolan's piecewise integration approach with the\n    Zolotarev :math:`M` parameterization by default. There is also the option\n    to use direct numerical integration of the standard parameterization of the\n    characteristic function or to evaluate by taking the FFT of the\n    characteristic function.\n\n    The default method can changed by setting the class variable\n    ``levy_stable.pdf_default_method`` to one of 'piecewise' for Nolan's\n    approach, 'dni' for direct numerical integration, or 'fft-simpson' for the\n    FFT based approach. For the sake of backwards compatibility, the methods\n    'best' and 'zolotarev' are equivalent to 'piecewise' and the method\n    'quadrature' is equivalent to 'dni'.\n\n    The parameterization can be changed  by setting the class variable\n    ``levy_stable.parameterization`` to either 'S0' or 'S1'.\n    The default is 'S1'.\n\n    To improve performance of piecewise and direct numerical integration one\n    can specify ``levy_stable.quad_eps`` (defaults to 1.2e-14). This is used\n    as both the absolute and relative quadrature tolerance for direct numerical\n    integration and as the relative quadrature tolerance for the piecewise\n    method. One can also specify ``levy_stable.piecewise_x_tol_near_zeta``\n    (defaults to 0.005) for how close x is to zeta before it is considered the\n    same as x [NO]. The exact check is\n    ``abs(x0 - zeta) < piecewise_x_tol_near_zeta*alpha**(1/alpha)``. One can\n    also specify ``levy_stable.piecewise_alpha_tol_near_one`` (defaults to\n    0.005) for how close alpha is to 1 before being considered equal to 1.\n\n    To increase accuracy of FFT calculation one can specify\n    ``levy_stable.pdf_fft_grid_spacing`` (defaults to 0.001) and\n    ``pdf_fft_n_points_two_power`` (defaults to None which means a value is\n    calculated that sufficiently covers the input range).\n\n    Further control over FFT calculation is available by setting\n    ``pdf_fft_interpolation_degree`` (defaults to 3) for spline order and\n    ``pdf_fft_interpolation_level`` for determining the number of points to use\n    in the Newton-Cotes formula when approximating the characteristic function\n    (considered experimental).\n\n    Evaluation of the cdf uses Nolan's piecewise integration approach with the\n    Zolatarev :math:`S_0` parameterization by default. There is also the option\n    to evaluate through integration of an interpolated spline of the pdf\n    calculated by means of the FFT method. The settings affecting FFT\n    calculation are the same as for pdf calculation. The default cdf method can\n    be changed by setting ``levy_stable.cdf_default_method`` to either\n    'piecewise' or 'fft-simpson'.  For cdf calculations the Zolatarev method is\n    superior in accuracy, so FFT is disabled by default.\n\n    Fitting estimate uses quantile estimation method in [MC]. MLE estimation of\n    parameters in fit method uses this quantile estimate initially. Note that\n    MLE doesn't always converge if using FFT for pdf calculations; this will be\n    the case if alpha <= 1 where the FFT approach doesn't give good\n    approximations.\n\n    Any non-missing value for the attribute\n    ``levy_stable.pdf_fft_min_points_threshold`` will set\n    ``levy_stable.pdf_default_method`` to 'fft-simpson' if a valid\n    default method is not otherwise set.\n\n\n\n    .. warning::\n\n        For pdf calculations FFT calculation is considered experimental.\n\n        For cdf calculations FFT calculation is considered experimental. Use\n        Zolatarev's method instead (default).\n\n    The probability density above is defined in the \"standardized\" form. To\n    shift and/or scale the distribution use the ``loc`` and ``scale``\n    parameters.\n    Generally ``levy_stable.pdf(x, alpha, beta, loc, scale)`` is identically\n    equivalent to ``levy_stable.pdf(y, alpha, beta) / scale`` with\n    ``y = (x - loc) / scale``, except in the ``S1`` parameterization if\n    ``alpha == 1``.  In that case ``levy_stable.pdf(x, alpha, beta, loc, scale)``\n    is identically equivalent to ``levy_stable.pdf(y, alpha, beta) / scale`` with\n    ``y = (x - loc - 2 * beta * scale * np.log(scale) / np.pi) / scale``.\n    See [NO2]_ Definition 1.8 for more information.\n    Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution.\n\n    References\n    ----------\n    .. [MC] McCulloch, J., 1986. Simple consistent estimators of stable\n        distribution parameters. Communications in Statistics - Simulation and\n        Computation 15, 11091136.\n    .. [WZ] Wang, Li and Zhang, Ji-Hong, 2008. Simpson's rule based FFT method\n        to compute densities of stable distribution.\n    .. [NO] Nolan, J., 1997. Numerical Calculation of Stable Densities and\n        distributions Functions.\n    .. [NO2] Nolan, J., 2018. Stable Distributions: Models for Heavy Tailed\n        Data.\n    .. [HO] Hopcraft, K. I., Jakeman, E., Tanner, R. M. J., 1999. L\u00e9vy random\n        walks with fluctuating step number and multiscale behavior.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import levy_stable\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> alpha, beta = 1.8, -0.5\n    >>> mean, var, skew, kurt = levy_stable.stats(alpha, beta, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(levy_stable.ppf(0.01, alpha, beta),\n    ...                 levy_stable.ppf(0.99, alpha, beta), 100)\n    >>> ax.plot(x, levy_stable.pdf(x, alpha, beta),\n    ...        'r-', lw=5, alpha=0.6, label='levy_stable pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = levy_stable(alpha, beta)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = levy_stable.ppf([0.001, 0.5, 0.999], alpha, beta)\n    >>> np.allclose([0.001, 0.5, 0.999], levy_stable.cdf(vals, alpha, beta))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = levy_stable.rvs(alpha, beta, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.linregress": "\n    Calculate a linear least-squares regression for two sets of measurements.\n\n    Parameters\n    ----------\n    x, y : array_like\n        Two sets of measurements.  Both arrays should have the same length N.  If\n        only `x` is given (and ``y=None``), then it must be a two-dimensional\n        array where one dimension has length 2.  The two sets of measurements\n        are then found by splitting the array along the length-2 dimension. In\n        the case where ``y=None`` and `x` is a 2xN array, ``linregress(x)`` is\n        equivalent to ``linregress(x[0], x[1])``.\n\n        .. deprecated:: 1.14.0\n            Inference of the two sets of measurements from a single argument `x`\n            is deprecated will result in an error in SciPy 1.16.0; the sets\n            must be specified separately as `x` and `y`.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n        The following options are available:\n\n        * 'two-sided': the slope of the regression line is nonzero\n        * 'less': the slope of the regression line is less than zero\n        * 'greater':  the slope of the regression line is greater than zero\n\n        .. versionadded:: 1.7.0\n\n    Returns\n    -------\n    result : ``LinregressResult`` instance\n        The return value is an object with the following attributes:\n\n        slope : float\n            Slope of the regression line.\n        intercept : float\n            Intercept of the regression line.\n        rvalue : float\n            The Pearson correlation coefficient. The square of ``rvalue``\n            is equal to the coefficient of determination.\n        pvalue : float\n            The p-value for a hypothesis test whose null hypothesis is\n            that the slope is zero, using Wald Test with t-distribution of\n            the test statistic. See `alternative` above for alternative\n            hypotheses.\n        stderr : float\n            Standard error of the estimated slope (gradient), under the\n            assumption of residual normality.\n        intercept_stderr : float\n            Standard error of the estimated intercept, under the assumption\n            of residual normality.\n\n    See Also\n    --------\n    scipy.optimize.curve_fit :\n        Use non-linear least squares to fit a function to data.\n    scipy.optimize.leastsq :\n        Minimize the sum of squares of a set of equations.\n\n    Notes\n    -----\n    For compatibility with older versions of SciPy, the return value acts\n    like a ``namedtuple`` of length 5, with fields ``slope``, ``intercept``,\n    ``rvalue``, ``pvalue`` and ``stderr``, so one can continue to write::\n\n        slope, intercept, r, p, se = linregress(x, y)\n\n    With that style, however, the standard error of the intercept is not\n    available.  To have access to all the computed values, including the\n    standard error of the intercept, use the return value as an object\n    with attributes, e.g.::\n\n        result = linregress(x, y)\n        print(result.intercept, result.intercept_stderr)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n\n    Generate some data:\n\n    >>> x = rng.random(10)\n    >>> y = 1.6*x + rng.random(10)\n\n    Perform the linear regression:\n\n    >>> res = stats.linregress(x, y)\n\n    Coefficient of determination (R-squared):\n\n    >>> print(f\"R-squared: {res.rvalue**2:.6f}\")\n    R-squared: 0.717533\n\n    Plot the data along with the fitted line:\n\n    >>> plt.plot(x, y, 'o', label='original data')\n    >>> plt.plot(x, res.intercept + res.slope*x, 'r', label='fitted line')\n    >>> plt.legend()\n    >>> plt.show()\n\n    Calculate 95% confidence interval on slope and intercept:\n\n    >>> # Two-sided inverse Students t-distribution\n    >>> # p - probability, df - degrees of freedom\n    >>> from scipy.stats import t\n    >>> tinv = lambda p, df: abs(t.ppf(p/2, df))\n\n    >>> ts = tinv(0.05, len(x)-2)\n    >>> print(f\"slope (95%): {res.slope:.6f} +/- {ts*res.stderr:.6f}\")\n    slope (95%): 1.453392 +/- 0.743465\n    >>> print(f\"intercept (95%): {res.intercept:.6f}\"\n    ...       f\" +/- {ts*res.intercept_stderr:.6f}\")\n    intercept (95%): 0.616950 +/- 0.544475\n\n    ",
    "scipy.stats.loggamma": "A log gamma continuous random variable.\n\n    As an instance of the `rv_continuous` class, `loggamma` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `loggamma` is:\n\n    .. math::\n\n        f(x, c) = \\frac{\\exp(c x - \\exp(x))}\n                       {\\Gamma(c)}\n\n    for all :math:`x, c > 0`. Here, :math:`\\Gamma` is the\n    gamma function (`scipy.special.gamma`).\n\n    `loggamma` takes ``c`` as a shape parameter for :math:`c`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``loggamma.pdf(x, c, loc, scale)`` is identically\n    equivalent to ``loggamma.pdf(y, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import loggamma\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c = 0.414\n    >>> mean, var, skew, kurt = loggamma.stats(c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(loggamma.ppf(0.01, c),\n    ...                 loggamma.ppf(0.99, c), 100)\n    >>> ax.plot(x, loggamma.pdf(x, c),\n    ...        'r-', lw=5, alpha=0.6, label='loggamma pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = loggamma(c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = loggamma.ppf([0.001, 0.5, 0.999], c)\n    >>> np.allclose([0.001, 0.5, 0.999], loggamma.cdf(vals, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = loggamma.rvs(c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.logistic": "A logistic (or Sech-squared) continuous random variable.\n\n    As an instance of the `rv_continuous` class, `logistic` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `logistic` is:\n\n    .. math::\n\n        f(x) = \\frac{\\exp(-x)}\n                    {(1+\\exp(-x))^2}\n\n    `logistic` is a special case of `genlogistic` with ``c=1``.\n\n    Remark that the survival function (``logistic.sf``) is equal to the\n    Fermi-Dirac distribution describing fermionic statistics.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``logistic.pdf(x, loc, scale)`` is identically\n    equivalent to ``logistic.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import logistic\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    \n    >>> mean, var, skew, kurt = logistic.stats(moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(logistic.ppf(0.01),\n    ...                 logistic.ppf(0.99), 100)\n    >>> ax.plot(x, logistic.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='logistic pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = logistic()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = logistic.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], logistic.cdf(vals))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = logistic.rvs(size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.loglaplace": "A log-Laplace continuous random variable.\n\n    As an instance of the `rv_continuous` class, `loglaplace` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `loglaplace` is:\n\n    .. math::\n\n        f(x, c) = \\begin{cases}\\frac{c}{2} x^{ c-1}  &\\text{for } 0 < x < 1\\\\\n                               \\frac{c}{2} x^{-c-1}  &\\text{for } x \\ge 1\n                  \\end{cases}\n\n    for :math:`c > 0`.\n\n    `loglaplace` takes ``c`` as a shape parameter for :math:`c`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``loglaplace.pdf(x, c, loc, scale)`` is identically\n    equivalent to ``loglaplace.pdf(y, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Suppose a random variable ``X`` follows the Laplace distribution with\n    location ``a`` and scale ``b``.  Then ``Y = exp(X)`` follows the\n    log-Laplace distribution with ``c = 1 / b`` and ``scale = exp(a)``.\n\n    References\n    ----------\n    T.J. Kozubowski and K. Podgorski, \"A log-Laplace growth rate model\",\n    The Mathematical Scientist, vol. 28, pp. 49-60, 2003.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import loglaplace\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c = 3.25\n    >>> mean, var, skew, kurt = loglaplace.stats(c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(loglaplace.ppf(0.01, c),\n    ...                 loglaplace.ppf(0.99, c), 100)\n    >>> ax.plot(x, loglaplace.pdf(x, c),\n    ...        'r-', lw=5, alpha=0.6, label='loglaplace pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = loglaplace(c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = loglaplace.ppf([0.001, 0.5, 0.999], c)\n    >>> np.allclose([0.001, 0.5, 0.999], loglaplace.cdf(vals, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = loglaplace.rvs(c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.lognorm": "A lognormal continuous random variable.\n\n    As an instance of the `rv_continuous` class, `lognorm` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(s, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, s, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, s, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, s, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, s, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, s, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, s, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, s, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, s, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, s, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(s, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(s, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(s,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(s, loc=0, scale=1)\n        Median of the distribution.\n    mean(s, loc=0, scale=1)\n        Mean of the distribution.\n    var(s, loc=0, scale=1)\n        Variance of the distribution.\n    std(s, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, s, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `lognorm` is:\n\n    .. math::\n\n        f(x, s) = \\frac{1}{s x \\sqrt{2\\pi}}\n                  \\exp\\left(-\\frac{\\log^2(x)}{2s^2}\\right)\n\n    for :math:`x > 0`, :math:`s > 0`.\n\n    `lognorm` takes ``s`` as a shape parameter for :math:`s`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``lognorm.pdf(x, s, loc, scale)`` is identically\n    equivalent to ``lognorm.pdf(y, s) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Suppose a normally distributed random variable ``X`` has  mean ``mu`` and\n    standard deviation ``sigma``. Then ``Y = exp(X)`` is lognormally\n    distributed with ``s = sigma`` and ``scale = exp(mu)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import lognorm\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> s = 0.954\n    >>> mean, var, skew, kurt = lognorm.stats(s, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(lognorm.ppf(0.01, s),\n    ...                 lognorm.ppf(0.99, s), 100)\n    >>> ax.plot(x, lognorm.pdf(x, s),\n    ...        'r-', lw=5, alpha=0.6, label='lognorm pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = lognorm(s)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = lognorm.ppf([0.001, 0.5, 0.999], s)\n    >>> np.allclose([0.001, 0.5, 0.999], lognorm.cdf(vals, s))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = lognorm.rvs(s, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    The logarithm of a log-normally distributed random variable is\n    normally distributed:\n\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> fig, ax = plt.subplots(1, 1)\n    >>> mu, sigma = 2, 0.5\n    >>> X = stats.norm(loc=mu, scale=sigma)\n    >>> Y = stats.lognorm(s=sigma, scale=np.exp(mu))\n    >>> x = np.linspace(*X.interval(0.999))\n    >>> y = Y.rvs(size=10000)\n    >>> ax.plot(x, X.pdf(x), label='X (pdf)')\n    >>> ax.hist(np.log(y), density=True, bins=x, label='log(Y) (histogram)')\n    >>> ax.legend()\n    >>> plt.show()\n\n    ",
    "scipy.stats.logrank": "Compare the survival distributions of two samples via the logrank test.\n\n    Parameters\n    ----------\n    x, y : array_like or CensoredData\n        Samples to compare based on their empirical survival functions.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n\n        The null hypothesis is that the survival distributions of the two\n        groups, say *X* and *Y*, are identical.\n\n        The following alternative hypotheses [4]_ are available (default is\n        'two-sided'):\n\n        * 'two-sided': the survival distributions of the two groups are not\n          identical.\n        * 'less': survival of group *X* is favored: the group *X* failure rate\n          function is less than the group *Y* failure rate function at some\n          times.\n        * 'greater': survival of group *Y* is favored: the group *X* failure\n          rate function is greater than the group *Y* failure rate function at\n          some times.\n\n    Returns\n    -------\n    res : `~scipy.stats._result_classes.LogRankResult`\n        An object containing attributes:\n\n        statistic : float ndarray\n            The computed statistic (defined below). Its magnitude is the\n            square root of the magnitude returned by most other logrank test\n            implementations.\n        pvalue : float ndarray\n            The computed p-value of the test.\n\n    See Also\n    --------\n    scipy.stats.ecdf\n\n    Notes\n    -----\n    The logrank test [1]_ compares the observed number of events to\n    the expected number of events under the null hypothesis that the two\n    samples were drawn from the same distribution. The statistic is\n\n    .. math::\n\n        Z_i = \\frac{\\sum_{j=1}^J(O_{i,j}-E_{i,j})}{\\sqrt{\\sum_{j=1}^J V_{i,j}}}\n        \\rightarrow \\mathcal{N}(0,1)\n\n    where\n\n    .. math::\n\n        E_{i,j} = O_j \\frac{N_{i,j}}{N_j},\n        \\qquad\n        V_{i,j} = E_{i,j} \\left(\\frac{N_j-O_j}{N_j}\\right)\n        \\left(\\frac{N_j-N_{i,j}}{N_j-1}\\right),\n\n    :math:`i` denotes the group (i.e. it may assume values :math:`x` or\n    :math:`y`, or it may be omitted to refer to the combined sample)\n    :math:`j` denotes the time (at which an event occurred),\n    :math:`N` is the number of subjects at risk just before an event occurred,\n    and :math:`O` is the observed number of events at that time.\n\n    The ``statistic`` :math:`Z_x` returned by `logrank` is the (signed) square\n    root of the statistic returned by many other implementations. Under the\n    null hypothesis, :math:`Z_x**2` is asymptotically distributed according to\n    the chi-squared distribution with one degree of freedom. Consequently,\n    :math:`Z_x` is asymptotically distributed according to the standard normal\n    distribution. The advantage of using :math:`Z_x` is that the sign\n    information (i.e. whether the observed number of events tends to be less\n    than or greater than the number expected under the null hypothesis) is\n    preserved, allowing `scipy.stats.logrank` to offer one-sided alternative\n    hypotheses.\n\n    References\n    ----------\n    .. [1] Mantel N. \"Evaluation of survival data and two new rank order\n           statistics arising in its consideration.\"\n           Cancer Chemotherapy Reports, 50(3):163-170, PMID: 5910392, 1966\n    .. [2] Bland, Altman, \"The logrank test\", BMJ, 328:1073,\n           :doi:`10.1136/bmj.328.7447.1073`, 2004\n    .. [3] \"Logrank test\", Wikipedia,\n           https://en.wikipedia.org/wiki/Logrank_test\n    .. [4] Brown, Mark. \"On the choice of variance for the log rank test.\"\n           Biometrika 71.1 (1984): 65-74.\n    .. [5] Klein, John P., and Melvin L. Moeschberger. Survival analysis:\n           techniques for censored and truncated data. Vol. 1230. New York:\n           Springer, 2003.\n\n    Examples\n    --------\n    Reference [2]_ compared the survival times of patients with two different\n    types of recurrent malignant gliomas. The samples below record the time\n    (number of weeks) for which each patient participated in the study. The\n    `scipy.stats.CensoredData` class is used because the data is\n    right-censored: the uncensored observations correspond with observed deaths\n    whereas the censored observations correspond with the patient leaving the\n    study for another reason.\n\n    >>> from scipy import stats\n    >>> x = stats.CensoredData(\n    ...     uncensored=[6, 13, 21, 30, 37, 38, 49, 50,\n    ...                 63, 79, 86, 98, 202, 219],\n    ...     right=[31, 47, 80, 82, 82, 149]\n    ... )\n    >>> y = stats.CensoredData(\n    ...     uncensored=[10, 10, 12, 13, 14, 15, 16, 17, 18, 20, 24, 24,\n    ...                 25, 28,30, 33, 35, 37, 40, 40, 46, 48, 76, 81,\n    ...                 82, 91, 112, 181],\n    ...     right=[34, 40, 70]\n    ... )\n\n    We can calculate and visualize the empirical survival functions\n    of both groups as follows.\n\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> ax = plt.subplot()\n    >>> ecdf_x = stats.ecdf(x)\n    >>> ecdf_x.sf.plot(ax, label='Astrocytoma')\n    >>> ecdf_y = stats.ecdf(y)\n    >>> ecdf_y.sf.plot(ax, label='Glioblastoma')\n    >>> ax.set_xlabel('Time to death (weeks)')\n    >>> ax.set_ylabel('Empirical SF')\n    >>> plt.legend()\n    >>> plt.show()\n\n    Visual inspection of the empirical survival functions suggests that the\n    survival times tend to be different between the two groups. To formally\n    assess whether the difference is significant at the 1% level, we use the\n    logrank test.\n\n    >>> res = stats.logrank(x=x, y=y)\n    >>> res.statistic\n    -2.73799\n    >>> res.pvalue\n    0.00618\n\n    The p-value is less than 1%, so we can consider the data to be evidence\n    against the null hypothesis in favor of the alternative that there is a\n    difference between the two survival functions.\n\n    ",
    "scipy.stats.logser": "A Logarithmic (Log-Series, Series) discrete random variable.\n\n    As an instance of the `rv_discrete` class, `logser` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(p, loc=0, size=1, random_state=None)\n        Random variates.\n    pmf(k, p, loc=0)\n        Probability mass function.\n    logpmf(k, p, loc=0)\n        Log of the probability mass function.\n    cdf(k, p, loc=0)\n        Cumulative distribution function.\n    logcdf(k, p, loc=0)\n        Log of the cumulative distribution function.\n    sf(k, p, loc=0)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(k, p, loc=0)\n        Log of the survival function.\n    ppf(q, p, loc=0)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, p, loc=0)\n        Inverse survival function (inverse of ``sf``).\n    stats(p, loc=0, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(p, loc=0)\n        (Differential) entropy of the RV.\n    expect(func, args=(p,), loc=0, lb=None, ub=None, conditional=False)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(p, loc=0)\n        Median of the distribution.\n    mean(p, loc=0)\n        Mean of the distribution.\n    var(p, loc=0)\n        Variance of the distribution.\n    std(p, loc=0)\n        Standard deviation of the distribution.\n    interval(confidence, p, loc=0)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability mass function for `logser` is:\n\n    .. math::\n\n        f(k) = - \\frac{p^k}{k \\log(1-p)}\n\n    for :math:`k \\ge 1`, :math:`0 < p < 1`\n\n    `logser` takes :math:`p` as shape parameter,\n    where :math:`p` is the probability of a single success\n    and :math:`1-p` is the probability of a single failure.\n\n    The probability mass function above is defined in the \"standardized\" form.\n    To shift distribution use the ``loc`` parameter.\n    Specifically, ``logser.pmf(k, p, loc)`` is identically\n    equivalent to ``logser.pmf(k - loc, p)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import logser\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> p = 0.6\n    >>> mean, var, skew, kurt = logser.stats(p, moments='mvsk')\n    \n    Display the probability mass function (``pmf``):\n    \n    >>> x = np.arange(logser.ppf(0.01, p),\n    ...               logser.ppf(0.99, p))\n    >>> ax.plot(x, logser.pmf(x, p), 'bo', ms=8, label='logser pmf')\n    >>> ax.vlines(x, 0, logser.pmf(x, p), colors='b', lw=5, alpha=0.5)\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape and location. This returns a \"frozen\" RV object holding\n    the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pmf``:\n    \n    >>> rv = logser(p)\n    >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n    ...         label='frozen pmf')\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> prob = logser.cdf(x, p)\n    >>> np.allclose(x, logser.ppf(prob, p))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = logser.rvs(p, size=1000)\n\n    ",
    "scipy.stats.loguniform": "A loguniform or reciprocal continuous random variable.\n\n    As an instance of the `rv_continuous` class, `loguniform` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, a, b, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, a, b, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, a, b, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, a, b, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, a, b, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, a, b, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, a, b, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, b, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, a, b, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(a, b, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, b, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, b, loc=0, scale=1)\n        Median of the distribution.\n    mean(a, b, loc=0, scale=1)\n        Mean of the distribution.\n    var(a, b, loc=0, scale=1)\n        Variance of the distribution.\n    std(a, b, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, a, b, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for this class is:\n\n    .. math::\n\n        f(x, a, b) = \\frac{1}{x \\log(b/a)}\n\n    for :math:`a \\le x \\le b`, :math:`b > a > 0`. This class takes\n    :math:`a` and :math:`b` as shape parameters.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``loguniform.pdf(x, a, b, loc, scale)`` is identically\n    equivalent to ``loguniform.pdf(y, a, b) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import loguniform\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a, b = 0.01, 1.25\n    >>> mean, var, skew, kurt = loguniform.stats(a, b, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(loguniform.ppf(0.01, a, b),\n    ...                 loguniform.ppf(0.99, a, b), 100)\n    >>> ax.plot(x, loguniform.pdf(x, a, b),\n    ...        'r-', lw=5, alpha=0.6, label='loguniform pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = loguniform(a, b)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = loguniform.ppf([0.001, 0.5, 0.999], a, b)\n    >>> np.allclose([0.001, 0.5, 0.999], loguniform.cdf(vals, a, b))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = loguniform.rvs(a, b, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    This doesn't show the equal probability of ``0.01``, ``0.1`` and\n    ``1``. This is best when the x-axis is log-scaled:\n\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    >>> ax.hist(np.log10(r))\n    >>> ax.set_ylabel(\"Frequency\")\n    >>> ax.set_xlabel(\"Value of random variable\")\n    >>> ax.xaxis.set_major_locator(plt.FixedLocator([-2, -1, 0]))\n    >>> ticks = [\"$10^{{ {} }}$\".format(i) for i in [-2, -1, 0]]\n    >>> ax.set_xticklabels(ticks)  # doctest: +SKIP\n    >>> plt.show()\n\n    This random variable will be log-uniform regardless of the base chosen for\n    ``a`` and ``b``. Let's specify with base ``2`` instead:\n\n    >>> rvs = loguniform(2**-2, 2**0).rvs(size=1000)\n\n    Values of ``1/4``, ``1/2`` and ``1`` are equally likely with this random\n    variable.  Here's the histogram:\n\n    >>> fig, ax = plt.subplots(1, 1)\n    >>> ax.hist(np.log2(rvs))\n    >>> ax.set_ylabel(\"Frequency\")\n    >>> ax.set_xlabel(\"Value of random variable\")\n    >>> ax.xaxis.set_major_locator(plt.FixedLocator([-2, -1, 0]))\n    >>> ticks = [\"$2^{{ {} }}$\".format(i) for i in [-2, -1, 0]]\n    >>> ax.set_xticklabels(ticks)  # doctest: +SKIP\n    >>> plt.show()\n\n    ",
    "scipy.stats.lomax": "A Lomax (Pareto of the second kind) continuous random variable.\n\n    As an instance of the `rv_continuous` class, `lomax` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `lomax` is:\n\n    .. math::\n\n        f(x, c) = \\frac{c}{(1+x)^{c+1}}\n\n    for :math:`x \\ge 0`, :math:`c > 0`.\n\n    `lomax` takes ``c`` as a shape parameter for :math:`c`.\n\n    `lomax` is a special case of `pareto` with ``loc=-1.0``.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``lomax.pdf(x, c, loc, scale)`` is identically\n    equivalent to ``lomax.pdf(y, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import lomax\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c = 1.88\n    >>> mean, var, skew, kurt = lomax.stats(c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(lomax.ppf(0.01, c),\n    ...                 lomax.ppf(0.99, c), 100)\n    >>> ax.plot(x, lomax.pdf(x, c),\n    ...        'r-', lw=5, alpha=0.6, label='lomax pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = lomax(c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = lomax.ppf([0.001, 0.5, 0.999], c)\n    >>> np.allclose([0.001, 0.5, 0.999], lomax.cdf(vals, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = lomax.rvs(c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.mannwhitneyu": "    \n\n\nPerform the Mann-Whitney U rank test on two independent samples.\n\nThe Mann-Whitney U test is a nonparametric test of the null hypothesis\nthat the distribution underlying sample `x` is the same as the\ndistribution underlying sample `y`. It is often used as a test of\ndifference in location between distributions.\n\nParameters\n----------\nx, y : array-like\n    N-d arrays of samples. The arrays must be broadcastable except along\n    the dimension given by `axis`.\nuse_continuity : bool, optional\n    Whether a continuity correction (1/2) should be applied.\n    Default is True when `method` is ``'asymptotic'``; has no effect\n    otherwise.\nalternative : {'two-sided', 'less', 'greater'}, optional\n    Defines the alternative hypothesis. Default is 'two-sided'.\n    Let *F(u)* and *G(u)* be the cumulative distribution functions of the\n    distributions underlying `x` and `y`, respectively. Then the following\n    alternative hypotheses are available:\n    \n    * 'two-sided': the distributions are not equal, i.e. *F(u) \u2260 G(u)* for\n      at least one *u*.\n    * 'less': the distribution underlying `x` is stochastically less\n      than the distribution underlying `y`, i.e. *F(u) > G(u)* for all *u*.\n    * 'greater': the distribution underlying `x` is stochastically greater\n      than the distribution underlying `y`, i.e. *F(u) < G(u)* for all *u*.\n    \n    Note that the mathematical expressions in the alternative hypotheses\n    above describe the CDFs of the underlying distributions. The directions\n    of the inequalities appear inconsistent with the natural language\n    description at first glance, but they are not. For example, suppose\n    *X* and *Y* are random variables that follow distributions with CDFs\n    *F* and *G*, respectively. If *F(u) > G(u)* for all *u*, samples drawn\n    from *X* tend to be less than those drawn from *Y*.\n    \n    Under a more restrictive set of assumptions, the alternative hypotheses\n    can be expressed in terms of the locations of the distributions;\n    see [5] section 5.1.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nmethod : {'auto', 'asymptotic', 'exact'} or `PermutationMethod` instance, optional\n    Selects the method used to calculate the *p*-value.\n    Default is 'auto'. The following options are available.\n    \n    * ``'asymptotic'``: compares the standardized test statistic\n      against the normal distribution, correcting for ties.\n    * ``'exact'``: computes the exact *p*-value by comparing the observed\n      :math:`U` statistic against the exact distribution of the :math:`U`\n      statistic under the null hypothesis. No correction is made for ties.\n    * ``'auto'``: chooses ``'exact'`` when the size of one of the samples\n      is less than or equal to 8 and there are no ties;\n      chooses ``'asymptotic'`` otherwise.\n    * `PermutationMethod` instance. In this case, the p-value\n      is computed using `permutation_test` with the provided\n      configuration options and other appropriate settings.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nres : MannwhitneyuResult\n    An object containing attributes:\n    \n    statistic : float\n        The Mann-Whitney U statistic corresponding with sample `x`. See\n        Notes for the test statistic corresponding with sample `y`.\n    pvalue : float\n        The associated *p*-value for the chosen `alternative`.\n\nSee Also\n--------\n\n:func:`scipy.stats.wilcoxon`, :func:`scipy.stats.ranksums`, :func:`scipy.stats.ttest_ind`\n    ..\n\nNotes\n-----\nIf ``U1`` is the statistic corresponding with sample `x`, then the\nstatistic corresponding with sample `y` is\n``U2 = x.shape[axis] * y.shape[axis] - U1``.\n\n`mannwhitneyu` is for independent samples. For related / paired samples,\nconsider `scipy.stats.wilcoxon`.\n\n`method` ``'exact'`` is recommended when there are no ties and when either\nsample size is less than 8 [1]_. The implementation follows the algorithm\nreported in [3]_.\nNote that the exact method is *not* corrected for ties, but\n`mannwhitneyu` will not raise errors or warnings if there are ties in the\ndata. If there are ties and either samples is small (fewer than ~10\nobservations), consider passing an instance of `PermutationMethod`\nas the `method` to perform a permutation test.\n\nThe Mann-Whitney U test is a non-parametric version of the t-test for\nindependent samples. When the means of samples from the populations\nare normally distributed, consider `scipy.stats.ttest_ind`.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] H.B. Mann and D.R. Whitney, \"On a test of whether one of two random\n       variables is stochastically larger than the other\", The Annals of\n       Mathematical Statistics, Vol. 18, pp. 50-60, 1947.\n.. [2] Mann-Whitney U Test, Wikipedia,\n       http://en.wikipedia.org/wiki/Mann-Whitney_U_test\n.. [3] Andreas L\u00f6ffler,\n       \"\u00dcber eine Partition der nat. Zahlen und ihr Anwendung beim U-Test\",\n       Wiss. Z. Univ. Halle, XXXII'83 pp. 87-89.\n.. [4] Rosie Shier, \"Statistics: 2.3 The Mann-Whitney U Test\", Mathematics\n       Learning Support Centre, 2004.\n.. [5] Michael P. Fay and Michael A. Proschan. \"Wilcoxon-Mann-Whitney\n       or t-test? On assumptions for hypothesis tests and multiple \\\n       interpretations of decision rules.\" Statistics surveys, Vol. 4, pp.\n       1-39, 2010. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2857732/\n\nExamples\n--------\nWe follow the example from [4]_: nine randomly sampled young adults were\ndiagnosed with type II diabetes at the ages below.\n\n>>> males = [19, 22, 16, 29, 24]\n>>> females = [20, 11, 17, 12]\n\nWe use the Mann-Whitney U test to assess whether there is a statistically\nsignificant difference in the diagnosis age of males and females.\nThe null hypothesis is that the distribution of male diagnosis ages is\nthe same as the distribution of female diagnosis ages. We decide\nthat a confidence level of 95% is required to reject the null hypothesis\nin favor of the alternative that the distributions are different.\nSince the number of samples is very small and there are no ties in the\ndata, we can compare the observed test statistic against the *exact*\ndistribution of the test statistic under the null hypothesis.\n\n>>> from scipy.stats import mannwhitneyu\n>>> U1, p = mannwhitneyu(males, females, method=\"exact\")\n>>> print(U1)\n17.0\n\n`mannwhitneyu` always reports the statistic associated with the first\nsample, which, in this case, is males. This agrees with :math:`U_M = 17`\nreported in [4]_. The statistic associated with the second statistic\ncan be calculated:\n\n>>> nx, ny = len(males), len(females)\n>>> U2 = nx*ny - U1\n>>> print(U2)\n3.0\n\nThis agrees with :math:`U_F = 3` reported in [4]_. The two-sided\n*p*-value can be calculated from either statistic, and the value produced\nby `mannwhitneyu` agrees with :math:`p = 0.11` reported in [4]_.\n\n>>> print(p)\n0.1111111111111111\n\nThe exact distribution of the test statistic is asymptotically normal, so\nthe example continues by comparing the exact *p*-value against the\n*p*-value produced using the normal approximation.\n\n>>> _, pnorm = mannwhitneyu(males, females, method=\"asymptotic\")\n>>> print(pnorm)\n0.11134688653314041\n\nHere `mannwhitneyu`'s reported *p*-value appears to conflict with the\nvalue :math:`p = 0.09` given in [4]_. The reason is that [4]_\ndoes not apply the continuity correction performed by `mannwhitneyu`;\n`mannwhitneyu` reduces the distance between the test statistic and the\nmean :math:`\\mu = n_x n_y / 2` by 0.5 to correct for the fact that the\ndiscrete statistic is being compared against a continuous distribution.\nHere, the :math:`U` statistic used is less than the mean, so we reduce\nthe distance by adding 0.5 in the numerator.\n\n>>> import numpy as np\n>>> from scipy.stats import norm\n>>> U = min(U1, U2)\n>>> N = nx + ny\n>>> z = (U - nx*ny/2 + 0.5) / np.sqrt(nx*ny * (N + 1)/ 12)\n>>> p = 2 * norm.cdf(z)  # use CDF to get p-value from smaller statistic\n>>> print(p)\n0.11134688653314041\n\nIf desired, we can disable the continuity correction to get a result\nthat agrees with that reported in [4]_.\n\n>>> _, pnorm = mannwhitneyu(males, females, use_continuity=False,\n...                         method=\"asymptotic\")\n>>> print(pnorm)\n0.0864107329737\n\nRegardless of whether we perform an exact or asymptotic test, the\nprobability of the test statistic being as extreme or more extreme by\nchance exceeds 5%, so we do not consider the results statistically\nsignificant.\n\nSuppose that, before seeing the data, we had hypothesized that females\nwould tend to be diagnosed at a younger age than males.\nIn that case, it would be natural to provide the female ages as the\nfirst input, and we would have performed a one-sided test using\n``alternative = 'less'``: females are diagnosed at an age that is\nstochastically less than that of males.\n\n>>> res = mannwhitneyu(females, males, alternative=\"less\", method=\"exact\")\n>>> print(res)\nMannwhitneyuResult(statistic=3.0, pvalue=0.05555555555555555)\n\nAgain, the probability of getting a sufficiently low value of the\ntest statistic by chance under the null hypothesis is greater than 5%,\nso we do not reject the null hypothesis in favor of our alternative.\n\nIf it is reasonable to assume that the means of samples from the\npopulations are normally distributed, we could have used a t-test to\nperform the analysis.\n\n>>> from scipy.stats import ttest_ind\n>>> res = ttest_ind(females, males, alternative=\"less\")\n>>> print(res)\nTtestResult(statistic=-2.239334696520584,\n            pvalue=0.030068441095757924,\n            df=7.0)\n\nUnder this assumption, the *p*-value would be low enough to reject the\nnull hypothesis in favor of the alternative.\n",
    "scipy.stats.matrix_normal": "A matrix normal random variable.\n\n    The `mean` keyword specifies the mean. The `rowcov` keyword specifies the\n    among-row covariance matrix. The 'colcov' keyword specifies the\n    among-column covariance matrix.\n\n    Methods\n    -------\n    pdf(X, mean=None, rowcov=1, colcov=1)\n        Probability density function.\n    logpdf(X, mean=None, rowcov=1, colcov=1)\n        Log of the probability density function.\n    rvs(mean=None, rowcov=1, colcov=1, size=1, random_state=None)\n        Draw random samples.\n    entropy(rowcol=1, colcov=1)\n        Differential entropy.\n\n    Parameters\n    ----------\n    mean : array_like, optional\n        Mean of the distribution (default: `None`)\n    rowcov : array_like, optional\n        Among-row covariance matrix of the distribution (default: `1`)\n    colcov : array_like, optional\n        Among-column covariance matrix of the distribution (default: `1`)\n    seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n        Used for drawing random variates.\n        If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used, seeded\n        with seed.\n        If `seed` is already a ``RandomState`` or ``Generator`` instance,\n        then that object is used.\n        Default is `None`.\n\n    Notes\n    -----\n    If `mean` is set to `None` then a matrix of zeros is used for the mean.\n    The dimensions of this matrix are inferred from the shape of `rowcov` and\n    `colcov`, if these are provided, or set to `1` if ambiguous.\n    \n    `rowcov` and `colcov` can be two-dimensional array_likes specifying the\n    covariance matrices directly. Alternatively, a one-dimensional array will\n    be be interpreted as the entries of a diagonal matrix, and a scalar or\n    zero-dimensional array will be interpreted as this value times the\n    identity matrix.\n\n    The covariance matrices specified by `rowcov` and `colcov` must be\n    (symmetric) positive definite. If the samples in `X` are\n    :math:`m \\times n`, then `rowcov` must be :math:`m \\times m` and\n    `colcov` must be :math:`n \\times n`. `mean` must be the same shape as `X`.\n\n    The probability density function for `matrix_normal` is\n\n    .. math::\n\n        f(X) = (2 \\pi)^{-\\frac{mn}{2}}|U|^{-\\frac{n}{2}} |V|^{-\\frac{m}{2}}\n               \\exp\\left( -\\frac{1}{2} \\mathrm{Tr}\\left[ U^{-1} (X-M) V^{-1}\n               (X-M)^T \\right] \\right),\n\n    where :math:`M` is the mean, :math:`U` the among-row covariance matrix,\n    :math:`V` the among-column covariance matrix.\n\n    The `allow_singular` behaviour of the `multivariate_normal`\n    distribution is not currently supported. Covariance matrices must be\n    full rank.\n\n    The `matrix_normal` distribution is closely related to the\n    `multivariate_normal` distribution. Specifically, :math:`\\mathrm{Vec}(X)`\n    (the vector formed by concatenating the columns  of :math:`X`) has a\n    multivariate normal distribution with mean :math:`\\mathrm{Vec}(M)`\n    and covariance :math:`V \\otimes U` (where :math:`\\otimes` is the Kronecker\n    product). Sampling and pdf evaluation are\n    :math:`\\mathcal{O}(m^3 + n^3 + m^2 n + m n^2)` for the matrix normal, but\n    :math:`\\mathcal{O}(m^3 n^3)` for the equivalent multivariate normal,\n    making this equivalent form algorithmically inefficient.\n\n    .. versionadded:: 0.17.0\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> from scipy.stats import matrix_normal\n\n    >>> M = np.arange(6).reshape(3,2); M\n    array([[0, 1],\n           [2, 3],\n           [4, 5]])\n    >>> U = np.diag([1,2,3]); U\n    array([[1, 0, 0],\n           [0, 2, 0],\n           [0, 0, 3]])\n    >>> V = 0.3*np.identity(2); V\n    array([[ 0.3,  0. ],\n           [ 0. ,  0.3]])\n    >>> X = M + 0.1; X\n    array([[ 0.1,  1.1],\n           [ 2.1,  3.1],\n           [ 4.1,  5.1]])\n    >>> matrix_normal.pdf(X, mean=M, rowcov=U, colcov=V)\n    0.023410202050005054\n\n    >>> # Equivalent multivariate normal\n    >>> from scipy.stats import multivariate_normal\n    >>> vectorised_X = X.T.flatten()\n    >>> equiv_mean = M.T.flatten()\n    >>> equiv_cov = np.kron(V,U)\n    >>> multivariate_normal.pdf(vectorised_X, mean=equiv_mean, cov=equiv_cov)\n    0.023410202050005054\n\n    Alternatively, the object may be called (as a function) to fix the mean\n    and covariance parameters, returning a \"frozen\" matrix normal\n    random variable:\n\n    >>> rv = matrix_normal(mean=None, rowcov=1, colcov=1)\n    >>> # Frozen object with the same methods but holding the given\n    >>> # mean and covariance fixed.\n\n    ",
    "scipy.stats.maxwell": "A Maxwell continuous random variable.\n\n    As an instance of the `rv_continuous` class, `maxwell` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    A special case of a `chi` distribution,  with ``df=3``, ``loc=0.0``,\n    and given ``scale = a``, where ``a`` is the parameter used in the\n    Mathworld description [1]_.\n\n    The probability density function for `maxwell` is:\n\n    .. math::\n\n        f(x) = \\sqrt{2/\\pi}x^2 \\exp(-x^2/2)\n\n    for :math:`x >= 0`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``maxwell.pdf(x, loc, scale)`` is identically\n    equivalent to ``maxwell.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] http://mathworld.wolfram.com/MaxwellDistribution.html\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import maxwell\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    \n    >>> mean, var, skew, kurt = maxwell.stats(moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(maxwell.ppf(0.01),\n    ...                 maxwell.ppf(0.99), 100)\n    >>> ax.plot(x, maxwell.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='maxwell pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = maxwell()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = maxwell.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], maxwell.cdf(vals))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = maxwell.rvs(size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n    ",
    "scipy.stats.median_abs_deviation": "\n    Compute the median absolute deviation of the data along the given axis.\n\n    The median absolute deviation (MAD, [1]_) computes the median over the\n    absolute deviations from the median. It is a measure of dispersion\n    similar to the standard deviation but more robust to outliers [2]_.\n\n    The MAD of an empty array is ``np.nan``.\n\n    .. versionadded:: 1.5.0\n\n    Parameters\n    ----------\n    x : array_like\n        Input array or object that can be converted to an array.\n    axis : int or None, optional\n        Axis along which the range is computed. Default is 0. If None, compute\n        the MAD over the entire array.\n    center : callable, optional\n        A function that will return the central value. The default is to use\n        np.median. Any user defined function used will need to have the\n        function signature ``func(arr, axis)``.\n    scale : scalar or str, optional\n        The numerical value of scale will be divided out of the final\n        result. The default is 1.0. The string \"normal\" is also accepted,\n        and results in `scale` being the inverse of the standard normal\n        quantile function at 0.75, which is approximately 0.67449.\n        Array-like scale is also allowed, as long as it broadcasts correctly\n        to the output such that ``out / scale`` is a valid operation. The\n        output dimensions depend on the input array, `x`, and the `axis`\n        argument.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    mad : scalar or ndarray\n        If ``axis=None``, a scalar is returned. If the input contains\n        integers or floats of smaller precision than ``np.float64``, then the\n        output data-type is ``np.float64``. Otherwise, the output data-type is\n        the same as that of the input.\n\n    See Also\n    --------\n    numpy.std, numpy.var, numpy.median, scipy.stats.iqr, scipy.stats.tmean,\n    scipy.stats.tstd, scipy.stats.tvar\n\n    Notes\n    -----\n    The `center` argument only affects the calculation of the central value\n    around which the MAD is calculated. That is, passing in ``center=np.mean``\n    will calculate the MAD around the mean - it will not calculate the *mean*\n    absolute deviation.\n\n    The input array may contain `inf`, but if `center` returns `inf`, the\n    corresponding MAD for that data will be `nan`.\n\n    References\n    ----------\n    .. [1] \"Median absolute deviation\",\n           https://en.wikipedia.org/wiki/Median_absolute_deviation\n    .. [2] \"Robust measures of scale\",\n           https://en.wikipedia.org/wiki/Robust_measures_of_scale\n\n    Examples\n    --------\n    When comparing the behavior of `median_abs_deviation` with ``np.std``,\n    the latter is affected when we change a single value of an array to have an\n    outlier value while the MAD hardly changes:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = stats.norm.rvs(size=100, scale=1, random_state=123456)\n    >>> x.std()\n    0.9973906394005013\n    >>> stats.median_abs_deviation(x)\n    0.82832610097857\n    >>> x[0] = 345.6\n    >>> x.std()\n    34.42304872314415\n    >>> stats.median_abs_deviation(x)\n    0.8323442311590675\n\n    Axis handling example:\n\n    >>> x = np.array([[10, 7, 4], [3, 2, 1]])\n    >>> x\n    array([[10,  7,  4],\n           [ 3,  2,  1]])\n    >>> stats.median_abs_deviation(x)\n    array([3.5, 2.5, 1.5])\n    >>> stats.median_abs_deviation(x, axis=None)\n    2.0\n\n    Scale normal example:\n\n    >>> x = stats.norm.rvs(size=1000000, scale=2, random_state=123456)\n    >>> stats.median_abs_deviation(x)\n    1.3487398527041636\n    >>> stats.median_abs_deviation(x, scale='normal')\n    1.9996446978061115\n\n    ",
    "scipy.stats.median_test": "Perform a Mood's median test.\n\n    Test that two or more samples come from populations with the same median.\n\n    Let ``n = len(samples)`` be the number of samples.  The \"grand median\" of\n    all the data is computed, and a contingency table is formed by\n    classifying the values in each sample as being above or below the grand\n    median.  The contingency table, along with `correction` and `lambda_`,\n    are passed to `scipy.stats.chi2_contingency` to compute the test statistic\n    and p-value.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n        The set of samples.  There must be at least two samples.\n        Each sample must be a one-dimensional sequence containing at least\n        one value.  The samples are not required to have the same length.\n    ties : str, optional\n        Determines how values equal to the grand median are classified in\n        the contingency table.  The string must be one of::\n\n            \"below\":\n                Values equal to the grand median are counted as \"below\".\n            \"above\":\n                Values equal to the grand median are counted as \"above\".\n            \"ignore\":\n                Values equal to the grand median are not counted.\n\n        The default is \"below\".\n    correction : bool, optional\n        If True, *and* there are just two samples, apply Yates' correction\n        for continuity when computing the test statistic associated with\n        the contingency table.  Default is True.\n    lambda_ : float or str, optional\n        By default, the statistic computed in this test is Pearson's\n        chi-squared statistic.  `lambda_` allows a statistic from the\n        Cressie-Read power divergence family to be used instead.  See\n        `power_divergence` for details.\n        Default is 1 (Pearson's chi-squared statistic).\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan. 'propagate' returns nan,\n        'raise' throws an error, 'omit' performs the calculations ignoring nan\n        values. Default is 'propagate'.\n\n    Returns\n    -------\n    res : MedianTestResult\n        An object containing attributes:\n\n        statistic : float\n            The test statistic.  The statistic that is returned is determined\n            by `lambda_`.  The default is Pearson's chi-squared statistic.\n        pvalue : float\n            The p-value of the test.\n        median : float\n            The grand median.\n        table : ndarray\n            The contingency table.  The shape of the table is (2, n), where\n            n is the number of samples.  The first row holds the counts of the\n            values above the grand median, and the second row holds the counts\n            of the values below the grand median.  The table allows further\n            analysis with, for example, `scipy.stats.chi2_contingency`, or with\n            `scipy.stats.fisher_exact` if there are two samples, without having\n            to recompute the table.  If ``nan_policy`` is \"propagate\" and there\n            are nans in the input, the return value for ``table`` is ``None``.\n\n    See Also\n    --------\n    kruskal : Compute the Kruskal-Wallis H-test for independent samples.\n    mannwhitneyu : Computes the Mann-Whitney rank test on samples x and y.\n\n    Notes\n    -----\n    .. versionadded:: 0.15.0\n\n    References\n    ----------\n    .. [1] Mood, A. M., Introduction to the Theory of Statistics. McGraw-Hill\n        (1950), pp. 394-399.\n    .. [2] Zar, J. H., Biostatistical Analysis, 5th ed. Prentice Hall (2010).\n        See Sections 8.12 and 10.15.\n\n    Examples\n    --------\n    A biologist runs an experiment in which there are three groups of plants.\n    Group 1 has 16 plants, group 2 has 15 plants, and group 3 has 17 plants.\n    Each plant produces a number of seeds.  The seed counts for each group\n    are::\n\n        Group 1: 10 14 14 18 20 22 24 25 31 31 32 39 43 43 48 49\n        Group 2: 28 30 31 33 34 35 36 40 44 55 57 61 91 92 99\n        Group 3:  0  3  9 22 23 25 25 33 34 34 40 45 46 48 62 67 84\n\n    The following code applies Mood's median test to these samples.\n\n    >>> g1 = [10, 14, 14, 18, 20, 22, 24, 25, 31, 31, 32, 39, 43, 43, 48, 49]\n    >>> g2 = [28, 30, 31, 33, 34, 35, 36, 40, 44, 55, 57, 61, 91, 92, 99]\n    >>> g3 = [0, 3, 9, 22, 23, 25, 25, 33, 34, 34, 40, 45, 46, 48, 62, 67, 84]\n    >>> from scipy.stats import median_test\n    >>> res = median_test(g1, g2, g3)\n\n    The median is\n\n    >>> res.median\n    34.0\n\n    and the contingency table is\n\n    >>> res.table\n    array([[ 5, 10,  7],\n           [11,  5, 10]])\n\n    `p` is too large to conclude that the medians are not the same:\n\n    >>> res.pvalue\n    0.12609082774093244\n\n    The \"G-test\" can be performed by passing ``lambda_=\"log-likelihood\"`` to\n    `median_test`.\n\n    >>> res = median_test(g1, g2, g3, lambda_=\"log-likelihood\")\n    >>> res.pvalue\n    0.12224779737117837\n\n    The median occurs several times in the data, so we'll get a different\n    result if, for example, ``ties=\"above\"`` is used:\n\n    >>> res = median_test(g1, g2, g3, ties=\"above\")\n    >>> res.pvalue\n    0.063873276069553273\n\n    >>> res.table\n    array([[ 5, 11,  9],\n           [11,  4,  8]])\n\n    This example demonstrates that if the data set is not large and there\n    are values equal to the median, the p-value can be sensitive to the\n    choice of `ties`.\n\n    ",
    "scipy.stats.mielke": "A Mielke Beta-Kappa / Dagum continuous random variable.\n\n    As an instance of the `rv_continuous` class, `mielke` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(k, s, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, k, s, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, k, s, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, k, s, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, k, s, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, k, s, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, k, s, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, k, s, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, k, s, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, k, s, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(k, s, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(k, s, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(k, s), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(k, s, loc=0, scale=1)\n        Median of the distribution.\n    mean(k, s, loc=0, scale=1)\n        Mean of the distribution.\n    var(k, s, loc=0, scale=1)\n        Variance of the distribution.\n    std(k, s, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, k, s, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `mielke` is:\n\n    .. math::\n\n        f(x, k, s) = \\frac{k x^{k-1}}{(1+x^s)^{1+k/s}}\n\n    for :math:`x > 0` and :math:`k, s > 0`. The distribution is sometimes\n    called Dagum distribution ([2]_). It was already defined in [3]_, called\n    a Burr Type III distribution (`burr` with parameters ``c=s`` and\n    ``d=k/s``).\n\n    `mielke` takes ``k`` and ``s`` as shape parameters.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``mielke.pdf(x, k, s, loc, scale)`` is identically\n    equivalent to ``mielke.pdf(y, k, s) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] Mielke, P.W., 1973 \"Another Family of Distributions for Describing\n           and Analyzing Precipitation Data.\" J. Appl. Meteor., 12, 275-280\n    .. [2] Dagum, C., 1977 \"A new model for personal income distribution.\"\n           Economie Appliquee, 33, 327-367.\n    .. [3] Burr, I. W. \"Cumulative frequency functions\", Annals of\n           Mathematical Statistics, 13(2), pp 215-232 (1942).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import mielke\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> k, s = 10.4, 4.6\n    >>> mean, var, skew, kurt = mielke.stats(k, s, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(mielke.ppf(0.01, k, s),\n    ...                 mielke.ppf(0.99, k, s), 100)\n    >>> ax.plot(x, mielke.pdf(x, k, s),\n    ...        'r-', lw=5, alpha=0.6, label='mielke pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = mielke(k, s)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = mielke.ppf([0.001, 0.5, 0.999], k, s)\n    >>> np.allclose([0.001, 0.5, 0.999], mielke.cdf(vals, k, s))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = mielke.rvs(k, s, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.mode": "    \n\n\nReturn an array of the modal (most common) value in the passed array.\n\nIf there is more than one such value, only one is returned.\nThe bin-count for the modal bins is also returned.\n\nParameters\n----------\na : array_like\n    Numeric, n-dimensional array of which to find mode(s).\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nmode : ndarray\n    Array of modal values.\ncount : ndarray\n    Array of counts for each mode.\n\nNotes\n-----\nThe mode  is calculated using `numpy.unique`.\nIn NumPy versions 1.21 and after, all NaNs - even those with different\nbinary representations - are treated as equivalent and counted as separate\ninstances of the same value.\n\nBy convention, the mode of an empty array is NaN, and the associated count\nis zero.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nExamples\n--------\n>>> import numpy as np\n>>> a = np.array([[3, 0, 3, 7],\n...               [3, 2, 6, 2],\n...               [1, 7, 2, 8],\n...               [3, 0, 6, 1],\n...               [3, 2, 5, 5]])\n>>> from scipy import stats\n>>> stats.mode(a, keepdims=True)\nModeResult(mode=array([[3, 0, 6, 1]]), count=array([[4, 2, 2, 1]]))\n\nTo get mode of whole array, specify ``axis=None``:\n\n>>> stats.mode(a, axis=None, keepdims=True)\nModeResult(mode=[[3]], count=[[5]])\n>>> stats.mode(a, axis=None, keepdims=False)\nModeResult(mode=3, count=5)\n",
    "scipy.stats.moment": "    \n\n\nCalculate the nth moment about the mean for a sample.\n\nA moment is a specific quantitative measure of the shape of a set of\npoints. It is often used to calculate coefficients of skewness and kurtosis\ndue to its close relationship with them.\n\nParameters\n----------\na : array_like\n    Input array.\norder : int or 1-D array_like of ints, optional\n    Order of central moment that is returned. Default is 1.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\ncenter : float or None, optional\n    The point about which moments are taken. This can be the sample mean,\n    the origin, or any other be point. If `None` (default) compute the\n    center as the sample mean.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nn-th moment about the `center` : ndarray or float\n    The appropriate moment along the given axis or over all values if axis\n    is None. The denominator for the moment calculation is the number of\n    observations, no degrees of freedom correction is done.\n\nSee Also\n--------\n\n:func:`kurtosis`, :func:`skew`, :func:`describe`\n    ..\n\nNotes\n-----\nThe k-th moment of a data sample is:\n\n.. math::\n\n    m_k = \\frac{1}{n} \\sum_{i = 1}^n (x_i - c)^k\n\nWhere `n` is the number of samples, and `c` is the center around which the\nmoment is calculated. This function uses exponentiation by squares [1]_ for\nefficiency.\n\nNote that, if `a` is an empty array (``a.size == 0``), array `moment` with\none element (`moment.size == 1`) is treated the same as scalar `moment`\n(``np.isscalar(moment)``). This might produce arrays of unexpected shape.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] https://eli.thegreenplace.net/2009/03/21/efficient-integer-exponentiation-algorithms\n\nExamples\n--------\n>>> from scipy.stats import moment\n>>> moment([1, 2, 3, 4, 5], order=1)\n0.0\n>>> moment([1, 2, 3, 4, 5], order=2)\n2.0\n",
    "scipy.stats.monte_carlo_test": "Perform a Monte Carlo hypothesis test.\n\n    `data` contains a sample or a sequence of one or more samples. `rvs`\n    specifies the distribution(s) of the sample(s) in `data` under the null\n    hypothesis. The value of `statistic` for the given `data` is compared\n    against a Monte Carlo null distribution: the value of the statistic for\n    each of `n_resamples` sets of samples generated using `rvs`. This gives\n    the p-value, the probability of observing such an extreme value of the\n    test statistic under the null hypothesis.\n\n    Parameters\n    ----------\n    data : array-like or sequence of array-like\n        An array or sequence of arrays of observations.\n    rvs : callable or tuple of callables\n        A callable or sequence of callables that generates random variates\n        under the null hypothesis. Each element of `rvs` must be a callable\n        that accepts keyword argument ``size`` (e.g. ``rvs(size=(m, n))``) and\n        returns an N-d array sample of that shape. If `rvs` is a sequence, the\n        number of callables in `rvs` must match the number of samples in\n        `data`, i.e. ``len(rvs) == len(data)``. If `rvs` is a single callable,\n        `data` is treated as a single sample.\n    statistic : callable\n        Statistic for which the p-value of the hypothesis test is to be\n        calculated. `statistic` must be a callable that accepts a sample\n        (e.g. ``statistic(sample)``) or ``len(rvs)`` separate samples (e.g.\n        ``statistic(samples1, sample2)`` if `rvs` contains two callables and\n        `data` contains two samples) and returns the resulting statistic.\n        If `vectorized` is set ``True``, `statistic` must also accept a keyword\n        argument `axis` and be vectorized to compute the statistic along the\n        provided `axis` of the samples in `data`.\n    vectorized : bool, optional\n        If `vectorized` is set ``False``, `statistic` will not be passed\n        keyword argument `axis` and is expected to calculate the statistic\n        only for 1D samples. If ``True``, `statistic` will be passed keyword\n        argument `axis` and is expected to calculate the statistic along `axis`\n        when passed ND sample arrays. If ``None`` (default), `vectorized`\n        will be set ``True`` if ``axis`` is a parameter of `statistic`. Use of\n        a vectorized statistic typically reduces computation time.\n    n_resamples : int, default: 9999\n        Number of samples drawn from each of the callables of `rvs`.\n        Equivalently, the number statistic values under the null hypothesis\n        used as the Monte Carlo null distribution.\n    batch : int, optional\n        The number of Monte Carlo samples to process in each call to\n        `statistic`. Memory usage is O( `batch` * ``sample.size[axis]`` ). Default\n        is ``None``, in which case `batch` equals `n_resamples`.\n    alternative : {'two-sided', 'less', 'greater'}\n        The alternative hypothesis for which the p-value is calculated.\n        For each alternative, the p-value is defined as follows.\n\n        - ``'greater'`` : the percentage of the null distribution that is\n          greater than or equal to the observed value of the test statistic.\n        - ``'less'`` : the percentage of the null distribution that is\n          less than or equal to the observed value of the test statistic.\n        - ``'two-sided'`` : twice the smaller of the p-values above.\n\n    axis : int, default: 0\n        The axis of `data` (or each sample within `data`) over which to\n        calculate the statistic.\n\n    Returns\n    -------\n    res : MonteCarloTestResult\n        An object with attributes:\n\n        statistic : float or ndarray\n            The test statistic of the observed `data`.\n        pvalue : float or ndarray\n            The p-value for the given alternative.\n        null_distribution : ndarray\n            The values of the test statistic generated under the null\n            hypothesis.\n\n    .. warning::\n        The p-value is calculated by counting the elements of the null\n        distribution that are as extreme or more extreme than the observed\n        value of the statistic. Due to the use of finite precision arithmetic,\n        some statistic functions return numerically distinct values when the\n        theoretical values would be exactly equal. In some cases, this could\n        lead to a large error in the calculated p-value. `monte_carlo_test`\n        guards against this by considering elements in the null distribution\n        that are \"close\" (within a relative tolerance of 100 times the\n        floating point epsilon of inexact dtypes) to the observed\n        value of the test statistic as equal to the observed value of the\n        test statistic. However, the user is advised to inspect the null\n        distribution to assess whether this method of comparison is\n        appropriate, and if not, calculate the p-value manually.\n\n    References\n    ----------\n\n    .. [1] B. Phipson and G. K. Smyth. \"Permutation P-values Should Never Be\n       Zero: Calculating Exact P-values When Permutations Are Randomly Drawn.\"\n       Statistical Applications in Genetics and Molecular Biology 9.1 (2010).\n\n    Examples\n    --------\n\n    Suppose we wish to test whether a small sample has been drawn from a normal\n    distribution. We decide that we will use the skew of the sample as a\n    test statistic, and we will consider a p-value of 0.05 to be statistically\n    significant.\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> def statistic(x, axis):\n    ...     return stats.skew(x, axis)\n\n    After collecting our data, we calculate the observed value of the test\n    statistic.\n\n    >>> rng = np.random.default_rng()\n    >>> x = stats.skewnorm.rvs(a=1, size=50, random_state=rng)\n    >>> statistic(x, axis=0)\n    0.12457412450240658\n\n    To determine the probability of observing such an extreme value of the\n    skewness by chance if the sample were drawn from the normal distribution,\n    we can perform a Monte Carlo hypothesis test. The test will draw many\n    samples at random from their normal distribution, calculate the skewness\n    of each sample, and compare our original skewness against this\n    distribution to determine an approximate p-value.\n\n    >>> from scipy.stats import monte_carlo_test\n    >>> # because our statistic is vectorized, we pass `vectorized=True`\n    >>> rvs = lambda size: stats.norm.rvs(size=size, random_state=rng)\n    >>> res = monte_carlo_test(x, rvs, statistic, vectorized=True)\n    >>> print(res.statistic)\n    0.12457412450240658\n    >>> print(res.pvalue)\n    0.7012\n\n    The probability of obtaining a test statistic less than or equal to the\n    observed value under the null hypothesis is ~70%. This is greater than\n    our chosen threshold of 5%, so we cannot consider this to be significant\n    evidence against the null hypothesis.\n\n    Note that this p-value essentially matches that of\n    `scipy.stats.skewtest`, which relies on an asymptotic distribution of a\n    test statistic based on the sample skewness.\n\n    >>> stats.skewtest(x).pvalue\n    0.6892046027110614\n\n    This asymptotic approximation is not valid for small sample sizes, but\n    `monte_carlo_test` can be used with samples of any size.\n\n    >>> x = stats.skewnorm.rvs(a=1, size=7, random_state=rng)\n    >>> # stats.skewtest(x) would produce an error due to small sample\n    >>> res = monte_carlo_test(x, rvs, statistic, vectorized=True)\n\n    The Monte Carlo distribution of the test statistic is provided for\n    further investigation.\n\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots()\n    >>> ax.hist(res.null_distribution, bins=50)\n    >>> ax.set_title(\"Monte Carlo distribution of test statistic\")\n    >>> ax.set_xlabel(\"Value of Statistic\")\n    >>> ax.set_ylabel(\"Frequency\")\n    >>> plt.show()\n\n    ",
    "scipy.stats.mood": "    \n\n\nPerform Mood's test for equal scale parameters.\n\nMood's two-sample test for scale parameters is a non-parametric\ntest for the null hypothesis that two samples are drawn from the\nsame distribution with the same scale parameter.\n\nParameters\n----------\nx, y : array_like\n    Arrays of sample data. There must be at least three observations\n    total.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nalternative : {'two-sided', 'less', 'greater'}, optional\n    Defines the alternative hypothesis. Default is 'two-sided'.\n    The following options are available:\n    \n    * 'two-sided': the scales of the distributions underlying `x` and `y`\n      are different.\n    * 'less': the scale of the distribution underlying `x` is less than\n      the scale of the distribution underlying `y`.\n    * 'greater': the scale of the distribution underlying `x` is greater\n      than the scale of the distribution underlying `y`.\n    \n    .. versionadded:: 1.7.0\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nres : SignificanceResult\n    An object containing attributes:\n    \n    statistic : scalar or ndarray\n        The z-score for the hypothesis test.  For 1-D inputs a scalar is\n        returned.\n    pvalue : scalar ndarray\n        The p-value for the hypothesis test.\n\nSee Also\n--------\n\n:func:`fligner`\n    A non-parametric test for the equality of k variances\n:func:`ansari`\n    A non-parametric test for the equality of 2 variances\n:func:`bartlett`\n    A parametric test for equality of k variances in normal samples\n:func:`levene`\n    A parametric test for equality of k variances\n\n\nNotes\n-----\nThe data are assumed to be drawn from probability distributions ``f(x)``\nand ``f(x/s) / s`` respectively, for some probability density function f.\nThe null hypothesis is that ``s == 1``.\n\nFor multi-dimensional arrays, if the inputs are of shapes\n``(n0, n1, n2, n3)``  and ``(n0, m1, n2, n3)``, then if ``axis=1``, the\nresulting z and p values will have shape ``(n0, n2, n3)``.  Note that\n``n1`` and ``m1`` don't have to be equal, but the other dimensions do.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n[1] Mielke, Paul W. \"Note on Some Squared Rank Tests with Existing Ties.\"\n    Technometrics, vol. 9, no. 2, 1967, pp. 312-14. JSTOR,\n    https://doi.org/10.2307/1266427. Accessed 18 May 2022.\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy import stats\n>>> rng = np.random.default_rng()\n>>> x2 = rng.standard_normal((2, 45, 6, 7))\n>>> x1 = rng.standard_normal((2, 30, 6, 7))\n>>> res = stats.mood(x1, x2, axis=1)\n>>> res.pvalue.shape\n(2, 6, 7)\n\nFind the number of points where the difference in scale is not significant:\n\n>>> (res.pvalue > 0.1).sum()\n78\n\nPerform the test with different scales:\n\n>>> x1 = rng.standard_normal((2, 30))\n>>> x2 = rng.standard_normal((2, 35)) * 10.0\n>>> stats.mood(x1, x2, axis=1)\nSignificanceResult(statistic=array([-5.76174136, -6.12650783]),\n                   pvalue=array([8.32505043e-09, 8.98287869e-10]))\n",
    "scipy.stats.moyal": "A Moyal continuous random variable.\n\n    As an instance of the `rv_continuous` class, `moyal` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `moyal` is:\n\n    .. math::\n\n        f(x) = \\exp(-(x + \\exp(-x))/2) / \\sqrt{2\\pi}\n\n    for a real number :math:`x`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``moyal.pdf(x, loc, scale)`` is identically\n    equivalent to ``moyal.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    This distribution has utility in high-energy physics and radiation\n    detection. It describes the energy loss of a charged relativistic\n    particle due to ionization of the medium [1]_. It also provides an\n    approximation for the Landau distribution. For an in depth description\n    see [2]_. For additional description, see [3]_.\n\n    References\n    ----------\n    .. [1] J.E. Moyal, \"XXX. Theory of ionization fluctuations\",\n           The London, Edinburgh, and Dublin Philosophical Magazine\n           and Journal of Science, vol 46, 263-280, (1955).\n           :doi:`10.1080/14786440308521076` (gated)\n    .. [2] G. Cordeiro et al., \"The beta Moyal: a useful skew distribution\",\n           International Journal of Research and Reviews in Applied Sciences,\n           vol 10, 171-192, (2012).\n           http://www.arpapress.com/Volumes/Vol10Issue2/IJRRAS_10_2_02.pdf\n    .. [3] C. Walck, \"Handbook on Statistical Distributions for\n           Experimentalists; International Report SUF-PFY/96-01\", Chapter 26,\n           University of Stockholm: Stockholm, Sweden, (2007).\n           http://www.stat.rice.edu/~dobelman/textfiles/DistributionsHandbook.pdf\n\n    .. versionadded:: 1.1.0\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import moyal\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    \n    >>> mean, var, skew, kurt = moyal.stats(moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(moyal.ppf(0.01),\n    ...                 moyal.ppf(0.99), 100)\n    >>> ax.plot(x, moyal.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='moyal pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = moyal()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = moyal.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], moyal.cdf(vals))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = moyal.rvs(size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.multinomial": "A multinomial random variable.\n\n    Methods\n    -------\n    pmf(x, n, p)\n        Probability mass function.\n    logpmf(x, n, p)\n        Log of the probability mass function.\n    rvs(n, p, size=1, random_state=None)\n        Draw random samples from a multinomial distribution.\n    entropy(n, p)\n        Compute the entropy of the multinomial distribution.\n    cov(n, p)\n        Compute the covariance matrix of the multinomial distribution.\n\n    Parameters\n    ----------\n    n : int\n        Number of trials\n    p : array_like\n        Probability of a trial falling into each category; should sum to 1\n    seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n        Used for drawing random variates.\n        If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used, seeded\n        with seed.\n        If `seed` is already a ``RandomState`` or ``Generator`` instance,\n        then that object is used.\n        Default is `None`.\n\n    Notes\n    -----\n    `n` should be a nonnegative integer. Each element of `p` should be in the\n    interval :math:`[0,1]` and the elements should sum to 1. If they do not sum to\n    1, the last element of the `p` array is not used and is replaced with the\n    remaining probability left over from the earlier elements.\n\n    The probability mass function for `multinomial` is\n\n    .. math::\n\n        f(x) = \\frac{n!}{x_1! \\cdots x_k!} p_1^{x_1} \\cdots p_k^{x_k},\n\n    supported on :math:`x=(x_1, \\ldots, x_k)` where each :math:`x_i` is a\n    nonnegative integer and their sum is :math:`n`.\n\n    .. versionadded:: 0.19.0\n\n    Examples\n    --------\n\n    >>> from scipy.stats import multinomial\n    >>> rv = multinomial(8, [0.3, 0.2, 0.5])\n    >>> rv.pmf([1, 3, 4])\n    0.042000000000000072\n\n    The multinomial distribution for :math:`k=2` is identical to the\n    corresponding binomial distribution (tiny numerical differences\n    notwithstanding):\n\n    >>> from scipy.stats import binom\n    >>> multinomial.pmf([3, 4], n=7, p=[0.4, 0.6])\n    0.29030399999999973\n    >>> binom.pmf(3, 7, 0.4)\n    0.29030400000000012\n\n    The functions ``pmf``, ``logpmf``, ``entropy``, and ``cov`` support\n    broadcasting, under the convention that the vector parameters (``x`` and\n    ``p``) are interpreted as if each row along the last axis is a single\n    object. For instance:\n\n    >>> multinomial.pmf([[3, 4], [3, 5]], n=[7, 8], p=[.3, .7])\n    array([0.2268945,  0.25412184])\n\n    Here, ``x.shape == (2, 2)``, ``n.shape == (2,)``, and ``p.shape == (2,)``,\n    but following the rules mentioned above they behave as if the rows\n    ``[3, 4]`` and ``[3, 5]`` in ``x`` and ``[.3, .7]`` in ``p`` were a single\n    object, and as if we had ``x.shape = (2,)``, ``n.shape = (2,)``, and\n    ``p.shape = ()``. To obtain the individual elements without broadcasting,\n    we would do this:\n\n    >>> multinomial.pmf([3, 4], n=7, p=[.3, .7])\n    0.2268945\n    >>> multinomial.pmf([3, 5], 8, p=[.3, .7])\n    0.25412184\n\n    This broadcasting also works for ``cov``, where the output objects are\n    square matrices of size ``p.shape[-1]``. For example:\n\n    >>> multinomial.cov([4, 5], [[.3, .7], [.4, .6]])\n    array([[[ 0.84, -0.84],\n            [-0.84,  0.84]],\n           [[ 1.2 , -1.2 ],\n            [-1.2 ,  1.2 ]]])\n\n    In this example, ``n.shape == (2,)`` and ``p.shape == (2, 2)``, and\n    following the rules above, these broadcast as if ``p.shape == (2,)``.\n    Thus the result should also be of shape ``(2,)``, but since each output is\n    a :math:`2 \\times 2` matrix, the result in fact has shape ``(2, 2, 2)``,\n    where ``result[0]`` is equal to ``multinomial.cov(n=4, p=[.3, .7])`` and\n    ``result[1]`` is equal to ``multinomial.cov(n=5, p=[.4, .6])``.\n\n    Alternatively, the object may be called (as a function) to fix the `n` and\n    `p` parameters, returning a \"frozen\" multinomial random variable:\n\n    >>> rv = multinomial(n=7, p=[.3, .7])\n    >>> # Frozen object with the same methods but holding the given\n    >>> # degrees of freedom and scale fixed.\n\n    See also\n    --------\n    scipy.stats.binom : The binomial distribution.\n    numpy.random.Generator.multinomial : Sampling from the multinomial distribution.\n    scipy.stats.multivariate_hypergeom :\n        The multivariate hypergeometric distribution.\n    ",
    "scipy.stats.multiscale_graphcorr": "Computes the Multiscale Graph Correlation (MGC) test statistic.\n\n    Specifically, for each point, MGC finds the :math:`k`-nearest neighbors for\n    one property (e.g. cloud density), and the :math:`l`-nearest neighbors for\n    the other property (e.g. grass wetness) [1]_. This pair :math:`(k, l)` is\n    called the \"scale\". A priori, however, it is not know which scales will be\n    most informative. So, MGC computes all distance pairs, and then efficiently\n    computes the distance correlations for all scales. The local correlations\n    illustrate which scales are relatively informative about the relationship.\n    The key, therefore, to successfully discover and decipher relationships\n    between disparate data modalities is to adaptively determine which scales\n    are the most informative, and the geometric implication for the most\n    informative scales. Doing so not only provides an estimate of whether the\n    modalities are related, but also provides insight into how the\n    determination was made. This is especially important in high-dimensional\n    data, where simple visualizations do not reveal relationships to the\n    unaided human eye. Characterizations of this implementation in particular\n    have been derived from and benchmarked within in [2]_.\n\n    Parameters\n    ----------\n    x, y : ndarray\n        If ``x`` and ``y`` have shapes ``(n, p)`` and ``(n, q)`` where `n` is\n        the number of samples and `p` and `q` are the number of dimensions,\n        then the MGC independence test will be run.  Alternatively, ``x`` and\n        ``y`` can have shapes ``(n, n)`` if they are distance or similarity\n        matrices, and ``compute_distance`` must be sent to ``None``. If ``x``\n        and ``y`` have shapes ``(n, p)`` and ``(m, p)``, an unpaired\n        two-sample MGC test will be run.\n    compute_distance : callable, optional\n        A function that computes the distance or similarity among the samples\n        within each data matrix. Set to ``None`` if ``x`` and ``y`` are\n        already distance matrices. The default uses the euclidean norm metric.\n        If you are calling a custom function, either create the distance\n        matrix before-hand or create a function of the form\n        ``compute_distance(x)`` where `x` is the data matrix for which\n        pairwise distances are calculated.\n    reps : int, optional\n        The number of replications used to estimate the null when using the\n        permutation test. The default is ``1000``.\n    workers : int or map-like callable, optional\n        If ``workers`` is an int the population is subdivided into ``workers``\n        sections and evaluated in parallel (uses ``multiprocessing.Pool\n        <multiprocessing>``). Supply ``-1`` to use all cores available to the\n        Process. Alternatively supply a map-like callable, such as\n        ``multiprocessing.Pool.map`` for evaluating the p-value in parallel.\n        This evaluation is carried out as ``workers(func, iterable)``.\n        Requires that `func` be pickleable. The default is ``1``.\n    is_twosamp : bool, optional\n        If `True`, a two sample test will be run. If ``x`` and ``y`` have\n        shapes ``(n, p)`` and ``(m, p)``, this optional will be overridden and\n        set to ``True``. Set to ``True`` if ``x`` and ``y`` both have shapes\n        ``(n, p)`` and a two sample test is desired. The default is ``False``.\n        Note that this will not run if inputs are distance matrices.\n    random_state : {None, int, `numpy.random.Generator`,\n                    `numpy.random.RandomState`}, optional\n\n        If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n        singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used,\n        seeded with `seed`.\n        If `seed` is already a ``Generator`` or ``RandomState`` instance then\n        that instance is used.\n\n    Returns\n    -------\n    res : MGCResult\n        An object containing attributes:\n\n        statistic : float\n            The sample MGC test statistic within `[-1, 1]`.\n        pvalue : float\n            The p-value obtained via permutation.\n        mgc_dict : dict\n            Contains additional useful results:\n\n                - mgc_map : ndarray\n                    A 2D representation of the latent geometry of the\n                    relationship.\n                - opt_scale : (int, int)\n                    The estimated optimal scale as a `(x, y)` pair.\n                - null_dist : list\n                    The null distribution derived from the permuted matrices.\n\n    See Also\n    --------\n    pearsonr : Pearson correlation coefficient and p-value for testing\n               non-correlation.\n    kendalltau : Calculates Kendall's tau.\n    spearmanr : Calculates a Spearman rank-order correlation coefficient.\n\n    Notes\n    -----\n    A description of the process of MGC and applications on neuroscience data\n    can be found in [1]_. It is performed using the following steps:\n\n    #. Two distance matrices :math:`D^X` and :math:`D^Y` are computed and\n       modified to be mean zero columnwise. This results in two\n       :math:`n \\times n` distance matrices :math:`A` and :math:`B` (the\n       centering and unbiased modification) [3]_.\n\n    #. For all values :math:`k` and :math:`l` from :math:`1, ..., n`,\n\n       * The :math:`k`-nearest neighbor and :math:`l`-nearest neighbor graphs\n         are calculated for each property. Here, :math:`G_k (i, j)` indicates\n         the :math:`k`-smallest values of the :math:`i`-th row of :math:`A`\n         and :math:`H_l (i, j)` indicates the :math:`l` smallested values of\n         the :math:`i`-th row of :math:`B`\n\n       * Let :math:`\\circ` denotes the entry-wise matrix product, then local\n         correlations are summed and normalized using the following statistic:\n\n    .. math::\n\n        c^{kl} = \\frac{\\sum_{ij} A G_k B H_l}\n                      {\\sqrt{\\sum_{ij} A^2 G_k \\times \\sum_{ij} B^2 H_l}}\n\n    #. The MGC test statistic is the smoothed optimal local correlation of\n       :math:`\\{ c^{kl} \\}`. Denote the smoothing operation as :math:`R(\\cdot)`\n       (which essentially set all isolated large correlations) as 0 and\n       connected large correlations the same as before, see [3]_.) MGC is,\n\n    .. math::\n\n        MGC_n (x, y) = \\max_{(k, l)} R \\left(c^{kl} \\left( x_n, y_n \\right)\n                                                    \\right)\n\n    The test statistic returns a value between :math:`(-1, 1)` since it is\n    normalized.\n\n    The p-value returned is calculated using a permutation test. This process\n    is completed by first randomly permuting :math:`y` to estimate the null\n    distribution and then calculating the probability of observing a test\n    statistic, under the null, at least as extreme as the observed test\n    statistic.\n\n    MGC requires at least 5 samples to run with reliable results. It can also\n    handle high-dimensional data sets.\n    In addition, by manipulating the input data matrices, the two-sample\n    testing problem can be reduced to the independence testing problem [4]_.\n    Given sample data :math:`U` and :math:`V` of sizes :math:`p \\times n`\n    :math:`p \\times m`, data matrix :math:`X` and :math:`Y` can be created as\n    follows:\n\n    .. math::\n\n        X = [U | V] \\in \\mathcal{R}^{p \\times (n + m)}\n        Y = [0_{1 \\times n} | 1_{1 \\times m}] \\in \\mathcal{R}^{(n + m)}\n\n    Then, the MGC statistic can be calculated as normal. This methodology can\n    be extended to similar tests such as distance correlation [4]_.\n\n    .. versionadded:: 1.4.0\n\n    References\n    ----------\n    .. [1] Vogelstein, J. T., Bridgeford, E. W., Wang, Q., Priebe, C. E.,\n           Maggioni, M., & Shen, C. (2019). Discovering and deciphering\n           relationships across disparate data modalities. ELife.\n    .. [2] Panda, S., Palaniappan, S., Xiong, J., Swaminathan, A.,\n           Ramachandran, S., Bridgeford, E. W., ... Vogelstein, J. T. (2019).\n           mgcpy: A Comprehensive High Dimensional Independence Testing Python\n           Package. :arXiv:`1907.02088`\n    .. [3] Shen, C., Priebe, C.E., & Vogelstein, J. T. (2019). From distance\n           correlation to multiscale graph correlation. Journal of the American\n           Statistical Association.\n    .. [4] Shen, C. & Vogelstein, J. T. (2018). The Exact Equivalence of\n           Distance and Kernel Methods for Hypothesis Testing.\n           :arXiv:`1806.05514`\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import multiscale_graphcorr\n    >>> x = np.arange(100)\n    >>> y = x\n    >>> res = multiscale_graphcorr(x, y)\n    >>> res.statistic, res.pvalue\n    (1.0, 0.001)\n\n    To run an unpaired two-sample test,\n\n    >>> x = np.arange(100)\n    >>> y = np.arange(79)\n    >>> res = multiscale_graphcorr(x, y)\n    >>> res.statistic, res.pvalue  # doctest: +SKIP\n    (0.033258146255703246, 0.023)\n\n    or, if shape of the inputs are the same,\n\n    >>> x = np.arange(100)\n    >>> y = x\n    >>> res = multiscale_graphcorr(x, y, is_twosamp=True)\n    >>> res.statistic, res.pvalue  # doctest: +SKIP\n    (-0.008021809890200488, 1.0)\n\n    ",
    "scipy.stats.multivariate_hypergeom": "A multivariate hypergeometric random variable.\n\n    Methods\n    -------\n    pmf(x, m, n)\n        Probability mass function.\n    logpmf(x, m, n)\n        Log of the probability mass function.\n    rvs(m, n, size=1, random_state=None)\n        Draw random samples from a multivariate hypergeometric\n        distribution.\n    mean(m, n)\n        Mean of the multivariate hypergeometric distribution.\n    var(m, n)\n        Variance of the multivariate hypergeometric distribution.\n    cov(m, n)\n        Compute the covariance matrix of the multivariate\n        hypergeometric distribution.\n\n    Parameters\n    ----------\n    m : array_like\n        The number of each type of object in the population.\n        That is, :math:`m[i]` is the number of objects of\n        type :math:`i`.\n    n : array_like\n        The number of samples taken from the population.\n    seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n        Used for drawing random variates.\n        If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used, seeded\n        with seed.\n        If `seed` is already a ``RandomState`` or ``Generator`` instance,\n        then that object is used.\n        Default is `None`.\n\n    Notes\n    -----\n    `m` must be an array of positive integers. If the quantile\n    :math:`i` contains values out of the range :math:`[0, m_i]`\n    where :math:`m_i` is the number of objects of type :math:`i`\n    in the population or if the parameters are inconsistent with one\n    another (e.g. ``x.sum() != n``), methods return the appropriate\n    value (e.g. ``0`` for ``pmf``). If `m` or `n` contain negative\n    values, the result will contain ``nan`` there.\n\n    The probability mass function for `multivariate_hypergeom` is\n\n    .. math::\n\n        P(X_1 = x_1, X_2 = x_2, \\ldots, X_k = x_k) = \\frac{\\binom{m_1}{x_1}\n        \\binom{m_2}{x_2} \\cdots \\binom{m_k}{x_k}}{\\binom{M}{n}}, \\\\ \\quad\n        (x_1, x_2, \\ldots, x_k) \\in \\mathbb{N}^k \\text{ with }\n        \\sum_{i=1}^k x_i = n\n\n    where :math:`m_i` are the number of objects of type :math:`i`, :math:`M`\n    is the total number of objects in the population (sum of all the\n    :math:`m_i`), and :math:`n` is the size of the sample to be taken\n    from the population.\n\n    .. versionadded:: 1.6.0\n\n    Examples\n    --------\n    To evaluate the probability mass function of the multivariate\n    hypergeometric distribution, with a dichotomous population of size\n    :math:`10` and :math:`20`, at a sample of size :math:`12` with\n    :math:`8` objects of the first type and :math:`4` objects of the\n    second type, use:\n\n    >>> from scipy.stats import multivariate_hypergeom\n    >>> multivariate_hypergeom.pmf(x=[8, 4], m=[10, 20], n=12)\n    0.0025207176631464523\n\n    The `multivariate_hypergeom` distribution is identical to the\n    corresponding `hypergeom` distribution (tiny numerical differences\n    notwithstanding) when only two types (good and bad) of objects\n    are present in the population as in the example above. Consider\n    another example for a comparison with the hypergeometric distribution:\n\n    >>> from scipy.stats import hypergeom\n    >>> multivariate_hypergeom.pmf(x=[3, 1], m=[10, 5], n=4)\n    0.4395604395604395\n    >>> hypergeom.pmf(k=3, M=15, n=4, N=10)\n    0.43956043956044005\n\n    The functions ``pmf``, ``logpmf``, ``mean``, ``var``, ``cov``, and ``rvs``\n    support broadcasting, under the convention that the vector parameters\n    (``x``, ``m``, and ``n``) are interpreted as if each row along the last\n    axis is a single object. For instance, we can combine the previous two\n    calls to `multivariate_hypergeom` as\n\n    >>> multivariate_hypergeom.pmf(x=[[8, 4], [3, 1]], m=[[10, 20], [10, 5]],\n    ...                            n=[12, 4])\n    array([0.00252072, 0.43956044])\n\n    This broadcasting also works for ``cov``, where the output objects are\n    square matrices of size ``m.shape[-1]``. For example:\n\n    >>> multivariate_hypergeom.cov(m=[[7, 9], [10, 15]], n=[8, 12])\n    array([[[ 1.05, -1.05],\n            [-1.05,  1.05]],\n           [[ 1.56, -1.56],\n            [-1.56,  1.56]]])\n\n    That is, ``result[0]`` is equal to\n    ``multivariate_hypergeom.cov(m=[7, 9], n=8)`` and ``result[1]`` is equal\n    to ``multivariate_hypergeom.cov(m=[10, 15], n=12)``.\n\n    Alternatively, the object may be called (as a function) to fix the `m`\n    and `n` parameters, returning a \"frozen\" multivariate hypergeometric\n    random variable.\n\n    >>> rv = multivariate_hypergeom(m=[10, 20], n=12)\n    >>> rv.pmf(x=[8, 4])\n    0.0025207176631464523\n\n    See Also\n    --------\n    scipy.stats.hypergeom : The hypergeometric distribution.\n    scipy.stats.multinomial : The multinomial distribution.\n\n    References\n    ----------\n    .. [1] The Multivariate Hypergeometric Distribution,\n           http://www.randomservices.org/random/urn/MultiHypergeometric.html\n    .. [2] Thomas J. Sargent and John Stachurski, 2020,\n           Multivariate Hypergeometric Distribution\n           https://python.quantecon.org/multi_hyper.html\n    ",
    "scipy.stats.multivariate_normal": "A multivariate normal random variable.\n\n    The `mean` keyword specifies the mean. The `cov` keyword specifies the\n    covariance matrix.\n\n    Methods\n    -------\n    pdf(x, mean=None, cov=1, allow_singular=False)\n        Probability density function.\n    logpdf(x, mean=None, cov=1, allow_singular=False)\n        Log of the probability density function.\n    cdf(x, mean=None, cov=1, allow_singular=False, maxpts=1000000*dim, abseps=1e-5, releps=1e-5, lower_limit=None)\n        Cumulative distribution function.\n    logcdf(x, mean=None, cov=1, allow_singular=False, maxpts=1000000*dim, abseps=1e-5, releps=1e-5)\n        Log of the cumulative distribution function.\n    rvs(mean=None, cov=1, size=1, random_state=None)\n        Draw random samples from a multivariate normal distribution.\n    entropy(mean=None, cov=1)\n        Compute the differential entropy of the multivariate normal.\n    fit(x, fix_mean=None, fix_cov=None)\n        Fit a multivariate normal distribution to data.\n\n    Parameters\n    ----------\n    mean : array_like, default: ``[0]``\n        Mean of the distribution.\n    cov : array_like or `Covariance`, default: ``[1]``\n        Symmetric positive (semi)definite covariance matrix of the distribution.\n    allow_singular : bool, default: ``False``\n        Whether to allow a singular covariance matrix. This is ignored if `cov` is\n        a `Covariance` object.\n    seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n        Used for drawing random variates.\n        If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used, seeded\n        with seed.\n        If `seed` is already a ``RandomState`` or ``Generator`` instance,\n        then that object is used.\n        Default is `None`.\n\n    Notes\n    -----\n    Setting the parameter `mean` to `None` is equivalent to having `mean`\n    be the zero-vector. The parameter `cov` can be a scalar, in which case\n    the covariance matrix is the identity times that value, a vector of\n    diagonal entries for the covariance matrix, a two-dimensional array_like,\n    or a `Covariance` object.\n\n    The covariance matrix `cov` may be an instance of a subclass of\n    `Covariance`, e.g. `scipy.stats.CovViaPrecision`. If so, `allow_singular`\n    is ignored.\n\n    Otherwise, `cov` must be a symmetric positive semidefinite\n    matrix when `allow_singular` is True; it must be (strictly) positive\n    definite when `allow_singular` is False.\n    Symmetry is not checked; only the lower triangular portion is used.\n    The determinant and inverse of `cov` are computed\n    as the pseudo-determinant and pseudo-inverse, respectively, so\n    that `cov` does not need to have full rank.\n\n    The probability density function for `multivariate_normal` is\n\n    .. math::\n\n        f(x) = \\frac{1}{\\sqrt{(2 \\pi)^k \\det \\Sigma}}\n               \\exp\\left( -\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right),\n\n    where :math:`\\mu` is the mean, :math:`\\Sigma` the covariance matrix,\n    :math:`k` the rank of :math:`\\Sigma`. In case of singular :math:`\\Sigma`,\n    SciPy extends this definition according to [1]_.\n\n    .. versionadded:: 0.14.0\n\n    References\n    ----------\n    .. [1] Multivariate Normal Distribution - Degenerate Case, Wikipedia,\n           https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Degenerate_case\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy.stats import multivariate_normal\n\n    >>> x = np.linspace(0, 5, 10, endpoint=False)\n    >>> y = multivariate_normal.pdf(x, mean=2.5, cov=0.5); y\n    array([ 0.00108914,  0.01033349,  0.05946514,  0.20755375,  0.43939129,\n            0.56418958,  0.43939129,  0.20755375,  0.05946514,  0.01033349])\n    >>> fig1 = plt.figure()\n    >>> ax = fig1.add_subplot(111)\n    >>> ax.plot(x, y)\n    >>> plt.show()\n\n    Alternatively, the object may be called (as a function) to fix the mean\n    and covariance parameters, returning a \"frozen\" multivariate normal\n    random variable:\n\n    >>> rv = multivariate_normal(mean=None, cov=1, allow_singular=False)\n    >>> # Frozen object with the same methods but holding the given\n    >>> # mean and covariance fixed.\n\n    The input quantiles can be any shape of array, as long as the last\n    axis labels the components.  This allows us for instance to\n    display the frozen pdf for a non-isotropic random variable in 2D as\n    follows:\n\n    >>> x, y = np.mgrid[-1:1:.01, -1:1:.01]\n    >>> pos = np.dstack((x, y))\n    >>> rv = multivariate_normal([0.5, -0.2], [[2.0, 0.3], [0.3, 0.5]])\n    >>> fig2 = plt.figure()\n    >>> ax2 = fig2.add_subplot(111)\n    >>> ax2.contourf(x, y, rv.pdf(pos))\n\n    ",
    "scipy.stats.multivariate_t": "A multivariate t-distributed random variable.\n\n    The `loc` parameter specifies the location. The `shape` parameter specifies\n    the positive semidefinite shape matrix. The `df` parameter specifies the\n    degrees of freedom.\n\n    In addition to calling the methods below, the object itself may be called\n    as a function to fix the location, shape matrix, and degrees of freedom\n    parameters, returning a \"frozen\" multivariate t-distribution random.\n\n    Methods\n    -------\n    pdf(x, loc=None, shape=1, df=1, allow_singular=False)\n        Probability density function.\n    logpdf(x, loc=None, shape=1, df=1, allow_singular=False)\n        Log of the probability density function.\n    cdf(x, loc=None, shape=1, df=1, allow_singular=False, *,\n        maxpts=None, lower_limit=None, random_state=None)\n        Cumulative distribution function.\n    rvs(loc=None, shape=1, df=1, size=1, random_state=None)\n        Draw random samples from a multivariate t-distribution.\n    entropy(loc=None, shape=1, df=1)\n        Differential entropy of a multivariate t-distribution.\n\n    Parameters\n    ----------\n    loc : array_like, optional\n        Location of the distribution. (default ``0``)\n    shape : array_like, optional\n        Positive semidefinite matrix of the distribution. (default ``1``)\n    df : float, optional\n        Degrees of freedom of the distribution; must be greater than zero.\n        If ``np.inf`` then results are multivariate normal. The default is ``1``.\n    allow_singular : bool, optional\n        Whether to allow a singular matrix. (default ``False``)\n    seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n        Used for drawing random variates.\n        If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used, seeded\n        with seed.\n        If `seed` is already a ``RandomState`` or ``Generator`` instance,\n        then that object is used.\n        Default is `None`.\n\n    Notes\n    -----\n    Setting the parameter `loc` to ``None`` is equivalent to having `loc`\n    be the zero-vector. The parameter `shape` can be a scalar, in which case\n    the shape matrix is the identity times that value, a vector of\n    diagonal entries for the shape matrix, or a two-dimensional array_like.\n    The matrix `shape` must be a (symmetric) positive semidefinite matrix. The\n    determinant and inverse of `shape` are computed as the pseudo-determinant\n    and pseudo-inverse, respectively, so that `shape` does not need to have\n    full rank.\n\n    The probability density function for `multivariate_t` is\n\n    .. math::\n\n        f(x) = \\frac{\\Gamma((\\nu + p)/2)}{\\Gamma(\\nu/2)\\nu^{p/2}\\pi^{p/2}|\\Sigma|^{1/2}}\n               \\left[1 + \\frac{1}{\\nu} (\\mathbf{x} - \\boldsymbol{\\mu})^{\\top}\n               \\boldsymbol{\\Sigma}^{-1}\n               (\\mathbf{x} - \\boldsymbol{\\mu}) \\right]^{-(\\nu + p)/2},\n\n    where :math:`p` is the dimension of :math:`\\mathbf{x}`,\n    :math:`\\boldsymbol{\\mu}` is the :math:`p`-dimensional location,\n    :math:`\\boldsymbol{\\Sigma}` the :math:`p \\times p`-dimensional shape\n    matrix, and :math:`\\nu` is the degrees of freedom.\n\n    .. versionadded:: 1.6.0\n\n    References\n    ----------\n    .. [1] Arellano-Valle et al. \"Shannon Entropy and Mutual Information for\n           Multivariate Skew-Elliptical Distributions\". Scandinavian Journal\n           of Statistics. Vol. 40, issue 1.\n\n    Examples\n    --------\n    The object may be called (as a function) to fix the `loc`, `shape`,\n    `df`, and `allow_singular` parameters, returning a \"frozen\"\n    multivariate_t random variable:\n\n    >>> import numpy as np\n    >>> from scipy.stats import multivariate_t\n    >>> rv = multivariate_t([1.0, -0.5], [[2.1, 0.3], [0.3, 1.5]], df=2)\n    >>> # Frozen object with the same methods but holding the given location,\n    >>> # scale, and degrees of freedom fixed.\n\n    Create a contour plot of the PDF.\n\n    >>> import matplotlib.pyplot as plt\n    >>> x, y = np.mgrid[-1:3:.01, -2:1.5:.01]\n    >>> pos = np.dstack((x, y))\n    >>> fig, ax = plt.subplots(1, 1)\n    >>> ax.set_aspect('equal')\n    >>> plt.contourf(x, y, rv.pdf(pos))\n\n    ",
    "scipy.stats.mvsdist": "\n    'Frozen' distributions for mean, variance, and standard deviation of data.\n\n    Parameters\n    ----------\n    data : array_like\n        Input array. Converted to 1-D using ravel.\n        Requires 2 or more data-points.\n\n    Returns\n    -------\n    mdist : \"frozen\" distribution object\n        Distribution object representing the mean of the data.\n    vdist : \"frozen\" distribution object\n        Distribution object representing the variance of the data.\n    sdist : \"frozen\" distribution object\n        Distribution object representing the standard deviation of the data.\n\n    See Also\n    --------\n    bayes_mvs\n\n    Notes\n    -----\n    The return values from ``bayes_mvs(data)`` is equivalent to\n    ``tuple((x.mean(), x.interval(0.90)) for x in mvsdist(data))``.\n\n    In other words, calling ``<dist>.mean()`` and ``<dist>.interval(0.90)``\n    on the three distribution objects returned from this function will give\n    the same results that are returned from `bayes_mvs`.\n\n    References\n    ----------\n    T.E. Oliphant, \"A Bayesian perspective on estimating mean, variance, and\n    standard-deviation from data\", https://scholarsarchive.byu.edu/facpub/278,\n    2006.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> data = [6, 9, 12, 7, 8, 8, 13]\n    >>> mean, var, std = stats.mvsdist(data)\n\n    We now have frozen distribution objects \"mean\", \"var\" and \"std\" that we can\n    examine:\n\n    >>> mean.mean()\n    9.0\n    >>> mean.interval(0.95)\n    (6.6120585482655692, 11.387941451734431)\n    >>> mean.std()\n    1.1952286093343936\n\n    ",
    "scipy.stats.nakagami": "A Nakagami continuous random variable.\n\n    As an instance of the `rv_continuous` class, `nakagami` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(nu, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, nu, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, nu, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, nu, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, nu, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, nu, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, nu, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, nu, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, nu, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, nu, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(nu, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(nu, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(nu,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(nu, loc=0, scale=1)\n        Median of the distribution.\n    mean(nu, loc=0, scale=1)\n        Mean of the distribution.\n    var(nu, loc=0, scale=1)\n        Variance of the distribution.\n    std(nu, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, nu, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `nakagami` is:\n\n    .. math::\n\n        f(x, \\nu) = \\frac{2 \\nu^\\nu}{\\Gamma(\\nu)} x^{2\\nu-1} \\exp(-\\nu x^2)\n\n    for :math:`x >= 0`, :math:`\\nu > 0`. The distribution was introduced in\n    [2]_, see also [1]_ for further information.\n\n    `nakagami` takes ``nu`` as a shape parameter for :math:`\\nu`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``nakagami.pdf(x, nu, loc, scale)`` is identically\n    equivalent to ``nakagami.pdf(y, nu) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] \"Nakagami distribution\", Wikipedia\n           https://en.wikipedia.org/wiki/Nakagami_distribution\n    .. [2] M. Nakagami, \"The m-distribution - A general formula of intensity\n           distribution of rapid fading\", Statistical methods in radio wave\n           propagation, Pergamon Press, 1960, 3-36.\n           :doi:`10.1016/B978-0-08-009306-2.50005-4`\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import nakagami\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> nu = 4.97\n    >>> mean, var, skew, kurt = nakagami.stats(nu, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(nakagami.ppf(0.01, nu),\n    ...                 nakagami.ppf(0.99, nu), 100)\n    >>> ax.plot(x, nakagami.pdf(x, nu),\n    ...        'r-', lw=5, alpha=0.6, label='nakagami pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = nakagami(nu)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = nakagami.ppf([0.001, 0.5, 0.999], nu)\n    >>> np.allclose([0.001, 0.5, 0.999], nakagami.cdf(vals, nu))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = nakagami.rvs(nu, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.nbinom": "A negative binomial discrete random variable.\n\n    As an instance of the `rv_discrete` class, `nbinom` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(n, p, loc=0, size=1, random_state=None)\n        Random variates.\n    pmf(k, n, p, loc=0)\n        Probability mass function.\n    logpmf(k, n, p, loc=0)\n        Log of the probability mass function.\n    cdf(k, n, p, loc=0)\n        Cumulative distribution function.\n    logcdf(k, n, p, loc=0)\n        Log of the cumulative distribution function.\n    sf(k, n, p, loc=0)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(k, n, p, loc=0)\n        Log of the survival function.\n    ppf(q, n, p, loc=0)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, n, p, loc=0)\n        Inverse survival function (inverse of ``sf``).\n    stats(n, p, loc=0, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(n, p, loc=0)\n        (Differential) entropy of the RV.\n    expect(func, args=(n, p), loc=0, lb=None, ub=None, conditional=False)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(n, p, loc=0)\n        Median of the distribution.\n    mean(n, p, loc=0)\n        Mean of the distribution.\n    var(n, p, loc=0)\n        Variance of the distribution.\n    std(n, p, loc=0)\n        Standard deviation of the distribution.\n    interval(confidence, n, p, loc=0)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    Negative binomial distribution describes a sequence of i.i.d. Bernoulli\n    trials, repeated until a predefined, non-random number of successes occurs.\n\n    The probability mass function of the number of failures for `nbinom` is:\n\n    .. math::\n\n       f(k) = \\binom{k+n-1}{n-1} p^n (1-p)^k\n\n    for :math:`k \\ge 0`, :math:`0 < p \\leq 1`\n\n    `nbinom` takes :math:`n` and :math:`p` as shape parameters where :math:`n`\n    is the number of successes, :math:`p` is the probability of a single\n    success, and :math:`1-p` is the probability of a single failure.\n\n    Another common parameterization of the negative binomial distribution is\n    in terms of the mean number of failures :math:`\\mu` to achieve :math:`n`\n    successes. The mean :math:`\\mu` is related to the probability of success\n    as\n\n    .. math::\n\n       p = \\frac{n}{n + \\mu}\n\n    The number of successes :math:`n` may also be specified in terms of a\n    \"dispersion\", \"heterogeneity\", or \"aggregation\" parameter :math:`\\alpha`,\n    which relates the mean :math:`\\mu` to the variance :math:`\\sigma^2`,\n    e.g. :math:`\\sigma^2 = \\mu + \\alpha \\mu^2`. Regardless of the convention\n    used for :math:`\\alpha`,\n\n    .. math::\n\n       p &= \\frac{\\mu}{\\sigma^2} \\\\\n       n &= \\frac{\\mu^2}{\\sigma^2 - \\mu}\n\n    The probability mass function above is defined in the \"standardized\" form.\n    To shift distribution use the ``loc`` parameter.\n    Specifically, ``nbinom.pmf(k, n, p, loc)`` is identically\n    equivalent to ``nbinom.pmf(k - loc, n, p)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import nbinom\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> n, p = 5, 0.5\n    >>> mean, var, skew, kurt = nbinom.stats(n, p, moments='mvsk')\n    \n    Display the probability mass function (``pmf``):\n    \n    >>> x = np.arange(nbinom.ppf(0.01, n, p),\n    ...               nbinom.ppf(0.99, n, p))\n    >>> ax.plot(x, nbinom.pmf(x, n, p), 'bo', ms=8, label='nbinom pmf')\n    >>> ax.vlines(x, 0, nbinom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape and location. This returns a \"frozen\" RV object holding\n    the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pmf``:\n    \n    >>> rv = nbinom(n, p)\n    >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n    ...         label='frozen pmf')\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> prob = nbinom.cdf(x, n, p)\n    >>> np.allclose(x, nbinom.ppf(prob, n, p))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = nbinom.rvs(n, p, size=1000)\n\n    See Also\n    --------\n    hypergeom, binom, nhypergeom\n\n    ",
    "scipy.stats.ncf": "A non-central F distribution continuous random variable.\n\n    As an instance of the `rv_continuous` class, `ncf` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(dfn, dfd, nc, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, dfn, dfd, nc, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, dfn, dfd, nc, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, dfn, dfd, nc, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, dfn, dfd, nc, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, dfn, dfd, nc, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, dfn, dfd, nc, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, dfn, dfd, nc, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, dfn, dfd, nc, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, dfn, dfd, nc, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(dfn, dfd, nc, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(dfn, dfd, nc, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(dfn, dfd, nc), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(dfn, dfd, nc, loc=0, scale=1)\n        Median of the distribution.\n    mean(dfn, dfd, nc, loc=0, scale=1)\n        Mean of the distribution.\n    var(dfn, dfd, nc, loc=0, scale=1)\n        Variance of the distribution.\n    std(dfn, dfd, nc, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, dfn, dfd, nc, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    scipy.stats.f : Fisher distribution\n\n    Notes\n    -----\n    The probability density function for `ncf` is:\n\n    .. math::\n\n        f(x, n_1, n_2, \\lambda) =\n            \\exp\\left(\\frac{\\lambda}{2} +\n                      \\lambda n_1 \\frac{x}{2(n_1 x + n_2)}\n                \\right)\n            n_1^{n_1/2} n_2^{n_2/2} x^{n_1/2 - 1} \\\\\n            (n_2 + n_1 x)^{-(n_1 + n_2)/2}\n            \\gamma(n_1/2) \\gamma(1 + n_2/2) \\\\\n            \\frac{L^{\\frac{n_1}{2}-1}_{n_2/2}\n                \\left(-\\lambda n_1 \\frac{x}{2(n_1 x + n_2)}\\right)}\n            {B(n_1/2, n_2/2)\n                \\gamma\\left(\\frac{n_1 + n_2}{2}\\right)}\n\n    for :math:`n_1, n_2 > 0`, :math:`\\lambda \\ge 0`.  Here :math:`n_1` is the\n    degrees of freedom in the numerator, :math:`n_2` the degrees of freedom in\n    the denominator, :math:`\\lambda` the non-centrality parameter,\n    :math:`\\gamma` is the logarithm of the Gamma function, :math:`L_n^k` is a\n    generalized Laguerre polynomial and :math:`B` is the beta function.\n\n    `ncf` takes ``df1``, ``df2`` and ``nc`` as shape parameters. If ``nc=0``,\n    the distribution becomes equivalent to the Fisher distribution.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``ncf.pdf(x, dfn, dfd, nc, loc, scale)`` is identically\n    equivalent to ``ncf.pdf(y, dfn, dfd, nc) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import ncf\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> dfn, dfd, nc = 27, 27, 0.416\n    >>> mean, var, skew, kurt = ncf.stats(dfn, dfd, nc, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(ncf.ppf(0.01, dfn, dfd, nc),\n    ...                 ncf.ppf(0.99, dfn, dfd, nc), 100)\n    >>> ax.plot(x, ncf.pdf(x, dfn, dfd, nc),\n    ...        'r-', lw=5, alpha=0.6, label='ncf pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = ncf(dfn, dfd, nc)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = ncf.ppf([0.001, 0.5, 0.999], dfn, dfd, nc)\n    >>> np.allclose([0.001, 0.5, 0.999], ncf.cdf(vals, dfn, dfd, nc))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = ncf.rvs(dfn, dfd, nc, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.nchypergeom_fisher": "A Fisher's noncentral hypergeometric discrete random variable.\n\n    Fisher's noncentral hypergeometric distribution models drawing objects of\n    two types from a bin. `M` is the total number of objects, `n` is the\n    number of Type I objects, and `odds` is the odds ratio: the odds of\n    selecting a Type I object rather than a Type II object when there is only\n    one object of each type.\n    The random variate represents the number of Type I objects drawn if we\n    take a handful of objects from the bin at once and find out afterwards\n    that we took `N` objects.\n\n    As an instance of the `rv_discrete` class, `nchypergeom_fisher` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(M, n, N, odds, loc=0, size=1, random_state=None)\n        Random variates.\n    pmf(k, M, n, N, odds, loc=0)\n        Probability mass function.\n    logpmf(k, M, n, N, odds, loc=0)\n        Log of the probability mass function.\n    cdf(k, M, n, N, odds, loc=0)\n        Cumulative distribution function.\n    logcdf(k, M, n, N, odds, loc=0)\n        Log of the cumulative distribution function.\n    sf(k, M, n, N, odds, loc=0)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(k, M, n, N, odds, loc=0)\n        Log of the survival function.\n    ppf(q, M, n, N, odds, loc=0)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, M, n, N, odds, loc=0)\n        Inverse survival function (inverse of ``sf``).\n    stats(M, n, N, odds, loc=0, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(M, n, N, odds, loc=0)\n        (Differential) entropy of the RV.\n    expect(func, args=(M, n, N, odds), loc=0, lb=None, ub=None, conditional=False)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(M, n, N, odds, loc=0)\n        Median of the distribution.\n    mean(M, n, N, odds, loc=0)\n        Mean of the distribution.\n    var(M, n, N, odds, loc=0)\n        Variance of the distribution.\n    std(M, n, N, odds, loc=0)\n        Standard deviation of the distribution.\n    interval(confidence, M, n, N, odds, loc=0)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    nchypergeom_wallenius, hypergeom, nhypergeom\n\n    Notes\n    -----\n    Let mathematical symbols :math:`N`, :math:`n`, and :math:`M` correspond\n    with parameters `N`, `n`, and `M` (respectively) as defined above.\n\n    The probability mass function is defined as\n\n    .. math::\n\n        p(x; M, n, N, \\omega) =\n        \\frac{\\binom{n}{x}\\binom{M - n}{N-x}\\omega^x}{P_0},\n\n    for\n    :math:`x \\in [x_l, x_u]`,\n    :math:`M \\in {\\mathbb N}`,\n    :math:`n \\in [0, M]`,\n    :math:`N \\in [0, M]`,\n    :math:`\\omega > 0`,\n    where\n    :math:`x_l = \\max(0, N - (M - n))`,\n    :math:`x_u = \\min(N, n)`,\n\n    .. math::\n\n        P_0 = \\sum_{y=x_l}^{x_u} \\binom{n}{y}\\binom{M - n}{N-y}\\omega^y,\n\n    and the binomial coefficients are defined as\n\n    .. math:: \\binom{n}{k} \\equiv \\frac{n!}{k! (n - k)!}.\n\n    `nchypergeom_fisher` uses the BiasedUrn package by Agner Fog with\n    permission for it to be distributed under SciPy's license.\n\n    The symbols used to denote the shape parameters (`N`, `n`, and `M`) are not\n    universally accepted; they are chosen for consistency with `hypergeom`.\n\n    Note that Fisher's noncentral hypergeometric distribution is distinct\n    from Wallenius' noncentral hypergeometric distribution, which models\n    drawing a pre-determined `N` objects from a bin one by one.\n    When the odds ratio is unity, however, both distributions reduce to the\n    ordinary hypergeometric distribution.\n\n    The probability mass function above is defined in the \"standardized\" form.\n    To shift distribution use the ``loc`` parameter.\n    Specifically, ``nchypergeom_fisher.pmf(k, M, n, N, odds, loc)`` is identically\n    equivalent to ``nchypergeom_fisher.pmf(k - loc, M, n, N, odds)``.\n\n    References\n    ----------\n    .. [1] Agner Fog, \"Biased Urn Theory\".\n           https://cran.r-project.org/web/packages/BiasedUrn/vignettes/UrnTheory.pdf\n\n    .. [2] \"Fisher's noncentral hypergeometric distribution\", Wikipedia,\n           https://en.wikipedia.org/wiki/Fisher's_noncentral_hypergeometric_distribution\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import nchypergeom_fisher\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> M, n, N, odds = 140, 80, 60, 0.5\n    >>> mean, var, skew, kurt = nchypergeom_fisher.stats(M, n, N, odds, moments='mvsk')\n    \n    Display the probability mass function (``pmf``):\n    \n    >>> x = np.arange(nchypergeom_fisher.ppf(0.01, M, n, N, odds),\n    ...               nchypergeom_fisher.ppf(0.99, M, n, N, odds))\n    >>> ax.plot(x, nchypergeom_fisher.pmf(x, M, n, N, odds), 'bo', ms=8, label='nchypergeom_fisher pmf')\n    >>> ax.vlines(x, 0, nchypergeom_fisher.pmf(x, M, n, N, odds), colors='b', lw=5, alpha=0.5)\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape and location. This returns a \"frozen\" RV object holding\n    the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pmf``:\n    \n    >>> rv = nchypergeom_fisher(M, n, N, odds)\n    >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n    ...         label='frozen pmf')\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> prob = nchypergeom_fisher.cdf(x, M, n, N, odds)\n    >>> np.allclose(x, nchypergeom_fisher.ppf(prob, M, n, N, odds))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = nchypergeom_fisher.rvs(M, n, N, odds, size=1000)\n\n    ",
    "scipy.stats.nchypergeom_wallenius": "A Wallenius' noncentral hypergeometric discrete random variable.\n\n    Wallenius' noncentral hypergeometric distribution models drawing objects of\n    two types from a bin. `M` is the total number of objects, `n` is the\n    number of Type I objects, and `odds` is the odds ratio: the odds of\n    selecting a Type I object rather than a Type II object when there is only\n    one object of each type.\n    The random variate represents the number of Type I objects drawn if we\n    draw a pre-determined `N` objects from a bin one by one.\n\n    As an instance of the `rv_discrete` class, `nchypergeom_wallenius` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(M, n, N, odds, loc=0, size=1, random_state=None)\n        Random variates.\n    pmf(k, M, n, N, odds, loc=0)\n        Probability mass function.\n    logpmf(k, M, n, N, odds, loc=0)\n        Log of the probability mass function.\n    cdf(k, M, n, N, odds, loc=0)\n        Cumulative distribution function.\n    logcdf(k, M, n, N, odds, loc=0)\n        Log of the cumulative distribution function.\n    sf(k, M, n, N, odds, loc=0)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(k, M, n, N, odds, loc=0)\n        Log of the survival function.\n    ppf(q, M, n, N, odds, loc=0)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, M, n, N, odds, loc=0)\n        Inverse survival function (inverse of ``sf``).\n    stats(M, n, N, odds, loc=0, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(M, n, N, odds, loc=0)\n        (Differential) entropy of the RV.\n    expect(func, args=(M, n, N, odds), loc=0, lb=None, ub=None, conditional=False)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(M, n, N, odds, loc=0)\n        Median of the distribution.\n    mean(M, n, N, odds, loc=0)\n        Mean of the distribution.\n    var(M, n, N, odds, loc=0)\n        Variance of the distribution.\n    std(M, n, N, odds, loc=0)\n        Standard deviation of the distribution.\n    interval(confidence, M, n, N, odds, loc=0)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    nchypergeom_fisher, hypergeom, nhypergeom\n\n    Notes\n    -----\n    Let mathematical symbols :math:`N`, :math:`n`, and :math:`M` correspond\n    with parameters `N`, `n`, and `M` (respectively) as defined above.\n\n    The probability mass function is defined as\n\n    .. math::\n\n        p(x; N, n, M) = \\binom{n}{x} \\binom{M - n}{N-x}\n        \\int_0^1 \\left(1-t^{\\omega/D}\\right)^x\\left(1-t^{1/D}\\right)^{N-x} dt\n\n    for\n    :math:`x \\in [x_l, x_u]`,\n    :math:`M \\in {\\mathbb N}`,\n    :math:`n \\in [0, M]`,\n    :math:`N \\in [0, M]`,\n    :math:`\\omega > 0`,\n    where\n    :math:`x_l = \\max(0, N - (M - n))`,\n    :math:`x_u = \\min(N, n)`,\n\n    .. math::\n\n        D = \\omega(n - x) + ((M - n)-(N-x)),\n\n    and the binomial coefficients are defined as\n\n    .. math:: \\binom{n}{k} \\equiv \\frac{n!}{k! (n - k)!}.\n\n    `nchypergeom_wallenius` uses the BiasedUrn package by Agner Fog with\n    permission for it to be distributed under SciPy's license.\n\n    The symbols used to denote the shape parameters (`N`, `n`, and `M`) are not\n    universally accepted; they are chosen for consistency with `hypergeom`.\n\n    Note that Wallenius' noncentral hypergeometric distribution is distinct\n    from Fisher's noncentral hypergeometric distribution, which models\n    take a handful of objects from the bin at once, finding out afterwards\n    that `N` objects were taken.\n    When the odds ratio is unity, however, both distributions reduce to the\n    ordinary hypergeometric distribution.\n\n    The probability mass function above is defined in the \"standardized\" form.\n    To shift distribution use the ``loc`` parameter.\n    Specifically, ``nchypergeom_wallenius.pmf(k, M, n, N, odds, loc)`` is identically\n    equivalent to ``nchypergeom_wallenius.pmf(k - loc, M, n, N, odds)``.\n\n    References\n    ----------\n    .. [1] Agner Fog, \"Biased Urn Theory\".\n           https://cran.r-project.org/web/packages/BiasedUrn/vignettes/UrnTheory.pdf\n\n    .. [2] \"Wallenius' noncentral hypergeometric distribution\", Wikipedia,\n           https://en.wikipedia.org/wiki/Wallenius'_noncentral_hypergeometric_distribution\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import nchypergeom_wallenius\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> M, n, N, odds = 140, 80, 60, 0.5\n    >>> mean, var, skew, kurt = nchypergeom_wallenius.stats(M, n, N, odds, moments='mvsk')\n    \n    Display the probability mass function (``pmf``):\n    \n    >>> x = np.arange(nchypergeom_wallenius.ppf(0.01, M, n, N, odds),\n    ...               nchypergeom_wallenius.ppf(0.99, M, n, N, odds))\n    >>> ax.plot(x, nchypergeom_wallenius.pmf(x, M, n, N, odds), 'bo', ms=8, label='nchypergeom_wallenius pmf')\n    >>> ax.vlines(x, 0, nchypergeom_wallenius.pmf(x, M, n, N, odds), colors='b', lw=5, alpha=0.5)\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape and location. This returns a \"frozen\" RV object holding\n    the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pmf``:\n    \n    >>> rv = nchypergeom_wallenius(M, n, N, odds)\n    >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n    ...         label='frozen pmf')\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> prob = nchypergeom_wallenius.cdf(x, M, n, N, odds)\n    >>> np.allclose(x, nchypergeom_wallenius.ppf(prob, M, n, N, odds))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = nchypergeom_wallenius.rvs(M, n, N, odds, size=1000)\n\n    ",
    "scipy.stats.nct": "A non-central Student's t continuous random variable.\n\n    As an instance of the `rv_continuous` class, `nct` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(df, nc, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, df, nc, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, df, nc, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, df, nc, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, df, nc, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, df, nc, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, df, nc, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, df, nc, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, df, nc, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, df, nc, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(df, nc, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(df, nc, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(df, nc), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(df, nc, loc=0, scale=1)\n        Median of the distribution.\n    mean(df, nc, loc=0, scale=1)\n        Mean of the distribution.\n    var(df, nc, loc=0, scale=1)\n        Variance of the distribution.\n    std(df, nc, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, df, nc, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    If :math:`Y` is a standard normal random variable and :math:`V` is\n    an independent chi-square random variable (`chi2`) with :math:`k` degrees\n    of freedom, then\n\n    .. math::\n\n        X = \\frac{Y + c}{\\sqrt{V/k}}\n\n    has a non-central Student's t distribution on the real line.\n    The degrees of freedom parameter :math:`k` (denoted ``df`` in the\n    implementation) satisfies :math:`k > 0` and the noncentrality parameter\n    :math:`c` (denoted ``nc`` in the implementation) is a real number.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``nct.pdf(x, df, nc, loc, scale)`` is identically\n    equivalent to ``nct.pdf(y, df, nc) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import nct\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> df, nc = 14, 0.24\n    >>> mean, var, skew, kurt = nct.stats(df, nc, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(nct.ppf(0.01, df, nc),\n    ...                 nct.ppf(0.99, df, nc), 100)\n    >>> ax.plot(x, nct.pdf(x, df, nc),\n    ...        'r-', lw=5, alpha=0.6, label='nct pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = nct(df, nc)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = nct.ppf([0.001, 0.5, 0.999], df, nc)\n    >>> np.allclose([0.001, 0.5, 0.999], nct.cdf(vals, df, nc))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = nct.rvs(df, nc, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.ncx2": "A non-central chi-squared continuous random variable.\n\n    As an instance of the `rv_continuous` class, `ncx2` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(df, nc, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, df, nc, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, df, nc, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, df, nc, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, df, nc, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, df, nc, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, df, nc, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, df, nc, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, df, nc, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, df, nc, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(df, nc, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(df, nc, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(df, nc), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(df, nc, loc=0, scale=1)\n        Median of the distribution.\n    mean(df, nc, loc=0, scale=1)\n        Mean of the distribution.\n    var(df, nc, loc=0, scale=1)\n        Variance of the distribution.\n    std(df, nc, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, df, nc, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `ncx2` is:\n\n    .. math::\n\n        f(x, k, \\lambda) = \\frac{1}{2} \\exp(-(\\lambda+x)/2)\n            (x/\\lambda)^{(k-2)/4}  I_{(k-2)/2}(\\sqrt{\\lambda x})\n\n    for :math:`x >= 0`, :math:`k > 0` and :math:`\\lambda \\ge 0`.\n    :math:`k` specifies the degrees of freedom (denoted ``df`` in the\n    implementation) and :math:`\\lambda` is the non-centrality parameter\n    (denoted ``nc`` in the implementation). :math:`I_\\nu` denotes the\n    modified Bessel function of first order of degree :math:`\\nu`\n    (`scipy.special.iv`).\n\n    `ncx2` takes ``df`` and ``nc`` as shape parameters.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``ncx2.pdf(x, df, nc, loc, scale)`` is identically\n    equivalent to ``ncx2.pdf(y, df, nc) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import ncx2\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> df, nc = 21, 1.06\n    >>> mean, var, skew, kurt = ncx2.stats(df, nc, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(ncx2.ppf(0.01, df, nc),\n    ...                 ncx2.ppf(0.99, df, nc), 100)\n    >>> ax.plot(x, ncx2.pdf(x, df, nc),\n    ...        'r-', lw=5, alpha=0.6, label='ncx2 pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = ncx2(df, nc)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = ncx2.ppf([0.001, 0.5, 0.999], df, nc)\n    >>> np.allclose([0.001, 0.5, 0.999], ncx2.cdf(vals, df, nc))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = ncx2.rvs(df, nc, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.nhypergeom": "A negative hypergeometric discrete random variable.\n\n    Consider a box containing :math:`M` balls:, :math:`n` red and\n    :math:`M-n` blue. We randomly sample balls from the box, one\n    at a time and *without* replacement, until we have picked :math:`r`\n    blue balls. `nhypergeom` is the distribution of the number of\n    red balls :math:`k` we have picked.\n\n    As an instance of the `rv_discrete` class, `nhypergeom` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(M, n, r, loc=0, size=1, random_state=None)\n        Random variates.\n    pmf(k, M, n, r, loc=0)\n        Probability mass function.\n    logpmf(k, M, n, r, loc=0)\n        Log of the probability mass function.\n    cdf(k, M, n, r, loc=0)\n        Cumulative distribution function.\n    logcdf(k, M, n, r, loc=0)\n        Log of the cumulative distribution function.\n    sf(k, M, n, r, loc=0)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(k, M, n, r, loc=0)\n        Log of the survival function.\n    ppf(q, M, n, r, loc=0)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, M, n, r, loc=0)\n        Inverse survival function (inverse of ``sf``).\n    stats(M, n, r, loc=0, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(M, n, r, loc=0)\n        (Differential) entropy of the RV.\n    expect(func, args=(M, n, r), loc=0, lb=None, ub=None, conditional=False)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(M, n, r, loc=0)\n        Median of the distribution.\n    mean(M, n, r, loc=0)\n        Mean of the distribution.\n    var(M, n, r, loc=0)\n        Variance of the distribution.\n    std(M, n, r, loc=0)\n        Standard deviation of the distribution.\n    interval(confidence, M, n, r, loc=0)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The symbols used to denote the shape parameters (`M`, `n`, and `r`) are not\n    universally accepted. See the Examples for a clarification of the\n    definitions used here.\n\n    The probability mass function is defined as,\n\n    .. math:: f(k; M, n, r) = \\frac{{{k+r-1}\\choose{k}}{{M-r-k}\\choose{n-k}}}\n                                   {{M \\choose n}}\n\n    for :math:`k \\in [0, n]`, :math:`n \\in [0, M]`, :math:`r \\in [0, M-n]`,\n    and the binomial coefficient is:\n\n    .. math:: \\binom{n}{k} \\equiv \\frac{n!}{k! (n - k)!}.\n\n    It is equivalent to observing :math:`k` successes in :math:`k+r-1`\n    samples with :math:`k+r`'th sample being a failure. The former\n    can be modelled as a hypergeometric distribution. The probability\n    of the latter is simply the number of failures remaining\n    :math:`M-n-(r-1)` divided by the size of the remaining population\n    :math:`M-(k+r-1)`. This relationship can be shown as:\n\n    .. math:: NHG(k;M,n,r) = HG(k;M,n,k+r-1)\\frac{(M-n-(r-1))}{(M-(k+r-1))}\n\n    where :math:`NHG` is probability mass function (PMF) of the\n    negative hypergeometric distribution and :math:`HG` is the\n    PMF of the hypergeometric distribution.\n\n    The probability mass function above is defined in the \"standardized\" form.\n    To shift distribution use the ``loc`` parameter.\n    Specifically, ``nhypergeom.pmf(k, M, n, r, loc)`` is identically\n    equivalent to ``nhypergeom.pmf(k - loc, M, n, r)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import nhypergeom\n    >>> import matplotlib.pyplot as plt\n\n    Suppose we have a collection of 20 animals, of which 7 are dogs.\n    Then if we want to know the probability of finding a given number\n    of dogs (successes) in a sample with exactly 12 animals that\n    aren't dogs (failures), we can initialize a frozen distribution\n    and plot the probability mass function:\n\n    >>> M, n, r = [20, 7, 12]\n    >>> rv = nhypergeom(M, n, r)\n    >>> x = np.arange(0, n+2)\n    >>> pmf_dogs = rv.pmf(x)\n\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> ax.plot(x, pmf_dogs, 'bo')\n    >>> ax.vlines(x, 0, pmf_dogs, lw=2)\n    >>> ax.set_xlabel('# of dogs in our group with given 12 failures')\n    >>> ax.set_ylabel('nhypergeom PMF')\n    >>> plt.show()\n\n    Instead of using a frozen distribution we can also use `nhypergeom`\n    methods directly.  To for example obtain the probability mass\n    function, use:\n\n    >>> prb = nhypergeom.pmf(x, M, n, r)\n\n    And to generate random numbers:\n\n    >>> R = nhypergeom.rvs(M, n, r, size=10)\n\n    To verify the relationship between `hypergeom` and `nhypergeom`, use:\n\n    >>> from scipy.stats import hypergeom, nhypergeom\n    >>> M, n, r = 45, 13, 8\n    >>> k = 6\n    >>> nhypergeom.pmf(k, M, n, r)\n    0.06180776620271643\n    >>> hypergeom.pmf(k, M, n, k+r-1) * (M - n - (r-1)) / (M - (k+r-1))\n    0.06180776620271644\n\n    See Also\n    --------\n    hypergeom, binom, nbinom\n\n    References\n    ----------\n    .. [1] Negative Hypergeometric Distribution on Wikipedia\n           https://en.wikipedia.org/wiki/Negative_hypergeometric_distribution\n\n    .. [2] Negative Hypergeometric Distribution from\n           http://www.math.wm.edu/~leemis/chart/UDR/PDFs/Negativehypergeometric.pdf\n\n    ",
    "scipy.stats.norm": "A normal continuous random variable.\n\n    The location (``loc``) keyword specifies the mean.\n    The scale (``scale``) keyword specifies the standard deviation.\n\n    As an instance of the `rv_continuous` class, `norm` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `norm` is:\n\n    .. math::\n\n        f(x) = \\frac{\\exp(-x^2/2)}{\\sqrt{2\\pi}}\n\n    for a real number :math:`x`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``norm.pdf(x, loc, scale)`` is identically\n    equivalent to ``norm.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import norm\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    \n    >>> mean, var, skew, kurt = norm.stats(moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(norm.ppf(0.01),\n    ...                 norm.ppf(0.99), 100)\n    >>> ax.plot(x, norm.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='norm pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = norm()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = norm.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], norm.cdf(vals))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = norm.rvs(size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.normaltest": "    \n\n\nTest whether a sample differs from a normal distribution.\n\nThis function tests the null hypothesis that a sample comes\nfrom a normal distribution.  It is based on D'Agostino and\nPearson's [1]_, [2]_ test that combines skew and kurtosis to\nproduce an omnibus test of normality.\n\nParameters\n----------\na : array_like\n    The array containing the sample to be tested. Must contain\n    at least eight observations.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nstatistic : float or array\n    ``s^2 + k^2``, where ``s`` is the z-score returned by `skewtest` and\n    ``k`` is the z-score returned by `kurtosistest`.\npvalue : float or array\n    A 2-sided chi squared probability for the hypothesis test.\n\nNotes\n-----\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] D'Agostino, R. B. (1971), \"An omnibus test of normality for\n       moderate and large sample size\", Biometrika, 58, 341-348\n.. [2] D'Agostino, R. and Pearson, E. S. (1973), \"Tests for departure from\n       normality\", Biometrika, 60, 613-622\n.. [3] Shapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test\n       for normality (complete samples). Biometrika, 52(3/4), 591-611.\n.. [4] B. Phipson and G. K. Smyth. \"Permutation P-values Should Never Be\n       Zero: Calculating Exact P-values When Permutations Are Randomly\n       Drawn.\" Statistical Applications in Genetics and Molecular Biology\n       9.1 (2010).\n.. [5] Panagiotakos, D. B. (2008). The value of p-value in biomedical\n       research. The open cardiovascular medicine journal, 2, 97.\n\nExamples\n--------\nSuppose we wish to infer from measurements whether the weights of adult\nhuman males in a medical study are not normally distributed [3]_.\nThe weights (lbs) are recorded in the array ``x`` below.\n\n>>> import numpy as np\n>>> x = np.array([148, 154, 158, 160, 161, 162, 166, 170, 182, 195, 236])\n\nThe normality test of [1]_ and [2]_ begins by computing a statistic based\non the sample skewness and kurtosis.\n\n>>> from scipy import stats\n>>> res = stats.normaltest(x)\n>>> res.statistic\n13.034263121192582\n\n(The test warns that our sample has too few observations to perform the\ntest. We'll return to this at the end of the example.)\nBecause the normal distribution has zero skewness and zero\n(\"excess\" or \"Fisher\") kurtosis, the value of this statistic tends to be\nlow for samples drawn from a normal distribution.\n\nThe test is performed by comparing the observed value of the statistic\nagainst the null distribution: the distribution of statistic values derived\nunder the null hypothesis that the weights were drawn from a normal\ndistribution.\nFor this normality test, the null distribution for very large samples is\nthe chi-squared distribution with two degrees of freedom.\n\n>>> import matplotlib.pyplot as plt\n>>> dist = stats.chi2(df=2)\n>>> stat_vals = np.linspace(0, 16, 100)\n>>> pdf = dist.pdf(stat_vals)\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> def plot(ax):  # we'll reuse this\n...     ax.plot(stat_vals, pdf)\n...     ax.set_title(\"Normality Test Null Distribution\")\n...     ax.set_xlabel(\"statistic\")\n...     ax.set_ylabel(\"probability density\")\n>>> plot(ax)\n>>> plt.show()\n\nThe comparison is quantified by the p-value: the proportion of values in\nthe null distribution greater than or equal to the observed value of the\nstatistic.\n\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> plot(ax)\n>>> pvalue = dist.sf(res.statistic)\n>>> annotation = (f'p-value={pvalue:.6f}\\n(shaded area)')\n>>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n>>> _ = ax.annotate(annotation, (13.5, 5e-4), (14, 5e-3), arrowprops=props)\n>>> i = stat_vals >= res.statistic  # index more extreme statistic values\n>>> ax.fill_between(stat_vals[i], y1=0, y2=pdf[i])\n>>> ax.set_xlim(8, 16)\n>>> ax.set_ylim(0, 0.01)\n>>> plt.show()\n>>> res.pvalue\n0.0014779023013100172\n\nIf the p-value is \"small\" - that is, if there is a low probability of\nsampling data from a normally distributed population that produces such an\nextreme value of the statistic - this may be taken as evidence against\nthe null hypothesis in favor of the alternative: the weights were not\ndrawn from a normal distribution. Note that:\n\n- The inverse is not true; that is, the test is not used to provide\n  evidence for the null hypothesis.\n- The threshold for values that will be considered \"small\" is a choice that\n  should be made before the data is analyzed [4]_ with consideration of the\n  risks of both false positives (incorrectly rejecting the null hypothesis)\n  and false negatives (failure to reject a false null hypothesis).\n\nNote that the chi-squared distribution provides an asymptotic\napproximation of the null distribution; it is only accurate for samples\nwith many observations. This is the reason we received a warning at the\nbeginning of the example; our sample is quite small. In this case,\n`scipy.stats.monte_carlo_test` may provide a more accurate, albeit\nstochastic, approximation of the exact p-value.\n\n>>> def statistic(x, axis):\n...     # Get only the `normaltest` statistic; ignore approximate p-value\n...     return stats.normaltest(x, axis=axis).statistic\n>>> res = stats.monte_carlo_test(x, stats.norm.rvs, statistic,\n...                              alternative='greater')\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> plot(ax)\n>>> ax.hist(res.null_distribution, np.linspace(0, 25, 50),\n...         density=True)\n>>> ax.legend(['aymptotic approximation (many observations)',\n...            'Monte Carlo approximation (11 observations)'])\n>>> ax.set_xlim(0, 14)\n>>> plt.show()\n>>> res.pvalue\n0.0082  # may vary\n\nFurthermore, despite their stochastic nature, p-values computed in this way\ncan be used to exactly control the rate of false rejections of the null\nhypothesis [5]_.\n",
    "scipy.stats.norminvgauss": "A Normal Inverse Gaussian continuous random variable.\n\n    As an instance of the `rv_continuous` class, `norminvgauss` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, a, b, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, a, b, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, a, b, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, a, b, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, a, b, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, a, b, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, a, b, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, b, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, a, b, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(a, b, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, b, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, b, loc=0, scale=1)\n        Median of the distribution.\n    mean(a, b, loc=0, scale=1)\n        Mean of the distribution.\n    var(a, b, loc=0, scale=1)\n        Variance of the distribution.\n    std(a, b, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, a, b, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `norminvgauss` is:\n\n    .. math::\n\n        f(x, a, b) = \\frac{a \\, K_1(a \\sqrt{1 + x^2})}{\\pi \\sqrt{1 + x^2}} \\,\n                     \\exp(\\sqrt{a^2 - b^2} + b x)\n\n    where :math:`x` is a real number, the parameter :math:`a` is the tail\n    heaviness and :math:`b` is the asymmetry parameter satisfying\n    :math:`a > 0` and :math:`|b| <= a`.\n    :math:`K_1` is the modified Bessel function of second kind\n    (`scipy.special.k1`).\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``norminvgauss.pdf(x, a, b, loc, scale)`` is identically\n    equivalent to ``norminvgauss.pdf(y, a, b) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    A normal inverse Gaussian random variable `Y` with parameters `a` and `b`\n    can be expressed as a normal mean-variance mixture:\n    `Y = b * V + sqrt(V) * X` where `X` is `norm(0,1)` and `V` is\n    `invgauss(mu=1/sqrt(a**2 - b**2))`. This representation is used\n    to generate random variates.\n\n    Another common parametrization of the distribution (see Equation 2.1 in\n    [2]_) is given by the following expression of the pdf:\n\n    .. math::\n\n        g(x, \\alpha, \\beta, \\delta, \\mu) =\n        \\frac{\\alpha\\delta K_1\\left(\\alpha\\sqrt{\\delta^2 + (x - \\mu)^2}\\right)}\n        {\\pi \\sqrt{\\delta^2 + (x - \\mu)^2}} \\,\n        e^{\\delta \\sqrt{\\alpha^2 - \\beta^2} + \\beta (x - \\mu)}\n\n    In SciPy, this corresponds to\n    `a = alpha * delta, b = beta * delta, loc = mu, scale=delta`.\n\n    References\n    ----------\n    .. [1] O. Barndorff-Nielsen, \"Hyperbolic Distributions and Distributions on\n           Hyperbolae\", Scandinavian Journal of Statistics, Vol. 5(3),\n           pp. 151-157, 1978.\n\n    .. [2] O. Barndorff-Nielsen, \"Normal Inverse Gaussian Distributions and\n           Stochastic Volatility Modelling\", Scandinavian Journal of\n           Statistics, Vol. 24, pp. 1-13, 1997.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import norminvgauss\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a, b = 1.25, 0.5\n    >>> mean, var, skew, kurt = norminvgauss.stats(a, b, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(norminvgauss.ppf(0.01, a, b),\n    ...                 norminvgauss.ppf(0.99, a, b), 100)\n    >>> ax.plot(x, norminvgauss.pdf(x, a, b),\n    ...        'r-', lw=5, alpha=0.6, label='norminvgauss pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = norminvgauss(a, b)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = norminvgauss.ppf([0.001, 0.5, 0.999], a, b)\n    >>> np.allclose([0.001, 0.5, 0.999], norminvgauss.cdf(vals, a, b))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = norminvgauss.rvs(a, b, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.obrientransform": "Compute the O'Brien transform on input data (any number of arrays).\n\n    Used to test for homogeneity of variance prior to running one-way stats.\n    Each array in ``*samples`` is one level of a factor.\n    If `f_oneway` is run on the transformed data and found significant,\n    the variances are unequal.  From Maxwell and Delaney [1]_, p.112.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n        Any number of arrays.\n\n    Returns\n    -------\n    obrientransform : ndarray\n        Transformed data for use in an ANOVA.  The first dimension\n        of the result corresponds to the sequence of transformed\n        arrays.  If the arrays given are all 1-D of the same length,\n        the return value is a 2-D array; otherwise it is a 1-D array\n        of type object, with each element being an ndarray.\n\n    References\n    ----------\n    .. [1] S. E. Maxwell and H. D. Delaney, \"Designing Experiments and\n           Analyzing Data: A Model Comparison Perspective\", Wadsworth, 1990.\n\n    Examples\n    --------\n    We'll test the following data sets for differences in their variance.\n\n    >>> x = [10, 11, 13, 9, 7, 12, 12, 9, 10]\n    >>> y = [13, 21, 5, 10, 8, 14, 10, 12, 7, 15]\n\n    Apply the O'Brien transform to the data.\n\n    >>> from scipy.stats import obrientransform\n    >>> tx, ty = obrientransform(x, y)\n\n    Use `scipy.stats.f_oneway` to apply a one-way ANOVA test to the\n    transformed data.\n\n    >>> from scipy.stats import f_oneway\n    >>> F, p = f_oneway(tx, ty)\n    >>> p\n    0.1314139477040335\n\n    If we require that ``p < 0.05`` for significance, we cannot conclude\n    that the variances are different.\n\n    ",
    "scipy.stats.ortho_group": "An Orthogonal matrix (O(N)) random variable.\n\n    Return a random orthogonal matrix, drawn from the O(N) Haar\n    distribution (the only uniform distribution on O(N)).\n\n    The `dim` keyword specifies the dimension N.\n\n    Methods\n    -------\n    rvs(dim=None, size=1, random_state=None)\n        Draw random samples from O(N).\n\n    Parameters\n    ----------\n    dim : scalar\n        Dimension of matrices\n    seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n        Used for drawing random variates.\n        If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used, seeded\n        with seed.\n        If `seed` is already a ``RandomState`` or ``Generator`` instance,\n        then that object is used.\n        Default is `None`.\n\n    Notes\n    -----\n    This class is closely related to `special_ortho_group`.\n\n    Some care is taken to avoid numerical error, as per the paper by Mezzadri.\n\n    References\n    ----------\n    .. [1] F. Mezzadri, \"How to generate random matrices from the classical\n           compact groups\", :arXiv:`math-ph/0609050v2`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import ortho_group\n    >>> x = ortho_group.rvs(3)\n\n    >>> np.dot(x, x.T)\n    array([[  1.00000000e+00,   1.13231364e-17,  -2.86852790e-16],\n           [  1.13231364e-17,   1.00000000e+00,  -1.46845020e-16],\n           [ -2.86852790e-16,  -1.46845020e-16,   1.00000000e+00]])\n\n    >>> import scipy.linalg\n    >>> np.fabs(scipy.linalg.det(x))\n    1.0\n\n    This generates one random matrix from O(3). It is orthogonal and\n    has a determinant of +1 or -1.\n\n    Alternatively, the object may be called (as a function) to fix the `dim`\n    parameter, returning a \"frozen\" ortho_group random variable:\n\n    >>> rv = ortho_group(5)\n    >>> # Frozen object with the same methods but holding the\n    >>> # dimension parameter fixed.\n\n    See Also\n    --------\n    special_ortho_group\n    ",
    "scipy.stats.page_trend_test": "\n    Perform Page's Test, a measure of trend in observations between treatments.\n\n    Page's Test (also known as Page's :math:`L` test) is useful when:\n\n    * there are :math:`n \\geq 3` treatments,\n    * :math:`m \\geq 2` subjects are observed for each treatment, and\n    * the observations are hypothesized to have a particular order.\n\n    Specifically, the test considers the null hypothesis that\n\n    .. math::\n\n        m_1 = m_2 = m_3 \\cdots = m_n,\n\n    where :math:`m_j` is the mean of the observed quantity under treatment\n    :math:`j`, against the alternative hypothesis that\n\n    .. math::\n\n        m_1 \\leq m_2 \\leq m_3 \\leq \\cdots \\leq m_n,\n\n    where at least one inequality is strict.\n\n    As noted by [4]_, Page's :math:`L` test has greater statistical power than\n    the Friedman test against the alternative that there is a difference in\n    trend, as Friedman's test only considers a difference in the means of the\n    observations without considering their order. Whereas Spearman :math:`\\rho`\n    considers the correlation between the ranked observations of two variables\n    (e.g. the airspeed velocity of a swallow vs. the weight of the coconut it\n    carries), Page's :math:`L` is concerned with a trend in an observation\n    (e.g. the airspeed velocity of a swallow) across several distinct\n    treatments (e.g. carrying each of five coconuts of different weight) even\n    as the observation is repeated with multiple subjects (e.g. one European\n    swallow and one African swallow).\n\n    Parameters\n    ----------\n    data : array-like\n        A :math:`m \\times n` array; the element in row :math:`i` and\n        column :math:`j` is the observation corresponding with subject\n        :math:`i` and treatment :math:`j`. By default, the columns are\n        assumed to be arranged in order of increasing predicted mean.\n\n    ranked : boolean, optional\n        By default, `data` is assumed to be observations rather than ranks;\n        it will be ranked with `scipy.stats.rankdata` along ``axis=1``. If\n        `data` is provided in the form of ranks, pass argument ``True``.\n\n    predicted_ranks : array-like, optional\n        The predicted ranks of the column means. If not specified,\n        the columns are assumed to be arranged in order of increasing\n        predicted mean, so the default `predicted_ranks` are\n        :math:`[1, 2, \\dots, n-1, n]`.\n\n    method : {'auto', 'asymptotic', 'exact'}, optional\n        Selects the method used to calculate the *p*-value. The following\n        options are available.\n\n        * 'auto': selects between 'exact' and 'asymptotic' to\n          achieve reasonably accurate results in reasonable time (default)\n        * 'asymptotic': compares the standardized test statistic against\n          the normal distribution\n        * 'exact': computes the exact *p*-value by comparing the observed\n          :math:`L` statistic against those realized by all possible\n          permutations of ranks (under the null hypothesis that each\n          permutation is equally likely)\n\n    Returns\n    -------\n    res : PageTrendTestResult\n        An object containing attributes:\n\n        statistic : float\n            Page's :math:`L` test statistic.\n        pvalue : float\n            The associated *p*-value\n        method : {'asymptotic', 'exact'}\n            The method used to compute the *p*-value\n\n    See Also\n    --------\n    rankdata, friedmanchisquare, spearmanr\n\n    Notes\n    -----\n    As noted in [1]_, \"the :math:`n` 'treatments' could just as well represent\n    :math:`n` objects or events or performances or persons or trials ranked.\"\n    Similarly, the :math:`m` 'subjects' could equally stand for :math:`m`\n    \"groupings by ability or some other control variable, or judges doing\n    the ranking, or random replications of some other sort.\"\n\n    The procedure for calculating the :math:`L` statistic, adapted from\n    [1]_, is:\n\n    1. \"Predetermine with careful logic the appropriate hypotheses\n       concerning the predicted ordering of the experimental results.\n       If no reasonable basis for ordering any treatments is known, the\n       :math:`L` test is not appropriate.\"\n    2. \"As in other experiments, determine at what level of confidence\n       you will reject the null hypothesis that there is no agreement of\n       experimental results with the monotonic hypothesis.\"\n    3. \"Cast the experimental material into a two-way table of :math:`n`\n       columns (treatments, objects ranked, conditions) and :math:`m`\n       rows (subjects, replication groups, levels of control variables).\"\n    4. \"When experimental observations are recorded, rank them across each\n       row\", e.g. ``ranks = scipy.stats.rankdata(data, axis=1)``.\n    5. \"Add the ranks in each column\", e.g.\n       ``colsums = np.sum(ranks, axis=0)``.\n    6. \"Multiply each sum of ranks by the predicted rank for that same\n       column\", e.g. ``products = predicted_ranks * colsums``.\n    7. \"Sum all such products\", e.g. ``L = products.sum()``.\n\n    [1]_ continues by suggesting use of the standardized statistic\n\n    .. math::\n\n        \\chi_L^2 = \\frac{\\left[12L-3mn(n+1)^2\\right]^2}{mn^2(n^2-1)(n+1)}\n\n    \"which is distributed approximately as chi-square with 1 degree of\n    freedom. The ordinary use of :math:`\\chi^2` tables would be\n    equivalent to a two-sided test of agreement. If a one-sided test\n    is desired, *as will almost always be the case*, the probability\n    discovered in the chi-square table should be *halved*.\"\n\n    However, this standardized statistic does not distinguish between the\n    observed values being well correlated with the predicted ranks and being\n    _anti_-correlated with the predicted ranks. Instead, we follow [2]_\n    and calculate the standardized statistic\n\n    .. math::\n\n        \\Lambda = \\frac{L - E_0}{\\sqrt{V_0}},\n\n    where :math:`E_0 = \\frac{1}{4} mn(n+1)^2` and\n    :math:`V_0 = \\frac{1}{144} mn^2(n+1)(n^2-1)`, \"which is asymptotically\n    normal under the null hypothesis\".\n\n    The *p*-value for ``method='exact'`` is generated by comparing the observed\n    value of :math:`L` against the :math:`L` values generated for all\n    :math:`(n!)^m` possible permutations of ranks. The calculation is performed\n    using the recursive method of [5].\n\n    The *p*-values are not adjusted for the possibility of ties. When\n    ties are present, the reported  ``'exact'`` *p*-values may be somewhat\n    larger (i.e. more conservative) than the true *p*-value [2]_. The\n    ``'asymptotic'``` *p*-values, however, tend to be smaller (i.e. less\n    conservative) than the ``'exact'`` *p*-values.\n\n    References\n    ----------\n    .. [1] Ellis Batten Page, \"Ordered hypotheses for multiple treatments:\n       a significant test for linear ranks\", *Journal of the American\n       Statistical Association* 58(301), p. 216--230, 1963.\n\n    .. [2] Markus Neuhauser, *Nonparametric Statistical Test: A computational\n       approach*, CRC Press, p. 150--152, 2012.\n\n    .. [3] Statext LLC, \"Page's L Trend Test - Easy Statistics\", *Statext -\n       Statistics Study*, https://www.statext.com/practice/PageTrendTest03.php,\n       Accessed July 12, 2020.\n\n    .. [4] \"Page's Trend Test\", *Wikipedia*, WikimediaFoundation,\n       https://en.wikipedia.org/wiki/Page%27s_trend_test,\n       Accessed July 12, 2020.\n\n    .. [5] Robert E. Odeh, \"The exact distribution of Page's L-statistic in\n       the two-way layout\", *Communications in Statistics - Simulation and\n       Computation*,  6(1), p. 49--61, 1977.\n\n    Examples\n    --------\n    We use the example from [3]_: 10 students are asked to rate three\n    teaching methods - tutorial, lecture, and seminar - on a scale of 1-5,\n    with 1 being the lowest and 5 being the highest. We have decided that\n    a confidence level of 99% is required to reject the null hypothesis in\n    favor of our alternative: that the seminar will have the highest ratings\n    and the tutorial will have the lowest. Initially, the data have been\n    tabulated with each row representing an individual student's ratings of\n    the three methods in the following order: tutorial, lecture, seminar.\n\n    >>> table = [[3, 4, 3],\n    ...          [2, 2, 4],\n    ...          [3, 3, 5],\n    ...          [1, 3, 2],\n    ...          [2, 3, 2],\n    ...          [2, 4, 5],\n    ...          [1, 2, 4],\n    ...          [3, 4, 4],\n    ...          [2, 4, 5],\n    ...          [1, 3, 4]]\n\n    Because the tutorial is hypothesized to have the lowest ratings, the\n    column corresponding with tutorial rankings should be first; the seminar\n    is hypothesized to have the highest ratings, so its column should be last.\n    Since the columns are already arranged in this order of increasing\n    predicted mean, we can pass the table directly into `page_trend_test`.\n\n    >>> from scipy.stats import page_trend_test\n    >>> res = page_trend_test(table)\n    >>> res\n    PageTrendTestResult(statistic=133.5, pvalue=0.0018191161948127822,\n                        method='exact')\n\n    This *p*-value indicates that there is a 0.1819% chance that\n    the :math:`L` statistic would reach such an extreme value under the null\n    hypothesis. Because 0.1819% is less than 1%, we have evidence to reject\n    the null hypothesis in favor of our alternative at a 99% confidence level.\n\n    The value of the :math:`L` statistic is 133.5. To check this manually,\n    we rank the data such that high scores correspond with high ranks, settling\n    ties with an average rank:\n\n    >>> from scipy.stats import rankdata\n    >>> ranks = rankdata(table, axis=1)\n    >>> ranks\n    array([[1.5, 3. , 1.5],\n           [1.5, 1.5, 3. ],\n           [1.5, 1.5, 3. ],\n           [1. , 3. , 2. ],\n           [1.5, 3. , 1.5],\n           [1. , 2. , 3. ],\n           [1. , 2. , 3. ],\n           [1. , 2.5, 2.5],\n           [1. , 2. , 3. ],\n           [1. , 2. , 3. ]])\n\n    We add the ranks within each column, multiply the sums by the\n    predicted ranks, and sum the products.\n\n    >>> import numpy as np\n    >>> m, n = ranks.shape\n    >>> predicted_ranks = np.arange(1, n+1)\n    >>> L = (predicted_ranks * np.sum(ranks, axis=0)).sum()\n    >>> res.statistic == L\n    True\n\n    As presented in [3]_, the asymptotic approximation of the *p*-value is the\n    survival function of the normal distribution evaluated at the standardized\n    test statistic:\n\n    >>> from scipy.stats import norm\n    >>> E0 = (m*n*(n+1)**2)/4\n    >>> V0 = (m*n**2*(n+1)*(n**2-1))/144\n    >>> Lambda = (L-E0)/np.sqrt(V0)\n    >>> p = norm.sf(Lambda)\n    >>> p\n    0.0012693433690751756\n\n    This does not precisely match the *p*-value reported by `page_trend_test`\n    above. The asymptotic distribution is not very accurate, nor conservative,\n    for :math:`m \\leq 12` and :math:`n \\leq 8`, so `page_trend_test` chose to\n    use ``method='exact'`` based on the dimensions of the table and the\n    recommendations in Page's original paper [1]_. To override\n    `page_trend_test`'s choice, provide the `method` argument.\n\n    >>> res = page_trend_test(table, method=\"asymptotic\")\n    >>> res\n    PageTrendTestResult(statistic=133.5, pvalue=0.0012693433690751756,\n                        method='asymptotic')\n\n    If the data are already ranked, we can pass in the ``ranks`` instead of\n    the ``table`` to save computation time.\n\n    >>> res = page_trend_test(ranks,             # ranks of data\n    ...                       ranked=True,       # data is already ranked\n    ...                       )\n    >>> res\n    PageTrendTestResult(statistic=133.5, pvalue=0.0018191161948127822,\n                        method='exact')\n\n    Suppose the raw data had been tabulated in an order different from the\n    order of predicted means, say lecture, seminar, tutorial.\n\n    >>> table = np.asarray(table)[:, [1, 2, 0]]\n\n    Since the arrangement of this table is not consistent with the assumed\n    ordering, we can either rearrange the table or provide the\n    `predicted_ranks`. Remembering that the lecture is predicted\n    to have the middle rank, the seminar the highest, and tutorial the lowest,\n    we pass:\n\n    >>> res = page_trend_test(table,             # data as originally tabulated\n    ...                       predicted_ranks=[2, 3, 1],  # our predicted order\n    ...                       )\n    >>> res\n    PageTrendTestResult(statistic=133.5, pvalue=0.0018191161948127822,\n                        method='exact')\n\n    ",
    "scipy.stats.pareto": "A Pareto continuous random variable.\n\n    As an instance of the `rv_continuous` class, `pareto` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(b, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, b, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, b, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, b, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, b, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, b, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, b, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, b, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, b, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, b, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(b, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(b, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(b,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(b, loc=0, scale=1)\n        Median of the distribution.\n    mean(b, loc=0, scale=1)\n        Mean of the distribution.\n    var(b, loc=0, scale=1)\n        Variance of the distribution.\n    std(b, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, b, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `pareto` is:\n\n    .. math::\n\n        f(x, b) = \\frac{b}{x^{b+1}}\n\n    for :math:`x \\ge 1`, :math:`b > 0`.\n\n    `pareto` takes ``b`` as a shape parameter for :math:`b`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``pareto.pdf(x, b, loc, scale)`` is identically\n    equivalent to ``pareto.pdf(y, b) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import pareto\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> b = 2.62\n    >>> mean, var, skew, kurt = pareto.stats(b, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(pareto.ppf(0.01, b),\n    ...                 pareto.ppf(0.99, b), 100)\n    >>> ax.plot(x, pareto.pdf(x, b),\n    ...        'r-', lw=5, alpha=0.6, label='pareto pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = pareto(b)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = pareto.ppf([0.001, 0.5, 0.999], b)\n    >>> np.allclose([0.001, 0.5, 0.999], pareto.cdf(vals, b))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = pareto.rvs(b, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.pearson3": "A pearson type III continuous random variable.\n\n    As an instance of the `rv_continuous` class, `pearson3` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(skew, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, skew, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, skew, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, skew, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, skew, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, skew, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, skew, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, skew, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, skew, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, skew, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(skew, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(skew, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(skew,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(skew, loc=0, scale=1)\n        Median of the distribution.\n    mean(skew, loc=0, scale=1)\n        Mean of the distribution.\n    var(skew, loc=0, scale=1)\n        Variance of the distribution.\n    std(skew, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, skew, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `pearson3` is:\n\n    .. math::\n\n        f(x, \\kappa) = \\frac{|\\beta|}{\\Gamma(\\alpha)}\n                       (\\beta (x - \\zeta))^{\\alpha - 1}\n                       \\exp(-\\beta (x - \\zeta))\n\n    where:\n\n    .. math::\n\n            \\beta = \\frac{2}{\\kappa}\n\n            \\alpha = \\beta^2 = \\frac{4}{\\kappa^2}\n\n            \\zeta = -\\frac{\\alpha}{\\beta} = -\\beta\n\n    :math:`\\Gamma` is the gamma function (`scipy.special.gamma`).\n    Pass the skew :math:`\\kappa` into `pearson3` as the shape parameter\n    ``skew``.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``pearson3.pdf(x, skew, loc, scale)`` is identically\n    equivalent to ``pearson3.pdf(y, skew) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import pearson3\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> skew = -2\n    >>> mean, var, skew, kurt = pearson3.stats(skew, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(pearson3.ppf(0.01, skew),\n    ...                 pearson3.ppf(0.99, skew), 100)\n    >>> ax.plot(x, pearson3.pdf(x, skew),\n    ...        'r-', lw=5, alpha=0.6, label='pearson3 pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = pearson3(skew)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = pearson3.ppf([0.001, 0.5, 0.999], skew)\n    >>> np.allclose([0.001, 0.5, 0.999], pearson3.cdf(vals, skew))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = pearson3.rvs(skew, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    References\n    ----------\n    R.W. Vogel and D.E. McMartin, \"Probability Plot Goodness-of-Fit and\n    Skewness Estimation Procedures for the Pearson Type 3 Distribution\", Water\n    Resources Research, Vol.27, 3149-3158 (1991).\n\n    L.R. Salvosa, \"Tables of Pearson's Type III Function\", Ann. Math. Statist.,\n    Vol.1, 191-198 (1930).\n\n    \"Using Modern Computing Tools to Fit the Pearson Type III Distribution to\n    Aviation Loads Data\", Office of Aviation Research (2003).\n\n    ",
    "scipy.stats.pearsonr": "\n    Pearson correlation coefficient and p-value for testing non-correlation.\n\n    The Pearson correlation coefficient [1]_ measures the linear relationship\n    between two datasets. Like other correlation\n    coefficients, this one varies between -1 and +1 with 0 implying no\n    correlation. Correlations of -1 or +1 imply an exact linear relationship.\n    Positive correlations imply that as x increases, so does y. Negative\n    correlations imply that as x increases, y decreases.\n\n    This function also performs a test of the null hypothesis that the\n    distributions underlying the samples are uncorrelated and normally\n    distributed. (See Kowalski [3]_\n    for a discussion of the effects of non-normality of the input on the\n    distribution of the correlation coefficient.)\n    The p-value roughly indicates the probability of an uncorrelated system\n    producing datasets that have a Pearson correlation at least as extreme\n    as the one computed from these datasets.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array.\n    y : array_like\n        Input array.\n    axis : int or None, default\n        Axis along which to perform the calculation. Default is 0.\n        If None, ravel both arrays before performing the calculation.\n\n        .. versionadded:: 1.13.0\n    alternative : {'two-sided', 'greater', 'less'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n        The following options are available:\n\n        * 'two-sided': the correlation is nonzero\n        * 'less': the correlation is negative (less than zero)\n        * 'greater':  the correlation is positive (greater than zero)\n\n        .. versionadded:: 1.9.0\n    method : ResamplingMethod, optional\n        Defines the method used to compute the p-value. If `method` is an\n        instance of `PermutationMethod`/`MonteCarloMethod`, the p-value is\n        computed using\n        `scipy.stats.permutation_test`/`scipy.stats.monte_carlo_test` with the\n        provided configuration options and other appropriate settings.\n        Otherwise, the p-value is computed as documented in the notes.\n\n        .. versionadded:: 1.11.0\n\n    Returns\n    -------\n    result : `~scipy.stats._result_classes.PearsonRResult`\n        An object with the following attributes:\n\n        statistic : float\n            Pearson product-moment correlation coefficient.\n        pvalue : float\n            The p-value associated with the chosen alternative.\n\n        The object has the following method:\n\n        confidence_interval(confidence_level, method)\n            This computes the confidence interval of the correlation\n            coefficient `statistic` for the given confidence level.\n            The confidence interval is returned in a ``namedtuple`` with\n            fields `low` and `high`. If `method` is not provided, the\n            confidence interval is computed using the Fisher transformation\n            [1]_. If `method` is an instance of `BootstrapMethod`, the\n            confidence interval is computed using `scipy.stats.bootstrap` with\n            the provided configuration options and other appropriate settings.\n            In some cases, confidence limits may be NaN due to a degenerate\n            resample, and this is typical for very small samples (~6\n            observations).\n\n    Warns\n    -----\n    `~scipy.stats.ConstantInputWarning`\n        Raised if an input is a constant array.  The correlation coefficient\n        is not defined in this case, so ``np.nan`` is returned.\n\n    `~scipy.stats.NearConstantInputWarning`\n        Raised if an input is \"nearly\" constant.  The array ``x`` is considered\n        nearly constant if ``norm(x - mean(x)) < 1e-13 * abs(mean(x))``.\n        Numerical errors in the calculation ``x - mean(x)`` in this case might\n        result in an inaccurate calculation of r.\n\n    See Also\n    --------\n    spearmanr : Spearman rank-order correlation coefficient.\n    kendalltau : Kendall's tau, a correlation measure for ordinal data.\n\n    Notes\n    -----\n    The correlation coefficient is calculated as follows:\n\n    .. math::\n\n        r = \\frac{\\sum (x - m_x) (y - m_y)}\n                 {\\sqrt{\\sum (x - m_x)^2 \\sum (y - m_y)^2}}\n\n    where :math:`m_x` is the mean of the vector x and :math:`m_y` is\n    the mean of the vector y.\n\n    Under the assumption that x and y are drawn from\n    independent normal distributions (so the population correlation coefficient\n    is 0), the probability density function of the sample correlation\n    coefficient r is ([1]_, [2]_):\n\n    .. math::\n        f(r) = \\frac{{(1-r^2)}^{n/2-2}}{\\mathrm{B}(\\frac{1}{2},\\frac{n}{2}-1)}\n\n    where n is the number of samples, and B is the beta function.  This\n    is sometimes referred to as the exact distribution of r.  This is\n    the distribution that is used in `pearsonr` to compute the p-value when\n    the `method` parameter is left at its default value (None).\n    The distribution is a beta distribution on the interval [-1, 1],\n    with equal shape parameters a = b = n/2 - 1.  In terms of SciPy's\n    implementation of the beta distribution, the distribution of r is::\n\n        dist = scipy.stats.beta(n/2 - 1, n/2 - 1, loc=-1, scale=2)\n\n    The default p-value returned by `pearsonr` is a two-sided p-value. For a\n    given sample with correlation coefficient r, the p-value is\n    the probability that abs(r') of a random sample x' and y' drawn from\n    the population with zero correlation would be greater than or equal\n    to abs(r). In terms of the object ``dist`` shown above, the p-value\n    for a given r and length n can be computed as::\n\n        p = 2*dist.cdf(-abs(r))\n\n    When n is 2, the above continuous distribution is not well-defined.\n    One can interpret the limit of the beta distribution as the shape\n    parameters a and b approach a = b = 0 as a discrete distribution with\n    equal probability masses at r = 1 and r = -1.  More directly, one\n    can observe that, given the data x = [x1, x2] and y = [y1, y2], and\n    assuming x1 != x2 and y1 != y2, the only possible values for r are 1\n    and -1.  Because abs(r') for any sample x' and y' with length 2 will\n    be 1, the two-sided p-value for a sample of length 2 is always 1.\n\n    For backwards compatibility, the object that is returned also behaves\n    like a tuple of length two that holds the statistic and the p-value.\n\n    References\n    ----------\n    .. [1] \"Pearson correlation coefficient\", Wikipedia,\n           https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n    .. [2] Student, \"Probable error of a correlation coefficient\",\n           Biometrika, Volume 6, Issue 2-3, 1 September 1908, pp. 302-310.\n    .. [3] C. J. Kowalski, \"On the Effects of Non-Normality on the Distribution\n           of the Sample Product-Moment Correlation Coefficient\"\n           Journal of the Royal Statistical Society. Series C (Applied\n           Statistics), Vol. 21, No. 1 (1972), pp. 1-12.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x, y = [1, 2, 3, 4, 5, 6, 7], [10, 9, 2.5, 6, 4, 3, 2]\n    >>> res = stats.pearsonr(x, y)\n    >>> res\n    PearsonRResult(statistic=-0.828503883588428, pvalue=0.021280260007523286)\n\n    To perform an exact permutation version of the test:\n\n    >>> rng = np.random.default_rng(7796654889291491997)\n    >>> method = stats.PermutationMethod(n_resamples=np.inf, random_state=rng)\n    >>> stats.pearsonr(x, y, method=method)\n    PearsonRResult(statistic=-0.828503883588428, pvalue=0.028174603174603175)\n\n    To perform the test under the null hypothesis that the data were drawn from\n    *uniform* distributions:\n\n    >>> method = stats.MonteCarloMethod(rvs=(rng.uniform, rng.uniform))\n    >>> stats.pearsonr(x, y, method=method)\n    PearsonRResult(statistic=-0.828503883588428, pvalue=0.0188)\n\n    To produce an asymptotic 90% confidence interval:\n\n    >>> res.confidence_interval(confidence_level=0.9)\n    ConfidenceInterval(low=-0.9644331982722841, high=-0.3460237473272273)\n\n    And for a bootstrap confidence interval:\n\n    >>> method = stats.BootstrapMethod(method='BCa', random_state=rng)\n    >>> res.confidence_interval(confidence_level=0.9, method=method)\n    ConfidenceInterval(low=-0.9983163756488651, high=-0.22771001702132443)  # may vary\n\n    If N-dimensional arrays are provided, multiple tests are performed in a\n    single call according to the same conventions as most `scipy.stats` functions:\n\n    >>> rng = np.random.default_rng(2348246935601934321)\n    >>> x = rng.standard_normal((8, 15))\n    >>> y = rng.standard_normal((8, 15))\n    >>> stats.pearsonr(x, y, axis=0).statistic.shape  # between corresponding columns\n    (15,)\n    >>> stats.pearsonr(x, y, axis=1).statistic.shape  # between corresponding rows\n    (8,)\n\n    To perform all pairwise comparisons between slices of the arrays,\n    use standard NumPy broadcasting techniques. For instance, to compute the\n    correlation between all pairs of rows:\n\n    >>> stats.pearsonr(x[:, np.newaxis, :], y, axis=-1).statistic.shape\n    (8, 8)\n\n    There is a linear dependence between x and y if y = a + b*x + e, where\n    a,b are constants and e is a random error term, assumed to be independent\n    of x. For simplicity, assume that x is standard normal, a=0, b=1 and let\n    e follow a normal distribution with mean zero and standard deviation s>0.\n\n    >>> rng = np.random.default_rng()\n    >>> s = 0.5\n    >>> x = stats.norm.rvs(size=500, random_state=rng)\n    >>> e = stats.norm.rvs(scale=s, size=500, random_state=rng)\n    >>> y = x + e\n    >>> stats.pearsonr(x, y).statistic\n    0.9001942438244763\n\n    This should be close to the exact value given by\n\n    >>> 1/np.sqrt(1 + s**2)\n    0.8944271909999159\n\n    For s=0.5, we observe a high level of correlation. In general, a large\n    variance of the noise reduces the correlation, while the correlation\n    approaches one as the variance of the error goes to zero.\n\n    It is important to keep in mind that no correlation does not imply\n    independence unless (x, y) is jointly normal. Correlation can even be zero\n    when there is a very simple dependence structure: if X follows a\n    standard normal distribution, let y = abs(x). Note that the correlation\n    between x and y is zero. Indeed, since the expectation of x is zero,\n    cov(x, y) = E[x*y]. By definition, this equals E[x*abs(x)] which is zero\n    by symmetry. The following lines of code illustrate this observation:\n\n    >>> y = np.abs(x)\n    >>> stats.pearsonr(x, y)\n    PearsonRResult(statistic=-0.05444919272687482, pvalue=0.22422294836207743)\n\n    A non-zero correlation coefficient can be misleading. For example, if X has\n    a standard normal distribution, define y = x if x < 0 and y = 0 otherwise.\n    A simple calculation shows that corr(x, y) = sqrt(2/Pi) = 0.797...,\n    implying a high level of correlation:\n\n    >>> y = np.where(x < 0, x, 0)\n    >>> stats.pearsonr(x, y)\n    PearsonRResult(statistic=0.861985781588, pvalue=4.813432002751103e-149)\n\n    This is unintuitive since there is no dependence of x and y if x is larger\n    than zero which happens in about half of the cases if we sample x and y.\n\n    ",
    "scipy.stats.percentileofscore": "Compute the percentile rank of a score relative to a list of scores.\n\n    A `percentileofscore` of, for example, 80% means that 80% of the\n    scores in `a` are below the given score. In the case of gaps or\n    ties, the exact definition depends on the optional keyword, `kind`.\n\n    Parameters\n    ----------\n    a : array_like\n        A 1-D array to which `score` is compared.\n    score : array_like\n        Scores to compute percentiles for.\n    kind : {'rank', 'weak', 'strict', 'mean'}, optional\n        Specifies the interpretation of the resulting score.\n        The following options are available (default is 'rank'):\n\n          * 'rank': Average percentage ranking of score.  In case of multiple\n            matches, average the percentage rankings of all matching scores.\n          * 'weak': This kind corresponds to the definition of a cumulative\n            distribution function.  A percentileofscore of 80% means that 80%\n            of values are less than or equal to the provided score.\n          * 'strict': Similar to \"weak\", except that only values that are\n            strictly less than the given score are counted.\n          * 'mean': The average of the \"weak\" and \"strict\" scores, often used\n            in testing.  See https://en.wikipedia.org/wiki/Percentile_rank\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Specifies how to treat `nan` values in `a`.\n        The following options are available (default is 'propagate'):\n\n          * 'propagate': returns nan (for each value in `score`).\n          * 'raise': throws an error\n          * 'omit': performs the calculations ignoring nan values\n\n    Returns\n    -------\n    pcos : float\n        Percentile-position of score (0-100) relative to `a`.\n\n    See Also\n    --------\n    numpy.percentile\n    scipy.stats.scoreatpercentile, scipy.stats.rankdata\n\n    Examples\n    --------\n    Three-quarters of the given values lie below a given score:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> stats.percentileofscore([1, 2, 3, 4], 3)\n    75.0\n\n    With multiple matches, note how the scores of the two matches, 0.6\n    and 0.8 respectively, are averaged:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3)\n    70.0\n\n    Only 2/5 values are strictly less than 3:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='strict')\n    40.0\n\n    But 4/5 values are less than or equal to 3:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='weak')\n    80.0\n\n    The average between the weak and the strict scores is:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='mean')\n    60.0\n\n    Score arrays (of any dimensionality) are supported:\n\n    >>> stats.percentileofscore([1, 2, 3, 3, 4], [2, 3])\n    array([40., 70.])\n\n    The inputs can be infinite:\n\n    >>> stats.percentileofscore([-np.inf, 0, 1, np.inf], [1, 2, np.inf])\n    array([75., 75., 100.])\n\n    If `a` is empty, then the resulting percentiles are all `nan`:\n\n    >>> stats.percentileofscore([], [1, 2])\n    array([nan, nan])\n    ",
    "scipy.stats.permutation_test": "\n    Performs a permutation test of a given statistic on provided data.\n\n    For independent sample statistics, the null hypothesis is that the data are\n    randomly sampled from the same distribution.\n    For paired sample statistics, two null hypothesis can be tested:\n    that the data are paired at random or that the data are assigned to samples\n    at random.\n\n    Parameters\n    ----------\n    data : iterable of array-like\n        Contains the samples, each of which is an array of observations.\n        Dimensions of sample arrays must be compatible for broadcasting except\n        along `axis`.\n    statistic : callable\n        Statistic for which the p-value of the hypothesis test is to be\n        calculated. `statistic` must be a callable that accepts samples\n        as separate arguments (e.g. ``statistic(*data)``) and returns the\n        resulting statistic.\n        If `vectorized` is set ``True``, `statistic` must also accept a keyword\n        argument `axis` and be vectorized to compute the statistic along the\n        provided `axis` of the sample arrays.\n    permutation_type : {'independent', 'samples', 'pairings'}, optional\n        The type of permutations to be performed, in accordance with the\n        null hypothesis. The first two permutation types are for paired sample\n        statistics, in which all samples contain the same number of\n        observations and observations with corresponding indices along `axis`\n        are considered to be paired; the third is for independent sample\n        statistics.\n\n        - ``'samples'`` : observations are assigned to different samples\n          but remain paired with the same observations from other samples.\n          This permutation type is appropriate for paired sample hypothesis\n          tests such as the Wilcoxon signed-rank test and the paired t-test.\n        - ``'pairings'`` : observations are paired with different observations,\n          but they remain within the same sample. This permutation type is\n          appropriate for association/correlation tests with statistics such\n          as Spearman's :math:`\\rho`, Kendall's :math:`\\tau`, and Pearson's\n          :math:`r`.\n        - ``'independent'`` (default) : observations are assigned to different\n          samples. Samples may contain different numbers of observations. This\n          permutation type is appropriate for independent sample hypothesis\n          tests such as the Mann-Whitney :math:`U` test and the independent\n          sample t-test.\n\n          Please see the Notes section below for more detailed descriptions\n          of the permutation types.\n\n    vectorized : bool, optional\n        If `vectorized` is set ``False``, `statistic` will not be passed\n        keyword argument `axis` and is expected to calculate the statistic\n        only for 1D samples. If ``True``, `statistic` will be passed keyword\n        argument `axis` and is expected to calculate the statistic along `axis`\n        when passed an ND sample array. If ``None`` (default), `vectorized`\n        will be set ``True`` if ``axis`` is a parameter of `statistic`. Use\n        of a vectorized statistic typically reduces computation time.\n    n_resamples : int or np.inf, default: 9999\n        Number of random permutations (resamples) used to approximate the null\n        distribution. If greater than or equal to the number of distinct\n        permutations, the exact null distribution will be computed.\n        Note that the number of distinct permutations grows very rapidly with\n        the sizes of samples, so exact tests are feasible only for very small\n        data sets.\n    batch : int, optional\n        The number of permutations to process in each call to `statistic`.\n        Memory usage is O( `batch` * ``n`` ), where ``n`` is the total size\n        of all samples, regardless of the value of `vectorized`. Default is\n        ``None``, in which case ``batch`` is the number of permutations.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        The alternative hypothesis for which the p-value is calculated.\n        For each alternative, the p-value is defined for exact tests as\n        follows.\n\n        - ``'greater'`` : the percentage of the null distribution that is\n          greater than or equal to the observed value of the test statistic.\n        - ``'less'`` : the percentage of the null distribution that is\n          less than or equal to the observed value of the test statistic.\n        - ``'two-sided'`` (default) : twice the smaller of the p-values above.\n\n        Note that p-values for randomized tests are calculated according to the\n        conservative (over-estimated) approximation suggested in [2]_ and [3]_\n        rather than the unbiased estimator suggested in [4]_. That is, when\n        calculating the proportion of the randomized null distribution that is\n        as extreme as the observed value of the test statistic, the values in\n        the numerator and denominator are both increased by one. An\n        interpretation of this adjustment is that the observed value of the\n        test statistic is always included as an element of the randomized\n        null distribution.\n        The convention used for two-sided p-values is not universal;\n        the observed test statistic and null distribution are returned in\n        case a different definition is preferred.\n\n    axis : int, default: 0\n        The axis of the (broadcasted) samples over which to calculate the\n        statistic. If samples have a different number of dimensions,\n        singleton dimensions are prepended to samples with fewer dimensions\n        before `axis` is considered.\n    random_state : {None, int, `numpy.random.Generator`,\n                    `numpy.random.RandomState`}, optional\n\n        Pseudorandom number generator state used to generate permutations.\n\n        If `random_state` is ``None`` (default), the\n        `numpy.random.RandomState` singleton is used.\n        If `random_state` is an int, a new ``RandomState`` instance is used,\n        seeded with `random_state`.\n        If `random_state` is already a ``Generator`` or ``RandomState``\n        instance then that instance is used.\n\n    Returns\n    -------\n    res : PermutationTestResult\n        An object with attributes:\n\n        statistic : float or ndarray\n            The observed test statistic of the data.\n        pvalue : float or ndarray\n            The p-value for the given alternative.\n        null_distribution : ndarray\n            The values of the test statistic generated under the null\n            hypothesis.\n\n    Notes\n    -----\n\n    The three types of permutation tests supported by this function are\n    described below.\n\n    **Unpaired statistics** (``permutation_type='independent'``):\n\n    The null hypothesis associated with this permutation type is that all\n    observations are sampled from the same underlying distribution and that\n    they have been assigned to one of the samples at random.\n\n    Suppose ``data`` contains two samples; e.g. ``a, b = data``.\n    When ``1 < n_resamples < binom(n, k)``, where\n\n    * ``k`` is the number of observations in ``a``,\n    * ``n`` is the total number of observations in ``a`` and ``b``, and\n    * ``binom(n, k)`` is the binomial coefficient (``n`` choose ``k``),\n\n    the data are pooled (concatenated), randomly assigned to either the first\n    or second sample, and the statistic is calculated. This process is\n    performed repeatedly, `permutation` times, generating a distribution of the\n    statistic under the null hypothesis. The statistic of the original\n    data is compared to this distribution to determine the p-value.\n\n    When ``n_resamples >= binom(n, k)``, an exact test is performed: the data\n    are *partitioned* between the samples in each distinct way exactly once,\n    and the exact null distribution is formed.\n    Note that for a given partitioning of the data between the samples,\n    only one ordering/permutation of the data *within* each sample is\n    considered. For statistics that do not depend on the order of the data\n    within samples, this dramatically reduces computational cost without\n    affecting the shape of the null distribution (because the frequency/count\n    of each value is affected by the same factor).\n\n    For ``a = [a1, a2, a3, a4]`` and ``b = [b1, b2, b3]``, an example of this\n    permutation type is ``x = [b3, a1, a2, b2]`` and ``y = [a4, b1, a3]``.\n    Because only one ordering/permutation of the data *within* each sample\n    is considered in an exact test, a resampling like ``x = [b3, a1, b2, a2]``\n    and ``y = [a4, a3, b1]`` would *not* be considered distinct from the\n    example above.\n\n    ``permutation_type='independent'`` does not support one-sample statistics,\n    but it can be applied to statistics with more than two samples. In this\n    case, if ``n`` is an array of the number of observations within each\n    sample, the number of distinct partitions is::\n\n        np.prod([binom(sum(n[i:]), sum(n[i+1:])) for i in range(len(n)-1)])\n\n    **Paired statistics, permute pairings** (``permutation_type='pairings'``):\n\n    The null hypothesis associated with this permutation type is that\n    observations within each sample are drawn from the same underlying\n    distribution and that pairings with elements of other samples are\n    assigned at random.\n\n    Suppose ``data`` contains only one sample; e.g. ``a, = data``, and we\n    wish to consider all possible pairings of elements of ``a`` with elements\n    of a second sample, ``b``. Let ``n`` be the number of observations in\n    ``a``, which must also equal the number of observations in ``b``.\n\n    When ``1 < n_resamples < factorial(n)``, the elements of ``a`` are\n    randomly permuted. The user-supplied statistic accepts one data argument,\n    say ``a_perm``, and calculates the statistic considering ``a_perm`` and\n    ``b``. This process is performed repeatedly, `permutation` times,\n    generating a distribution of the statistic under the null hypothesis.\n    The statistic of the original data is compared to this distribution to\n    determine the p-value.\n\n    When ``n_resamples >= factorial(n)``, an exact test is performed:\n    ``a`` is permuted in each distinct way exactly once. Therefore, the\n    `statistic` is computed for each unique pairing of samples between ``a``\n    and ``b`` exactly once.\n\n    For ``a = [a1, a2, a3]`` and ``b = [b1, b2, b3]``, an example of this\n    permutation type is ``a_perm = [a3, a1, a2]`` while ``b`` is left\n    in its original order.\n\n    ``permutation_type='pairings'`` supports ``data`` containing any number\n    of samples, each of which must contain the same number of observations.\n    All samples provided in ``data`` are permuted *independently*. Therefore,\n    if ``m`` is the number of samples and ``n`` is the number of observations\n    within each sample, then the number of permutations in an exact test is::\n\n        factorial(n)**m\n\n    Note that if a two-sample statistic, for example, does not inherently\n    depend on the order in which observations are provided - only on the\n    *pairings* of observations - then only one of the two samples should be\n    provided in ``data``. This dramatically reduces computational cost without\n    affecting the shape of the null distribution (because the frequency/count\n    of each value is affected by the same factor).\n\n    **Paired statistics, permute samples** (``permutation_type='samples'``):\n\n    The null hypothesis associated with this permutation type is that\n    observations within each pair are drawn from the same underlying\n    distribution and that the sample to which they are assigned is random.\n\n    Suppose ``data`` contains two samples; e.g. ``a, b = data``.\n    Let ``n`` be the number of observations in ``a``, which must also equal\n    the number of observations in ``b``.\n\n    When ``1 < n_resamples < 2**n``, the elements of ``a`` are ``b`` are\n    randomly swapped between samples (maintaining their pairings) and the\n    statistic is calculated. This process is performed repeatedly,\n    `permutation` times,  generating a distribution of the statistic under the\n    null hypothesis. The statistic of the original data is compared to this\n    distribution to determine the p-value.\n\n    When ``n_resamples >= 2**n``, an exact test is performed: the observations\n    are assigned to the two samples in each distinct way (while maintaining\n    pairings) exactly once.\n\n    For ``a = [a1, a2, a3]`` and ``b = [b1, b2, b3]``, an example of this\n    permutation type is ``x = [b1, a2, b3]`` and ``y = [a1, b2, a3]``.\n\n    ``permutation_type='samples'`` supports ``data`` containing any number\n    of samples, each of which must contain the same number of observations.\n    If ``data`` contains more than one sample, paired observations within\n    ``data`` are exchanged between samples *independently*. Therefore, if ``m``\n    is the number of samples and ``n`` is the number of observations within\n    each sample, then the number of permutations in an exact test is::\n\n        factorial(m)**n\n\n    Several paired-sample statistical tests, such as the Wilcoxon signed rank\n    test and paired-sample t-test, can be performed considering only the\n    *difference* between two paired elements. Accordingly, if ``data`` contains\n    only one sample, then the null distribution is formed by independently\n    changing the *sign* of each observation.\n\n    .. warning::\n        The p-value is calculated by counting the elements of the null\n        distribution that are as extreme or more extreme than the observed\n        value of the statistic. Due to the use of finite precision arithmetic,\n        some statistic functions return numerically distinct values when the\n        theoretical values would be exactly equal. In some cases, this could\n        lead to a large error in the calculated p-value. `permutation_test`\n        guards against this by considering elements in the null distribution\n        that are \"close\" (within a relative tolerance of 100 times the\n        floating point epsilon of inexact dtypes) to the observed\n        value of the test statistic as equal to the observed value of the\n        test statistic. However, the user is advised to inspect the null\n        distribution to assess whether this method of comparison is\n        appropriate, and if not, calculate the p-value manually. See example\n        below.\n\n    References\n    ----------\n\n    .. [1] R. A. Fisher. The Design of Experiments, 6th Ed (1951).\n    .. [2] B. Phipson and G. K. Smyth. \"Permutation P-values Should Never Be\n       Zero: Calculating Exact P-values When Permutations Are Randomly Drawn.\"\n       Statistical Applications in Genetics and Molecular Biology 9.1 (2010).\n    .. [3] M. D. Ernst. \"Permutation Methods: A Basis for Exact Inference\".\n       Statistical Science (2004).\n    .. [4] B. Efron and R. J. Tibshirani. An Introduction to the Bootstrap\n       (1993).\n\n    Examples\n    --------\n\n    Suppose we wish to test whether two samples are drawn from the same\n    distribution. Assume that the underlying distributions are unknown to us,\n    and that before observing the data, we hypothesized that the mean of the\n    first sample would be less than that of the second sample. We decide that\n    we will use the difference between the sample means as a test statistic,\n    and we will consider a p-value of 0.05 to be statistically significant.\n\n    For efficiency, we write the function defining the test statistic in a\n    vectorized fashion: the samples ``x`` and ``y`` can be ND arrays, and the\n    statistic will be calculated for each axis-slice along `axis`.\n\n    >>> import numpy as np\n    >>> def statistic(x, y, axis):\n    ...     return np.mean(x, axis=axis) - np.mean(y, axis=axis)\n\n    After collecting our data, we calculate the observed value of the test\n    statistic.\n\n    >>> from scipy.stats import norm\n    >>> rng = np.random.default_rng()\n    >>> x = norm.rvs(size=5, random_state=rng)\n    >>> y = norm.rvs(size=6, loc = 3, random_state=rng)\n    >>> statistic(x, y, 0)\n    -3.5411688580987266\n\n    Indeed, the test statistic is negative, suggesting that the true mean of\n    the distribution underlying ``x`` is less than that of the distribution\n    underlying ``y``. To determine the probability of this occurring by chance\n    if the two samples were drawn from the same distribution, we perform\n    a permutation test.\n\n    >>> from scipy.stats import permutation_test\n    >>> # because our statistic is vectorized, we pass `vectorized=True`\n    >>> # `n_resamples=np.inf` indicates that an exact test is to be performed\n    >>> res = permutation_test((x, y), statistic, vectorized=True,\n    ...                        n_resamples=np.inf, alternative='less')\n    >>> print(res.statistic)\n    -3.5411688580987266\n    >>> print(res.pvalue)\n    0.004329004329004329\n\n    The probability of obtaining a test statistic less than or equal to the\n    observed value under the null hypothesis is 0.4329%. This is less than our\n    chosen threshold of 5%, so we consider this to be significant evidence\n    against the null hypothesis in favor of the alternative.\n\n    Because the size of the samples above was small, `permutation_test` could\n    perform an exact test. For larger samples, we resort to a randomized\n    permutation test.\n\n    >>> x = norm.rvs(size=100, random_state=rng)\n    >>> y = norm.rvs(size=120, loc=0.2, random_state=rng)\n    >>> res = permutation_test((x, y), statistic, n_resamples=9999,\n    ...                        vectorized=True, alternative='less',\n    ...                        random_state=rng)\n    >>> print(res.statistic)\n    -0.4230459671240913\n    >>> print(res.pvalue)\n    0.0015\n\n    The approximate probability of obtaining a test statistic less than or\n    equal to the observed value under the null hypothesis is 0.0225%. This is\n    again less than our chosen threshold of 5%, so again we have significant\n    evidence to reject the null hypothesis in favor of the alternative.\n\n    For large samples and number of permutations, the result is comparable to\n    that of the corresponding asymptotic test, the independent sample t-test.\n\n    >>> from scipy.stats import ttest_ind\n    >>> res_asymptotic = ttest_ind(x, y, alternative='less')\n    >>> print(res_asymptotic.pvalue)\n    0.0014669545224902675\n\n    The permutation distribution of the test statistic is provided for\n    further investigation.\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.hist(res.null_distribution, bins=50)\n    >>> plt.title(\"Permutation distribution of test statistic\")\n    >>> plt.xlabel(\"Value of Statistic\")\n    >>> plt.ylabel(\"Frequency\")\n    >>> plt.show()\n\n    Inspection of the null distribution is essential if the statistic suffers\n    from inaccuracy due to limited machine precision. Consider the following\n    case:\n\n    >>> from scipy.stats import pearsonr\n    >>> x = [1, 2, 4, 3]\n    >>> y = [2, 4, 6, 8]\n    >>> def statistic(x, y, axis=-1):\n    ...     return pearsonr(x, y, axis=axis).statistic\n    >>> res = permutation_test((x, y), statistic, vectorized=True,\n    ...                        permutation_type='pairings',\n    ...                        alternative='greater')\n    >>> r, pvalue, null = res.statistic, res.pvalue, res.null_distribution\n\n    In this case, some elements of the null distribution differ from the\n    observed value of the correlation coefficient ``r`` due to numerical noise.\n    We manually inspect the elements of the null distribution that are nearly\n    the same as the observed value of the test statistic.\n\n    >>> r\n    0.7999999999999999\n    >>> unique = np.unique(null)\n    >>> unique\n    array([-1. , -1. , -0.8, -0.8, -0.8, -0.6, -0.4, -0.4, -0.2, -0.2, -0.2,\n        0. ,  0.2,  0.2,  0.2,  0.4,  0.4,  0.6,  0.8,  0.8,  0.8,  1. ,\n        1. ])  # may vary\n    >>> unique[np.isclose(r, unique)].tolist()\n    [0.7999999999999998, 0.7999999999999999, 0.8]  # may vary\n\n    If `permutation_test` were to perform the comparison naively, the\n    elements of the null distribution with value ``0.7999999999999998`` would\n    not be considered as extreme or more extreme as the observed value of the\n    statistic, so the calculated p-value would be too small.\n\n    >>> incorrect_pvalue = np.count_nonzero(null >= r) / len(null)\n    >>> incorrect_pvalue\n    0.14583333333333334  # may vary\n\n    Instead, `permutation_test` treats elements of the null distribution that\n    are within ``max(1e-14, abs(r)*1e-14)`` of the observed value of the\n    statistic ``r`` to be equal to ``r``.\n\n    >>> correct_pvalue = np.count_nonzero(null >= r - 1e-14) / len(null)\n    >>> correct_pvalue\n    0.16666666666666666\n    >>> res.pvalue == correct_pvalue\n    True\n\n    This method of comparison is expected to be accurate in most practical\n    situations, but the user is advised to assess this by inspecting the\n    elements of the null distribution that are close to the observed value\n    of the statistic. Also, consider the use of statistics that can be\n    calculated using exact arithmetic (e.g. integer statistics).\n\n    ",
    "scipy.stats.planck": "A Planck discrete exponential random variable.\n\n    As an instance of the `rv_discrete` class, `planck` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(lambda_, loc=0, size=1, random_state=None)\n        Random variates.\n    pmf(k, lambda_, loc=0)\n        Probability mass function.\n    logpmf(k, lambda_, loc=0)\n        Log of the probability mass function.\n    cdf(k, lambda_, loc=0)\n        Cumulative distribution function.\n    logcdf(k, lambda_, loc=0)\n        Log of the cumulative distribution function.\n    sf(k, lambda_, loc=0)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(k, lambda_, loc=0)\n        Log of the survival function.\n    ppf(q, lambda_, loc=0)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, lambda_, loc=0)\n        Inverse survival function (inverse of ``sf``).\n    stats(lambda_, loc=0, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(lambda_, loc=0)\n        (Differential) entropy of the RV.\n    expect(func, args=(lambda_,), loc=0, lb=None, ub=None, conditional=False)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(lambda_, loc=0)\n        Median of the distribution.\n    mean(lambda_, loc=0)\n        Mean of the distribution.\n    var(lambda_, loc=0)\n        Variance of the distribution.\n    std(lambda_, loc=0)\n        Standard deviation of the distribution.\n    interval(confidence, lambda_, loc=0)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability mass function for `planck` is:\n\n    .. math::\n\n        f(k) = (1-\\exp(-\\lambda)) \\exp(-\\lambda k)\n\n    for :math:`k \\ge 0` and :math:`\\lambda > 0`.\n\n    `planck` takes :math:`\\lambda` as shape parameter. The Planck distribution\n    can be written as a geometric distribution (`geom`) with\n    :math:`p = 1 - \\exp(-\\lambda)` shifted by ``loc = -1``.\n\n    The probability mass function above is defined in the \"standardized\" form.\n    To shift distribution use the ``loc`` parameter.\n    Specifically, ``planck.pmf(k, lambda_, loc)`` is identically\n    equivalent to ``planck.pmf(k - loc, lambda_)``.\n\n    See Also\n    --------\n    geom\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import planck\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> lambda_ = 0.51\n    >>> mean, var, skew, kurt = planck.stats(lambda_, moments='mvsk')\n    \n    Display the probability mass function (``pmf``):\n    \n    >>> x = np.arange(planck.ppf(0.01, lambda_),\n    ...               planck.ppf(0.99, lambda_))\n    >>> ax.plot(x, planck.pmf(x, lambda_), 'bo', ms=8, label='planck pmf')\n    >>> ax.vlines(x, 0, planck.pmf(x, lambda_), colors='b', lw=5, alpha=0.5)\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape and location. This returns a \"frozen\" RV object holding\n    the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pmf``:\n    \n    >>> rv = planck(lambda_)\n    >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n    ...         label='frozen pmf')\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> prob = planck.cdf(x, lambda_)\n    >>> np.allclose(x, planck.ppf(prob, lambda_))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = planck.rvs(lambda_, size=1000)\n\n    ",
    "scipy.stats.pmean": "    \n\n\nCalculate the weighted power mean along the specified axis.\n\nThe weighted power mean of the array :math:`a_i` associated to weights\n:math:`w_i` is:\n\n.. math::\n\n    \\left( \\frac{ \\sum_{i=1}^n w_i a_i^p }{ \\sum_{i=1}^n w_i }\n          \\right)^{ 1 / p } \\, ,\n\nand, with equal weights, it gives:\n\n.. math::\n\n    \\left( \\frac{ 1 }{ n } \\sum_{i=1}^n a_i^p \\right)^{ 1 / p }  \\, .\n\nWhen ``p=0``, it returns the geometric mean.\n\nThis mean is also called generalized mean or H\u00f6lder mean, and must not be\nconfused with the Kolmogorov generalized mean, also called\nquasi-arithmetic mean or generalized f-mean [3]_.\n\nParameters\n----------\na : array_like\n    Input array, masked array or object that can be converted to an array.\np : int or float\n    Exponent.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\ndtype : dtype, optional\n    Type of the returned array and of the accumulator in which the\n    elements are summed. If `dtype` is not specified, it defaults to the\n    dtype of `a`, unless `a` has an integer `dtype` with a precision less\n    than that of the default platform integer. In that case, the default\n    platform integer is used.\nweights : array_like, optional\n    The weights array can either be 1-D (in which case its length must be\n    the size of `a` along the given `axis`) or of the same shape as `a`.\n    Default is None, which gives each value a weight of 1.0.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\npmean : ndarray, see `dtype` parameter above.\n    Output array containing the power mean values.\n\nSee Also\n--------\n\n:func:`numpy.average`\n    Weighted average\n:func:`gmean`\n    Geometric mean\n:func:`hmean`\n    Harmonic mean\n\n\nNotes\n-----\nThe power mean is computed over a single dimension of the input\narray, ``axis=0`` by default, or all values in the array if ``axis=None``.\nfloat64 intermediate and return values are used for integer inputs.\n\n.. versionadded:: 1.9\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] \"Generalized Mean\", *Wikipedia*,\n       https://en.wikipedia.org/wiki/Generalized_mean\n.. [2] Norris, N., \"Convexity properties of generalized mean value\n       functions\", The Annals of Mathematical Statistics, vol. 8,\n       pp. 118-120, 1937\n.. [3] Bullen, P.S., Handbook of Means and Their Inequalities, 2003\n\nExamples\n--------\n>>> from scipy.stats import pmean, hmean, gmean\n>>> pmean([1, 4], 1.3)\n2.639372938300652\n>>> pmean([1, 2, 3, 4, 5, 6, 7], 1.3)\n4.157111214492084\n>>> pmean([1, 4, 7], -2, weights=[3, 1, 3])\n1.4969684896631954\n\nFor p=-1, power mean is equal to harmonic mean:\n\n>>> pmean([1, 4, 7], -1, weights=[3, 1, 3])\n1.9029126213592233\n>>> hmean([1, 4, 7], weights=[3, 1, 3])\n1.9029126213592233\n\nFor p=0, power mean is defined as the geometric mean:\n\n>>> pmean([1, 4, 7], 0, weights=[3, 1, 3])\n2.80668351922014\n>>> gmean([1, 4, 7], weights=[3, 1, 3])\n2.80668351922014\n",
    "scipy.stats.pointbiserialr": "Calculate a point biserial correlation coefficient and its p-value.\n\n    The point biserial correlation is used to measure the relationship\n    between a binary variable, x, and a continuous variable, y. Like other\n    correlation coefficients, this one varies between -1 and +1 with 0\n    implying no correlation. Correlations of -1 or +1 imply a determinative\n    relationship.\n\n    This function may be computed using a shortcut formula but produces the\n    same result as `pearsonr`.\n\n    Parameters\n    ----------\n    x : array_like of bools\n        Input array.\n    y : array_like\n        Input array.\n\n    Returns\n    -------\n    res: SignificanceResult\n        An object containing attributes:\n\n        statistic : float\n            The R value.\n        pvalue : float\n            The two-sided p-value.\n\n    Notes\n    -----\n    `pointbiserialr` uses a t-test with ``n-1`` degrees of freedom.\n    It is equivalent to `pearsonr`.\n\n    The value of the point-biserial correlation can be calculated from:\n\n    .. math::\n\n        r_{pb} = \\frac{\\overline{Y_1} - \\overline{Y_0}}\n                      {s_y}\n                 \\sqrt{\\frac{N_0 N_1}\n                            {N (N - 1)}}\n\n    Where :math:`\\overline{Y_{0}}` and :math:`\\overline{Y_{1}}` are means\n    of the metric observations coded 0 and 1 respectively; :math:`N_{0}` and\n    :math:`N_{1}` are number of observations coded 0 and 1 respectively;\n    :math:`N` is the total number of observations and :math:`s_{y}` is the\n    standard deviation of all the metric observations.\n\n    A value of :math:`r_{pb}` that is significantly different from zero is\n    completely equivalent to a significant difference in means between the two\n    groups. Thus, an independent groups t Test with :math:`N-2` degrees of\n    freedom may be used to test whether :math:`r_{pb}` is nonzero. The\n    relation between the t-statistic for comparing two independent groups and\n    :math:`r_{pb}` is given by:\n\n    .. math::\n\n        t = \\sqrt{N - 2}\\frac{r_{pb}}{\\sqrt{1 - r^{2}_{pb}}}\n\n    References\n    ----------\n    .. [1] J. Lev, \"The Point Biserial Coefficient of Correlation\", Ann. Math.\n           Statist., Vol. 20, no.1, pp. 125-126, 1949.\n\n    .. [2] R.F. Tate, \"Correlation Between a Discrete and a Continuous\n           Variable. Point-Biserial Correlation.\", Ann. Math. Statist., Vol. 25,\n           np. 3, pp. 603-607, 1954.\n\n    .. [3] D. Kornbrot \"Point Biserial Correlation\", In Wiley StatsRef:\n           Statistics Reference Online (eds N. Balakrishnan, et al.), 2014.\n           :doi:`10.1002/9781118445112.stat06227`\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> a = np.array([0, 0, 0, 1, 1, 1, 1])\n    >>> b = np.arange(7)\n    >>> stats.pointbiserialr(a, b)\n    (0.8660254037844386, 0.011724811003954652)\n    >>> stats.pearsonr(a, b)\n    (0.86602540378443871, 0.011724811003954626)\n    >>> np.corrcoef(a, b)\n    array([[ 1.       ,  0.8660254],\n           [ 0.8660254,  1.       ]])\n\n    ",
    "scipy.stats.poisson": "A Poisson discrete random variable.\n\n    As an instance of the `rv_discrete` class, `poisson` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(mu, loc=0, size=1, random_state=None)\n        Random variates.\n    pmf(k, mu, loc=0)\n        Probability mass function.\n    logpmf(k, mu, loc=0)\n        Log of the probability mass function.\n    cdf(k, mu, loc=0)\n        Cumulative distribution function.\n    logcdf(k, mu, loc=0)\n        Log of the cumulative distribution function.\n    sf(k, mu, loc=0)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(k, mu, loc=0)\n        Log of the survival function.\n    ppf(q, mu, loc=0)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, mu, loc=0)\n        Inverse survival function (inverse of ``sf``).\n    stats(mu, loc=0, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(mu, loc=0)\n        (Differential) entropy of the RV.\n    expect(func, args=(mu,), loc=0, lb=None, ub=None, conditional=False)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(mu, loc=0)\n        Median of the distribution.\n    mean(mu, loc=0)\n        Mean of the distribution.\n    var(mu, loc=0)\n        Variance of the distribution.\n    std(mu, loc=0)\n        Standard deviation of the distribution.\n    interval(confidence, mu, loc=0)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability mass function for `poisson` is:\n\n    .. math::\n\n        f(k) = \\exp(-\\mu) \\frac{\\mu^k}{k!}\n\n    for :math:`k \\ge 0`.\n\n    `poisson` takes :math:`\\mu \\geq 0` as shape parameter.\n    When :math:`\\mu = 0`, the ``pmf`` method\n    returns ``1.0`` at quantile :math:`k = 0`.\n\n    The probability mass function above is defined in the \"standardized\" form.\n    To shift distribution use the ``loc`` parameter.\n    Specifically, ``poisson.pmf(k, mu, loc)`` is identically\n    equivalent to ``poisson.pmf(k - loc, mu)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import poisson\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> mu = 0.6\n    >>> mean, var, skew, kurt = poisson.stats(mu, moments='mvsk')\n    \n    Display the probability mass function (``pmf``):\n    \n    >>> x = np.arange(poisson.ppf(0.01, mu),\n    ...               poisson.ppf(0.99, mu))\n    >>> ax.plot(x, poisson.pmf(x, mu), 'bo', ms=8, label='poisson pmf')\n    >>> ax.vlines(x, 0, poisson.pmf(x, mu), colors='b', lw=5, alpha=0.5)\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape and location. This returns a \"frozen\" RV object holding\n    the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pmf``:\n    \n    >>> rv = poisson(mu)\n    >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n    ...         label='frozen pmf')\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> prob = poisson.cdf(x, mu)\n    >>> np.allclose(x, poisson.ppf(prob, mu))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = poisson.rvs(mu, size=1000)\n\n    ",
    "scipy.stats.poisson_means_test": "\n    Performs the Poisson means test, AKA the \"E-test\".\n\n    This is a test of the null hypothesis that the difference between means of\n    two Poisson distributions is `diff`. The samples are provided as the\n    number of events `k1` and `k2` observed within measurement intervals\n    (e.g. of time, space, number of observations) of sizes `n1` and `n2`.\n\n    Parameters\n    ----------\n    k1 : int\n        Number of events observed from distribution 1.\n    n1: float\n        Size of sample from distribution 1.\n    k2 : int\n        Number of events observed from distribution 2.\n    n2 : float\n        Size of sample from distribution 2.\n    diff : float, default=0\n        The hypothesized difference in means between the distributions\n        underlying the samples.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n        The following options are available (default is 'two-sided'):\n\n          * 'two-sided': the difference between distribution means is not\n            equal to `diff`\n          * 'less': the difference between distribution means is less than\n            `diff`\n          * 'greater': the difference between distribution means is greater\n            than `diff`\n\n    Returns\n    -------\n    statistic : float\n        The test statistic (see [1]_ equation 3.3).\n    pvalue : float\n        The probability of achieving such an extreme value of the test\n        statistic under the null hypothesis.\n\n    Notes\n    -----\n\n    Let:\n\n    .. math:: X_1 \\sim \\mbox{Poisson}(\\mathtt{n1}\\lambda_1)\n\n    be a random variable independent of\n\n    .. math:: X_2  \\sim \\mbox{Poisson}(\\mathtt{n2}\\lambda_2)\n\n    and let ``k1`` and ``k2`` be the observed values of :math:`X_1`\n    and :math:`X_2`, respectively. Then `poisson_means_test` uses the number\n    of observed events ``k1`` and ``k2`` from samples of size ``n1`` and\n    ``n2``, respectively, to test the null hypothesis that\n\n    .. math::\n       H_0: \\lambda_1 - \\lambda_2 = \\mathtt{diff}\n\n    A benefit of the E-test is that it has good power for small sample sizes,\n    which can reduce sampling costs [1]_. It has been evaluated and determined\n    to be more powerful than the comparable C-test, sometimes referred to as\n    the Poisson exact test.\n\n    References\n    ----------\n    .. [1]  Krishnamoorthy, K., & Thomson, J. (2004). A more powerful test for\n       comparing two Poisson means. Journal of Statistical Planning and\n       Inference, 119(1), 23-35.\n\n    .. [2]  Przyborowski, J., & Wilenski, H. (1940). Homogeneity of results in\n       testing samples from Poisson series: With an application to testing\n       clover seed for dodder. Biometrika, 31(3/4), 313-323.\n\n    Examples\n    --------\n\n    Suppose that a gardener wishes to test the number of dodder (weed) seeds\n    in a sack of clover seeds that they buy from a seed company. It has\n    previously been established that the number of dodder seeds in clover\n    follows the Poisson distribution.\n\n    A 100 gram sample is drawn from the sack before being shipped to the\n    gardener. The sample is analyzed, and it is found to contain no dodder\n    seeds; that is, `k1` is 0. However, upon arrival, the gardener draws\n    another 100 gram sample from the sack. This time, three dodder seeds are\n    found in the sample; that is, `k2` is 3. The gardener would like to\n    know if the difference is significant and not due to chance. The\n    null hypothesis is that the difference between the two samples is merely\n    due to chance, or that :math:`\\lambda_1 - \\lambda_2 = \\mathtt{diff}`\n    where :math:`\\mathtt{diff} = 0`. The alternative hypothesis is that the\n    difference is not due to chance, or :math:`\\lambda_1 - \\lambda_2 \\ne 0`.\n    The gardener selects a significance level of 5% to reject the null\n    hypothesis in favor of the alternative [2]_.\n\n    >>> import scipy.stats as stats\n    >>> res = stats.poisson_means_test(0, 100, 3, 100)\n    >>> res.statistic, res.pvalue\n    (-1.7320508075688772, 0.08837900929018157)\n\n    The p-value is .088, indicating a near 9% chance of observing a value of\n    the test statistic under the null hypothesis. This exceeds 5%, so the\n    gardener does not reject the null hypothesis as the difference cannot be\n    regarded as significant at this level.\n    ",
    "scipy.stats.power": "Simulate the power of a hypothesis test under an alternative hypothesis.\n\n    Parameters\n    ----------\n    test : callable\n        Hypothesis test for which the power is to be simulated.\n        `test` must be a callable that accepts a sample (e.g. ``test(sample)``)\n        or ``len(rvs)`` separate samples (e.g. ``test(samples1, sample2)`` if\n        `rvs` contains two callables and `n_observations` contains two values)\n        and returns the p-value of the test.\n        If `vectorized` is set to ``True``, `test` must also accept a keyword\n        argument `axis` and be vectorized to perform the test along the\n        provided `axis` of the samples.\n        Any callable from `scipy.stats` with an `axis` argument that returns an\n        object with a `pvalue` attribute is also acceptable.\n    rvs : callable or tuple of callables\n        A callable or sequence of callables that generate(s) random variates\n        under the alternative hypothesis. Each element of `rvs` must accept\n        keyword argument ``size`` (e.g. ``rvs(size=(m, n))``) and return an\n        N-d array of that shape. If `rvs` is a sequence, the number of callables\n        in `rvs` must match the number of elements of `n_observations`, i.e.\n        ``len(rvs) == len(n_observations)``. If `rvs` is a single callable,\n        `n_observations` is treated as a single element.\n    n_observations : tuple of ints or tuple of integer arrays\n        If a sequence of ints, each is the sizes of a sample to be passed to `test`.\n        If a sequence of integer arrays, the power is simulated for each\n        set of corresponding sample sizes. See Examples.\n    significance : float or array_like of floats, default: 0.01\n        The threshold for significance; i.e., the p-value below which the\n        hypothesis test results will be considered as evidence against the null\n        hypothesis. Equivalently, the acceptable rate of Type I error under\n        the null hypothesis. If an array, the power is simulated for each\n        significance threshold.\n    kwargs : dict, optional\n        Keyword arguments to be passed to `rvs` and/or `test` callables.\n        Introspection is used to determine which keyword arguments may be\n        passed to each callable.\n        The value corresponding with each keyword must be an array.\n        Arrays must be broadcastable with one another and with each array in\n        `n_observations`. The power is simulated for each set of corresponding\n        sample sizes and arguments. See Examples.\n    vectorized : bool, optional\n        If `vectorized` is set to ``False``, `test` will not be passed keyword\n        argument `axis` and is expected to perform the test only for 1D samples.\n        If ``True``, `test` will be passed keyword argument `axis` and is\n        expected to perform the test along `axis` when passed N-D sample arrays.\n        If ``None`` (default), `vectorized` will be set ``True`` if ``axis`` is\n        a parameter of `test`. Use of a vectorized test typically reduces\n        computation time.\n    n_resamples : int, default: 10000\n        Number of samples drawn from each of the callables of `rvs`.\n        Equivalently, the number tests performed under the alternative\n        hypothesis to approximate the power.\n    batch : int, optional\n        The number of samples to process in each call to `test`. Memory usage is\n        proportional to the product of `batch` and the largest sample size. Default\n        is ``None``, in which case `batch` equals `n_resamples`.\n\n    Returns\n    -------\n    res : PowerResult\n        An object with attributes:\n\n        power : float or ndarray\n            The estimated power against the alternative.\n        pvalues : ndarray\n            The p-values observed under the alternative hypothesis.\n\n    Notes\n    -----\n    The power is simulated as follows:\n\n    - Draw many random samples (or sets of samples), each of the size(s)\n      specified by `n_observations`, under the alternative specified by\n      `rvs`.\n    - For each sample (or set of samples), compute the p-value according to\n      `test`. These p-values are recorded in the ``pvalues`` attribute of\n      the result object.\n    - Compute the proportion of p-values that are less than the `significance`\n      level. This is the power recorded in the ``power`` attribute of the\n      result object.\n\n    Suppose that `significance` is an array with shape ``shape1``, the elements\n    of `kwargs` and `n_observations` are mutually broadcastable to shape ``shape2``,\n    and `test` returns an array of p-values of shape ``shape3``. Then the result\n    object ``power`` attribute will be of shape ``shape1 + shape2 + shape3``, and\n    the ``pvalues`` attribute will be of shape ``shape2 + shape3 + (n_resamples,)``.\n\n    Examples\n    --------\n    Suppose we wish to simulate the power of the independent sample t-test\n    under the following conditions:\n\n    - The first sample has 10 observations drawn from a normal distribution\n      with mean 0.\n    - The second sample has 12 observations drawn from a normal distribution\n      with mean 1.0.\n    - The threshold on p-values for significance is 0.05.\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng(2549598345528)\n    >>>\n    >>> test = stats.ttest_ind\n    >>> n_observations = (10, 12)\n    >>> rvs1 = rng.normal\n    >>> rvs2 = lambda size: rng.normal(loc=1, size=size)\n    >>> rvs = (rvs1, rvs2)\n    >>> res = stats.power(test, rvs, n_observations, significance=0.05)\n    >>> res.power\n    0.6116\n\n    With samples of size 10 and 12, respectively, the power of the t-test\n    with a significance threshold of 0.05 is approximately 60% under the chosen\n    alternative. We can investigate the effect of sample size on the power\n    by passing sample size arrays.\n\n    >>> import matplotlib.pyplot as plt\n    >>> nobs_x = np.arange(5, 21)\n    >>> nobs_y = nobs_x\n    >>> n_observations = (nobs_x, nobs_y)\n    >>> res = stats.power(test, rvs, n_observations, significance=0.05)\n    >>> ax = plt.subplot()\n    >>> ax.plot(nobs_x, res.power)\n    >>> ax.set_xlabel('Sample Size')\n    >>> ax.set_ylabel('Simulated Power')\n    >>> ax.set_title('Simulated Power of `ttest_ind` with Equal Sample Sizes')\n    >>> plt.show()\n\n    Alternatively, we can investigate the impact that effect size has on the power.\n    In this case, the effect size is the location of the distribution underlying\n    the second sample.\n\n    >>> n_observations = (10, 12)\n    >>> loc = np.linspace(0, 1, 20)\n    >>> rvs2 = lambda size, loc: rng.normal(loc=loc, size=size)\n    >>> rvs = (rvs1, rvs2)\n    >>> res = stats.power(test, rvs, n_observations, significance=0.05,\n    ...                   kwargs={'loc': loc})\n    >>> ax = plt.subplot()\n    >>> ax.plot(loc, res.power)\n    >>> ax.set_xlabel('Effect Size')\n    >>> ax.set_ylabel('Simulated Power')\n    >>> ax.set_title('Simulated Power of `ttest_ind`, Varying Effect Size')\n    >>> plt.show()\n\n    We can also use `power` to estimate the Type I error rate (also referred to by the\n    ambiguous term \"size\") of a test and assess whether it matches the nominal level.\n    For example, the null hypothesis of `jarque_bera` is that the sample was drawn from\n    a distribution with the same skewness and kurtosis as the normal distribution. To\n    estimate the Type I error rate, we can consider the null hypothesis to be a true\n    *alternative* hypothesis and calculate the power.\n\n    >>> test = stats.jarque_bera\n    >>> n_observations = 10\n    >>> rvs = rng.normal\n    >>> significance = np.linspace(0.0001, 0.1, 1000)\n    >>> res = stats.power(test, rvs, n_observations, significance=significance)\n    >>> size = res.power\n\n    As shown below, the Type I error rate of the test is far below the nominal level\n    for such a small sample, as mentioned in its documentation.\n\n    >>> ax = plt.subplot()\n    >>> ax.plot(significance, size)\n    >>> ax.plot([0, 0.1], [0, 0.1], '--')\n    >>> ax.set_xlabel('nominal significance level')\n    >>> ax.set_ylabel('estimated test size (Type I error rate)')\n    >>> ax.set_title('Estimated test size vs nominal significance level')\n    >>> ax.set_aspect('equal', 'box')\n    >>> ax.legend(('`ttest_1samp`', 'ideal test'))\n    >>> plt.show()\n\n    As one might expect from such a conservative test, the power is quite low with\n    respect to some alternatives. For example, the power of the test under the\n    alternative that the sample was drawn from the Laplace distribution may not\n    be much greater than the Type I error rate.\n\n    >>> rvs = rng.laplace\n    >>> significance = np.linspace(0.0001, 0.1, 1000)\n    >>> res = stats.power(test, rvs, n_observations, significance=0.05)\n    >>> print(res.power)\n    0.0587\n\n    This is not a mistake in SciPy's implementation; it is simply due to the fact\n    that the null distribution of the test statistic is derived under the assumption\n    that the sample size is large (i.e. approaches infinity), and this asymptotic\n    approximation is not accurate for small samples. In such cases, resampling\n    and Monte Carlo methods (e.g. `permutation_test`, `goodness_of_fit`,\n    `monte_carlo_test`) may be more appropriate.\n\n    ",
    "scipy.stats.power_divergence": "Cressie-Read power divergence statistic and goodness of fit test.\n\n    This function tests the null hypothesis that the categorical data\n    has the given frequencies, using the Cressie-Read power divergence\n    statistic.\n\n    Parameters\n    ----------\n    f_obs : array_like\n        Observed frequencies in each category.\n\n        .. deprecated:: 1.14.0\n            Support for masked array input was deprecated in\n            SciPy 1.14.0 and will be removed in version 1.16.0.\n\n    f_exp : array_like, optional\n        Expected frequencies in each category.  By default the categories are\n        assumed to be equally likely.\n\n        .. deprecated:: 1.14.0\n            Support for masked array input was deprecated in\n            SciPy 1.14.0 and will be removed in version 1.16.0.\n\n    ddof : int, optional\n        \"Delta degrees of freedom\": adjustment to the degrees of freedom\n        for the p-value.  The p-value is computed using a chi-squared\n        distribution with ``k - 1 - ddof`` degrees of freedom, where `k`\n        is the number of observed frequencies.  The default value of `ddof`\n        is 0.\n    axis : int or None, optional\n        The axis of the broadcast result of `f_obs` and `f_exp` along which to\n        apply the test.  If axis is None, all values in `f_obs` are treated\n        as a single data set.  Default is 0.\n    lambda_ : float or str, optional\n        The power in the Cressie-Read power divergence statistic.  The default\n        is 1.  For convenience, `lambda_` may be assigned one of the following\n        strings, in which case the corresponding numerical value is used:\n\n        * ``\"pearson\"`` (value 1)\n            Pearson's chi-squared statistic. In this case, the function is\n            equivalent to `chisquare`.\n        * ``\"log-likelihood\"`` (value 0)\n            Log-likelihood ratio. Also known as the G-test [3]_.\n        * ``\"freeman-tukey\"`` (value -1/2)\n            Freeman-Tukey statistic.\n        * ``\"mod-log-likelihood\"`` (value -1)\n            Modified log-likelihood ratio.\n        * ``\"neyman\"`` (value -2)\n            Neyman's statistic.\n        * ``\"cressie-read\"`` (value 2/3)\n            The power recommended in [5]_.\n\n    Returns\n    -------\n    res: Power_divergenceResult\n        An object containing attributes:\n\n        statistic : float or ndarray\n            The Cressie-Read power divergence test statistic.  The value is\n            a float if `axis` is None or if` `f_obs` and `f_exp` are 1-D.\n        pvalue : float or ndarray\n            The p-value of the test.  The value is a float if `ddof` and the\n            return value `stat` are scalars.\n\n    See Also\n    --------\n    chisquare\n\n    Notes\n    -----\n    This test is invalid when the observed or expected frequencies in each\n    category are too small.  A typical rule is that all of the observed\n    and expected frequencies should be at least 5.\n\n    Also, the sum of the observed and expected frequencies must be the same\n    for the test to be valid; `power_divergence` raises an error if the sums\n    do not agree within a relative tolerance of ``eps**0.5``, where ``eps``\n    is the precision of the input dtype.\n\n    When `lambda_` is less than zero, the formula for the statistic involves\n    dividing by `f_obs`, so a warning or error may be generated if any value\n    in `f_obs` is 0.\n\n    Similarly, a warning or error may be generated if any value in `f_exp` is\n    zero when `lambda_` >= 0.\n\n    The default degrees of freedom, k-1, are for the case when no parameters\n    of the distribution are estimated. If p parameters are estimated by\n    efficient maximum likelihood then the correct degrees of freedom are\n    k-1-p. If the parameters are estimated in a different way, then the\n    dof can be between k-1-p and k-1. However, it is also possible that\n    the asymptotic distribution is not a chisquare, in which case this\n    test is not appropriate.\n\n    References\n    ----------\n    .. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n           Statistics\". Chapter 8.\n           https://web.archive.org/web/20171015035606/http://faculty.vassar.edu/lowry/ch8pt1.html\n    .. [2] \"Chi-squared test\", https://en.wikipedia.org/wiki/Chi-squared_test\n    .. [3] \"G-test\", https://en.wikipedia.org/wiki/G-test\n    .. [4] Sokal, R. R. and Rohlf, F. J. \"Biometry: the principles and\n           practice of statistics in biological research\", New York: Freeman\n           (1981)\n    .. [5] Cressie, N. and Read, T. R. C., \"Multinomial Goodness-of-Fit\n           Tests\", J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),\n           pp. 440-464.\n\n    Examples\n    --------\n    (See `chisquare` for more examples.)\n\n    When just `f_obs` is given, it is assumed that the expected frequencies\n    are uniform and given by the mean of the observed frequencies.  Here we\n    perform a G-test (i.e. use the log-likelihood ratio statistic):\n\n    >>> import numpy as np\n    >>> from scipy.stats import power_divergence\n    >>> power_divergence([16, 18, 16, 14, 12, 12], lambda_='log-likelihood')\n    (2.006573162632538, 0.84823476779463769)\n\n    The expected frequencies can be given with the `f_exp` argument:\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12],\n    ...                  f_exp=[16, 16, 16, 16, 16, 8],\n    ...                  lambda_='log-likelihood')\n    (3.3281031458963746, 0.6495419288047497)\n\n    When `f_obs` is 2-D, by default the test is applied to each column.\n\n    >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n    >>> obs.shape\n    (6, 2)\n    >>> power_divergence(obs, lambda_=\"log-likelihood\")\n    (array([ 2.00657316,  6.77634498]), array([ 0.84823477,  0.23781225]))\n\n    By setting ``axis=None``, the test is applied to all data in the array,\n    which is equivalent to applying the test to the flattened array.\n\n    >>> power_divergence(obs, axis=None)\n    (23.31034482758621, 0.015975692534127565)\n    >>> power_divergence(obs.ravel())\n    (23.31034482758621, 0.015975692534127565)\n\n    `ddof` is the change to make to the default degrees of freedom.\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12], ddof=1)\n    (2.0, 0.73575888234288467)\n\n    The calculation of the p-values is done by broadcasting the\n    test statistic with `ddof`.\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12], ddof=[0,1,2])\n    (2.0, array([ 0.84914504,  0.73575888,  0.5724067 ]))\n\n    `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n    shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n    `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n    statistics, we must use ``axis=1``:\n\n    >>> power_divergence([16, 18, 16, 14, 12, 12],\n    ...                  f_exp=[[16, 16, 16, 16, 16, 8],\n    ...                         [8, 20, 20, 16, 12, 12]],\n    ...                  axis=1)\n    (array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846]))\n\n    ",
    "scipy.stats.powerlaw": "A power-function continuous random variable.\n\n    As an instance of the `rv_continuous` class, `powerlaw` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, a, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, a, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, a, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, a, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, a, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, a, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, a, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, a, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(a, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, loc=0, scale=1)\n        Median of the distribution.\n    mean(a, loc=0, scale=1)\n        Mean of the distribution.\n    var(a, loc=0, scale=1)\n        Variance of the distribution.\n    std(a, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, a, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    pareto\n\n    Notes\n    -----\n    The probability density function for `powerlaw` is:\n\n    .. math::\n\n        f(x, a) = a x^{a-1}\n\n    for :math:`0 \\le x \\le 1`, :math:`a > 0`.\n\n    `powerlaw` takes ``a`` as a shape parameter for :math:`a`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``powerlaw.pdf(x, a, loc, scale)`` is identically\n    equivalent to ``powerlaw.pdf(y, a) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    For example, the support of `powerlaw` can be adjusted from the default\n    interval ``[0, 1]`` to the interval ``[c, c+d]`` by setting ``loc=c`` and\n    ``scale=d``. For a power-law distribution with infinite support, see\n    `pareto`.\n\n    `powerlaw` is a special case of `beta` with ``b=1``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import powerlaw\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a = 0.659\n    >>> mean, var, skew, kurt = powerlaw.stats(a, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(powerlaw.ppf(0.01, a),\n    ...                 powerlaw.ppf(0.99, a), 100)\n    >>> ax.plot(x, powerlaw.pdf(x, a),\n    ...        'r-', lw=5, alpha=0.6, label='powerlaw pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = powerlaw(a)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = powerlaw.ppf([0.001, 0.5, 0.999], a)\n    >>> np.allclose([0.001, 0.5, 0.999], powerlaw.cdf(vals, a))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = powerlaw.rvs(a, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.powerlognorm": "A power log-normal continuous random variable.\n\n    As an instance of the `rv_continuous` class, `powerlognorm` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, s, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, s, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, s, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, s, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, s, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, s, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, s, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, s, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, s, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, s, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, s, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, s, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c, s), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, s, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, s, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, s, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, s, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, s, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `powerlognorm` is:\n\n    .. math::\n\n        f(x, c, s) = \\frac{c}{x s} \\phi(\\log(x)/s)\n                     (\\Phi(-\\log(x)/s))^{c-1}\n\n    where :math:`\\phi` is the normal pdf, and :math:`\\Phi` is the normal cdf,\n    and :math:`x > 0`, :math:`s, c > 0`.\n\n    `powerlognorm` takes :math:`c` and :math:`s` as shape parameters.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``powerlognorm.pdf(x, c, s, loc, scale)`` is identically\n    equivalent to ``powerlognorm.pdf(y, c, s) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import powerlognorm\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c, s = 2.14, 0.446\n    >>> mean, var, skew, kurt = powerlognorm.stats(c, s, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(powerlognorm.ppf(0.01, c, s),\n    ...                 powerlognorm.ppf(0.99, c, s), 100)\n    >>> ax.plot(x, powerlognorm.pdf(x, c, s),\n    ...        'r-', lw=5, alpha=0.6, label='powerlognorm pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = powerlognorm(c, s)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = powerlognorm.ppf([0.001, 0.5, 0.999], c, s)\n    >>> np.allclose([0.001, 0.5, 0.999], powerlognorm.cdf(vals, c, s))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = powerlognorm.rvs(c, s, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.powernorm": "A power normal continuous random variable.\n\n    As an instance of the `rv_continuous` class, `powernorm` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `powernorm` is:\n\n    .. math::\n\n        f(x, c) = c \\phi(x) (\\Phi(-x))^{c-1}\n\n    where :math:`\\phi` is the normal pdf, :math:`\\Phi` is the normal cdf,\n    :math:`x` is any real, and :math:`c > 0` [1]_.\n\n    `powernorm` takes ``c`` as a shape parameter for :math:`c`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``powernorm.pdf(x, c, loc, scale)`` is identically\n    equivalent to ``powernorm.pdf(y, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] NIST Engineering Statistics Handbook, Section 1.3.6.6.13,\n           https://www.itl.nist.gov/div898/handbook//eda/section3/eda366d.htm\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import powernorm\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c = 4.45\n    >>> mean, var, skew, kurt = powernorm.stats(c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(powernorm.ppf(0.01, c),\n    ...                 powernorm.ppf(0.99, c), 100)\n    >>> ax.plot(x, powernorm.pdf(x, c),\n    ...        'r-', lw=5, alpha=0.6, label='powernorm pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = powernorm(c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = powernorm.ppf([0.001, 0.5, 0.999], c)\n    >>> np.allclose([0.001, 0.5, 0.999], powernorm.cdf(vals, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = powernorm.rvs(c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.ppcc_max": "Calculate the shape parameter that maximizes the PPCC.\n\n    The probability plot correlation coefficient (PPCC) plot can be used\n    to determine the optimal shape parameter for a one-parameter family\n    of distributions. ``ppcc_max`` returns the shape parameter that would\n    maximize the probability plot correlation coefficient for the given\n    data to a one-parameter family of distributions.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array.\n    brack : tuple, optional\n        Triple (a,b,c) where (a<b<c). If bracket consists of two numbers (a, c)\n        then they are assumed to be a starting interval for a downhill bracket\n        search (see `scipy.optimize.brent`).\n    dist : str or stats.distributions instance, optional\n        Distribution or distribution function name.  Objects that look enough\n        like a stats.distributions instance (i.e. they have a ``ppf`` method)\n        are also accepted.  The default is ``'tukeylambda'``.\n\n    Returns\n    -------\n    shape_value : float\n        The shape parameter at which the probability plot correlation\n        coefficient reaches its max value.\n\n    See Also\n    --------\n    ppcc_plot, probplot, boxcox\n\n    Notes\n    -----\n    The brack keyword serves as a starting point which is useful in corner\n    cases. One can use a plot to obtain a rough visual estimate of the location\n    for the maximum to start the search near it.\n\n    References\n    ----------\n    .. [1] J.J. Filliben, \"The Probability Plot Correlation Coefficient Test\n           for Normality\", Technometrics, Vol. 17, pp. 111-117, 1975.\n    .. [2] Engineering Statistics Handbook, NIST/SEMATEC,\n           https://www.itl.nist.gov/div898/handbook/eda/section3/ppccplot.htm\n\n    Examples\n    --------\n    First we generate some random data from a Weibull distribution\n    with shape parameter 2.5:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n    >>> rng = np.random.default_rng()\n    >>> c = 2.5\n    >>> x = stats.weibull_min.rvs(c, scale=4, size=2000, random_state=rng)\n\n    Generate the PPCC plot for this data with the Weibull distribution.\n\n    >>> fig, ax = plt.subplots(figsize=(8, 6))\n    >>> res = stats.ppcc_plot(x, c/2, 2*c, dist='weibull_min', plot=ax)\n\n    We calculate the value where the shape should reach its maximum and a\n    red line is drawn there. The line should coincide with the highest\n    point in the PPCC graph.\n\n    >>> cmax = stats.ppcc_max(x, brack=(c/2, 2*c), dist='weibull_min')\n    >>> ax.axvline(cmax, color='r')\n    >>> plt.show()\n\n    ",
    "scipy.stats.ppcc_plot": "Calculate and optionally plot probability plot correlation coefficient.\n\n    The probability plot correlation coefficient (PPCC) plot can be used to\n    determine the optimal shape parameter for a one-parameter family of\n    distributions.  It cannot be used for distributions without shape\n    parameters\n    (like the normal distribution) or with multiple shape parameters.\n\n    By default a Tukey-Lambda distribution (`stats.tukeylambda`) is used. A\n    Tukey-Lambda PPCC plot interpolates from long-tailed to short-tailed\n    distributions via an approximately normal one, and is therefore\n    particularly useful in practice.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array.\n    a, b : scalar\n        Lower and upper bounds of the shape parameter to use.\n    dist : str or stats.distributions instance, optional\n        Distribution or distribution function name.  Objects that look enough\n        like a stats.distributions instance (i.e. they have a ``ppf`` method)\n        are also accepted.  The default is ``'tukeylambda'``.\n    plot : object, optional\n        If given, plots PPCC against the shape parameter.\n        `plot` is an object that has to have methods \"plot\" and \"text\".\n        The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n        or a custom object with the same methods.\n        Default is None, which means that no plot is created.\n    N : int, optional\n        Number of points on the horizontal axis (equally distributed from\n        `a` to `b`).\n\n    Returns\n    -------\n    svals : ndarray\n        The shape values for which `ppcc` was calculated.\n    ppcc : ndarray\n        The calculated probability plot correlation coefficient values.\n\n    See Also\n    --------\n    ppcc_max, probplot, boxcox_normplot, tukeylambda\n\n    References\n    ----------\n    J.J. Filliben, \"The Probability Plot Correlation Coefficient Test for\n    Normality\", Technometrics, Vol. 17, pp. 111-117, 1975.\n\n    Examples\n    --------\n    First we generate some random data from a Weibull distribution\n    with shape parameter 2.5, and plot the histogram of the data:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n    >>> rng = np.random.default_rng()\n    >>> c = 2.5\n    >>> x = stats.weibull_min.rvs(c, scale=4, size=2000, random_state=rng)\n\n    Take a look at the histogram of the data.\n\n    >>> fig1, ax = plt.subplots(figsize=(9, 4))\n    >>> ax.hist(x, bins=50)\n    >>> ax.set_title('Histogram of x')\n    >>> plt.show()\n\n    Now we explore this data with a PPCC plot as well as the related\n    probability plot and Box-Cox normplot.  A red line is drawn where we\n    expect the PPCC value to be maximal (at the shape parameter ``c``\n    used above):\n\n    >>> fig2 = plt.figure(figsize=(12, 4))\n    >>> ax1 = fig2.add_subplot(1, 3, 1)\n    >>> ax2 = fig2.add_subplot(1, 3, 2)\n    >>> ax3 = fig2.add_subplot(1, 3, 3)\n    >>> res = stats.probplot(x, plot=ax1)\n    >>> res = stats.boxcox_normplot(x, -4, 4, plot=ax2)\n    >>> res = stats.ppcc_plot(x, c/2, 2*c, dist='weibull_min', plot=ax3)\n    >>> ax3.axvline(c, color='r')\n    >>> plt.show()\n\n    ",
    "scipy.stats.probplot": "\n    Calculate quantiles for a probability plot, and optionally show the plot.\n\n    Generates a probability plot of sample data against the quantiles of a\n    specified theoretical distribution (the normal distribution by default).\n    `probplot` optionally calculates a best-fit line for the data and plots the\n    results using Matplotlib or a given plot function.\n\n    Parameters\n    ----------\n    x : array_like\n        Sample/response data from which `probplot` creates the plot.\n    sparams : tuple, optional\n        Distribution-specific shape parameters (shape parameters plus location\n        and scale).\n    dist : str or stats.distributions instance, optional\n        Distribution or distribution function name. The default is 'norm' for a\n        normal probability plot.  Objects that look enough like a\n        stats.distributions instance (i.e. they have a ``ppf`` method) are also\n        accepted.\n    fit : bool, optional\n        Fit a least-squares regression (best-fit) line to the sample data if\n        True (default).\n    plot : object, optional\n        If given, plots the quantiles.\n        If given and `fit` is True, also plots the least squares fit.\n        `plot` is an object that has to have methods \"plot\" and \"text\".\n        The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n        or a custom object with the same methods.\n        Default is None, which means that no plot is created.\n    rvalue : bool, optional\n        If `plot` is provided and `fit` is True, setting `rvalue` to True\n        includes the coefficient of determination on the plot.\n        Default is False.\n\n    Returns\n    -------\n    (osm, osr) : tuple of ndarrays\n        Tuple of theoretical quantiles (osm, or order statistic medians) and\n        ordered responses (osr).  `osr` is simply sorted input `x`.\n        For details on how `osm` is calculated see the Notes section.\n    (slope, intercept, r) : tuple of floats, optional\n        Tuple  containing the result of the least-squares fit, if that is\n        performed by `probplot`. `r` is the square root of the coefficient of\n        determination.  If ``fit=False`` and ``plot=None``, this tuple is not\n        returned.\n\n    Notes\n    -----\n    Even if `plot` is given, the figure is not shown or saved by `probplot`;\n    ``plt.show()`` or ``plt.savefig('figname.png')`` should be used after\n    calling `probplot`.\n\n    `probplot` generates a probability plot, which should not be confused with\n    a Q-Q or a P-P plot.  Statsmodels has more extensive functionality of this\n    type, see ``statsmodels.api.ProbPlot``.\n\n    The formula used for the theoretical quantiles (horizontal axis of the\n    probability plot) is Filliben's estimate::\n\n        quantiles = dist.ppf(val), for\n\n                0.5**(1/n),                  for i = n\n          val = (i - 0.3175) / (n + 0.365),  for i = 2, ..., n-1\n                1 - 0.5**(1/n),              for i = 1\n\n    where ``i`` indicates the i-th ordered value and ``n`` is the total number\n    of values.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n    >>> nsample = 100\n    >>> rng = np.random.default_rng()\n\n    A t distribution with small degrees of freedom:\n\n    >>> ax1 = plt.subplot(221)\n    >>> x = stats.t.rvs(3, size=nsample, random_state=rng)\n    >>> res = stats.probplot(x, plot=plt)\n\n    A t distribution with larger degrees of freedom:\n\n    >>> ax2 = plt.subplot(222)\n    >>> x = stats.t.rvs(25, size=nsample, random_state=rng)\n    >>> res = stats.probplot(x, plot=plt)\n\n    A mixture of two normal distributions with broadcasting:\n\n    >>> ax3 = plt.subplot(223)\n    >>> x = stats.norm.rvs(loc=[0,5], scale=[1,1.5],\n    ...                    size=(nsample//2,2), random_state=rng).ravel()\n    >>> res = stats.probplot(x, plot=plt)\n\n    A standard normal distribution:\n\n    >>> ax4 = plt.subplot(224)\n    >>> x = stats.norm.rvs(loc=0, scale=1, size=nsample, random_state=rng)\n    >>> res = stats.probplot(x, plot=plt)\n\n    Produce a new figure with a loggamma distribution, using the ``dist`` and\n    ``sparams`` keywords:\n\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> x = stats.loggamma.rvs(c=2.5, size=500, random_state=rng)\n    >>> res = stats.probplot(x, dist=stats.loggamma, sparams=(2.5,), plot=ax)\n    >>> ax.set_title(\"Probplot for loggamma dist with shape parameter 2.5\")\n\n    Show the results with Matplotlib:\n\n    >>> plt.show()\n\n    ",
    "scipy.stats.quantile_test": "\n    Perform a quantile test and compute a confidence interval of the quantile.\n\n    This function tests the null hypothesis that `q` is the value of the\n    quantile associated with probability `p` of the population underlying\n    sample `x`. For example, with default parameters, it tests that the\n    median of the population underlying `x` is zero. The function returns an\n    object including the test statistic, a p-value, and a method for computing\n    the confidence interval around the quantile.\n\n    Parameters\n    ----------\n    x : array_like\n        A one-dimensional sample.\n    q : float, default: 0\n        The hypothesized value of the quantile.\n    p : float, default: 0.5\n        The probability associated with the quantile; i.e. the proportion of\n        the population less than `q` is `p`. Must be strictly between 0 and\n        1.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n        The following options are available (default is 'two-sided'):\n\n        * 'two-sided': the quantile associated with the probability `p`\n          is not `q`.\n        * 'less': the quantile associated with the probability `p` is less\n          than `q`.\n        * 'greater': the quantile associated with the probability `p` is\n          greater than `q`.\n\n    Returns\n    -------\n    result : QuantileTestResult\n        An object with the following attributes:\n\n        statistic : float\n            One of two test statistics that may be used in the quantile test.\n            The first test statistic, ``T1``, is the proportion of samples in\n            `x` that are less than or equal to the hypothesized quantile\n            `q`. The second test statistic, ``T2``, is the proportion of\n            samples in `x` that are strictly less than the hypothesized\n            quantile `q`.\n\n            When ``alternative = 'greater'``, ``T1`` is used to calculate the\n            p-value and ``statistic`` is set to ``T1``.\n\n            When ``alternative = 'less'``, ``T2`` is used to calculate the\n            p-value and ``statistic`` is set to ``T2``.\n\n            When ``alternative = 'two-sided'``, both ``T1`` and ``T2`` are\n            considered, and the one that leads to the smallest p-value is used.\n\n        statistic_type : int\n            Either `1` or `2` depending on which of ``T1`` or ``T2`` was\n            used to calculate the p-value.\n\n        pvalue : float\n            The p-value associated with the given alternative.\n\n        The object also has the following method:\n\n        confidence_interval(confidence_level=0.95)\n            Computes a confidence interval around the the\n            population quantile associated with the probability `p`. The\n            confidence interval is returned in a ``namedtuple`` with\n            fields `low` and `high`.  Values are `nan` when there are\n            not enough observations to compute the confidence interval at\n            the desired confidence.\n\n    Notes\n    -----\n    This test and its method for computing confidence intervals are\n    non-parametric. They are valid if and only if the observations are i.i.d.\n\n    The implementation of the test follows Conover [1]_. Two test statistics\n    are considered.\n\n    ``T1``: The number of observations in `x` less than or equal to `q`.\n\n        ``T1 = (x <= q).sum()``\n\n    ``T2``: The number of observations in `x` strictly less than `q`.\n\n        ``T2 = (x < q).sum()``\n\n    The use of two test statistics is necessary to handle the possibility that\n    `x` was generated from a discrete or mixed distribution.\n\n    The null hypothesis for the test is:\n\n        H0: The :math:`p^{\\mathrm{th}}` population quantile is `q`.\n\n    and the null distribution for each test statistic is\n    :math:`\\mathrm{binom}\\left(n, p\\right)`. When ``alternative='less'``,\n    the alternative hypothesis is:\n\n        H1: The :math:`p^{\\mathrm{th}}` population quantile is less than `q`.\n\n    and the p-value is the probability that the binomial random variable\n\n    .. math::\n        Y \\sim \\mathrm{binom}\\left(n, p\\right)\n\n    is greater than or equal to the observed value ``T2``.\n\n    When ``alternative='greater'``, the alternative hypothesis is:\n\n        H1: The :math:`p^{\\mathrm{th}}` population quantile is greater than `q`\n\n    and the p-value is the probability that the binomial random variable Y\n    is less than or equal to the observed value ``T1``.\n\n    When ``alternative='two-sided'``, the alternative hypothesis is\n\n        H1: `q` is not the :math:`p^{\\mathrm{th}}` population quantile.\n\n    and the p-value is twice the smaller of the p-values for the ``'less'``\n    and ``'greater'`` cases. Both of these p-values can exceed 0.5 for the same\n    data, so the value is clipped into the interval :math:`[0, 1]`.\n\n    The approach for confidence intervals is attributed to Thompson [2]_ and\n    later proven to be applicable to any set of i.i.d. samples [3]_. The\n    computation is based on the observation that the probability of a quantile\n    :math:`q` to be larger than any observations :math:`x_m (1\\leq m \\leq N)`\n    can be computed as\n\n    .. math::\n\n        \\mathbb{P}(x_m \\leq q) = 1 - \\sum_{k=0}^{m-1} \\binom{N}{k}\n        q^k(1-q)^{N-k}\n\n    By default, confidence intervals are computed for a 95% confidence level.\n    A common interpretation of a 95% confidence intervals is that if i.i.d.\n    samples are drawn repeatedly from the same population and confidence\n    intervals are formed each time, the confidence interval will contain the\n    true value of the specified quantile in approximately 95% of trials.\n\n    A similar function is available in the QuantileNPCI R package [4]_. The\n    foundation is the same, but it computes the confidence interval bounds by\n    doing interpolations between the sample values, whereas this function uses\n    only sample values as bounds. Thus, ``quantile_test.confidence_interval``\n    returns more conservative intervals (i.e., larger).\n\n    The same computation of confidence intervals for quantiles is included in\n    the confintr package [5]_.\n\n    Two-sided confidence intervals are not guaranteed to be optimal; i.e.,\n    there may exist a tighter interval that may contain the quantile of\n    interest with probability larger than the confidence level.\n    Without further assumption on the samples (e.g., the nature of the\n    underlying distribution), the one-sided intervals are optimally tight.\n\n    References\n    ----------\n    .. [1] W. J. Conover. Practical Nonparametric Statistics, 3rd Ed. 1999.\n    .. [2] W. R. Thompson, \"On Confidence Ranges for the Median and Other\n       Expectation Distributions for Populations of Unknown Distribution\n       Form,\" The Annals of Mathematical Statistics, vol. 7, no. 3,\n       pp. 122-128, 1936, Accessed: Sep. 18, 2019. [Online]. Available:\n       https://www.jstor.org/stable/2957563.\n    .. [3] H. A. David and H. N. Nagaraja, \"Order Statistics in Nonparametric\n       Inference\" in Order Statistics, John Wiley & Sons, Ltd, 2005, pp.\n       159-170. Available:\n       https://onlinelibrary.wiley.com/doi/10.1002/0471722162.ch7.\n    .. [4] N. Hutson, A. Hutson, L. Yan, \"QuantileNPCI: Nonparametric\n       Confidence Intervals for Quantiles,\" R package,\n       https://cran.r-project.org/package=QuantileNPCI\n    .. [5] M. Mayer, \"confintr: Confidence Intervals,\" R package,\n       https://cran.r-project.org/package=confintr\n\n\n    Examples\n    --------\n\n    Suppose we wish to test the null hypothesis that the median of a population\n    is equal to 0.5. We choose a confidence level of 99%; that is, we will\n    reject the null hypothesis in favor of the alternative if the p-value is\n    less than 0.01.\n\n    When testing random variates from the standard uniform distribution, which\n    has a median of 0.5, we expect the data to be consistent with the null\n    hypothesis most of the time.\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng(6981396440634228121)\n    >>> rvs = stats.uniform.rvs(size=100, random_state=rng)\n    >>> stats.quantile_test(rvs, q=0.5, p=0.5)\n    QuantileTestResult(statistic=45, statistic_type=1, pvalue=0.36820161732669576)\n\n    As expected, the p-value is not below our threshold of 0.01, so\n    we cannot reject the null hypothesis.\n\n    When testing data from the standard *normal* distribution, which has a\n    median of 0, we would expect the null hypothesis to be rejected.\n\n    >>> rvs = stats.norm.rvs(size=100, random_state=rng)\n    >>> stats.quantile_test(rvs, q=0.5, p=0.5)\n    QuantileTestResult(statistic=67, statistic_type=2, pvalue=0.0008737198369123724)\n\n    Indeed, the p-value is lower than our threshold of 0.01, so we reject the\n    null hypothesis in favor of the default \"two-sided\" alternative: the median\n    of the population is *not* equal to 0.5.\n\n    However, suppose we were to test the null hypothesis against the\n    one-sided alternative that the median of the population is *greater* than\n    0.5. Since the median of the standard normal is less than 0.5, we would not\n    expect the null hypothesis to be rejected.\n\n    >>> stats.quantile_test(rvs, q=0.5, p=0.5, alternative='greater')\n    QuantileTestResult(statistic=67, statistic_type=1, pvalue=0.9997956114162866)\n\n    Unsurprisingly, with a p-value greater than our threshold, we would not\n    reject the null hypothesis in favor of the chosen alternative.\n\n    The quantile test can be used for any quantile, not only the median. For\n    example, we can test whether the third quartile of the distribution\n    underlying the sample is greater than 0.6.\n\n    >>> rvs = stats.uniform.rvs(size=100, random_state=rng)\n    >>> stats.quantile_test(rvs, q=0.6, p=0.75, alternative='greater')\n    QuantileTestResult(statistic=64, statistic_type=1, pvalue=0.00940696592998271)\n\n    The p-value is lower than the threshold. We reject the null hypothesis in\n    favor of the alternative: the third quartile of the distribution underlying\n    our sample is greater than 0.6.\n\n    `quantile_test` can also compute confidence intervals for any quantile.\n\n    >>> rvs = stats.norm.rvs(size=100, random_state=rng)\n    >>> res = stats.quantile_test(rvs, q=0.6, p=0.75)\n    >>> ci = res.confidence_interval(confidence_level=0.95)\n    >>> ci\n    ConfidenceInterval(low=0.284491604437432, high=0.8912531024914844)\n\n    When testing a one-sided alternative, the confidence interval contains\n    all observations such that if passed as `q`, the p-value of the\n    test would be greater than 0.05, and therefore the null hypothesis\n    would not be rejected. For example:\n\n    >>> rvs.sort()\n    >>> q, p, alpha = 0.6, 0.75, 0.95\n    >>> res = stats.quantile_test(rvs, q=q, p=p, alternative='less')\n    >>> ci = res.confidence_interval(confidence_level=alpha)\n    >>> for x in rvs[rvs <= ci.high]:\n    ...     res = stats.quantile_test(rvs, q=x, p=p, alternative='less')\n    ...     assert res.pvalue > 1-alpha\n    >>> for x in rvs[rvs > ci.high]:\n    ...     res = stats.quantile_test(rvs, q=x, p=p, alternative='less')\n    ...     assert res.pvalue < 1-alpha\n\n    Also, if a 95% confidence interval is repeatedly generated for random\n    samples, the confidence interval will contain the true quantile value in\n    approximately 95% of replications.\n\n    >>> dist = stats.rayleigh() # our \"unknown\" distribution\n    >>> p = 0.2\n    >>> true_stat = dist.ppf(p) # the true value of the statistic\n    >>> n_trials = 1000\n    >>> quantile_ci_contains_true_stat = 0\n    >>> for i in range(n_trials):\n    ...     data = dist.rvs(size=100, random_state=rng)\n    ...     res = stats.quantile_test(data, p=p)\n    ...     ci = res.confidence_interval(0.95)\n    ...     if ci[0] < true_stat < ci[1]:\n    ...         quantile_ci_contains_true_stat += 1\n    >>> quantile_ci_contains_true_stat >= 950\n    True\n\n    This works with any distribution and any quantile, as long as the samples\n    are i.i.d.\n    ",
    "scipy.stats.randint": "A uniform discrete random variable.\n\n    As an instance of the `rv_discrete` class, `randint` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(low, high, loc=0, size=1, random_state=None)\n        Random variates.\n    pmf(k, low, high, loc=0)\n        Probability mass function.\n    logpmf(k, low, high, loc=0)\n        Log of the probability mass function.\n    cdf(k, low, high, loc=0)\n        Cumulative distribution function.\n    logcdf(k, low, high, loc=0)\n        Log of the cumulative distribution function.\n    sf(k, low, high, loc=0)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(k, low, high, loc=0)\n        Log of the survival function.\n    ppf(q, low, high, loc=0)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, low, high, loc=0)\n        Inverse survival function (inverse of ``sf``).\n    stats(low, high, loc=0, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(low, high, loc=0)\n        (Differential) entropy of the RV.\n    expect(func, args=(low, high), loc=0, lb=None, ub=None, conditional=False)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(low, high, loc=0)\n        Median of the distribution.\n    mean(low, high, loc=0)\n        Mean of the distribution.\n    var(low, high, loc=0)\n        Variance of the distribution.\n    std(low, high, loc=0)\n        Standard deviation of the distribution.\n    interval(confidence, low, high, loc=0)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability mass function for `randint` is:\n\n    .. math::\n\n        f(k) = \\frac{1}{\\texttt{high} - \\texttt{low}}\n\n    for :math:`k \\in \\{\\texttt{low}, \\dots, \\texttt{high} - 1\\}`.\n\n    `randint` takes :math:`\\texttt{low}` and :math:`\\texttt{high}` as shape\n    parameters.\n\n    The probability mass function above is defined in the \"standardized\" form.\n    To shift distribution use the ``loc`` parameter.\n    Specifically, ``randint.pmf(k, low, high, loc)`` is identically\n    equivalent to ``randint.pmf(k - loc, low, high)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import randint\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n\n    Calculate the first four moments:\n\n    >>> low, high = 7, 31\n    >>> mean, var, skew, kurt = randint.stats(low, high, moments='mvsk')\n\n    Display the probability mass function (``pmf``):\n\n    >>> x = np.arange(low - 5, high + 5)\n    >>> ax.plot(x, randint.pmf(x, low, high), 'bo', ms=8, label='randint pmf')\n    >>> ax.vlines(x, 0, randint.pmf(x, low, high), colors='b', lw=5, alpha=0.5)\n\n    Alternatively, the distribution object can be called (as a function) to\n    fix the shape and location. This returns a \"frozen\" RV object holding the\n    given parameters fixed.\n\n    Freeze the distribution and display the frozen ``pmf``:\n\n    >>> rv = randint(low, high)\n    >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-',\n    ...           lw=1, label='frozen pmf')\n    >>> ax.legend(loc='lower center')\n    >>> plt.show()\n\n    Check the relationship between the cumulative distribution function\n    (``cdf``) and its inverse, the percent point function (``ppf``):\n\n    >>> q = np.arange(low, high)\n    >>> p = randint.cdf(q, low, high)\n    >>> np.allclose(q, randint.ppf(p, low, high))\n    True\n\n    Generate random numbers:\n\n    >>> r = randint.rvs(low, high, size=1000)\n\n    ",
    "scipy.stats.random_correlation": "A random correlation matrix.\n\n    Return a random correlation matrix, given a vector of eigenvalues.\n\n    The `eigs` keyword specifies the eigenvalues of the correlation matrix,\n    and implies the dimension.\n\n    Methods\n    -------\n    rvs(eigs=None, random_state=None)\n        Draw random correlation matrices, all with eigenvalues eigs.\n\n    Parameters\n    ----------\n    eigs : 1d ndarray\n        Eigenvalues of correlation matrix\n    seed : {None, int, `numpy.random.Generator`, `numpy.random.RandomState`}, optional\n        If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n        singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used,\n        seeded with `seed`.\n        If `seed` is already a ``Generator`` or ``RandomState`` instance\n        then that instance is used.\n    tol : float, optional\n        Tolerance for input parameter checks\n    diag_tol : float, optional\n        Tolerance for deviation of the diagonal of the resulting\n        matrix. Default: 1e-7\n\n    Raises\n    ------\n    RuntimeError\n        Floating point error prevented generating a valid correlation\n        matrix.\n\n    Returns\n    -------\n    rvs : ndarray or scalar\n        Random size N-dimensional matrices, dimension (size, dim, dim),\n        each having eigenvalues eigs.\n\n    Notes\n    -----\n\n    Generates a random correlation matrix following a numerically stable\n    algorithm spelled out by Davies & Higham. This algorithm uses a single O(N)\n    similarity transformation to construct a symmetric positive semi-definite\n    matrix, and applies a series of Givens rotations to scale it to have ones\n    on the diagonal.\n\n    References\n    ----------\n\n    .. [1] Davies, Philip I; Higham, Nicholas J; \"Numerically stable generation\n           of correlation matrices and their factors\", BIT 2000, Vol. 40,\n           No. 4, pp. 640 651\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import random_correlation\n    >>> rng = np.random.default_rng()\n    >>> x = random_correlation.rvs((.5, .8, 1.2, 1.5), random_state=rng)\n    >>> x\n    array([[ 1.        , -0.02423399,  0.03130519,  0.4946965 ],\n           [-0.02423399,  1.        ,  0.20334736,  0.04039817],\n           [ 0.03130519,  0.20334736,  1.        ,  0.02694275],\n           [ 0.4946965 ,  0.04039817,  0.02694275,  1.        ]])\n    >>> import scipy.linalg\n    >>> e, v = scipy.linalg.eigh(x)\n    >>> e\n    array([ 0.5,  0.8,  1.2,  1.5])\n\n    ",
    "scipy.stats.random_table": "Contingency tables from independent samples with fixed marginal sums.\n\n    This is the distribution of random tables with given row and column vector\n    sums. This distribution represents the set of random tables under the null\n    hypothesis that rows and columns are independent. It is used in hypothesis\n    tests of independence.\n\n    Because of assumed independence, the expected frequency of each table\n    element can be computed from the row and column sums, so that the\n    distribution is completely determined by these two vectors.\n\n    Methods\n    -------\n    logpmf(x)\n        Log-probability of table `x` to occur in the distribution.\n    pmf(x)\n        Probability of table `x` to occur in the distribution.\n    mean(row, col)\n        Mean table.\n    rvs(row, col, size=None, method=None, random_state=None)\n        Draw random tables with given row and column vector sums.\n\n    Parameters\n    ----------\n    row : array_like\n        Sum of table entries in each row.\n    col : array_like\n        Sum of table entries in each column.\n    seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n        Used for drawing random variates.\n        If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used, seeded\n        with seed.\n        If `seed` is already a ``RandomState`` or ``Generator`` instance,\n        then that object is used.\n        Default is `None`.\n\n    Notes\n    -----\n    The row and column vectors must be one-dimensional, not empty,\n    and each sum up to the same value. They cannot contain negative\n    or noninteger entries.\n\n    Random elements from the distribution are generated either with Boyett's\n    [1]_ or Patefield's algorithm [2]_. Boyett's algorithm has\n    O(N) time and space complexity, where N is the total sum of entries in the\n    table. Patefield's algorithm has O(K x log(N)) time complexity, where K is\n    the number of cells in the table and requires only a small constant work\n    space. By default, the `rvs` method selects the fastest algorithm based on\n    the input, but you can specify the algorithm with the keyword `method`.\n    Allowed values are \"boyett\" and \"patefield\".\n\n    .. versionadded:: 1.10.0\n\n    Examples\n    --------\n    >>> from scipy.stats import random_table\n\n    >>> row = [1, 5]\n    >>> col = [2, 3, 1]\n    >>> random_table.mean(row, col)\n    array([[0.33333333, 0.5       , 0.16666667],\n           [1.66666667, 2.5       , 0.83333333]])\n\n    Alternatively, the object may be called (as a function) to fix the row\n    and column vector sums, returning a \"frozen\" distribution.\n\n    >>> dist = random_table(row, col)\n    >>> dist.rvs(random_state=123)\n    array([[1., 0., 0.],\n           [1., 3., 1.]])\n\n    References\n    ----------\n    .. [1] J. Boyett, AS 144 Appl. Statist. 28 (1979) 329-332\n    .. [2] W.M. Patefield, AS 159 Appl. Statist. 30 (1981) 91-97\n    ",
    "scipy.stats.rankdata": "Assign ranks to data, dealing with ties appropriately.\n\n    By default (``axis=None``), the data array is first flattened, and a flat\n    array of ranks is returned. Separately reshape the rank array to the\n    shape of the data array if desired (see Examples).\n\n    Ranks begin at 1.  The `method` argument controls how ranks are assigned\n    to equal values.  See [1]_ for further discussion of ranking methods.\n\n    Parameters\n    ----------\n    a : array_like\n        The array of values to be ranked.\n    method : {'average', 'min', 'max', 'dense', 'ordinal'}, optional\n        The method used to assign ranks to tied elements.\n        The following methods are available (default is 'average'):\n\n          * 'average': The average of the ranks that would have been assigned to\n            all the tied values is assigned to each value.\n          * 'min': The minimum of the ranks that would have been assigned to all\n            the tied values is assigned to each value.  (This is also\n            referred to as \"competition\" ranking.)\n          * 'max': The maximum of the ranks that would have been assigned to all\n            the tied values is assigned to each value.\n          * 'dense': Like 'min', but the rank of the next highest element is\n            assigned the rank immediately after those assigned to the tied\n            elements.\n          * 'ordinal': All values are given a distinct rank, corresponding to\n            the order that the values occur in `a`.\n    axis : {None, int}, optional\n        Axis along which to perform the ranking. If ``None``, the data array\n        is first flattened.\n    nan_policy : {'propagate', 'omit', 'raise'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n          * 'propagate': propagates nans through the rank calculation\n          * 'omit': performs the calculations ignoring nan values\n          * 'raise': raises an error\n\n        .. note::\n\n            When `nan_policy` is 'propagate', the output is an array of *all*\n            nans because ranks relative to nans in the input are undefined.\n            When `nan_policy` is 'omit', nans in `a` are ignored when ranking\n            the other values, and the corresponding locations of the output\n            are nan.\n\n        .. versionadded:: 1.10\n\n    Returns\n    -------\n    ranks : ndarray\n         An array of size equal to the size of `a`, containing rank\n         scores.\n\n    References\n    ----------\n    .. [1] \"Ranking\", https://en.wikipedia.org/wiki/Ranking\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import rankdata\n    >>> rankdata([0, 2, 3, 2])\n    array([ 1. ,  2.5,  4. ,  2.5])\n    >>> rankdata([0, 2, 3, 2], method='min')\n    array([ 1,  2,  4,  2])\n    >>> rankdata([0, 2, 3, 2], method='max')\n    array([ 1,  3,  4,  3])\n    >>> rankdata([0, 2, 3, 2], method='dense')\n    array([ 1,  2,  3,  2])\n    >>> rankdata([0, 2, 3, 2], method='ordinal')\n    array([ 1,  2,  4,  3])\n    >>> rankdata([[0, 2], [3, 2]]).reshape(2,2)\n    array([[1. , 2.5],\n          [4. , 2.5]])\n    >>> rankdata([[0, 2, 2], [3, 2, 5]], axis=1)\n    array([[1. , 2.5, 2.5],\n           [2. , 1. , 3. ]])\n    >>> rankdata([0, 2, 3, np.nan, -2, np.nan], nan_policy=\"propagate\")\n    array([nan, nan, nan, nan, nan, nan])\n    >>> rankdata([0, 2, 3, np.nan, -2, np.nan], nan_policy=\"omit\")\n    array([ 2.,  3.,  4., nan,  1., nan])\n\n    ",
    "scipy.stats.ranksums": "    \n\n\nCompute the Wilcoxon rank-sum statistic for two samples.\n\nThe Wilcoxon rank-sum test tests the null hypothesis that two sets\nof measurements are drawn from the same distribution.  The alternative\nhypothesis is that values in one sample are more likely to be\nlarger than the values in the other sample.\n\nThis test should be used to compare two samples from continuous\ndistributions.  It does not handle ties between measurements\nin x and y.  For tie-handling and an optional continuity correction\nsee `scipy.stats.mannwhitneyu`.\n\nParameters\n----------\nx,y : array_like\n    The data from the two samples.\nalternative : {'two-sided', 'less', 'greater'}, optional\n    Defines the alternative hypothesis. Default is 'two-sided'.\n    The following options are available:\n    \n    * 'two-sided': one of the distributions (underlying `x` or `y`) is\n      stochastically greater than the other.\n    * 'less': the distribution underlying `x` is stochastically less\n      than the distribution underlying `y`.\n    * 'greater': the distribution underlying `x` is stochastically greater\n      than the distribution underlying `y`.\n    \n    .. versionadded:: 1.7.0\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nstatistic : float\n    The test statistic under the large-sample approximation that the\n    rank sum statistic is normally distributed.\npvalue : float\n    The p-value of the test.\n\nNotes\n-----\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] https://en.wikipedia.org/wiki/Wilcoxon_rank-sum_test\n\nExamples\n--------\nWe can test the hypothesis that two independent unequal-sized samples are\ndrawn from the same distribution with computing the Wilcoxon rank-sum\nstatistic.\n\n>>> import numpy as np\n>>> from scipy.stats import ranksums\n>>> rng = np.random.default_rng()\n>>> sample1 = rng.uniform(-1, 1, 200)\n>>> sample2 = rng.uniform(-0.5, 1.5, 300) # a shifted distribution\n>>> ranksums(sample1, sample2)\nRanksumsResult(statistic=-7.887059,\n               pvalue=3.09390448e-15) # may vary\n>>> ranksums(sample1, sample2, alternative='less')\nRanksumsResult(statistic=-7.750585297581713,\n               pvalue=4.573497606342543e-15) # may vary\n>>> ranksums(sample1, sample2, alternative='greater')\nRanksumsResult(statistic=-7.750585297581713,\n               pvalue=0.9999999999999954) # may vary\n\nThe p-value of less than ``0.05`` indicates that this test rejects the\nhypothesis at the 5% significance level.\n",
    "scipy.stats.rayleigh": "A Rayleigh continuous random variable.\n\n    As an instance of the `rv_continuous` class, `rayleigh` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `rayleigh` is:\n\n    .. math::\n\n        f(x) = x \\exp(-x^2/2)\n\n    for :math:`x \\ge 0`.\n\n    `rayleigh` is a special case of `chi` with ``df=2``.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``rayleigh.pdf(x, loc, scale)`` is identically\n    equivalent to ``rayleigh.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import rayleigh\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    \n    >>> mean, var, skew, kurt = rayleigh.stats(moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(rayleigh.ppf(0.01),\n    ...                 rayleigh.ppf(0.99), 100)\n    >>> ax.plot(x, rayleigh.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='rayleigh pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = rayleigh()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = rayleigh.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], rayleigh.cdf(vals))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = rayleigh.rvs(size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.rdist": "An R-distributed (symmetric beta) continuous random variable.\n\n    As an instance of the `rv_continuous` class, `rdist` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `rdist` is:\n\n    .. math::\n\n        f(x, c) = \\frac{(1-x^2)^{c/2-1}}{B(1/2, c/2)}\n\n    for :math:`-1 \\le x \\le 1`, :math:`c > 0`. `rdist` is also called the\n    symmetric beta distribution: if B has a `beta` distribution with\n    parameters (c/2, c/2), then X = 2*B - 1 follows a R-distribution with\n    parameter c.\n\n    `rdist` takes ``c`` as a shape parameter for :math:`c`.\n\n    This distribution includes the following distribution kernels as\n    special cases::\n\n        c = 2:  uniform\n        c = 3:  `semicircular`\n        c = 4:  Epanechnikov (parabolic)\n        c = 6:  quartic (biweight)\n        c = 8:  triweight\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``rdist.pdf(x, c, loc, scale)`` is identically\n    equivalent to ``rdist.pdf(y, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import rdist\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c = 1.6\n    >>> mean, var, skew, kurt = rdist.stats(c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(rdist.ppf(0.01, c),\n    ...                 rdist.ppf(0.99, c), 100)\n    >>> ax.plot(x, rdist.pdf(x, c),\n    ...        'r-', lw=5, alpha=0.6, label='rdist pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = rdist(c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = rdist.ppf([0.001, 0.5, 0.999], c)\n    >>> np.allclose([0.001, 0.5, 0.999], rdist.cdf(vals, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = rdist.rvs(c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.recipinvgauss": "A reciprocal inverse Gaussian continuous random variable.\n\n    As an instance of the `rv_continuous` class, `recipinvgauss` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(mu, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, mu, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, mu, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, mu, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, mu, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, mu, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, mu, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, mu, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, mu, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, mu, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(mu, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(mu, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(mu,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(mu, loc=0, scale=1)\n        Median of the distribution.\n    mean(mu, loc=0, scale=1)\n        Mean of the distribution.\n    var(mu, loc=0, scale=1)\n        Variance of the distribution.\n    std(mu, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, mu, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `recipinvgauss` is:\n\n    .. math::\n\n        f(x, \\mu) = \\frac{1}{\\sqrt{2\\pi x}}\n                    \\exp\\left(\\frac{-(1-\\mu x)^2}{2\\mu^2x}\\right)\n\n    for :math:`x \\ge 0`.\n\n    `recipinvgauss` takes ``mu`` as a shape parameter for :math:`\\mu`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``recipinvgauss.pdf(x, mu, loc, scale)`` is identically\n    equivalent to ``recipinvgauss.pdf(y, mu) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import recipinvgauss\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> mu = 0.63\n    >>> mean, var, skew, kurt = recipinvgauss.stats(mu, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(recipinvgauss.ppf(0.01, mu),\n    ...                 recipinvgauss.ppf(0.99, mu), 100)\n    >>> ax.plot(x, recipinvgauss.pdf(x, mu),\n    ...        'r-', lw=5, alpha=0.6, label='recipinvgauss pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = recipinvgauss(mu)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = recipinvgauss.ppf([0.001, 0.5, 0.999], mu)\n    >>> np.allclose([0.001, 0.5, 0.999], recipinvgauss.cdf(vals, mu))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = recipinvgauss.rvs(mu, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.reciprocal": "A loguniform or reciprocal continuous random variable.\n\n    As an instance of the `rv_continuous` class, `reciprocal` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, a, b, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, a, b, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, a, b, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, a, b, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, a, b, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, a, b, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, a, b, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, b, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, a, b, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(a, b, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, b, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, b, loc=0, scale=1)\n        Median of the distribution.\n    mean(a, b, loc=0, scale=1)\n        Mean of the distribution.\n    var(a, b, loc=0, scale=1)\n        Variance of the distribution.\n    std(a, b, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, a, b, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for this class is:\n\n    .. math::\n\n        f(x, a, b) = \\frac{1}{x \\log(b/a)}\n\n    for :math:`a \\le x \\le b`, :math:`b > a > 0`. This class takes\n    :math:`a` and :math:`b` as shape parameters.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``reciprocal.pdf(x, a, b, loc, scale)`` is identically\n    equivalent to ``reciprocal.pdf(y, a, b) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import reciprocal\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a, b = 0.01, 1.25\n    >>> mean, var, skew, kurt = reciprocal.stats(a, b, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(reciprocal.ppf(0.01, a, b),\n    ...                 reciprocal.ppf(0.99, a, b), 100)\n    >>> ax.plot(x, reciprocal.pdf(x, a, b),\n    ...        'r-', lw=5, alpha=0.6, label='reciprocal pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = reciprocal(a, b)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = reciprocal.ppf([0.001, 0.5, 0.999], a, b)\n    >>> np.allclose([0.001, 0.5, 0.999], reciprocal.cdf(vals, a, b))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = reciprocal.rvs(a, b, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    This doesn't show the equal probability of ``0.01``, ``0.1`` and\n    ``1``. This is best when the x-axis is log-scaled:\n\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    >>> ax.hist(np.log10(r))\n    >>> ax.set_ylabel(\"Frequency\")\n    >>> ax.set_xlabel(\"Value of random variable\")\n    >>> ax.xaxis.set_major_locator(plt.FixedLocator([-2, -1, 0]))\n    >>> ticks = [\"$10^{{ {} }}$\".format(i) for i in [-2, -1, 0]]\n    >>> ax.set_xticklabels(ticks)  # doctest: +SKIP\n    >>> plt.show()\n\n    This random variable will be log-uniform regardless of the base chosen for\n    ``a`` and ``b``. Let's specify with base ``2`` instead:\n\n    >>> rvs = reciprocal(2**-2, 2**0).rvs(size=1000)\n\n    Values of ``1/4``, ``1/2`` and ``1`` are equally likely with this random\n    variable.  Here's the histogram:\n\n    >>> fig, ax = plt.subplots(1, 1)\n    >>> ax.hist(np.log2(rvs))\n    >>> ax.set_ylabel(\"Frequency\")\n    >>> ax.set_xlabel(\"Value of random variable\")\n    >>> ax.xaxis.set_major_locator(plt.FixedLocator([-2, -1, 0]))\n    >>> ticks = [\"$2^{{ {} }}$\".format(i) for i in [-2, -1, 0]]\n    >>> ax.set_xticklabels(ticks)  # doctest: +SKIP\n    >>> plt.show()\n\n    ",
    "scipy.stats.rel_breitwigner": "A relativistic Breit-Wigner random variable.\n\n    As an instance of the `rv_continuous` class, `rel_breitwigner` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(rho, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, rho, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, rho, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, rho, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, rho, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, rho, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, rho, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, rho, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, rho, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, rho, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(rho, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(rho, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(rho,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(rho, loc=0, scale=1)\n        Median of the distribution.\n    mean(rho, loc=0, scale=1)\n        Mean of the distribution.\n    var(rho, loc=0, scale=1)\n        Variance of the distribution.\n    std(rho, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, rho, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    cauchy: Cauchy distribution, also known as the Breit-Wigner distribution.\n\n    Notes\n    -----\n\n    The probability density function for `rel_breitwigner` is\n\n    .. math::\n\n        f(x, \\rho) = \\frac{k}{(x^2 - \\rho^2)^2 + \\rho^2}\n\n    where\n\n    .. math::\n        k = \\frac{2\\sqrt{2}\\rho^2\\sqrt{\\rho^2 + 1}}\n            {\\pi\\sqrt{\\rho^2 + \\rho\\sqrt{\\rho^2 + 1}}}\n\n    The relativistic Breit-Wigner distribution is used in high energy physics\n    to model resonances [1]_. It gives the uncertainty in the invariant mass,\n    :math:`M` [2]_, of a resonance with characteristic mass :math:`M_0` and\n    decay-width :math:`\\Gamma`, where :math:`M`, :math:`M_0` and :math:`\\Gamma`\n    are expressed in natural units. In SciPy's parametrization, the shape\n    parameter :math:`\\rho` is equal to :math:`M_0/\\Gamma` and takes values in\n    :math:`(0, \\infty)`.\n\n    Equivalently, the relativistic Breit-Wigner distribution is said to give\n    the uncertainty in the center-of-mass energy :math:`E_{\\text{cm}}`. In\n    natural units, the speed of light :math:`c` is equal to 1 and the invariant\n    mass :math:`M` is equal to the rest energy :math:`Mc^2`. In the\n    center-of-mass frame, the rest energy is equal to the total energy [3]_.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``rel_breitwigner.pdf(x, rho, loc, scale)`` is identically\n    equivalent to ``rel_breitwigner.pdf(y, rho) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    :math:`\\rho = M/\\Gamma` and :math:`\\Gamma` is the scale parameter. For\n    example, if one seeks to model the :math:`Z^0` boson with :math:`M_0\n    \\approx 91.1876 \\text{ GeV}` and :math:`\\Gamma \\approx 2.4952\\text{ GeV}`\n    [4]_ one can set ``rho=91.1876/2.4952`` and ``scale=2.4952``.\n\n    To ensure a physically meaningful result when using the `fit` method, one\n    should set ``floc=0`` to fix the location parameter to 0.\n\n    References\n    ----------\n    .. [1] Relativistic Breit-Wigner distribution, Wikipedia,\n           https://en.wikipedia.org/wiki/Relativistic_Breit-Wigner_distribution\n    .. [2] Invariant mass, Wikipedia,\n           https://en.wikipedia.org/wiki/Invariant_mass\n    .. [3] Center-of-momentum frame, Wikipedia,\n           https://en.wikipedia.org/wiki/Center-of-momentum_frame\n    .. [4] M. Tanabashi et al. (Particle Data Group) Phys. Rev. D 98, 030001 -\n           Published 17 August 2018\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import rel_breitwigner\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> rho = 36.5\n    >>> mean, var, skew, kurt = rel_breitwigner.stats(rho, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(rel_breitwigner.ppf(0.01, rho),\n    ...                 rel_breitwigner.ppf(0.99, rho), 100)\n    >>> ax.plot(x, rel_breitwigner.pdf(x, rho),\n    ...        'r-', lw=5, alpha=0.6, label='rel_breitwigner pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = rel_breitwigner(rho)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = rel_breitwigner.ppf([0.001, 0.5, 0.999], rho)\n    >>> np.allclose([0.001, 0.5, 0.999], rel_breitwigner.cdf(vals, rho))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = rel_breitwigner.rvs(rho, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.relfreq": "Return a relative frequency histogram, using the histogram function.\n\n    A relative frequency  histogram is a mapping of the number of\n    observations in each of the bins relative to the total of observations.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    numbins : int, optional\n        The number of bins to use for the histogram. Default is 10.\n    defaultreallimits : tuple (lower, upper), optional\n        The lower and upper values for the range of the histogram.\n        If no value is given, a range slightly larger than the range of the\n        values in a is used. Specifically ``(a.min() - s, a.max() + s)``,\n        where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.\n    weights : array_like, optional\n        The weights for each value in `a`. Default is None, which gives each\n        value a weight of 1.0\n\n    Returns\n    -------\n    frequency : ndarray\n        Binned values of relative frequency.\n    lowerlimit : float\n        Lower real limit.\n    binsize : float\n        Width of each bin.\n    extrapoints : int\n        Extra points.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy import stats\n    >>> rng = np.random.default_rng()\n    >>> a = np.array([2, 4, 1, 2, 3, 2])\n    >>> res = stats.relfreq(a, numbins=4)\n    >>> res.frequency\n    array([ 0.16666667, 0.5       , 0.16666667,  0.16666667])\n    >>> np.sum(res.frequency)  # relative frequencies should add up to 1\n    1.0\n\n    Create a normal distribution with 1000 random values\n\n    >>> samples = stats.norm.rvs(size=1000, random_state=rng)\n\n    Calculate relative frequencies\n\n    >>> res = stats.relfreq(samples, numbins=25)\n\n    Calculate space of values for x\n\n    >>> x = res.lowerlimit + np.linspace(0, res.binsize*res.frequency.size,\n    ...                                  res.frequency.size)\n\n    Plot relative frequency histogram\n\n    >>> fig = plt.figure(figsize=(5, 4))\n    >>> ax = fig.add_subplot(1, 1, 1)\n    >>> ax.bar(x, res.frequency, width=res.binsize)\n    >>> ax.set_title('Relative frequency histogram')\n    >>> ax.set_xlim([x.min(), x.max()])\n\n    >>> plt.show()\n\n    ",
    "scipy.stats.rice": "A Rice continuous random variable.\n\n    As an instance of the `rv_continuous` class, `rice` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(b, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, b, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, b, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, b, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, b, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, b, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, b, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, b, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, b, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, b, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(b, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(b, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(b,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(b, loc=0, scale=1)\n        Median of the distribution.\n    mean(b, loc=0, scale=1)\n        Mean of the distribution.\n    var(b, loc=0, scale=1)\n        Variance of the distribution.\n    std(b, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, b, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `rice` is:\n\n    .. math::\n\n        f(x, b) = x \\exp(- \\frac{x^2 + b^2}{2}) I_0(x b)\n\n    for :math:`x >= 0`, :math:`b > 0`. :math:`I_0` is the modified Bessel\n    function of order zero (`scipy.special.i0`).\n\n    `rice` takes ``b`` as a shape parameter for :math:`b`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``rice.pdf(x, b, loc, scale)`` is identically\n    equivalent to ``rice.pdf(y, b) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    The Rice distribution describes the length, :math:`r`, of a 2-D vector with\n    components :math:`(U+u, V+v)`, where :math:`U, V` are constant, :math:`u,\n    v` are independent Gaussian random variables with standard deviation\n    :math:`s`.  Let :math:`R = \\sqrt{U^2 + V^2}`. Then the pdf of :math:`r` is\n    ``rice.pdf(x, R/s, scale=s)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import rice\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> b = 0.775\n    >>> mean, var, skew, kurt = rice.stats(b, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(rice.ppf(0.01, b),\n    ...                 rice.ppf(0.99, b), 100)\n    >>> ax.plot(x, rice.pdf(x, b),\n    ...        'r-', lw=5, alpha=0.6, label='rice pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = rice(b)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = rice.ppf([0.001, 0.5, 0.999], b)\n    >>> np.allclose([0.001, 0.5, 0.999], rice.cdf(vals, b))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = rice.rvs(b, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.rv_continuous": "A generic continuous random variable class meant for subclassing.\n\n    `rv_continuous` is a base class to construct specific distribution classes\n    and instances for continuous random variables. It cannot be used\n    directly as a distribution.\n\n    Parameters\n    ----------\n    momtype : int, optional\n        The type of generic moment calculation to use: 0 for pdf, 1 (default)\n        for ppf.\n    a : float, optional\n        Lower bound of the support of the distribution, default is minus\n        infinity.\n    b : float, optional\n        Upper bound of the support of the distribution, default is plus\n        infinity.\n    xtol : float, optional\n        The tolerance for fixed point calculation for generic ppf.\n    badvalue : float, optional\n        The value in a result arrays that indicates a value that for which\n        some argument restriction is violated, default is np.nan.\n    name : str, optional\n        The name of the instance. This string is used to construct the default\n        example for distributions.\n    longname : str, optional\n        This string is used as part of the first line of the docstring returned\n        when a subclass has no docstring of its own. Note: `longname` exists\n        for backwards compatibility, do not use for new subclasses.\n    shapes : str, optional\n        The shape of the distribution. For example ``\"m, n\"`` for a\n        distribution that takes two integers as the two shape arguments for all\n        its methods. If not provided, shape parameters will be inferred from\n        the signature of the private methods, ``_pdf`` and ``_cdf`` of the\n        instance.\n    seed : {None, int, `numpy.random.Generator`, `numpy.random.RandomState`}, optional\n        If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n        singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used,\n        seeded with `seed`.\n        If `seed` is already a ``Generator`` or ``RandomState`` instance then\n        that instance is used.\n\n    Methods\n    -------\n    rvs\n    pdf\n    logpdf\n    cdf\n    logcdf\n    sf\n    logsf\n    ppf\n    isf\n    moment\n    stats\n    entropy\n    expect\n    median\n    mean\n    std\n    var\n    interval\n    __call__\n    fit\n    fit_loc_scale\n    nnlf\n    support\n\n    Notes\n    -----\n    Public methods of an instance of a distribution class (e.g., ``pdf``,\n    ``cdf``) check their arguments and pass valid arguments to private,\n    computational methods (``_pdf``, ``_cdf``). For ``pdf(x)``, ``x`` is valid\n    if it is within the support of the distribution.\n    Whether a shape parameter is valid is decided by an ``_argcheck`` method\n    (which defaults to checking that its arguments are strictly positive.)\n\n    **Subclassing**\n\n    New random variables can be defined by subclassing the `rv_continuous` class\n    and re-defining at least the ``_pdf`` or the ``_cdf`` method (normalized\n    to location 0 and scale 1).\n\n    If positive argument checking is not correct for your RV\n    then you will also need to re-define the ``_argcheck`` method.\n\n    For most of the scipy.stats distributions, the support interval doesn't\n    depend on the shape parameters. ``x`` being in the support interval is\n    equivalent to ``self.a <= x <= self.b``.  If either of the endpoints of\n    the support do depend on the shape parameters, then\n    i) the distribution must implement the ``_get_support`` method; and\n    ii) those dependent endpoints must be omitted from the distribution's\n    call to the ``rv_continuous`` initializer.\n\n    Correct, but potentially slow defaults exist for the remaining\n    methods but for speed and/or accuracy you can over-ride::\n\n      _logpdf, _cdf, _logcdf, _ppf, _rvs, _isf, _sf, _logsf\n\n    The default method ``_rvs`` relies on the inverse of the cdf, ``_ppf``,\n    applied to a uniform random variate. In order to generate random variates\n    efficiently, either the default ``_ppf`` needs to be overwritten (e.g.\n    if the inverse cdf can expressed in an explicit form) or a sampling\n    method needs to be implemented in a custom ``_rvs`` method.\n\n    If possible, you should override ``_isf``, ``_sf`` or ``_logsf``.\n    The main reason would be to improve numerical accuracy: for example,\n    the survival function ``_sf`` is computed as ``1 - _cdf`` which can\n    result in loss of precision if ``_cdf(x)`` is close to one.\n\n    **Methods that can be overwritten by subclasses**\n    ::\n\n      _rvs\n      _pdf\n      _cdf\n      _sf\n      _ppf\n      _isf\n      _stats\n      _munp\n      _entropy\n      _argcheck\n      _get_support\n\n    There are additional (internal and private) generic methods that can\n    be useful for cross-checking and for debugging, but might work in all\n    cases when directly called.\n\n    A note on ``shapes``: subclasses need not specify them explicitly. In this\n    case, `shapes` will be automatically deduced from the signatures of the\n    overridden methods (`pdf`, `cdf` etc).\n    If, for some reason, you prefer to avoid relying on introspection, you can\n    specify ``shapes`` explicitly as an argument to the instance constructor.\n\n\n    **Frozen Distributions**\n\n    Normally, you must provide shape parameters (and, optionally, location and\n    scale parameters to each call of a method of a distribution.\n\n    Alternatively, the object may be called (as a function) to fix the shape,\n    location, and scale parameters returning a \"frozen\" continuous RV object:\n\n    rv = generic(<shape(s)>, loc=0, scale=1)\n        `rv_frozen` object with the same methods but holding the given shape,\n        location, and scale fixed\n\n    **Statistics**\n\n    Statistics are computed using numerical integration by default.\n    For speed you can redefine this using ``_stats``:\n\n     - take shape parameters and return mu, mu2, g1, g2\n     - If you can't compute one of these, return it as None\n     - Can also be defined with a keyword argument ``moments``, which is a\n       string composed of \"m\", \"v\", \"s\", and/or \"k\".\n       Only the components appearing in string should be computed and\n       returned in the order \"m\", \"v\", \"s\", or \"k\"  with missing values\n       returned as None.\n\n    Alternatively, you can override ``_munp``, which takes ``n`` and shape\n    parameters and returns the n-th non-central moment of the distribution.\n\n    **Deepcopying / Pickling**\n\n    If a distribution or frozen distribution is deepcopied (pickled/unpickled,\n    etc.), any underlying random number generator is deepcopied with it. An\n    implication is that if a distribution relies on the singleton RandomState\n    before copying, it will rely on a copy of that random state after copying,\n    and ``np.random.seed`` will no longer control the state.\n\n    Examples\n    --------\n    To create a new Gaussian distribution, we would do the following:\n\n    >>> from scipy.stats import rv_continuous\n    >>> class gaussian_gen(rv_continuous):\n    ...     \"Gaussian distribution\"\n    ...     def _pdf(self, x):\n    ...         return np.exp(-x**2 / 2.) / np.sqrt(2.0 * np.pi)\n    >>> gaussian = gaussian_gen(name='gaussian')\n\n    ``scipy.stats`` distributions are *instances*, so here we subclass\n    `rv_continuous` and create an instance. With this, we now have\n    a fully functional distribution with all relevant methods automagically\n    generated by the framework.\n\n    Note that above we defined a standard normal distribution, with zero mean\n    and unit variance. Shifting and scaling of the distribution can be done\n    by using ``loc`` and ``scale`` parameters: ``gaussian.pdf(x, loc, scale)``\n    essentially computes ``y = (x - loc) / scale`` and\n    ``gaussian._pdf(y) / scale``.\n\n    ",
    "scipy.stats.rv_discrete": "A generic discrete random variable class meant for subclassing.\n\n    `rv_discrete` is a base class to construct specific distribution classes\n    and instances for discrete random variables. It can also be used\n    to construct an arbitrary distribution defined by a list of support\n    points and corresponding probabilities.\n\n    Parameters\n    ----------\n    a : float, optional\n        Lower bound of the support of the distribution, default: 0\n    b : float, optional\n        Upper bound of the support of the distribution, default: plus infinity\n    moment_tol : float, optional\n        The tolerance for the generic calculation of moments.\n    values : tuple of two array_like, optional\n        ``(xk, pk)`` where ``xk`` are integers and ``pk`` are the non-zero\n        probabilities between 0 and 1 with ``sum(pk) = 1``. ``xk``\n        and ``pk`` must have the same shape, and ``xk`` must be unique.\n    inc : integer, optional\n        Increment for the support of the distribution.\n        Default is 1. (other values have not been tested)\n    badvalue : float, optional\n        The value in a result arrays that indicates a value that for which\n        some argument restriction is violated, default is np.nan.\n    name : str, optional\n        The name of the instance. This string is used to construct the default\n        example for distributions.\n    longname : str, optional\n        This string is used as part of the first line of the docstring returned\n        when a subclass has no docstring of its own. Note: `longname` exists\n        for backwards compatibility, do not use for new subclasses.\n    shapes : str, optional\n        The shape of the distribution. For example \"m, n\" for a distribution\n        that takes two integers as the two shape arguments for all its methods\n        If not provided, shape parameters will be inferred from\n        the signatures of the private methods, ``_pmf`` and ``_cdf`` of\n        the instance.\n    seed : {None, int, `numpy.random.Generator`, `numpy.random.RandomState`}, optional\n        If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n        singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used,\n        seeded with `seed`.\n        If `seed` is already a ``Generator`` or ``RandomState`` instance then\n        that instance is used.\n\n    Methods\n    -------\n    rvs\n    pmf\n    logpmf\n    cdf\n    logcdf\n    sf\n    logsf\n    ppf\n    isf\n    moment\n    stats\n    entropy\n    expect\n    median\n    mean\n    std\n    var\n    interval\n    __call__\n    support\n\n    Notes\n    -----\n    This class is similar to `rv_continuous`. Whether a shape parameter is\n    valid is decided by an ``_argcheck`` method (which defaults to checking\n    that its arguments are strictly positive.)\n    The main differences are as follows.\n\n    - The support of the distribution is a set of integers.\n    - Instead of the probability density function, ``pdf`` (and the\n      corresponding private ``_pdf``), this class defines the\n      *probability mass function*, `pmf` (and the corresponding\n      private ``_pmf``.)\n    - There is no ``scale`` parameter.\n    - The default implementations of methods (e.g. ``_cdf``) are not designed\n      for distributions with support that is unbounded below (i.e.\n      ``a=-np.inf``), so they must be overridden.\n\n    To create a new discrete distribution, we would do the following:\n\n    >>> from scipy.stats import rv_discrete\n    >>> class poisson_gen(rv_discrete):\n    ...     \"Poisson distribution\"\n    ...     def _pmf(self, k, mu):\n    ...         return exp(-mu) * mu**k / factorial(k)\n\n    and create an instance::\n\n    >>> poisson = poisson_gen(name=\"poisson\")\n\n    Note that above we defined the Poisson distribution in the standard form.\n    Shifting the distribution can be done by providing the ``loc`` parameter\n    to the methods of the instance. For example, ``poisson.pmf(x, mu, loc)``\n    delegates the work to ``poisson._pmf(x-loc, mu)``.\n\n    **Discrete distributions from a list of probabilities**\n\n    Alternatively, you can construct an arbitrary discrete rv defined\n    on a finite set of values ``xk`` with ``Prob{X=xk} = pk`` by using the\n    ``values`` keyword argument to the `rv_discrete` constructor.\n\n    **Deepcopying / Pickling**\n\n    If a distribution or frozen distribution is deepcopied (pickled/unpickled,\n    etc.), any underlying random number generator is deepcopied with it. An\n    implication is that if a distribution relies on the singleton RandomState\n    before copying, it will rely on a copy of that random state after copying,\n    and ``np.random.seed`` will no longer control the state.\n\n    Examples\n    --------\n    Custom made discrete distribution:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> xk = np.arange(7)\n    >>> pk = (0.1, 0.2, 0.3, 0.1, 0.1, 0.0, 0.2)\n    >>> custm = stats.rv_discrete(name='custm', values=(xk, pk))\n    >>>\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    >>> ax.plot(xk, custm.pmf(xk), 'ro', ms=12, mec='r')\n    >>> ax.vlines(xk, 0, custm.pmf(xk), colors='r', lw=4)\n    >>> plt.show()\n\n    Random number generation:\n\n    >>> R = custm.rvs(size=100)\n\n    ",
    "scipy.stats.rv_histogram": "\n    Generates a distribution given by a histogram.\n    This is useful to generate a template distribution from a binned\n    datasample.\n\n    As a subclass of the `rv_continuous` class, `rv_histogram` inherits from it\n    a collection of generic methods (see `rv_continuous` for the full list),\n    and implements them based on the properties of the provided binned\n    datasample.\n\n    Parameters\n    ----------\n    histogram : tuple of array_like\n        Tuple containing two array_like objects.\n        The first containing the content of n bins,\n        the second containing the (n+1) bin boundaries.\n        In particular, the return value of `numpy.histogram` is accepted.\n\n    density : bool, optional\n        If False, assumes the histogram is proportional to counts per bin;\n        otherwise, assumes it is proportional to a density.\n        For constant bin widths, these are equivalent, but the distinction\n        is important when bin widths vary (see Notes).\n        If None (default), sets ``density=True`` for backwards compatibility,\n        but warns if the bin widths are variable. Set `density` explicitly\n        to silence the warning.\n\n        .. versionadded:: 1.10.0\n\n    Notes\n    -----\n    When a histogram has unequal bin widths, there is a distinction between\n    histograms that are proportional to counts per bin and histograms that are\n    proportional to probability density over a bin. If `numpy.histogram` is\n    called with its default ``density=False``, the resulting histogram is the\n    number of counts per bin, so ``density=False`` should be passed to\n    `rv_histogram`. If `numpy.histogram` is called with ``density=True``, the\n    resulting histogram is in terms of probability density, so ``density=True``\n    should be passed to `rv_histogram`. To avoid warnings, always pass\n    ``density`` explicitly when the input histogram has unequal bin widths.\n\n    There are no additional shape parameters except for the loc and scale.\n    The pdf is defined as a stepwise function from the provided histogram.\n    The cdf is a linear interpolation of the pdf.\n\n    .. versionadded:: 0.19.0\n\n    Examples\n    --------\n\n    Create a scipy.stats distribution from a numpy histogram\n\n    >>> import scipy.stats\n    >>> import numpy as np\n    >>> data = scipy.stats.norm.rvs(size=100000, loc=0, scale=1.5,\n    ...                             random_state=123)\n    >>> hist = np.histogram(data, bins=100)\n    >>> hist_dist = scipy.stats.rv_histogram(hist, density=False)\n\n    Behaves like an ordinary scipy rv_continuous distribution\n\n    >>> hist_dist.pdf(1.0)\n    0.20538577847618705\n    >>> hist_dist.cdf(2.0)\n    0.90818568543056499\n\n    PDF is zero above (below) the highest (lowest) bin of the histogram,\n    defined by the max (min) of the original dataset\n\n    >>> hist_dist.pdf(np.max(data))\n    0.0\n    >>> hist_dist.cdf(np.max(data))\n    1.0\n    >>> hist_dist.pdf(np.min(data))\n    7.7591907244498314e-05\n    >>> hist_dist.cdf(np.min(data))\n    0.0\n\n    PDF and CDF follow the histogram\n\n    >>> import matplotlib.pyplot as plt\n    >>> X = np.linspace(-5.0, 5.0, 100)\n    >>> fig, ax = plt.subplots()\n    >>> ax.set_title(\"PDF from Template\")\n    >>> ax.hist(data, density=True, bins=100)\n    >>> ax.plot(X, hist_dist.pdf(X), label='PDF')\n    >>> ax.plot(X, hist_dist.cdf(X), label='CDF')\n    >>> ax.legend()\n    >>> fig.show()\n\n    ",
    "scipy.stats.rvs_ratio_uniforms": "\n    Generate random samples from a probability density function using the\n    ratio-of-uniforms method.\n\n    .. deprecated:: 1.12.0\n        `rvs_ratio_uniforms` is deprecated in favour of\n        `scipy.stats.sampling.RatioUniforms` from version 1.12.0 and will\n        be removed in SciPy 1.15.0\n\n    Parameters\n    ----------\n    pdf : callable\n        A function with signature `pdf(x)` that is proportional to the\n        probability density function of the distribution.\n    umax : float\n        The upper bound of the bounding rectangle in the u-direction.\n    vmin : float\n        The lower bound of the bounding rectangle in the v-direction.\n    vmax : float\n        The upper bound of the bounding rectangle in the v-direction.\n    size : int or tuple of ints, optional\n        Defining number of random variates (default is 1).\n    c : float, optional.\n        Shift parameter of ratio-of-uniforms method, see Notes. Default is 0.\n    random_state : {None, int, `numpy.random.Generator`,\n                    `numpy.random.RandomState`}, optional\n\n        If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n        singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used,\n        seeded with `seed`.\n        If `seed` is already a ``Generator`` or ``RandomState`` instance then\n        that instance is used.\n\n    Returns\n    -------\n    rvs : ndarray\n        The random variates distributed according to the probability\n        distribution defined by the pdf.\n\n    Notes\n    -----\n    Please refer to `scipy.stats.sampling.RatioUniforms` for the documentation.\n    ",
    "scipy.stats.scoreatpercentile": "Calculate the score at a given percentile of the input sequence.\n\n    For example, the score at `per=50` is the median. If the desired quantile\n    lies between two data points, we interpolate between them, according to\n    the value of `interpolation`. If the parameter `limit` is provided, it\n    should be a tuple (lower, upper) of two values.\n\n    Parameters\n    ----------\n    a : array_like\n        A 1-D array of values from which to extract score.\n    per : array_like\n        Percentile(s) at which to extract score.  Values should be in range\n        [0,100].\n    limit : tuple, optional\n        Tuple of two scalars, the lower and upper limits within which to\n        compute the percentile. Values of `a` outside\n        this (closed) interval will be ignored.\n    interpolation_method : {'fraction', 'lower', 'higher'}, optional\n        Specifies the interpolation method to use,\n        when the desired quantile lies between two data points `i` and `j`\n        The following options are available (default is 'fraction'):\n\n          * 'fraction': ``i + (j - i) * fraction`` where ``fraction`` is the\n            fractional part of the index surrounded by ``i`` and ``j``\n          * 'lower': ``i``\n          * 'higher': ``j``\n\n    axis : int, optional\n        Axis along which the percentiles are computed. Default is None. If\n        None, compute over the whole array `a`.\n\n    Returns\n    -------\n    score : float or ndarray\n        Score at percentile(s).\n\n    See Also\n    --------\n    percentileofscore, numpy.percentile\n\n    Notes\n    -----\n    This function will become obsolete in the future.\n    For NumPy 1.9 and higher, `numpy.percentile` provides all the functionality\n    that `scoreatpercentile` provides.  And it's significantly faster.\n    Therefore it's recommended to use `numpy.percentile` for users that have\n    numpy >= 1.9.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> a = np.arange(100)\n    >>> stats.scoreatpercentile(a, 50)\n    49.5\n\n    ",
    "scipy.stats.sem": "    \n\n\nCompute standard error of the mean.\n\nCalculate the standard error of the mean (or standard error of\nmeasurement) of the values in the input array.\n\nParameters\n----------\na : array_like\n    An array containing the values for which the standard error is\n    returned. Must contain at least two observations.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nddof : int, optional\n    Delta degrees-of-freedom. How many degrees of freedom to adjust\n    for bias in limited samples relative to the population estimate\n    of variance. Defaults to 1.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\ns : ndarray or float\n    The standard error of the mean in the sample(s), along the input axis.\n\nNotes\n-----\nThe default value for `ddof` is different to the default (0) used by other\nddof containing routines, such as np.std and np.nanstd.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nExamples\n--------\nFind standard error along the first axis:\n\n>>> import numpy as np\n>>> from scipy import stats\n>>> a = np.arange(20).reshape(5,4)\n>>> stats.sem(a)\narray([ 2.8284,  2.8284,  2.8284,  2.8284])\n\nFind standard error across the whole array, using n degrees of freedom:\n\n>>> stats.sem(a, axis=None, ddof=0)\n1.2893796958227628\n",
    "scipy.stats.semicircular": "A semicircular continuous random variable.\n\n    As an instance of the `rv_continuous` class, `semicircular` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    rdist\n\n    Notes\n    -----\n    The probability density function for `semicircular` is:\n\n    .. math::\n\n        f(x) = \\frac{2}{\\pi} \\sqrt{1-x^2}\n\n    for :math:`-1 \\le x \\le 1`.\n\n    The distribution is a special case of `rdist` with `c = 3`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``semicircular.pdf(x, loc, scale)`` is identically\n    equivalent to ``semicircular.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] \"Wigner semicircle distribution\",\n           https://en.wikipedia.org/wiki/Wigner_semicircle_distribution\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import semicircular\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    \n    >>> mean, var, skew, kurt = semicircular.stats(moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(semicircular.ppf(0.01),\n    ...                 semicircular.ppf(0.99), 100)\n    >>> ax.plot(x, semicircular.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='semicircular pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = semicircular()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = semicircular.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], semicircular.cdf(vals))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = semicircular.rvs(size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.shapiro": "    \n\n\nPerform the Shapiro-Wilk test for normality.\n\nThe Shapiro-Wilk test tests the null hypothesis that the\ndata was drawn from a normal distribution.\n\nParameters\n----------\nx : array_like\n    Array of sample data. Must contain at least three observations.\naxis : int or None, default: None\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nstatistic : float\n    The test statistic.\np-value : float\n    The p-value for the hypothesis test.\n\nSee Also\n--------\n\n:func:`anderson`\n    The Anderson-Darling test for normality\n:func:`kstest`\n    The Kolmogorov-Smirnov test for goodness of fit.\n\n\nNotes\n-----\nThe algorithm used is described in [4]_ but censoring parameters as\ndescribed are not implemented. For N > 5000 the W test statistic is\naccurate, but the p-value may not be.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm\n       :doi:`10.18434/M32189`\n.. [2] Shapiro, S. S. & Wilk, M.B, \"An analysis of variance test for\n       normality (complete samples)\", Biometrika, 1965, Vol. 52,\n       pp. 591-611, :doi:`10.2307/2333709`\n.. [3] Razali, N. M. & Wah, Y. B., \"Power comparisons of Shapiro-Wilk,\n       Kolmogorov-Smirnov, Lilliefors and Anderson-Darling tests\", Journal\n       of Statistical Modeling and Analytics, 2011, Vol. 2, pp. 21-33.\n.. [4] Royston P., \"Remark AS R94: A Remark on Algorithm AS 181: The\n       W-test for Normality\", 1995, Applied Statistics, Vol. 44,\n       :doi:`10.2307/2986146`\n.. [5] Phipson B., and Smyth, G. K., \"Permutation P-values Should Never Be\n       Zero: Calculating Exact P-values When Permutations Are Randomly\n       Drawn\", Statistical Applications in Genetics and Molecular Biology,\n       2010, Vol.9, :doi:`10.2202/1544-6115.1585`\n.. [6] Panagiotakos, D. B., \"The value of p-value in biomedical\n       research\", The Open Cardiovascular Medicine Journal, 2008, Vol.2,\n       pp. 97-99, :doi:`10.2174/1874192400802010097`\n\nExamples\n--------\nSuppose we wish to infer from measurements whether the weights of adult\nhuman males in a medical study are not normally distributed [2]_.\nThe weights (lbs) are recorded in the array ``x`` below.\n\n>>> import numpy as np\n>>> x = np.array([148, 154, 158, 160, 161, 162, 166, 170, 182, 195, 236])\n\nThe normality test of [1]_ and [2]_ begins by computing a statistic based\non the relationship between the observations and the expected order\nstatistics of a normal distribution.\n\n>>> from scipy import stats\n>>> res = stats.shapiro(x)\n>>> res.statistic\n0.7888147830963135\n\nThe value of this statistic tends to be high (close to 1) for samples drawn\nfrom a normal distribution.\n\nThe test is performed by comparing the observed value of the statistic\nagainst the null distribution: the distribution of statistic values formed\nunder the null hypothesis that the weights were drawn from a normal\ndistribution. For this normality test, the null distribution is not easy to\ncalculate exactly, so it is usually approximated by Monte Carlo methods,\nthat is, drawing many samples of the same size as ``x`` from a normal\ndistribution and computing the values of the statistic for each.\n\n>>> def statistic(x):\n...     # Get only the `shapiro` statistic; ignore its p-value\n...     return stats.shapiro(x).statistic\n>>> ref = stats.monte_carlo_test(x, stats.norm.rvs, statistic,\n...                              alternative='less')\n>>> import matplotlib.pyplot as plt\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> bins = np.linspace(0.65, 1, 50)\n>>> def plot(ax):  # we'll reuse this\n...     ax.hist(ref.null_distribution, density=True, bins=bins)\n...     ax.set_title(\"Shapiro-Wilk Test Null Distribution \\n\"\n...                  \"(Monte Carlo Approximation, 11 Observations)\")\n...     ax.set_xlabel(\"statistic\")\n...     ax.set_ylabel(\"probability density\")\n>>> plot(ax)\n>>> plt.show()\n\nThe comparison is quantified by the p-value: the proportion of values in\nthe null distribution less than or equal to the observed value of the\nstatistic.\n\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> plot(ax)\n>>> annotation = (f'p-value={res.pvalue:.6f}\\n(highlighted area)')\n>>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n>>> _ = ax.annotate(annotation, (0.75, 0.1), (0.68, 0.7), arrowprops=props)\n>>> i_extreme = np.where(bins <= res.statistic)[0]\n>>> for i in i_extreme:\n...     ax.patches[i].set_color('C1')\n>>> plt.xlim(0.65, 0.9)\n>>> plt.ylim(0, 4)\n>>> plt.show\n>>> res.pvalue\n0.006703833118081093\n\nIf the p-value is \"small\" - that is, if there is a low probability of\nsampling data from a normally distributed population that produces such an\nextreme value of the statistic - this may be taken as evidence against\nthe null hypothesis in favor of the alternative: the weights were not\ndrawn from a normal distribution. Note that:\n\n- The inverse is not true; that is, the test is not used to provide\n  evidence *for* the null hypothesis.\n- The threshold for values that will be considered \"small\" is a choice that\n  should be made before the data is analyzed [5]_ with consideration of the\n  risks of both false positives (incorrectly rejecting the null hypothesis)\n  and false negatives (failure to reject a false null hypothesis).\n",
    "scipy.stats.siegelslopes": "\n    Computes the Siegel estimator for a set of points (x, y).\n\n    `siegelslopes` implements a method for robust linear regression\n    using repeated medians (see [1]_) to fit a line to the points (x, y).\n    The method is robust to outliers with an asymptotic breakdown point\n    of 50%.\n\n    Parameters\n    ----------\n    y : array_like\n        Dependent variable.\n    x : array_like or None, optional\n        Independent variable. If None, use ``arange(len(y))`` instead.\n    method : {'hierarchical', 'separate'}\n        If 'hierarchical', estimate the intercept using the estimated\n        slope ``slope`` (default option).\n        If 'separate', estimate the intercept independent of the estimated\n        slope. See Notes for details.\n\n    Returns\n    -------\n    result : ``SiegelslopesResult`` instance\n        The return value is an object with the following attributes:\n\n        slope : float\n            Estimate of the slope of the regression line.\n        intercept : float\n            Estimate of the intercept of the regression line.\n\n    See Also\n    --------\n    theilslopes : a similar technique without repeated medians\n\n    Notes\n    -----\n    With ``n = len(y)``, compute ``m_j`` as the median of\n    the slopes from the point ``(x[j], y[j])`` to all other `n-1` points.\n    ``slope`` is then the median of all slopes ``m_j``.\n    Two ways are given to estimate the intercept in [1]_ which can be chosen\n    via the parameter ``method``.\n    The hierarchical approach uses the estimated slope ``slope``\n    and computes ``intercept`` as the median of ``y - slope*x``.\n    The other approach estimates the intercept separately as follows: for\n    each point ``(x[j], y[j])``, compute the intercepts of all the `n-1`\n    lines through the remaining points and take the median ``i_j``.\n    ``intercept`` is the median of the ``i_j``.\n\n    The implementation computes `n` times the median of a vector of size `n`\n    which can be slow for large vectors. There are more efficient algorithms\n    (see [2]_) which are not implemented here.\n\n    For compatibility with older versions of SciPy, the return value acts\n    like a ``namedtuple`` of length 2, with fields ``slope`` and\n    ``intercept``, so one can continue to write::\n\n        slope, intercept = siegelslopes(y, x)\n\n    References\n    ----------\n    .. [1] A. Siegel, \"Robust Regression Using Repeated Medians\",\n           Biometrika, Vol. 69, pp. 242-244, 1982.\n\n    .. [2] A. Stein and M. Werman, \"Finding the repeated median regression\n           line\", Proceedings of the Third Annual ACM-SIAM Symposium on\n           Discrete Algorithms, pp. 409-413, 1992.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n\n    >>> x = np.linspace(-5, 5, num=150)\n    >>> y = x + np.random.normal(size=x.size)\n    >>> y[11:15] += 10  # add outliers\n    >>> y[-5:] -= 7\n\n    Compute the slope and intercept.  For comparison, also compute the\n    least-squares fit with `linregress`:\n\n    >>> res = stats.siegelslopes(y, x)\n    >>> lsq_res = stats.linregress(x, y)\n\n    Plot the results. The Siegel regression line is shown in red. The green\n    line shows the least-squares fit for comparison.\n\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> ax.plot(x, y, 'b.')\n    >>> ax.plot(x, res[1] + res[0] * x, 'r-')\n    >>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, 'g-')\n    >>> plt.show()\n\n    ",
    "scipy.stats.sigmaclip": "Perform iterative sigma-clipping of array elements.\n\n    Starting from the full sample, all elements outside the critical range are\n    removed, i.e. all elements of the input array `c` that satisfy either of\n    the following conditions::\n\n        c < mean(c) - std(c)*low\n        c > mean(c) + std(c)*high\n\n    The iteration continues with the updated sample until no\n    elements are outside the (updated) range.\n\n    Parameters\n    ----------\n    a : array_like\n        Data array, will be raveled if not 1-D.\n    low : float, optional\n        Lower bound factor of sigma clipping. Default is 4.\n    high : float, optional\n        Upper bound factor of sigma clipping. Default is 4.\n\n    Returns\n    -------\n    clipped : ndarray\n        Input array with clipped elements removed.\n    lower : float\n        Lower threshold value use for clipping.\n    upper : float\n        Upper threshold value use for clipping.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import sigmaclip\n    >>> a = np.concatenate((np.linspace(9.5, 10.5, 31),\n    ...                     np.linspace(0, 20, 5)))\n    >>> fact = 1.5\n    >>> c, low, upp = sigmaclip(a, fact, fact)\n    >>> c\n    array([  9.96666667,  10.        ,  10.03333333,  10.        ])\n    >>> c.var(), c.std()\n    (0.00055555555555555165, 0.023570226039551501)\n    >>> low, c.mean() - fact*c.std(), c.min()\n    (9.9646446609406727, 9.9646446609406727, 9.9666666666666668)\n    >>> upp, c.mean() + fact*c.std(), c.max()\n    (10.035355339059327, 10.035355339059327, 10.033333333333333)\n\n    >>> a = np.concatenate((np.linspace(9.5, 10.5, 11),\n    ...                     np.linspace(-100, -50, 3)))\n    >>> c, low, upp = sigmaclip(a, 1.8, 1.8)\n    >>> (c == np.linspace(9.5, 10.5, 11)).all()\n    True\n\n    ",
    "scipy.stats.skellam": "A  Skellam discrete random variable.\n\n    As an instance of the `rv_discrete` class, `skellam` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(mu1, mu2, loc=0, size=1, random_state=None)\n        Random variates.\n    pmf(k, mu1, mu2, loc=0)\n        Probability mass function.\n    logpmf(k, mu1, mu2, loc=0)\n        Log of the probability mass function.\n    cdf(k, mu1, mu2, loc=0)\n        Cumulative distribution function.\n    logcdf(k, mu1, mu2, loc=0)\n        Log of the cumulative distribution function.\n    sf(k, mu1, mu2, loc=0)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(k, mu1, mu2, loc=0)\n        Log of the survival function.\n    ppf(q, mu1, mu2, loc=0)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, mu1, mu2, loc=0)\n        Inverse survival function (inverse of ``sf``).\n    stats(mu1, mu2, loc=0, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(mu1, mu2, loc=0)\n        (Differential) entropy of the RV.\n    expect(func, args=(mu1, mu2), loc=0, lb=None, ub=None, conditional=False)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(mu1, mu2, loc=0)\n        Median of the distribution.\n    mean(mu1, mu2, loc=0)\n        Mean of the distribution.\n    var(mu1, mu2, loc=0)\n        Variance of the distribution.\n    std(mu1, mu2, loc=0)\n        Standard deviation of the distribution.\n    interval(confidence, mu1, mu2, loc=0)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    Probability distribution of the difference of two correlated or\n    uncorrelated Poisson random variables.\n\n    Let :math:`k_1` and :math:`k_2` be two Poisson-distributed r.v. with\n    expected values :math:`\\lambda_1` and :math:`\\lambda_2`. Then,\n    :math:`k_1 - k_2` follows a Skellam distribution with parameters\n    :math:`\\mu_1 = \\lambda_1 - \\rho \\sqrt{\\lambda_1 \\lambda_2}` and\n    :math:`\\mu_2 = \\lambda_2 - \\rho \\sqrt{\\lambda_1 \\lambda_2}`, where\n    :math:`\\rho` is the correlation coefficient between :math:`k_1` and\n    :math:`k_2`. If the two Poisson-distributed r.v. are independent then\n    :math:`\\rho = 0`.\n\n    Parameters :math:`\\mu_1` and :math:`\\mu_2` must be strictly positive.\n\n    For details see: https://en.wikipedia.org/wiki/Skellam_distribution\n\n    `skellam` takes :math:`\\mu_1` and :math:`\\mu_2` as shape parameters.\n\n    The probability mass function above is defined in the \"standardized\" form.\n    To shift distribution use the ``loc`` parameter.\n    Specifically, ``skellam.pmf(k, mu1, mu2, loc)`` is identically\n    equivalent to ``skellam.pmf(k - loc, mu1, mu2)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import skellam\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> mu1, mu2 = 15, 8\n    >>> mean, var, skew, kurt = skellam.stats(mu1, mu2, moments='mvsk')\n    \n    Display the probability mass function (``pmf``):\n    \n    >>> x = np.arange(skellam.ppf(0.01, mu1, mu2),\n    ...               skellam.ppf(0.99, mu1, mu2))\n    >>> ax.plot(x, skellam.pmf(x, mu1, mu2), 'bo', ms=8, label='skellam pmf')\n    >>> ax.vlines(x, 0, skellam.pmf(x, mu1, mu2), colors='b', lw=5, alpha=0.5)\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape and location. This returns a \"frozen\" RV object holding\n    the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pmf``:\n    \n    >>> rv = skellam(mu1, mu2)\n    >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n    ...         label='frozen pmf')\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> prob = skellam.cdf(x, mu1, mu2)\n    >>> np.allclose(x, skellam.ppf(prob, mu1, mu2))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = skellam.rvs(mu1, mu2, size=1000)\n\n    ",
    "scipy.stats.skew": "    \n\n\nCompute the sample skewness of a data set.\n\nFor normally distributed data, the skewness should be about zero. For\nunimodal continuous distributions, a skewness value greater than zero means\nthat there is more weight in the right tail of the distribution. The\nfunction `skewtest` can be used to determine if the skewness value\nis close enough to zero, statistically speaking.\n\nParameters\n----------\na : ndarray\n    Input array.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nbias : bool, optional\n    If False, then the calculations are corrected for statistical bias.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nskewness : ndarray\n    The skewness of values along an axis, returning NaN where all values\n    are equal.\n\nNotes\n-----\nThe sample skewness is computed as the Fisher-Pearson coefficient\nof skewness, i.e.\n\n.. math::\n\n    g_1=\\frac{m_3}{m_2^{3/2}}\n\nwhere\n\n.. math::\n\n    m_i=\\frac{1}{N}\\sum_{n=1}^N(x[n]-\\bar{x})^i\n\nis the biased sample :math:`i\\texttt{th}` central moment, and\n:math:`\\bar{x}` is\nthe sample mean.  If ``bias`` is False, the calculations are\ncorrected for bias and the value computed is the adjusted\nFisher-Pearson standardized moment coefficient, i.e.\n\n.. math::\n\n    G_1=\\frac{k_3}{k_2^{3/2}}=\n        \\frac{\\sqrt{N(N-1)}}{N-2}\\frac{m_3}{m_2^{3/2}}.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n   Probability and Statistics Tables and Formulae. Chapman & Hall: New\n   York. 2000.\n   Section 2.2.24.1\n\nExamples\n--------\n>>> from scipy.stats import skew\n>>> skew([1, 2, 3, 4, 5])\n0.0\n>>> skew([2, 8, 0, 4, 1, 9, 9, 0])\n0.2650554122698573\n",
    "scipy.stats.skewcauchy": "A skewed Cauchy random variable.\n\n    As an instance of the `rv_continuous` class, `skewcauchy` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, a, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, a, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, a, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, a, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, a, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, a, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, a, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, a, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(a, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, loc=0, scale=1)\n        Median of the distribution.\n    mean(a, loc=0, scale=1)\n        Mean of the distribution.\n    var(a, loc=0, scale=1)\n        Variance of the distribution.\n    std(a, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, a, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    cauchy : Cauchy distribution\n\n    Notes\n    -----\n\n    The probability density function for `skewcauchy` is:\n\n    .. math::\n\n        f(x) = \\frac{1}{\\pi \\left(\\frac{x^2}{\\left(a\\, \\text{sign}(x) + 1\n                                                   \\right)^2} + 1 \\right)}\n\n    for a real number :math:`x` and skewness parameter :math:`-1 < a < 1`.\n\n    When :math:`a=0`, the distribution reduces to the usual Cauchy\n    distribution.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``skewcauchy.pdf(x, a, loc, scale)`` is identically\n    equivalent to ``skewcauchy.pdf(y, a) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] \"Skewed generalized *t* distribution\", Wikipedia\n       https://en.wikipedia.org/wiki/Skewed_generalized_t_distribution#Skewed_Cauchy_distribution\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import skewcauchy\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a = 0.5\n    >>> mean, var, skew, kurt = skewcauchy.stats(a, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(skewcauchy.ppf(0.01, a),\n    ...                 skewcauchy.ppf(0.99, a), 100)\n    >>> ax.plot(x, skewcauchy.pdf(x, a),\n    ...        'r-', lw=5, alpha=0.6, label='skewcauchy pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = skewcauchy(a)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = skewcauchy.ppf([0.001, 0.5, 0.999], a)\n    >>> np.allclose([0.001, 0.5, 0.999], skewcauchy.cdf(vals, a))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = skewcauchy.rvs(a, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.skewnorm": "A skew-normal random variable.\n\n    As an instance of the `rv_continuous` class, `skewnorm` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, a, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, a, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, a, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, a, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, a, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, a, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, a, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, a, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(a, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, loc=0, scale=1)\n        Median of the distribution.\n    mean(a, loc=0, scale=1)\n        Mean of the distribution.\n    var(a, loc=0, scale=1)\n        Variance of the distribution.\n    std(a, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, a, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The pdf is::\n\n        skewnorm.pdf(x, a) = 2 * norm.pdf(x) * norm.cdf(a*x)\n\n    `skewnorm` takes a real number :math:`a` as a skewness parameter\n    When ``a = 0`` the distribution is identical to a normal distribution\n    (`norm`). `rvs` implements the method of [1]_.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``skewnorm.pdf(x, a, loc, scale)`` is identically\n    equivalent to ``skewnorm.pdf(y, a) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import skewnorm\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a = 4\n    >>> mean, var, skew, kurt = skewnorm.stats(a, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(skewnorm.ppf(0.01, a),\n    ...                 skewnorm.ppf(0.99, a), 100)\n    >>> ax.plot(x, skewnorm.pdf(x, a),\n    ...        'r-', lw=5, alpha=0.6, label='skewnorm pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = skewnorm(a)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = skewnorm.ppf([0.001, 0.5, 0.999], a)\n    >>> np.allclose([0.001, 0.5, 0.999], skewnorm.cdf(vals, a))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = skewnorm.rvs(a, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    References\n    ----------\n    .. [1] A. Azzalini and A. Capitanio (1999). Statistical applications of\n        the multivariate skew-normal distribution. J. Roy. Statist. Soc.,\n        B 61, 579-602. :arxiv:`0911.2093`\n\n    ",
    "scipy.stats.skewtest": "    \n\n\nTest whether the skew is different from the normal distribution.\n\nThis function tests the null hypothesis that the skewness of\nthe population that the sample was drawn from is the same\nas that of a corresponding normal distribution.\n\nParameters\n----------\na : array\n    The data to be tested. Must contain at least eight observations.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nalternative : {'two-sided', 'less', 'greater'}, optional\n    Defines the alternative hypothesis. Default is 'two-sided'.\n    The following options are available:\n    \n    * 'two-sided': the skewness of the distribution underlying the sample\n      is different from that of the normal distribution (i.e. 0)\n    * 'less': the skewness of the distribution underlying the sample\n      is less than that of the normal distribution\n    * 'greater': the skewness of the distribution underlying the sample\n      is greater than that of the normal distribution\n    \n    .. versionadded:: 1.7.0\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nstatistic : float\n    The computed z-score for this test.\npvalue : float\n    The p-value for the hypothesis test.\n\nNotes\n-----\nThe sample size must be at least 8.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] R. B. D'Agostino, A. J. Belanger and R. B. D'Agostino Jr.,\n        \"A suggestion for using powerful and informative tests of\n        normality\", American Statistician 44, pp. 316-321, 1990.\n.. [2] Shapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test\n       for normality (complete samples). Biometrika, 52(3/4), 591-611.\n.. [3] B. Phipson and G. K. Smyth. \"Permutation P-values Should Never Be\n       Zero: Calculating Exact P-values When Permutations Are Randomly\n       Drawn.\" Statistical Applications in Genetics and Molecular Biology\n       9.1 (2010).\n\nExamples\n--------\nSuppose we wish to infer from measurements whether the weights of adult\nhuman males in a medical study are not normally distributed [2]_.\nThe weights (lbs) are recorded in the array ``x`` below.\n\n>>> import numpy as np\n>>> x = np.array([148, 154, 158, 160, 161, 162, 166, 170, 182, 195, 236])\n\nThe skewness test from [1]_ begins by computing a statistic based on the\nsample skewness.\n\n>>> from scipy import stats\n>>> res = stats.skewtest(x)\n>>> res.statistic\n2.7788579769903414\n\nBecause normal distributions have zero skewness, the magnitude of this\nstatistic tends to be low for samples drawn from a normal distribution.\n\nThe test is performed by comparing the observed value of the\nstatistic against the null distribution: the distribution of statistic\nvalues derived under the null hypothesis that the weights were drawn from\na normal distribution.\n\nFor this test, the null distribution of the statistic for very large\nsamples is the standard normal distribution.\n\n>>> import matplotlib.pyplot as plt\n>>> dist = stats.norm()\n>>> st_val = np.linspace(-5, 5, 100)\n>>> pdf = dist.pdf(st_val)\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> def st_plot(ax):  # we'll reuse this\n...     ax.plot(st_val, pdf)\n...     ax.set_title(\"Skew Test Null Distribution\")\n...     ax.set_xlabel(\"statistic\")\n...     ax.set_ylabel(\"probability density\")\n>>> st_plot(ax)\n>>> plt.show()\n\nThe comparison is quantified by the p-value: the proportion of values in\nthe null distribution as extreme or more extreme than the observed\nvalue of the statistic. In a two-sided test, elements of the null\ndistribution greater than the observed statistic and elements of the null\ndistribution less than the negative of the observed statistic are both\nconsidered \"more extreme\".\n\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> st_plot(ax)\n>>> pvalue = dist.cdf(-res.statistic) + dist.sf(res.statistic)\n>>> annotation = (f'p-value={pvalue:.3f}\\n(shaded area)')\n>>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n>>> _ = ax.annotate(annotation, (3, 0.005), (3.25, 0.02), arrowprops=props)\n>>> i = st_val >= res.statistic\n>>> ax.fill_between(st_val[i], y1=0, y2=pdf[i], color='C0')\n>>> i = st_val <= -res.statistic\n>>> ax.fill_between(st_val[i], y1=0, y2=pdf[i], color='C0')\n>>> ax.set_xlim(-5, 5)\n>>> ax.set_ylim(0, 0.1)\n>>> plt.show()\n>>> res.pvalue\n0.005455036974740185\n\nIf the p-value is \"small\" - that is, if there is a low probability of\nsampling data from a normally distributed population that produces such an\nextreme value of the statistic - this may be taken as evidence against\nthe null hypothesis in favor of the alternative: the weights were not\ndrawn from a normal distribution. Note that:\n\n- The inverse is not true; that is, the test is not used to provide\n  evidence for the null hypothesis.\n- The threshold for values that will be considered \"small\" is a choice that\n  should be made before the data is analyzed [3]_ with consideration of the\n  risks of both false positives (incorrectly rejecting the null hypothesis)\n  and false negatives (failure to reject a false null hypothesis).\n\nNote that the standard normal distribution provides an asymptotic\napproximation of the null distribution; it is only accurate for samples\nwith many observations. For small samples like ours,\n`scipy.stats.monte_carlo_test` may provide a more accurate, albeit\nstochastic, approximation of the exact p-value.\n\n>>> def statistic(x, axis):\n...     # get just the skewtest statistic; ignore the p-value\n...     return stats.skewtest(x, axis=axis).statistic\n>>> res = stats.monte_carlo_test(x, stats.norm.rvs, statistic)\n>>> fig, ax = plt.subplots(figsize=(8, 5))\n>>> st_plot(ax)\n>>> ax.hist(res.null_distribution, np.linspace(-5, 5, 50),\n...         density=True)\n>>> ax.legend(['aymptotic approximation\\n(many observations)',\n...            'Monte Carlo approximation\\n(11 observations)'])\n>>> plt.show()\n>>> res.pvalue\n0.0062  # may vary\n\nIn this case, the asymptotic approximation and Monte Carlo approximation\nagree fairly closely, even for our small sample.\n",
    "scipy.stats.sobol_indices": "Global sensitivity indices of Sobol'.\n\n    Parameters\n    ----------\n    func : callable or dict(str, array_like)\n        If `func` is a callable, function to compute the Sobol' indices from.\n        Its signature must be::\n\n            func(x: ArrayLike) -> ArrayLike\n\n        with ``x`` of shape ``(d, n)`` and output of shape ``(s, n)`` where:\n\n        - ``d`` is the input dimensionality of `func`\n          (number of input variables),\n        - ``s`` is the output dimensionality of `func`\n          (number of output variables), and\n        - ``n`` is the number of samples (see `n` below).\n\n        Function evaluation values must be finite.\n\n        If `func` is a dictionary, contains the function evaluations from three\n        different arrays. Keys must be: ``f_A``, ``f_B`` and ``f_AB``.\n        ``f_A`` and ``f_B`` should have a shape ``(s, n)`` and ``f_AB``\n        should have a shape ``(d, s, n)``.\n        This is an advanced feature and misuse can lead to wrong analysis.\n    n : int\n        Number of samples used to generate the matrices ``A`` and ``B``.\n        Must be a power of 2. The total number of points at which `func` is\n        evaluated will be ``n*(d+2)``.\n    dists : list(distributions), optional\n        List of each parameter's distribution. The distribution of parameters\n        depends on the application and should be carefully chosen.\n        Parameters are assumed to be independently distributed, meaning there\n        is no constraint nor relationship between their values.\n\n        Distributions must be an instance of a class with a ``ppf``\n        method.\n\n        Must be specified if `func` is a callable, and ignored otherwise.\n    method : Callable or str, default: 'saltelli_2010'\n        Method used to compute the first and total Sobol' indices.\n\n        If a callable, its signature must be::\n\n            func(f_A: np.ndarray, f_B: np.ndarray, f_AB: np.ndarray)\n            -> Tuple[np.ndarray, np.ndarray]\n\n        with ``f_A, f_B`` of shape ``(s, n)`` and ``f_AB`` of shape\n        ``(d, s, n)``.\n        These arrays contain the function evaluations from three different sets\n        of samples.\n        The output is a tuple of the first and total indices with\n        shape ``(s, d)``.\n        This is an advanced feature and misuse can lead to wrong analysis.\n    random_state : {None, int, `numpy.random.Generator`}, optional\n        If `random_state` is an int or None, a new `numpy.random.Generator` is\n        created using ``np.random.default_rng(random_state)``.\n        If `random_state` is already a ``Generator`` instance, then the\n        provided instance is used.\n\n    Returns\n    -------\n    res : SobolResult\n        An object with attributes:\n\n        first_order : ndarray of shape (s, d)\n            First order Sobol' indices.\n        total_order : ndarray of shape (s, d)\n            Total order Sobol' indices.\n\n        And method:\n\n        bootstrap(confidence_level: float, n_resamples: int)\n        -> BootstrapSobolResult\n\n            A method providing confidence intervals on the indices.\n            See `scipy.stats.bootstrap` for more details.\n\n            The bootstrapping is done on both first and total order indices,\n            and they are available in `BootstrapSobolResult` as attributes\n            ``first_order`` and ``total_order``.\n\n    Notes\n    -----\n    The Sobol' method [1]_, [2]_ is a variance-based Sensitivity Analysis which\n    obtains the contribution of each parameter to the variance of the\n    quantities of interest (QoIs; i.e., the outputs of `func`).\n    Respective contributions can be used to rank the parameters and\n    also gauge the complexity of the model by computing the\n    model's effective (or mean) dimension.\n\n    .. note::\n\n        Parameters are assumed to be independently distributed. Each\n        parameter can still follow any distribution. In fact, the distribution\n        is very important and should match the real distribution of the\n        parameters.\n\n    It uses a functional decomposition of the variance of the function to\n    explore\n\n    .. math::\n\n        \\mathbb{V}(Y) = \\sum_{i}^{d} \\mathbb{V}_i (Y) + \\sum_{i<j}^{d}\n        \\mathbb{V}_{ij}(Y) + ... + \\mathbb{V}_{1,2,...,d}(Y),\n\n    introducing conditional variances:\n\n    .. math::\n\n        \\mathbb{V}_i(Y) = \\mathbb{\\mathbb{V}}[\\mathbb{E}(Y|x_i)]\n        \\qquad\n        \\mathbb{V}_{ij}(Y) = \\mathbb{\\mathbb{V}}[\\mathbb{E}(Y|x_i x_j)]\n        - \\mathbb{V}_i(Y) - \\mathbb{V}_j(Y),\n\n    Sobol' indices are expressed as\n\n    .. math::\n\n        S_i = \\frac{\\mathbb{V}_i(Y)}{\\mathbb{V}[Y]}\n        \\qquad\n        S_{ij} =\\frac{\\mathbb{V}_{ij}(Y)}{\\mathbb{V}[Y]}.\n\n    :math:`S_{i}` corresponds to the first-order term which apprises the\n    contribution of the i-th parameter, while :math:`S_{ij}` corresponds to the\n    second-order term which informs about the contribution of interactions\n    between the i-th and the j-th parameters. These equations can be\n    generalized to compute higher order terms; however, they are expensive to\n    compute and their interpretation is complex.\n    This is why only first order indices are provided.\n\n    Total order indices represent the global contribution of the parameters\n    to the variance of the QoI and are defined as:\n\n    .. math::\n\n        S_{T_i} = S_i + \\sum_j S_{ij} + \\sum_{j,k} S_{ijk} + ...\n        = 1 - \\frac{\\mathbb{V}[\\mathbb{E}(Y|x_{\\sim i})]}{\\mathbb{V}[Y]}.\n\n    First order indices sum to at most 1, while total order indices sum to at\n    least 1. If there are no interactions, then first and total order indices\n    are equal, and both first and total order indices sum to 1.\n\n    .. warning::\n\n        Negative Sobol' values are due to numerical errors. Increasing the\n        number of points `n` should help.\n\n        The number of sample required to have a good analysis increases with\n        the dimensionality of the problem. e.g. for a 3 dimension problem,\n        consider at minima ``n >= 2**12``. The more complex the model is,\n        the more samples will be needed.\n\n        Even for a purely addiditive model, the indices may not sum to 1 due\n        to numerical noise.\n\n    References\n    ----------\n    .. [1] Sobol, I. M.. \"Sensitivity analysis for nonlinear mathematical\n       models.\" Mathematical Modeling and Computational Experiment, 1:407-414,\n       1993.\n    .. [2] Sobol, I. M. (2001). \"Global sensitivity indices for nonlinear\n       mathematical models and their Monte Carlo estimates.\" Mathematics\n       and Computers in Simulation, 55(1-3):271-280,\n       :doi:`10.1016/S0378-4754(00)00270-6`, 2001.\n    .. [3] Saltelli, A. \"Making best use of model evaluations to\n       compute sensitivity indices.\"  Computer Physics Communications,\n       145(2):280-297, :doi:`10.1016/S0010-4655(02)00280-1`, 2002.\n    .. [4] Saltelli, A., M. Ratto, T. Andres, F. Campolongo, J. Cariboni,\n       D. Gatelli, M. Saisana, and S. Tarantola. \"Global Sensitivity Analysis.\n       The Primer.\" 2007.\n    .. [5] Saltelli, A., P. Annoni, I. Azzini, F. Campolongo, M. Ratto, and\n       S. Tarantola. \"Variance based sensitivity analysis of model\n       output. Design and estimator for the total sensitivity index.\"\n       Computer Physics Communications, 181(2):259-270,\n       :doi:`10.1016/j.cpc.2009.09.018`, 2010.\n    .. [6] Ishigami, T. and T. Homma. \"An importance quantification technique\n       in uncertainty analysis for computer models.\" IEEE,\n       :doi:`10.1109/ISUMA.1990.151285`, 1990.\n\n    Examples\n    --------\n    The following is an example with the Ishigami function [6]_\n\n    .. math::\n\n        Y(\\mathbf{x}) = \\sin x_1 + 7 \\sin^2 x_2 + 0.1 x_3^4 \\sin x_1,\n\n    with :math:`\\mathbf{x} \\in [-\\pi, \\pi]^3`. This function exhibits strong\n    non-linearity and non-monotonicity.\n\n    Remember, Sobol' indices assumes that samples are independently\n    distributed. In this case we use a uniform distribution on each marginals.\n\n    >>> import numpy as np\n    >>> from scipy.stats import sobol_indices, uniform\n    >>> rng = np.random.default_rng()\n    >>> def f_ishigami(x):\n    ...     f_eval = (\n    ...         np.sin(x[0])\n    ...         + 7 * np.sin(x[1])**2\n    ...         + 0.1 * (x[2]**4) * np.sin(x[0])\n    ...     )\n    ...     return f_eval\n    >>> indices = sobol_indices(\n    ...     func=f_ishigami, n=1024,\n    ...     dists=[\n    ...         uniform(loc=-np.pi, scale=2*np.pi),\n    ...         uniform(loc=-np.pi, scale=2*np.pi),\n    ...         uniform(loc=-np.pi, scale=2*np.pi)\n    ...     ],\n    ...     random_state=rng\n    ... )\n    >>> indices.first_order\n    array([0.31637954, 0.43781162, 0.00318825])\n    >>> indices.total_order\n    array([0.56122127, 0.44287857, 0.24229595])\n\n    Confidence interval can be obtained using bootstrapping.\n\n    >>> boot = indices.bootstrap()\n\n    Then, this information can be easily visualized.\n\n    >>> import matplotlib.pyplot as plt\n    >>> fig, axs = plt.subplots(1, 2, figsize=(9, 4))\n    >>> _ = axs[0].errorbar(\n    ...     [1, 2, 3], indices.first_order, fmt='o',\n    ...     yerr=[\n    ...         indices.first_order - boot.first_order.confidence_interval.low,\n    ...         boot.first_order.confidence_interval.high - indices.first_order\n    ...     ],\n    ... )\n    >>> axs[0].set_ylabel(\"First order Sobol' indices\")\n    >>> axs[0].set_xlabel('Input parameters')\n    >>> axs[0].set_xticks([1, 2, 3])\n    >>> _ = axs[1].errorbar(\n    ...     [1, 2, 3], indices.total_order, fmt='o',\n    ...     yerr=[\n    ...         indices.total_order - boot.total_order.confidence_interval.low,\n    ...         boot.total_order.confidence_interval.high - indices.total_order\n    ...     ],\n    ... )\n    >>> axs[1].set_ylabel(\"Total order Sobol' indices\")\n    >>> axs[1].set_xlabel('Input parameters')\n    >>> axs[1].set_xticks([1, 2, 3])\n    >>> plt.tight_layout()\n    >>> plt.show()\n\n    .. note::\n\n        By default, `scipy.stats.uniform` has support ``[0, 1]``.\n        Using the parameters ``loc`` and ``scale``, one obtains the uniform\n        distribution on ``[loc, loc + scale]``.\n\n    This result is particularly interesting because the first order index\n    :math:`S_{x_3} = 0` whereas its total order is :math:`S_{T_{x_3}} = 0.244`.\n    This means that higher order interactions with :math:`x_3` are responsible\n    for the difference. Almost 25% of the observed variance\n    on the QoI is due to the correlations between :math:`x_3` and :math:`x_1`,\n    although :math:`x_3` by itself has no impact on the QoI.\n\n    The following gives a visual explanation of Sobol' indices on this\n    function. Let's generate 1024 samples in :math:`[-\\pi, \\pi]^3` and\n    calculate the value of the output.\n\n    >>> from scipy.stats import qmc\n    >>> n_dim = 3\n    >>> p_labels = ['$x_1$', '$x_2$', '$x_3$']\n    >>> sample = qmc.Sobol(d=n_dim, seed=rng).random(1024)\n    >>> sample = qmc.scale(\n    ...     sample=sample,\n    ...     l_bounds=[-np.pi, -np.pi, -np.pi],\n    ...     u_bounds=[np.pi, np.pi, np.pi]\n    ... )\n    >>> output = f_ishigami(sample.T)\n\n    Now we can do scatter plots of the output with respect to each parameter.\n    This gives a visual way to understand how each parameter impacts the\n    output of the function.\n\n    >>> fig, ax = plt.subplots(1, n_dim, figsize=(12, 4))\n    >>> for i in range(n_dim):\n    ...     xi = sample[:, i]\n    ...     ax[i].scatter(xi, output, marker='+')\n    ...     ax[i].set_xlabel(p_labels[i])\n    >>> ax[0].set_ylabel('Y')\n    >>> plt.tight_layout()\n    >>> plt.show()\n\n    Now Sobol' goes a step further:\n    by conditioning the output value by given values of the parameter\n    (black lines), the conditional output mean is computed. It corresponds to\n    the term :math:`\\mathbb{E}(Y|x_i)`. Taking the variance of this term gives\n    the numerator of the Sobol' indices.\n\n    >>> mini = np.min(output)\n    >>> maxi = np.max(output)\n    >>> n_bins = 10\n    >>> bins = np.linspace(-np.pi, np.pi, num=n_bins, endpoint=False)\n    >>> dx = bins[1] - bins[0]\n    >>> fig, ax = plt.subplots(1, n_dim, figsize=(12, 4))\n    >>> for i in range(n_dim):\n    ...     xi = sample[:, i]\n    ...     ax[i].scatter(xi, output, marker='+')\n    ...     ax[i].set_xlabel(p_labels[i])\n    ...     for bin_ in bins:\n    ...         idx = np.where((bin_ <= xi) & (xi <= bin_ + dx))\n    ...         xi_ = xi[idx]\n    ...         y_ = output[idx]\n    ...         ave_y_ = np.mean(y_)\n    ...         ax[i].plot([bin_ + dx/2] * 2, [mini, maxi], c='k')\n    ...         ax[i].scatter(bin_ + dx/2, ave_y_, c='r')\n    >>> ax[0].set_ylabel('Y')\n    >>> plt.tight_layout()\n    >>> plt.show()\n\n    Looking at :math:`x_3`, the variance\n    of the mean is zero leading to :math:`S_{x_3} = 0`. But we can further\n    observe that the variance of the output is not constant along the parameter\n    values of :math:`x_3`. This heteroscedasticity is explained by higher order\n    interactions. Moreover, an heteroscedasticity is also noticeable on\n    :math:`x_1` leading to an interaction between :math:`x_3` and :math:`x_1`.\n    On :math:`x_2`, the variance seems to be constant and thus null interaction\n    with this parameter can be supposed.\n\n    This case is fairly simple to analyse visually---although it is only a\n    qualitative analysis. Nevertheless, when the number of input parameters\n    increases such analysis becomes unrealistic as it would be difficult to\n    conclude on high-order terms. Hence the benefit of using Sobol' indices.\n\n    ",
    "scipy.stats.somersd": "Calculates Somers' D, an asymmetric measure of ordinal association.\n\n    Like Kendall's :math:`\\tau`, Somers' :math:`D` is a measure of the\n    correspondence between two rankings. Both statistics consider the\n    difference between the number of concordant and discordant pairs in two\n    rankings :math:`X` and :math:`Y`, and both are normalized such that values\n    close  to 1 indicate strong agreement and values close to -1 indicate\n    strong disagreement. They differ in how they are normalized. To show the\n    relationship, Somers' :math:`D` can be defined in terms of Kendall's\n    :math:`\\tau_a`:\n\n    .. math::\n        D(Y|X) = \\frac{\\tau_a(X, Y)}{\\tau_a(X, X)}\n\n    Suppose the first ranking :math:`X` has :math:`r` distinct ranks and the\n    second ranking :math:`Y` has :math:`s` distinct ranks. These two lists of\n    :math:`n` rankings can also be viewed as an :math:`r \\times s` contingency\n    table in which element :math:`i, j` is the number of rank pairs with rank\n    :math:`i` in ranking :math:`X` and rank :math:`j` in ranking :math:`Y`.\n    Accordingly, `somersd` also allows the input data to be supplied as a\n    single, 2D contingency table instead of as two separate, 1D rankings.\n\n    Note that the definition of Somers' :math:`D` is asymmetric: in general,\n    :math:`D(Y|X) \\neq D(X|Y)`. ``somersd(x, y)`` calculates Somers'\n    :math:`D(Y|X)`: the \"row\" variable :math:`X` is treated as an independent\n    variable, and the \"column\" variable :math:`Y` is dependent. For Somers'\n    :math:`D(X|Y)`, swap the input lists or transpose the input table.\n\n    Parameters\n    ----------\n    x : array_like\n        1D array of rankings, treated as the (row) independent variable.\n        Alternatively, a 2D contingency table.\n    y : array_like, optional\n        If `x` is a 1D array of rankings, `y` is a 1D array of rankings of the\n        same length, treated as the (column) dependent variable.\n        If `x` is 2D, `y` is ignored.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n        The following options are available:\n        * 'two-sided': the rank correlation is nonzero\n        * 'less': the rank correlation is negative (less than zero)\n        * 'greater':  the rank correlation is positive (greater than zero)\n\n    Returns\n    -------\n    res : SomersDResult\n        A `SomersDResult` object with the following fields:\n\n            statistic : float\n               The Somers' :math:`D` statistic.\n            pvalue : float\n               The p-value for a hypothesis test whose null\n               hypothesis is an absence of association, :math:`D=0`.\n               See notes for more information.\n            table : 2D array\n               The contingency table formed from rankings `x` and `y` (or the\n               provided contingency table, if `x` is a 2D array)\n\n    See Also\n    --------\n    kendalltau : Calculates Kendall's tau, another correlation measure.\n    weightedtau : Computes a weighted version of Kendall's tau.\n    spearmanr : Calculates a Spearman rank-order correlation coefficient.\n    pearsonr : Calculates a Pearson correlation coefficient.\n\n    Notes\n    -----\n    This function follows the contingency table approach of [2]_ and\n    [3]_. *p*-values are computed based on an asymptotic approximation of\n    the test statistic distribution under the null hypothesis :math:`D=0`.\n\n    Theoretically, hypothesis tests based on Kendall's :math:`tau` and Somers'\n    :math:`D` should be identical.\n    However, the *p*-values returned by `kendalltau` are based\n    on the null hypothesis of *independence* between :math:`X` and :math:`Y`\n    (i.e. the population from which pairs in :math:`X` and :math:`Y` are\n    sampled contains equal numbers of all possible pairs), which is more\n    specific than the null hypothesis :math:`D=0` used here. If the null\n    hypothesis of independence is desired, it is acceptable to use the\n    *p*-value returned by `kendalltau` with the statistic returned by\n    `somersd` and vice versa. For more information, see [2]_.\n\n    Contingency tables are formatted according to the convention used by\n    SAS and R: the first ranking supplied (``x``) is the \"row\" variable, and\n    the second ranking supplied (``y``) is the \"column\" variable. This is\n    opposite the convention of Somers' original paper [1]_.\n\n    References\n    ----------\n    .. [1] Robert H. Somers, \"A New Asymmetric Measure of Association for\n           Ordinal Variables\", *American Sociological Review*, Vol. 27, No. 6,\n           pp. 799--811, 1962.\n\n    .. [2] Morton B. Brown and Jacqueline K. Benedetti, \"Sampling Behavior of\n           Tests for Correlation in Two-Way Contingency Tables\", *Journal of\n           the American Statistical Association* Vol. 72, No. 358, pp.\n           309--315, 1977.\n\n    .. [3] SAS Institute, Inc., \"The FREQ Procedure (Book Excerpt)\",\n           *SAS/STAT 9.2 User's Guide, Second Edition*, SAS Publishing, 2009.\n\n    .. [4] Laerd Statistics, \"Somers' d using SPSS Statistics\", *SPSS\n           Statistics Tutorials and Statistical Guides*,\n           https://statistics.laerd.com/spss-tutorials/somers-d-using-spss-statistics.php,\n           Accessed July 31, 2020.\n\n    Examples\n    --------\n    We calculate Somers' D for the example given in [4]_, in which a hotel\n    chain owner seeks to determine the association between hotel room\n    cleanliness and customer satisfaction. The independent variable, hotel\n    room cleanliness, is ranked on an ordinal scale: \"below average (1)\",\n    \"average (2)\", or \"above average (3)\". The dependent variable, customer\n    satisfaction, is ranked on a second scale: \"very dissatisfied (1)\",\n    \"moderately dissatisfied (2)\", \"neither dissatisfied nor satisfied (3)\",\n    \"moderately satisfied (4)\", or \"very satisfied (5)\". 189 customers\n    respond to the survey, and the results are cast into a contingency table\n    with the hotel room cleanliness as the \"row\" variable and customer\n    satisfaction as the \"column\" variable.\n\n    +-----+-----+-----+-----+-----+-----+\n    |     | (1) | (2) | (3) | (4) | (5) |\n    +=====+=====+=====+=====+=====+=====+\n    | (1) | 27  | 25  | 14  | 7   | 0   |\n    +-----+-----+-----+-----+-----+-----+\n    | (2) | 7   | 14  | 18  | 35  | 12  |\n    +-----+-----+-----+-----+-----+-----+\n    | (3) | 1   | 3   | 2   | 7   | 17  |\n    +-----+-----+-----+-----+-----+-----+\n\n    For example, 27 customers assigned their room a cleanliness ranking of\n    \"below average (1)\" and a corresponding satisfaction of \"very\n    dissatisfied (1)\". We perform the analysis as follows.\n\n    >>> from scipy.stats import somersd\n    >>> table = [[27, 25, 14, 7, 0], [7, 14, 18, 35, 12], [1, 3, 2, 7, 17]]\n    >>> res = somersd(table)\n    >>> res.statistic\n    0.6032766111513396\n    >>> res.pvalue\n    1.0007091191074533e-27\n\n    The value of the Somers' D statistic is approximately 0.6, indicating\n    a positive correlation between room cleanliness and customer satisfaction\n    in the sample.\n    The *p*-value is very small, indicating a very small probability of\n    observing such an extreme value of the statistic under the null\n    hypothesis that the statistic of the entire population (from which\n    our sample of 189 customers is drawn) is zero. This supports the\n    alternative hypothesis that the true value of Somers' D for the population\n    is nonzero.\n\n    ",
    "scipy.stats.spearmanr": "Calculate a Spearman correlation coefficient with associated p-value.\n\n    The Spearman rank-order correlation coefficient is a nonparametric measure\n    of the monotonicity of the relationship between two datasets.\n    Like other correlation coefficients,\n    this one varies between -1 and +1 with 0 implying no correlation.\n    Correlations of -1 or +1 imply an exact monotonic relationship. Positive\n    correlations imply that as x increases, so does y. Negative correlations\n    imply that as x increases, y decreases.\n\n    The p-value roughly indicates the probability of an uncorrelated system\n    producing datasets that have a Spearman correlation at least as extreme\n    as the one computed from these datasets. Although calculation of the\n    p-value does not make strong assumptions about the distributions underlying\n    the samples, it is only accurate for very large samples (>500\n    observations). For smaller sample sizes, consider a permutation test (see\n    Examples section below).\n\n    Parameters\n    ----------\n    a, b : 1D or 2D array_like, b is optional\n        One or two 1-D or 2-D arrays containing multiple variables and\n        observations. When these are 1-D, each represents a vector of\n        observations of a single variable. For the behavior in the 2-D case,\n        see under ``axis``, below.\n        Both arrays need to have the same length in the ``axis`` dimension.\n    axis : int or None, optional\n        If axis=0 (default), then each column represents a variable, with\n        observations in the rows. If axis=1, the relationship is transposed:\n        each row represents a variable, while the columns contain observations.\n        If axis=None, then both arrays will be raveled.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is 'propagate'):\n\n        * 'propagate': returns nan\n        * 'raise': throws an error\n        * 'omit': performs the calculations ignoring nan values\n\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis. Default is 'two-sided'.\n        The following options are available:\n\n        * 'two-sided': the correlation is nonzero\n        * 'less': the correlation is negative (less than zero)\n        * 'greater':  the correlation is positive (greater than zero)\n\n        .. versionadded:: 1.7.0\n\n    Returns\n    -------\n    res : SignificanceResult\n        An object containing attributes:\n\n        statistic : float or ndarray (2-D square)\n            Spearman correlation matrix or correlation coefficient (if only 2\n            variables are given as parameters). Correlation matrix is square\n            with length equal to total number of variables (columns or rows) in\n            ``a`` and ``b`` combined.\n        pvalue : float\n            The p-value for a hypothesis test whose null hypothesis\n            is that two samples have no ordinal correlation. See\n            `alternative` above for alternative hypotheses. `pvalue` has the\n            same shape as `statistic`.\n\n    Warns\n    -----\n    `~scipy.stats.ConstantInputWarning`\n        Raised if an input is a constant array.  The correlation coefficient\n        is not defined in this case, so ``np.nan`` is returned.\n\n    References\n    ----------\n    .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n       Probability and Statistics Tables and Formulae. Chapman & Hall: New\n       York. 2000.\n       Section  14.7\n    .. [2] Kendall, M. G. and Stuart, A. (1973).\n       The Advanced Theory of Statistics, Volume 2: Inference and Relationship.\n       Griffin. 1973.\n       Section 31.18\n    .. [3] Kershenobich, D., Fierro, F. J., & Rojkind, M. (1970). The\n       relationship between the free pool of proline and collagen content in\n       human liver cirrhosis. The Journal of Clinical Investigation, 49(12),\n       2246-2249.\n    .. [4] Hollander, M., Wolfe, D. A., & Chicken, E. (2013). Nonparametric\n       statistical methods. John Wiley & Sons.\n    .. [5] B. Phipson and G. K. Smyth. \"Permutation P-values Should Never Be\n       Zero: Calculating Exact P-values When Permutations Are Randomly Drawn.\"\n       Statistical Applications in Genetics and Molecular Biology 9.1 (2010).\n    .. [6] Ludbrook, J., & Dudley, H. (1998). Why permutation tests are\n       superior to t and F tests in biomedical research. The American\n       Statistician, 52(2), 127-132.\n\n    Examples\n    --------\n    Consider the following data from [3]_, which studied the relationship\n    between free proline (an amino acid) and total collagen (a protein often\n    found in connective tissue) in unhealthy human livers.\n\n    The ``x`` and ``y`` arrays below record measurements of the two compounds.\n    The observations are paired: each free proline measurement was taken from\n    the same liver as the total collagen measurement at the same index.\n\n    >>> import numpy as np\n    >>> # total collagen (mg/g dry weight of liver)\n    >>> x = np.array([7.1, 7.1, 7.2, 8.3, 9.4, 10.5, 11.4])\n    >>> # free proline (\u03bc mole/g dry weight of liver)\n    >>> y = np.array([2.8, 2.9, 2.8, 2.6, 3.5, 4.6, 5.0])\n\n    These data were analyzed in [4]_ using Spearman's correlation coefficient,\n    a statistic sensitive to monotonic correlation between the samples.\n\n    >>> from scipy import stats\n    >>> res = stats.spearmanr(x, y)\n    >>> res.statistic\n    0.7000000000000001\n\n    The value of this statistic tends to be high (close to 1) for samples with\n    a strongly positive ordinal correlation, low (close to -1) for samples with\n    a strongly negative ordinal correlation, and small in magnitude (close to\n    zero) for samples with weak ordinal correlation.\n\n    The test is performed by comparing the observed value of the\n    statistic against the null distribution: the distribution of statistic\n    values derived under the null hypothesis that total collagen and free\n    proline measurements are independent.\n\n    For this test, the statistic can be transformed such that the null\n    distribution for large samples is Student's t distribution with\n    ``len(x) - 2`` degrees of freedom.\n\n    >>> import matplotlib.pyplot as plt\n    >>> dof = len(x)-2  # len(x) == len(y)\n    >>> dist = stats.t(df=dof)\n    >>> t_vals = np.linspace(-5, 5, 100)\n    >>> pdf = dist.pdf(t_vals)\n    >>> fig, ax = plt.subplots(figsize=(8, 5))\n    >>> def plot(ax):  # we'll reuse this\n    ...     ax.plot(t_vals, pdf)\n    ...     ax.set_title(\"Spearman's Rho Test Null Distribution\")\n    ...     ax.set_xlabel(\"statistic\")\n    ...     ax.set_ylabel(\"probability density\")\n    >>> plot(ax)\n    >>> plt.show()\n\n    The comparison is quantified by the p-value: the proportion of values in\n    the null distribution as extreme or more extreme than the observed\n    value of the statistic. In a two-sided test in which the statistic is\n    positive, elements of the null distribution greater than the transformed\n    statistic and elements of the null distribution less than the negative of\n    the observed statistic are both considered \"more extreme\".\n\n    >>> fig, ax = plt.subplots(figsize=(8, 5))\n    >>> plot(ax)\n    >>> rs = res.statistic  # original statistic\n    >>> transformed = rs * np.sqrt(dof / ((rs+1.0)*(1.0-rs)))\n    >>> pvalue = dist.cdf(-transformed) + dist.sf(transformed)\n    >>> annotation = (f'p-value={pvalue:.4f}\\n(shaded area)')\n    >>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n    >>> _ = ax.annotate(annotation, (2.7, 0.025), (3, 0.03), arrowprops=props)\n    >>> i = t_vals >= transformed\n    >>> ax.fill_between(t_vals[i], y1=0, y2=pdf[i], color='C0')\n    >>> i = t_vals <= -transformed\n    >>> ax.fill_between(t_vals[i], y1=0, y2=pdf[i], color='C0')\n    >>> ax.set_xlim(-5, 5)\n    >>> ax.set_ylim(0, 0.1)\n    >>> plt.show()\n    >>> res.pvalue\n    0.07991669030889909  # two-sided p-value\n\n    If the p-value is \"small\" - that is, if there is a low probability of\n    sampling data from independent distributions that produces such an extreme\n    value of the statistic - this may be taken as evidence against the null\n    hypothesis in favor of the alternative: the distribution of total collagen\n    and free proline are *not* independent. Note that:\n\n    - The inverse is not true; that is, the test is not used to provide\n      evidence for the null hypothesis.\n    - The threshold for values that will be considered \"small\" is a choice that\n      should be made before the data is analyzed [5]_ with consideration of the\n      risks of both false positives (incorrectly rejecting the null hypothesis)\n      and false negatives (failure to reject a false null hypothesis).\n    - Small p-values are not evidence for a *large* effect; rather, they can\n      only provide evidence for a \"significant\" effect, meaning that they are\n      unlikely to have occurred under the null hypothesis.\n\n    Suppose that before performing the experiment, the authors had reason\n    to predict a positive correlation between the total collagen and free\n    proline measurements, and that they had chosen to assess the plausibility\n    of the null hypothesis against a one-sided alternative: free proline has a\n    positive ordinal correlation with total collagen. In this case, only those\n    values in the null distribution that are as great or greater than the\n    observed statistic are considered to be more extreme.\n\n    >>> res = stats.spearmanr(x, y, alternative='greater')\n    >>> res.statistic\n    0.7000000000000001  # same statistic\n    >>> fig, ax = plt.subplots(figsize=(8, 5))\n    >>> plot(ax)\n    >>> pvalue = dist.sf(transformed)\n    >>> annotation = (f'p-value={pvalue:.6f}\\n(shaded area)')\n    >>> props = dict(facecolor='black', width=1, headwidth=5, headlength=8)\n    >>> _ = ax.annotate(annotation, (3, 0.018), (3.5, 0.03), arrowprops=props)\n    >>> i = t_vals >= transformed\n    >>> ax.fill_between(t_vals[i], y1=0, y2=pdf[i], color='C0')\n    >>> ax.set_xlim(1, 5)\n    >>> ax.set_ylim(0, 0.1)\n    >>> plt.show()\n    >>> res.pvalue\n    0.03995834515444954  # one-sided p-value; half of the two-sided p-value\n\n    Note that the t-distribution provides an asymptotic approximation of the\n    null distribution; it is only accurate for samples with many observations.\n    For small samples, it may be more appropriate to perform a permutation\n    test: Under the null hypothesis that total collagen and free proline are\n    independent, each of the free proline measurements were equally likely to\n    have been observed with any of the total collagen measurements. Therefore,\n    we can form an *exact* null distribution by calculating the statistic under\n    each possible pairing of elements between ``x`` and ``y``.\n\n    >>> def statistic(x):  # explore all possible pairings by permuting `x`\n    ...     rs = stats.spearmanr(x, y).statistic  # ignore pvalue\n    ...     transformed = rs * np.sqrt(dof / ((rs+1.0)*(1.0-rs)))\n    ...     return transformed\n    >>> ref = stats.permutation_test((x,), statistic, alternative='greater',\n    ...                              permutation_type='pairings')\n    >>> fig, ax = plt.subplots(figsize=(8, 5))\n    >>> plot(ax)\n    >>> ax.hist(ref.null_distribution, np.linspace(-5, 5, 26),\n    ...         density=True)\n    >>> ax.legend(['aymptotic approximation\\n(many observations)',\n    ...            f'exact \\n({len(ref.null_distribution)} permutations)'])\n    >>> plt.show()\n    >>> ref.pvalue\n    0.04563492063492063  # exact one-sided p-value\n\n    ",
    "scipy.stats.special_ortho_group": "A Special Orthogonal matrix (SO(N)) random variable.\n\n    Return a random rotation matrix, drawn from the Haar distribution\n    (the only uniform distribution on SO(N)) with a determinant of +1.\n\n    The `dim` keyword specifies the dimension N.\n\n    Methods\n    -------\n    rvs(dim=None, size=1, random_state=None)\n        Draw random samples from SO(N).\n\n    Parameters\n    ----------\n    dim : scalar\n        Dimension of matrices\n    seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n        Used for drawing random variates.\n        If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used, seeded\n        with seed.\n        If `seed` is already a ``RandomState`` or ``Generator`` instance,\n        then that object is used.\n        Default is `None`.\n\n    Notes\n    -----\n    This class is wrapping the random_rot code from the MDP Toolkit,\n    https://github.com/mdp-toolkit/mdp-toolkit\n\n    Return a random rotation matrix, drawn from the Haar distribution\n    (the only uniform distribution on SO(N)).\n    The algorithm is described in the paper\n    Stewart, G.W., \"The efficient generation of random orthogonal\n    matrices with an application to condition estimators\", SIAM Journal\n    on Numerical Analysis, 17(3), pp. 403-409, 1980.\n    For more information see\n    https://en.wikipedia.org/wiki/Orthogonal_matrix#Randomization\n\n    See also the similar `ortho_group`. For a random rotation in three\n    dimensions, see `scipy.spatial.transform.Rotation.random`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import special_ortho_group\n    >>> x = special_ortho_group.rvs(3)\n\n    >>> np.dot(x, x.T)\n    array([[  1.00000000e+00,   1.13231364e-17,  -2.86852790e-16],\n           [  1.13231364e-17,   1.00000000e+00,  -1.46845020e-16],\n           [ -2.86852790e-16,  -1.46845020e-16,   1.00000000e+00]])\n\n    >>> import scipy.linalg\n    >>> scipy.linalg.det(x)\n    1.0\n\n    This generates one random matrix from SO(3). It is orthogonal and\n    has a determinant of 1.\n\n    Alternatively, the object may be called (as a function) to fix the `dim`\n    parameter, returning a \"frozen\" special_ortho_group random variable:\n\n    >>> rv = special_ortho_group(5)\n    >>> # Frozen object with the same methods but holding the\n    >>> # dimension parameter fixed.\n\n    See Also\n    --------\n    ortho_group, scipy.spatial.transform.Rotation.random\n\n    ",
    "scipy.stats.studentized_range": "A studentized range continuous random variable.\n\n    As an instance of the `rv_continuous` class, `studentized_range` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(k, df, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, k, df, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, k, df, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, k, df, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, k, df, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, k, df, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, k, df, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, k, df, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, k, df, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, k, df, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(k, df, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(k, df, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(k, df), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(k, df, loc=0, scale=1)\n        Median of the distribution.\n    mean(k, df, loc=0, scale=1)\n        Mean of the distribution.\n    var(k, df, loc=0, scale=1)\n        Variance of the distribution.\n    std(k, df, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, k, df, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    t: Student's t distribution\n\n    Notes\n    -----\n    The probability density function for `studentized_range` is:\n\n    .. math::\n\n         f(x; k, \\nu) = \\frac{k(k-1)\\nu^{\\nu/2}}{\\Gamma(\\nu/2)\n                        2^{\\nu/2-1}} \\int_{0}^{\\infty} \\int_{-\\infty}^{\\infty}\n                        s^{\\nu} e^{-\\nu s^2/2} \\phi(z) \\phi(sx + z)\n                        [\\Phi(sx + z) - \\Phi(z)]^{k-2} \\,dz \\,ds\n\n    for :math:`x \u2265 0`, :math:`k > 1`, and :math:`\\nu > 0`.\n\n    `studentized_range` takes ``k`` for :math:`k` and ``df`` for :math:`\\nu`\n    as shape parameters.\n\n    When :math:`\\nu` exceeds 100,000, an asymptotic approximation (infinite\n    degrees of freedom) is used to compute the cumulative distribution\n    function [4]_ and probability distribution function.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``studentized_range.pdf(x, k, df, loc, scale)`` is identically\n    equivalent to ``studentized_range.pdf(y, k, df) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n\n    .. [1] \"Studentized range distribution\",\n           https://en.wikipedia.org/wiki/Studentized_range_distribution\n    .. [2] Batista, Ben D\u00eaivide, et al. \"Externally Studentized Normal Midrange\n           Distribution.\" Ci\u00eancia e Agrotecnologia, vol. 41, no. 4, 2017, pp.\n           378-389., doi:10.1590/1413-70542017414047716.\n    .. [3] Harter, H. Leon. \"Tables of Range and Studentized Range.\" The Annals\n           of Mathematical Statistics, vol. 31, no. 4, 1960, pp. 1122-1147.\n           JSTOR, www.jstor.org/stable/2237810. Accessed 18 Feb. 2021.\n    .. [4] Lund, R. E., and J. R. Lund. \"Algorithm AS 190: Probabilities and\n           Upper Quantiles for the Studentized Range.\" Journal of the Royal\n           Statistical Society. Series C (Applied Statistics), vol. 32, no. 2,\n           1983, pp. 204-210. JSTOR, www.jstor.org/stable/2347300. Accessed 18\n           Feb. 2021.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import studentized_range\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n\n    Display the probability density function (``pdf``):\n\n    >>> k, df = 3, 10\n    >>> x = np.linspace(studentized_range.ppf(0.01, k, df),\n    ...                 studentized_range.ppf(0.99, k, df), 100)\n    >>> ax.plot(x, studentized_range.pdf(x, k, df),\n    ...         'r-', lw=5, alpha=0.6, label='studentized_range pdf')\n\n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n\n    Freeze the distribution and display the frozen ``pdf``:\n\n    >>> rv = studentized_range(k, df)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n\n    Check accuracy of ``cdf`` and ``ppf``:\n\n    >>> vals = studentized_range.ppf([0.001, 0.5, 0.999], k, df)\n    >>> np.allclose([0.001, 0.5, 0.999], studentized_range.cdf(vals, k, df))\n    True\n\n    Rather than using (``studentized_range.rvs``) to generate random variates,\n    which is very slow for this distribution, we can approximate the inverse\n    CDF using an interpolator, and then perform inverse transform sampling\n    with this approximate inverse CDF.\n\n    This distribution has an infinite but thin right tail, so we focus our\n    attention on the leftmost 99.9 percent.\n\n    >>> a, b = studentized_range.ppf([0, .999], k, df)\n    >>> a, b\n    0, 7.41058083802274\n\n    >>> from scipy.interpolate import interp1d\n    >>> rng = np.random.default_rng()\n    >>> xs = np.linspace(a, b, 50)\n    >>> cdf = studentized_range.cdf(xs, k, df)\n    # Create an interpolant of the inverse CDF\n    >>> ppf = interp1d(cdf, xs, fill_value='extrapolate')\n    # Perform inverse transform sampling using the interpolant\n    >>> r = ppf(rng.uniform(size=1000))\n\n    And compare the histogram:\n\n    >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n\n    ",
    "scipy.stats.t": "A Student's t continuous random variable.\n\n    For the noncentral t distribution, see `nct`.\n\n    As an instance of the `rv_continuous` class, `t` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(df, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, df, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, df, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, df, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, df, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, df, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, df, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, df, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, df, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, df, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(df, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(df, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(df,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(df, loc=0, scale=1)\n        Median of the distribution.\n    mean(df, loc=0, scale=1)\n        Mean of the distribution.\n    var(df, loc=0, scale=1)\n        Variance of the distribution.\n    std(df, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, df, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    nct\n\n    Notes\n    -----\n    The probability density function for `t` is:\n\n    .. math::\n\n        f(x, \\nu) = \\frac{\\Gamma((\\nu+1)/2)}\n                        {\\sqrt{\\pi \\nu} \\Gamma(\\nu/2)}\n                    (1+x^2/\\nu)^{-(\\nu+1)/2}\n\n    where :math:`x` is a real number and the degrees of freedom parameter\n    :math:`\\nu` (denoted ``df`` in the implementation) satisfies\n    :math:`\\nu > 0`. :math:`\\Gamma` is the gamma function\n    (`scipy.special.gamma`).\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``t.pdf(x, df, loc, scale)`` is identically\n    equivalent to ``t.pdf(y, df) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import t\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> df = 2.74\n    >>> mean, var, skew, kurt = t.stats(df, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(t.ppf(0.01, df),\n    ...                 t.ppf(0.99, df), 100)\n    >>> ax.plot(x, t.pdf(x, df),\n    ...        'r-', lw=5, alpha=0.6, label='t pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = t(df)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = t.ppf([0.001, 0.5, 0.999], df)\n    >>> np.allclose([0.001, 0.5, 0.999], t.cdf(vals, df))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = t.rvs(df, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.test": "\n    Run tests for this namespace\n\n    ``scipy.test()`` runs tests for all of SciPy, with the default settings.\n    When used from a submodule (e.g., ``scipy.cluster.test()``, only the tests\n    for that namespace are run.\n\n    Parameters\n    ----------\n    label : {'fast', 'full'}, optional\n        Whether to run only the fast tests, or also those marked as slow.\n        Default is 'fast'.\n    verbose : int, optional\n        Test output verbosity. Default is 1.\n    extra_argv : list, optional\n        Arguments to pass through to Pytest.\n    doctests : bool, optional\n        Whether to run doctests or not. Default is False.\n    coverage : bool, optional\n        Whether to run tests with code coverage measurements enabled.\n        Default is False.\n    tests : list of str, optional\n        List of module names to run tests for. By default, uses the module\n        from which the ``test`` function is called.\n    parallel : int, optional\n        Run tests in parallel with pytest-xdist, if number given is larger than\n        1. Default is 1.\n\n    ",
    "scipy.stats.theilslopes": "\n    Computes the Theil-Sen estimator for a set of points (x, y).\n\n    `theilslopes` implements a method for robust linear regression.  It\n    computes the slope as the median of all slopes between paired values.\n\n    Parameters\n    ----------\n    y : array_like\n        Dependent variable.\n    x : array_like or None, optional\n        Independent variable. If None, use ``arange(len(y))`` instead.\n    alpha : float, optional\n        Confidence degree between 0 and 1. Default is 95% confidence.\n        Note that `alpha` is symmetric around 0.5, i.e. both 0.1 and 0.9 are\n        interpreted as \"find the 90% confidence interval\".\n    method : {'joint', 'separate'}, optional\n        Method to be used for computing estimate for intercept.\n        Following methods are supported,\n\n            * 'joint': Uses np.median(y - slope * x) as intercept.\n            * 'separate': Uses np.median(y) - slope * np.median(x)\n                          as intercept.\n\n        The default is 'separate'.\n\n        .. versionadded:: 1.8.0\n\n    Returns\n    -------\n    result : ``TheilslopesResult`` instance\n        The return value is an object with the following attributes:\n\n        slope : float\n            Theil slope.\n        intercept : float\n            Intercept of the Theil line.\n        low_slope : float\n            Lower bound of the confidence interval on `slope`.\n        high_slope : float\n            Upper bound of the confidence interval on `slope`.\n\n    See Also\n    --------\n    siegelslopes : a similar technique using repeated medians\n\n    Notes\n    -----\n    The implementation of `theilslopes` follows [1]_. The intercept is\n    not defined in [1]_, and here it is defined as ``median(y) -\n    slope*median(x)``, which is given in [3]_. Other definitions of\n    the intercept exist in the literature such as  ``median(y - slope*x)``\n    in [4]_. The approach to compute the intercept can be determined by the\n    parameter ``method``. A confidence interval for the intercept is not\n    given as this question is not addressed in [1]_.\n\n    For compatibility with older versions of SciPy, the return value acts\n    like a ``namedtuple`` of length 4, with fields ``slope``, ``intercept``,\n    ``low_slope``, and ``high_slope``, so one can continue to write::\n\n        slope, intercept, low_slope, high_slope = theilslopes(y, x)\n\n    References\n    ----------\n    .. [1] P.K. Sen, \"Estimates of the regression coefficient based on\n           Kendall's tau\", J. Am. Stat. Assoc., Vol. 63, pp. 1379-1389, 1968.\n    .. [2] H. Theil, \"A rank-invariant method of linear and polynomial\n           regression analysis I, II and III\",  Nederl. Akad. Wetensch., Proc.\n           53:, pp. 386-392, pp. 521-525, pp. 1397-1412, 1950.\n    .. [3] W.L. Conover, \"Practical nonparametric statistics\", 2nd ed.,\n           John Wiley and Sons, New York, pp. 493.\n    .. [4] https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n\n    >>> x = np.linspace(-5, 5, num=150)\n    >>> y = x + np.random.normal(size=x.size)\n    >>> y[11:15] += 10  # add outliers\n    >>> y[-5:] -= 7\n\n    Compute the slope, intercept and 90% confidence interval.  For comparison,\n    also compute the least-squares fit with `linregress`:\n\n    >>> res = stats.theilslopes(y, x, 0.90, method='separate')\n    >>> lsq_res = stats.linregress(x, y)\n\n    Plot the results. The Theil-Sen regression line is shown in red, with the\n    dashed red lines illustrating the confidence interval of the slope (note\n    that the dashed red lines are not the confidence interval of the regression\n    as the confidence interval of the intercept is not included). The green\n    line shows the least-squares fit for comparison.\n\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> ax.plot(x, y, 'b.')\n    >>> ax.plot(x, res[1] + res[0] * x, 'r-')\n    >>> ax.plot(x, res[1] + res[2] * x, 'r--')\n    >>> ax.plot(x, res[1] + res[3] * x, 'r--')\n    >>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, 'g-')\n    >>> plt.show()\n\n    ",
    "scipy.stats.tiecorrect": "Tie correction factor for Mann-Whitney U and Kruskal-Wallis H tests.\n\n    Parameters\n    ----------\n    rankvals : array_like\n        A 1-D sequence of ranks.  Typically this will be the array\n        returned by `~scipy.stats.rankdata`.\n\n    Returns\n    -------\n    factor : float\n        Correction factor for U or H.\n\n    See Also\n    --------\n    rankdata : Assign ranks to the data\n    mannwhitneyu : Mann-Whitney rank test\n    kruskal : Kruskal-Wallis H test\n\n    References\n    ----------\n    .. [1] Siegel, S. (1956) Nonparametric Statistics for the Behavioral\n           Sciences.  New York: McGraw-Hill.\n\n    Examples\n    --------\n    >>> from scipy.stats import tiecorrect, rankdata\n    >>> tiecorrect([1, 2.5, 2.5, 4])\n    0.9\n    >>> ranks = rankdata([1, 3, 2, 4, 5, 7, 2, 8, 4])\n    >>> ranks\n    array([ 1. ,  4. ,  2.5,  5.5,  7. ,  8. ,  2.5,  9. ,  5.5])\n    >>> tiecorrect(ranks)\n    0.9833333333333333\n\n    ",
    "scipy.stats.tmax": "    \n\n\nCompute the trimmed maximum.\n\nThis function computes the maximum value of an array along a given axis,\nwhile ignoring values larger than a specified upper limit.\n\nParameters\n----------\na : array_like\n    Array of values.\nupperlimit : None or float, optional\n    Values in the input array greater than the given limit will be ignored.\n    When upperlimit is None, then all values are used. The default value\n    is None.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\ninclusive : {True, False}, optional\n    This flag determines whether values exactly equal to the upper limit\n    are included.  The default value is True.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\ntmax : float, int or ndarray\n    Trimmed maximum.\n\nNotes\n-----\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy import stats\n>>> x = np.arange(20)\n>>> stats.tmax(x)\n19\n\n>>> stats.tmax(x, 13)\n13\n\n>>> stats.tmax(x, 13, inclusive=False)\n12\n",
    "scipy.stats.tmean": "    \n\n\nCompute the trimmed mean.\n\nThis function finds the arithmetic mean of given values, ignoring values\noutside the given `limits`.\n\nParameters\n----------\na : array_like\n    Array of values.\nlimits : None or (lower limit, upper limit), optional\n    Values in the input array less than the lower limit or greater than the\n    upper limit will be ignored.  When limits is None (default), then all\n    values are used.  Either of the limit values in the tuple can also be\n    None representing a half-open interval.\ninclusive : (bool, bool), optional\n    A tuple consisting of the (lower flag, upper flag).  These flags\n    determine whether values exactly equal to the lower or upper limits\n    are included.  The default value is (True, True).\naxis : int or None, default: None\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\ntmean : ndarray\n    Trimmed mean.\n\nSee Also\n--------\n\n:func:`trim_mean`\n    Returns mean after trimming a proportion from both tails.\n\n\nNotes\n-----\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy import stats\n>>> x = np.arange(20)\n>>> stats.tmean(x)\n9.5\n>>> stats.tmean(x, (3,17))\n10.0\n",
    "scipy.stats.tmin": "    \n\n\nCompute the trimmed minimum.\n\nThis function finds the minimum value of an array `a` along the\nspecified axis, but only considering values greater than a specified\nlower limit.\n\nParameters\n----------\na : array_like\n    Array of values.\nlowerlimit : None or float, optional\n    Values in the input array less than the given limit will be ignored.\n    When lowerlimit is None, then all values are used. The default value\n    is None.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\ninclusive : {True, False}, optional\n    This flag determines whether values exactly equal to the lower limit\n    are included.  The default value is True.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\ntmin : float, int or ndarray\n    Trimmed minimum.\n\nNotes\n-----\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy import stats\n>>> x = np.arange(20)\n>>> stats.tmin(x)\n0\n\n>>> stats.tmin(x, 13)\n13\n\n>>> stats.tmin(x, 13, inclusive=False)\n14\n",
    "scipy.stats.trapezoid": "A trapezoidal continuous random variable.\n\n    As an instance of the `rv_continuous` class, `trapezoid` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, d, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, d, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, d, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, d, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, d, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, d, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, d, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, d, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, d, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, d, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, d, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, d, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c, d), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, d, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, d, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, d, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, d, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, d, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The trapezoidal distribution can be represented with an up-sloping line\n    from ``loc`` to ``(loc + c*scale)``, then constant to ``(loc + d*scale)``\n    and then downsloping from ``(loc + d*scale)`` to ``(loc+scale)``.  This\n    defines the trapezoid base from ``loc`` to ``(loc+scale)`` and the flat\n    top from ``c`` to ``d`` proportional to the position along the base\n    with ``0 <= c <= d <= 1``.  When ``c=d``, this is equivalent to `triang`\n    with the same values for `loc`, `scale` and `c`.\n    The method of [1]_ is used for computing moments.\n\n    `trapezoid` takes :math:`c` and :math:`d` as shape parameters.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``trapezoid.pdf(x, c, d, loc, scale)`` is identically\n    equivalent to ``trapezoid.pdf(y, c, d) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    The standard form is in the range [0, 1] with c the mode.\n    The location parameter shifts the start to `loc`.\n    The scale parameter changes the width from 1 to `scale`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import trapezoid\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c, d = 0.2, 0.8\n    >>> mean, var, skew, kurt = trapezoid.stats(c, d, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(trapezoid.ppf(0.01, c, d),\n    ...                 trapezoid.ppf(0.99, c, d), 100)\n    >>> ax.plot(x, trapezoid.pdf(x, c, d),\n    ...        'r-', lw=5, alpha=0.6, label='trapezoid pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = trapezoid(c, d)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = trapezoid.ppf([0.001, 0.5, 0.999], c, d)\n    >>> np.allclose([0.001, 0.5, 0.999], trapezoid.cdf(vals, c, d))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = trapezoid.rvs(c, d, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    References\n    ----------\n    .. [1] Kacker, R.N. and Lawrence, J.F. (2007). Trapezoidal and triangular\n       distributions for Type B evaluation of standard uncertainty.\n       Metrologia 44, 117-127. :doi:`10.1088/0026-1394/44/2/003`\n\n\n    ",
    "scipy.stats.trapz": "\n\n    .. deprecated:: 1.14.0\n        `trapz` is deprecated and will be removed in SciPy 1.16.\n        Plese use `trapezoid` instead!\n    ",
    "scipy.stats.triang": "A triangular continuous random variable.\n\n    As an instance of the `rv_continuous` class, `triang` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The triangular distribution can be represented with an up-sloping line from\n    ``loc`` to ``(loc + c*scale)`` and then downsloping for ``(loc + c*scale)``\n    to ``(loc + scale)``.\n\n    `triang` takes ``c`` as a shape parameter for :math:`0 \\le c \\le 1`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``triang.pdf(x, c, loc, scale)`` is identically\n    equivalent to ``triang.pdf(y, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    The standard form is in the range [0, 1] with c the mode.\n    The location parameter shifts the start to `loc`.\n    The scale parameter changes the width from 1 to `scale`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import triang\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c = 0.158\n    >>> mean, var, skew, kurt = triang.stats(c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(triang.ppf(0.01, c),\n    ...                 triang.ppf(0.99, c), 100)\n    >>> ax.plot(x, triang.pdf(x, c),\n    ...        'r-', lw=5, alpha=0.6, label='triang pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = triang(c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = triang.ppf([0.001, 0.5, 0.999], c)\n    >>> np.allclose([0.001, 0.5, 0.999], triang.cdf(vals, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = triang.rvs(c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.trim1": "Slice off a proportion from ONE end of the passed array distribution.\n\n    If `proportiontocut` = 0.1, slices off 'leftmost' or 'rightmost'\n    10% of scores. The lowest or highest values are trimmed (depending on\n    the tail).\n    Slice off less if proportion results in a non-integer slice index\n    (i.e. conservatively slices off `proportiontocut` ).\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    proportiontocut : float\n        Fraction to cut off of 'left' or 'right' of distribution.\n    tail : {'left', 'right'}, optional\n        Defaults to 'right'.\n    axis : int or None, optional\n        Axis along which to trim data. Default is 0. If None, compute over\n        the whole array `a`.\n\n    Returns\n    -------\n    trim1 : ndarray\n        Trimmed version of array `a`. The order of the trimmed content is\n        undefined.\n\n    Examples\n    --------\n    Create an array of 10 values and trim 20% of its lowest values:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n    >>> stats.trim1(a, 0.2, 'left')\n    array([2, 4, 3, 5, 6, 7, 8, 9])\n\n    Note that the elements of the input array are trimmed by value, but the\n    output array is not necessarily sorted.\n\n    The proportion to trim is rounded down to the nearest integer. For\n    instance, trimming 25% of the values from an array of 10 values will\n    return an array of 8 values:\n\n    >>> b = np.arange(10)\n    >>> stats.trim1(b, 1/4).shape\n    (8,)\n\n    Multidimensional arrays can be trimmed along any axis or across the entire\n    array:\n\n    >>> c = [2, 4, 6, 8, 0, 1, 3, 5, 7, 9]\n    >>> d = np.array([a, b, c])\n    >>> stats.trim1(d, 0.8, axis=0).shape\n    (1, 10)\n    >>> stats.trim1(d, 0.8, axis=1).shape\n    (3, 2)\n    >>> stats.trim1(d, 0.8, axis=None).shape\n    (6,)\n\n    ",
    "scipy.stats.trim_mean": "Return mean of array after trimming a specified fraction of extreme values\n\n    Removes the specified proportion of elements from *each* end of the\n    sorted array, then computes the mean of the remaining elements.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    proportiontocut : float\n        Fraction of the most positive and most negative elements to remove.\n        When the specified proportion does not result in an integer number of\n        elements, the number of elements to trim is rounded down.\n    axis : int or None, default: 0\n        Axis along which the trimmed means are computed.\n        If None, compute over the raveled array.\n\n    Returns\n    -------\n    trim_mean : ndarray\n        Mean of trimmed array.\n\n    See Also\n    --------\n    trimboth : Remove a proportion of elements from each end of an array.\n    tmean : Compute the mean after trimming values outside specified limits.\n\n    Notes\n    -----\n    For 1-D array `a`, `trim_mean` is approximately equivalent to the following\n    calculation::\n\n        import numpy as np\n        a = np.sort(a)\n        m = int(proportiontocut * len(a))\n        np.mean(a[m: len(a) - m])\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = [1, 2, 3, 5]\n    >>> stats.trim_mean(x, 0.25)\n    2.5\n\n    When the specified proportion does not result in an integer number of\n    elements, the number of elements to trim is rounded down.\n\n    >>> stats.trim_mean(x, 0.24999) == np.mean(x)\n    True\n\n    Use `axis` to specify the axis along which the calculation is performed.\n\n    >>> x2 = [[1, 2, 3, 5],\n    ...       [10, 20, 30, 50]]\n    >>> stats.trim_mean(x2, 0.25)\n    array([ 5.5, 11. , 16.5, 27.5])\n    >>> stats.trim_mean(x2, 0.25, axis=1)\n    array([ 2.5, 25. ])\n\n    ",
    "scipy.stats.trimboth": "Slice off a proportion of items from both ends of an array.\n\n    Slice off the passed proportion of items from both ends of the passed\n    array (i.e., with `proportiontocut` = 0.1, slices leftmost 10% **and**\n    rightmost 10% of scores). The trimmed values are the lowest and\n    highest ones.\n    Slice off less if proportion results in a non-integer slice index (i.e.\n    conservatively slices off `proportiontocut`).\n\n    Parameters\n    ----------\n    a : array_like\n        Data to trim.\n    proportiontocut : float\n        Proportion (in range 0-1) of total data set to trim of each end.\n    axis : int or None, optional\n        Axis along which to trim data. Default is 0. If None, compute over\n        the whole array `a`.\n\n    Returns\n    -------\n    out : ndarray\n        Trimmed version of array `a`. The order of the trimmed content\n        is undefined.\n\n    See Also\n    --------\n    trim_mean\n\n    Examples\n    --------\n    Create an array of 10 values and trim 10% of those values from each end:\n\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n    >>> stats.trimboth(a, 0.1)\n    array([1, 3, 2, 4, 5, 6, 7, 8])\n\n    Note that the elements of the input array are trimmed by value, but the\n    output array is not necessarily sorted.\n\n    The proportion to trim is rounded down to the nearest integer. For\n    instance, trimming 25% of the values from each end of an array of 10\n    values will return an array of 6 values:\n\n    >>> b = np.arange(10)\n    >>> stats.trimboth(b, 1/4).shape\n    (6,)\n\n    Multidimensional arrays can be trimmed along any axis or across the entire\n    array:\n\n    >>> c = [2, 4, 6, 8, 0, 1, 3, 5, 7, 9]\n    >>> d = np.array([a, b, c])\n    >>> stats.trimboth(d, 0.4, axis=0).shape\n    (1, 10)\n    >>> stats.trimboth(d, 0.4, axis=1).shape\n    (3, 2)\n    >>> stats.trimboth(d, 0.4, axis=None).shape\n    (6,)\n\n    ",
    "scipy.stats.truncexpon": "A truncated exponential continuous random variable.\n\n    As an instance of the `rv_continuous` class, `truncexpon` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(b, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, b, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, b, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, b, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, b, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, b, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, b, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, b, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, b, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, b, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(b, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(b, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(b,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(b, loc=0, scale=1)\n        Median of the distribution.\n    mean(b, loc=0, scale=1)\n        Mean of the distribution.\n    var(b, loc=0, scale=1)\n        Variance of the distribution.\n    std(b, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, b, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `truncexpon` is:\n\n    .. math::\n\n        f(x, b) = \\frac{\\exp(-x)}{1 - \\exp(-b)}\n\n    for :math:`0 <= x <= b`.\n\n    `truncexpon` takes ``b`` as a shape parameter for :math:`b`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``truncexpon.pdf(x, b, loc, scale)`` is identically\n    equivalent to ``truncexpon.pdf(y, b) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import truncexpon\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> b = 4.69\n    >>> mean, var, skew, kurt = truncexpon.stats(b, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(truncexpon.ppf(0.01, b),\n    ...                 truncexpon.ppf(0.99, b), 100)\n    >>> ax.plot(x, truncexpon.pdf(x, b),\n    ...        'r-', lw=5, alpha=0.6, label='truncexpon pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = truncexpon(b)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = truncexpon.ppf([0.001, 0.5, 0.999], b)\n    >>> np.allclose([0.001, 0.5, 0.999], truncexpon.cdf(vals, b))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = truncexpon.rvs(b, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.truncnorm": "A truncated normal continuous random variable.\n\n    As an instance of the `rv_continuous` class, `truncnorm` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, a, b, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, a, b, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, a, b, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, a, b, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, a, b, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, a, b, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, a, b, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, b, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, a, b, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(a, b, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, b, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, b, loc=0, scale=1)\n        Median of the distribution.\n    mean(a, b, loc=0, scale=1)\n        Mean of the distribution.\n    var(a, b, loc=0, scale=1)\n        Variance of the distribution.\n    std(a, b, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, a, b, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    This distribution is the normal distribution centered on ``loc`` (default\n    0), with standard deviation ``scale`` (default 1), and truncated at ``a``\n    and ``b`` *standard deviations* from ``loc``. For arbitrary ``loc`` and\n    ``scale``, ``a`` and ``b`` are *not* the abscissae at which the shifted\n    and scaled distribution is truncated.\n\n    .. note::\n        If ``a_trunc`` and ``b_trunc`` are the abscissae at which we wish\n        to truncate the distribution (as opposed to the number of standard\n        deviations from ``loc``), then we can calculate the distribution\n        parameters ``a`` and ``b`` as follows::\n\n            a, b = (a_trunc - loc) / scale, (b_trunc - loc) / scale\n\n        This is a common point of confusion. For additional clarification,\n        please see the example below.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import truncnorm\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a, b = 0.1, 2\n    >>> mean, var, skew, kurt = truncnorm.stats(a, b, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(truncnorm.ppf(0.01, a, b),\n    ...                 truncnorm.ppf(0.99, a, b), 100)\n    >>> ax.plot(x, truncnorm.pdf(x, a, b),\n    ...        'r-', lw=5, alpha=0.6, label='truncnorm pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = truncnorm(a, b)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = truncnorm.ppf([0.001, 0.5, 0.999], a, b)\n    >>> np.allclose([0.001, 0.5, 0.999], truncnorm.cdf(vals, a, b))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = truncnorm.rvs(a, b, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    In the examples above, ``loc=0`` and ``scale=1``, so the plot is truncated\n    at ``a`` on the left and ``b`` on the right. However, suppose we were to\n    produce the same histogram with ``loc = 1`` and ``scale=0.5``.\n\n    >>> loc, scale = 1, 0.5\n    >>> rv = truncnorm(a, b, loc=loc, scale=scale)\n    >>> x = np.linspace(truncnorm.ppf(0.01, a, b),\n    ...                 truncnorm.ppf(0.99, a, b), 100)\n    >>> r = rv.rvs(size=1000)\n\n    >>> fig, ax = plt.subplots(1, 1)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim(a, b)\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n\n    Note that the distribution is no longer appears to be truncated at\n    abscissae ``a`` and ``b``. That is because the *standard* normal\n    distribution is first truncated at ``a`` and ``b``, *then* the resulting\n    distribution is scaled by ``scale`` and shifted by ``loc``. If we instead\n    want the shifted and scaled distribution to be truncated at ``a`` and\n    ``b``, we need to transform these values before passing them as the\n    distribution parameters.\n\n    >>> a_transformed, b_transformed = (a - loc) / scale, (b - loc) / scale\n    >>> rv = truncnorm(a_transformed, b_transformed, loc=loc, scale=scale)\n    >>> x = np.linspace(truncnorm.ppf(0.01, a, b),\n    ...                 truncnorm.ppf(0.99, a, b), 100)\n    >>> r = rv.rvs(size=10000)\n\n    >>> fig, ax = plt.subplots(1, 1)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim(a-0.1, b+0.1)\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    ",
    "scipy.stats.truncpareto": "An upper truncated Pareto continuous random variable.\n\n    As an instance of the `rv_continuous` class, `truncpareto` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(b, c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, b, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, b, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, b, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, b, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, b, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, b, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, b, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, b, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, b, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(b, c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(b, c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(b, c), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(b, c, loc=0, scale=1)\n        Median of the distribution.\n    mean(b, c, loc=0, scale=1)\n        Mean of the distribution.\n    var(b, c, loc=0, scale=1)\n        Variance of the distribution.\n    std(b, c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, b, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    pareto : Pareto distribution\n\n    Notes\n    -----\n    The probability density function for `truncpareto` is:\n\n    .. math::\n\n        f(x, b, c) = \\frac{b}{1 - c^{-b}} \\frac{1}{x^{b+1}}\n\n    for :math:`b > 0`, :math:`c > 1` and :math:`1 \\le x \\le c`.\n\n    `truncpareto` takes `b` and `c` as shape parameters for :math:`b` and\n    :math:`c`.\n\n    Notice that the upper truncation value :math:`c` is defined in\n    standardized form so that random values of an unscaled, unshifted variable\n    are within the range ``[1, c]``.\n    If ``u_r`` is the upper bound to a scaled and/or shifted variable,\n    then ``c = (u_r - loc) / scale``. In other words, the support of the\n    distribution becomes ``(scale + loc) <= x <= (c*scale + loc)`` when\n    `scale` and/or `loc` are provided.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``truncpareto.pdf(x, b, c, loc, scale)`` is identically\n    equivalent to ``truncpareto.pdf(y, b, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    .. [1] Burroughs, S. M., and Tebbens S. F.\n        \"Upper-truncated power laws in natural systems.\"\n        Pure and Applied Geophysics 158.4 (2001): 741-757.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import truncpareto\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> b, c = 2, 5\n    >>> mean, var, skew, kurt = truncpareto.stats(b, c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(truncpareto.ppf(0.01, b, c),\n    ...                 truncpareto.ppf(0.99, b, c), 100)\n    >>> ax.plot(x, truncpareto.pdf(x, b, c),\n    ...        'r-', lw=5, alpha=0.6, label='truncpareto pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = truncpareto(b, c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = truncpareto.ppf([0.001, 0.5, 0.999], b, c)\n    >>> np.allclose([0.001, 0.5, 0.999], truncpareto.cdf(vals, b, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = truncpareto.rvs(b, c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.truncweibull_min": "A doubly truncated Weibull minimum continuous random variable.\n\n    As an instance of the `rv_continuous` class, `truncweibull_min` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, a, b, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, a, b, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, a, b, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, a, b, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, a, b, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, a, b, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, a, b, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, a, b, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, a, b, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, a, b, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, a, b, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, a, b, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c, a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, a, b, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, a, b, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, a, b, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, a, b, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, a, b, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    weibull_min, truncexpon\n\n    Notes\n    -----\n    The probability density function for `truncweibull_min` is:\n\n    .. math::\n\n        f(x, a, b, c) = \\frac{c x^{c-1} \\exp(-x^c)}{\\exp(-a^c) - \\exp(-b^c)}\n\n    for :math:`a < x <= b`, :math:`0 \\le a < b` and :math:`c > 0`.\n\n    `truncweibull_min` takes :math:`a`, :math:`b`, and :math:`c` as shape\n    parameters.\n\n    Notice that the truncation values, :math:`a` and :math:`b`, are defined in\n    standardized form:\n\n    .. math::\n\n        a = (u_l - loc)/scale\n        b = (u_r - loc)/scale\n\n    where :math:`u_l` and :math:`u_r` are the specific left and right\n    truncation values, respectively. In other words, the support of the\n    distribution becomes :math:`(a*scale + loc) < x <= (b*scale + loc)` when\n    :math:`loc` and/or :math:`scale` are provided.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``truncweibull_min.pdf(x, c, a, b, loc, scale)`` is identically\n    equivalent to ``truncweibull_min.pdf(y, c, a, b) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n\n    .. [1] Rinne, H. \"The Weibull Distribution: A Handbook\". CRC Press (2009).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import truncweibull_min\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c, a, b = 2.5, 0.25, 1.75\n    >>> mean, var, skew, kurt = truncweibull_min.stats(c, a, b, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(truncweibull_min.ppf(0.01, c, a, b),\n    ...                 truncweibull_min.ppf(0.99, c, a, b), 100)\n    >>> ax.plot(x, truncweibull_min.pdf(x, c, a, b),\n    ...        'r-', lw=5, alpha=0.6, label='truncweibull_min pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = truncweibull_min(c, a, b)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = truncweibull_min.ppf([0.001, 0.5, 0.999], c, a, b)\n    >>> np.allclose([0.001, 0.5, 0.999], truncweibull_min.cdf(vals, c, a, b))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = truncweibull_min.rvs(c, a, b, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.tsem": "    \n\n\nCompute the trimmed standard error of the mean.\n\nThis function finds the standard error of the mean for given\nvalues, ignoring values outside the given `limits`.\n\nParameters\n----------\na : array_like\n    Array of values.\nlimits : None or (lower limit, upper limit), optional\n    Values in the input array less than the lower limit or greater than the\n    upper limit will be ignored. When limits is None, then all values are\n    used. Either of the limit values in the tuple can also be None\n    representing a half-open interval.  The default value is None.\ninclusive : (bool, bool), optional\n    A tuple consisting of the (lower flag, upper flag).  These flags\n    determine whether values exactly equal to the lower or upper limits\n    are included.  The default value is (True, True).\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nddof : int, optional\n    Delta degrees of freedom.  Default is 1.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\ntsem : float\n    Trimmed standard error of the mean.\n\nNotes\n-----\n`tsem` uses unbiased sample standard deviation, i.e. it uses a\ncorrection factor ``n / (n - 1)``.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy import stats\n>>> x = np.arange(20)\n>>> stats.tsem(x)\n1.3228756555322954\n>>> stats.tsem(x, (3,17))\n1.1547005383792515\n",
    "scipy.stats.tstd": "    \n\n\nCompute the trimmed sample standard deviation.\n\nThis function finds the sample standard deviation of given values,\nignoring values outside the given `limits`.\n\nParameters\n----------\na : array_like\n    Array of values.\nlimits : None or (lower limit, upper limit), optional\n    Values in the input array less than the lower limit or greater than the\n    upper limit will be ignored. When limits is None, then all values are\n    used. Either of the limit values in the tuple can also be None\n    representing a half-open interval.  The default value is None.\ninclusive : (bool, bool), optional\n    A tuple consisting of the (lower flag, upper flag).  These flags\n    determine whether values exactly equal to the lower or upper limits\n    are included.  The default value is (True, True).\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nddof : int, optional\n    Delta degrees of freedom.  Default is 1.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\ntstd : float\n    Trimmed sample standard deviation.\n\nNotes\n-----\n`tstd` computes the unbiased sample standard deviation, i.e. it uses a\ncorrection factor ``n / (n - 1)``.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy import stats\n>>> x = np.arange(20)\n>>> stats.tstd(x)\n5.9160797830996161\n>>> stats.tstd(x, (3,17))\n4.4721359549995796\n",
    "scipy.stats.ttest_1samp": "    \n\n\nCalculate the T-test for the mean of ONE group of scores.\n\nThis is a test for the null hypothesis that the expected value\n(mean) of a sample of independent observations `a` is equal to the given\npopulation mean, `popmean`.\n\nParameters\n----------\na : array_like\n    Sample observations.\npopmean : float or array_like\n    Expected value in null hypothesis. If array_like, then its length along\n    `axis` must equal 1, and it must otherwise be broadcastable with `a`.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nalternative : {'two-sided', 'less', 'greater'}, optional\n    Defines the alternative hypothesis.\n    The following options are available (default is 'two-sided'):\n    \n    * 'two-sided': the mean of the underlying distribution of the sample\n      is different than the given population mean (`popmean`)\n    * 'less': the mean of the underlying distribution of the sample is\n      less than the given population mean (`popmean`)\n    * 'greater': the mean of the underlying distribution of the sample is\n      greater than the given population mean (`popmean`)\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nresult : `~scipy.stats._result_classes.TtestResult`\n    An object with the following attributes:\n    \n    statistic : float or array\n        The t-statistic.\n    pvalue : float or array\n        The p-value associated with the given alternative.\n    df : float or array\n        The number of degrees of freedom used in calculation of the\n        t-statistic; this is one less than the size of the sample\n        (``a.shape[axis]``).\n    \n        .. versionadded:: 1.10.0\n    \n    The object also has the following method:\n    \n    confidence_interval(confidence_level=0.95)\n        Computes a confidence interval around the population\n        mean for the given confidence level.\n        The confidence interval is returned in a ``namedtuple`` with\n        fields `low` and `high`.\n    \n        .. versionadded:: 1.10.0\n\nNotes\n-----\nThe statistic is calculated as ``(np.mean(a) - popmean)/se``, where\n``se`` is the standard error. Therefore, the statistic will be positive\nwhen the sample mean is greater than the population mean and negative when\nthe sample mean is less than the population mean.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nExamples\n--------\nSuppose we wish to test the null hypothesis that the mean of a population\nis equal to 0.5. We choose a confidence level of 99%; that is, we will\nreject the null hypothesis in favor of the alternative if the p-value is\nless than 0.01.\n\nWhen testing random variates from the standard uniform distribution, which\nhas a mean of 0.5, we expect the data to be consistent with the null\nhypothesis most of the time.\n\n>>> import numpy as np\n>>> from scipy import stats\n>>> rng = np.random.default_rng()\n>>> rvs = stats.uniform.rvs(size=50, random_state=rng)\n>>> stats.ttest_1samp(rvs, popmean=0.5)\nTtestResult(statistic=2.456308468440, pvalue=0.017628209047638, df=49)\n\nAs expected, the p-value of 0.017 is not below our threshold of 0.01, so\nwe cannot reject the null hypothesis.\n\nWhen testing data from the standard *normal* distribution, which has a mean\nof 0, we would expect the null hypothesis to be rejected.\n\n>>> rvs = stats.norm.rvs(size=50, random_state=rng)\n>>> stats.ttest_1samp(rvs, popmean=0.5)\nTtestResult(statistic=-7.433605518875, pvalue=1.416760157221e-09, df=49)\n\nIndeed, the p-value is lower than our threshold of 0.01, so we reject the\nnull hypothesis in favor of the default \"two-sided\" alternative: the mean\nof the population is *not* equal to 0.5.\n\nHowever, suppose we were to test the null hypothesis against the\none-sided alternative that the mean of the population is *greater* than\n0.5. Since the mean of the standard normal is less than 0.5, we would not\nexpect the null hypothesis to be rejected.\n\n>>> stats.ttest_1samp(rvs, popmean=0.5, alternative='greater')\nTtestResult(statistic=-7.433605518875, pvalue=0.99999999929, df=49)\n\nUnsurprisingly, with a p-value greater than our threshold, we would not\nreject the null hypothesis.\n\nNote that when working with a confidence level of 99%, a true null\nhypothesis will be rejected approximately 1% of the time.\n\n>>> rvs = stats.uniform.rvs(size=(100, 50), random_state=rng)\n>>> res = stats.ttest_1samp(rvs, popmean=0.5, axis=1)\n>>> np.sum(res.pvalue < 0.01)\n1\n\nIndeed, even though all 100 samples above were drawn from the standard\nuniform distribution, which *does* have a population mean of 0.5, we would\nmistakenly reject the null hypothesis for one of them.\n\n`ttest_1samp` can also compute a confidence interval around the population\nmean.\n\n>>> rvs = stats.norm.rvs(size=50, random_state=rng)\n>>> res = stats.ttest_1samp(rvs, popmean=0)\n>>> ci = res.confidence_interval(confidence_level=0.95)\n>>> ci\nConfidenceInterval(low=-0.3193887540880017, high=0.2898583388980972)\n\nThe bounds of the 95% confidence interval are the\nminimum and maximum values of the parameter `popmean` for which the\np-value of the test would be 0.05.\n\n>>> res = stats.ttest_1samp(rvs, popmean=ci.low)\n>>> np.testing.assert_allclose(res.pvalue, 0.05)\n>>> res = stats.ttest_1samp(rvs, popmean=ci.high)\n>>> np.testing.assert_allclose(res.pvalue, 0.05)\n\nUnder certain assumptions about the population from which a sample\nis drawn, the confidence interval with confidence level 95% is expected\nto contain the true population mean in 95% of sample replications.\n\n>>> rvs = stats.norm.rvs(size=(50, 1000), loc=1, random_state=rng)\n>>> res = stats.ttest_1samp(rvs, popmean=0)\n>>> ci = res.confidence_interval()\n>>> contains_pop_mean = (ci.low < 1) & (ci.high > 1)\n>>> contains_pop_mean.sum()\n953\n",
    "scipy.stats.ttest_ind": "    \n\n\nCalculate the T-test for the means of *two independent* samples of scores.\n\nThis is a test for the null hypothesis that 2 independent samples\nhave identical average (expected) values. This test assumes that the\npopulations have identical variances by default.\n\nParameters\n----------\na, b : array_like\n    The arrays must have the same shape, except in the dimension\n    corresponding to `axis` (the first, by default).\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nequal_var : bool, optional\n    If True (default), perform a standard independent 2 sample test\n    that assumes equal population variances [1]_.\n    If False, perform Welch's t-test, which does not assume equal\n    population variance [2]_.\n    \n    .. versionadded:: 0.11.0\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\npermutations : non-negative int, np.inf, or None (default), optional\n    If 0 or None (default), use the t-distribution to calculate p-values.\n    Otherwise, `permutations` is  the number of random permutations that\n    will be used to estimate p-values using a permutation test. If\n    `permutations` equals or exceeds the number of distinct partitions of\n    the pooled data, an exact test is performed instead (i.e. each\n    distinct partition is used exactly once). See Notes for details.\n    \n    .. versionadded:: 1.7.0\nrandom_state : {None, int, `numpy.random.Generator`,\n        `numpy.random.RandomState`}, optional\n    \n    If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n    singleton is used.\n    If `seed` is an int, a new ``RandomState`` instance is used,\n    seeded with `seed`.\n    If `seed` is already a ``Generator`` or ``RandomState`` instance then\n    that instance is used.\n    \n    Pseudorandom number generator state used to generate permutations\n    (used only when `permutations` is not None).\n    \n    .. versionadded:: 1.7.0\nalternative : {'two-sided', 'less', 'greater'}, optional\n    Defines the alternative hypothesis.\n    The following options are available (default is 'two-sided'):\n    \n    * 'two-sided': the means of the distributions underlying the samples\n      are unequal.\n    * 'less': the mean of the distribution underlying the first sample\n      is less than the mean of the distribution underlying the second\n      sample.\n    * 'greater': the mean of the distribution underlying the first\n      sample is greater than the mean of the distribution underlying\n      the second sample.\n    \n    .. versionadded:: 1.6.0\ntrim : float, optional\n    If nonzero, performs a trimmed (Yuen's) t-test.\n    Defines the fraction of elements to be trimmed from each end of the\n    input samples. If 0 (default), no elements will be trimmed from either\n    side. The number of trimmed elements from each tail is the floor of the\n    trim times the number of elements. Valid range is [0, .5).\n    \n    .. versionadded:: 1.7\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nresult : `~scipy.stats._result_classes.TtestResult`\n    An object with the following attributes:\n    \n    statistic : float or ndarray\n        The t-statistic.\n    pvalue : float or ndarray\n        The p-value associated with the given alternative.\n    df : float or ndarray\n        The number of degrees of freedom used in calculation of the\n        t-statistic. This is always NaN for a permutation t-test.\n    \n        .. versionadded:: 1.11.0\n    \n    The object also has the following method:\n    \n    confidence_interval(confidence_level=0.95)\n        Computes a confidence interval around the difference in\n        population means for the given confidence level.\n        The confidence interval is returned in a ``namedtuple`` with\n        fields ``low`` and ``high``.\n        When a permutation t-test is performed, the confidence interval\n        is not computed, and fields ``low`` and ``high`` contain NaN.\n    \n        .. versionadded:: 1.11.0\n\nNotes\n-----\nSuppose we observe two independent samples, e.g. flower petal lengths, and\nwe are considering whether the two samples were drawn from the same\npopulation (e.g. the same species of flower or two species with similar\npetal characteristics) or two different populations.\n\nThe t-test quantifies the difference between the arithmetic means\nof the two samples. The p-value quantifies the probability of observing\nas or more extreme values assuming the null hypothesis, that the\nsamples are drawn from populations with the same population means, is true.\nA p-value larger than a chosen threshold (e.g. 5% or 1%) indicates that\nour observation is not so unlikely to have occurred by chance. Therefore,\nwe do not reject the null hypothesis of equal population means.\nIf the p-value is smaller than our threshold, then we have evidence\nagainst the null hypothesis of equal population means.\n\nBy default, the p-value is determined by comparing the t-statistic of the\nobserved data against a theoretical t-distribution.\nWhen ``1 < permutations < binom(n, k)``, where\n\n* ``k`` is the number of observations in `a`,\n* ``n`` is the total number of observations in `a` and `b`, and\n* ``binom(n, k)`` is the binomial coefficient (``n`` choose ``k``),\n\nthe data are pooled (concatenated), randomly assigned to either group `a`\nor `b`, and the t-statistic is calculated. This process is performed\nrepeatedly (`permutation` times), generating a distribution of the\nt-statistic under the null hypothesis, and the t-statistic of the observed\ndata is compared to this distribution to determine the p-value.\nSpecifically, the p-value reported is the \"achieved significance level\"\n(ASL) as defined in 4.4 of [3]_. Note that there are other ways of\nestimating p-values using randomized permutation tests; for other\noptions, see the more general `permutation_test`.\n\nWhen ``permutations >= binom(n, k)``, an exact test is performed: the data\nare partitioned between the groups in each distinct way exactly once.\n\nThe permutation test can be computationally expensive and not necessarily\nmore accurate than the analytical test, but it does not make strong\nassumptions about the shape of the underlying distribution.\n\nUse of trimming is commonly referred to as the trimmed t-test. At times\ncalled Yuen's t-test, this is an extension of Welch's t-test, with the\ndifference being the use of winsorized means in calculation of the variance\nand the trimmed sample size in calculation of the statistic. Trimming is\nrecommended if the underlying distribution is long-tailed or contaminated\nwith outliers [4]_.\n\nThe statistic is calculated as ``(np.mean(a) - np.mean(b))/se``, where\n``se`` is the standard error. Therefore, the statistic will be positive\nwhen the sample mean of `a` is greater than the sample mean of `b` and\nnegative when the sample mean of `a` is less than the sample mean of\n`b`.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] https://en.wikipedia.org/wiki/T-test#Independent_two-sample_t-test\n\n.. [2] https://en.wikipedia.org/wiki/Welch%27s_t-test\n\n.. [3] B. Efron and T. Hastie. Computer Age Statistical Inference. (2016).\n\n.. [4] Yuen, Karen K. \"The Two-Sample Trimmed t for Unequal Population\n       Variances.\" Biometrika, vol. 61, no. 1, 1974, pp. 165-170. JSTOR,\n       www.jstor.org/stable/2334299. Accessed 30 Mar. 2021.\n\n.. [5] Yuen, Karen K., and W. J. Dixon. \"The Approximate Behaviour and\n       Performance of the Two-Sample Trimmed t.\" Biometrika, vol. 60,\n       no. 2, 1973, pp. 369-374. JSTOR, www.jstor.org/stable/2334550.\n       Accessed 30 Mar. 2021.\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy import stats\n>>> rng = np.random.default_rng()\n\nTest with sample with identical means:\n\n>>> rvs1 = stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n>>> rvs2 = stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n>>> stats.ttest_ind(rvs1, rvs2)\nTtestResult(statistic=-0.4390847099199348,\n            pvalue=0.6606952038870015,\n            df=998.0)\n>>> stats.ttest_ind(rvs1, rvs2, equal_var=False)\nTtestResult(statistic=-0.4390847099199348,\n            pvalue=0.6606952553131064,\n            df=997.4602304121448)\n\n`ttest_ind` underestimates p for unequal variances:\n\n>>> rvs3 = stats.norm.rvs(loc=5, scale=20, size=500, random_state=rng)\n>>> stats.ttest_ind(rvs1, rvs3)\nTtestResult(statistic=-1.6370984482905417,\n            pvalue=0.1019251574705033,\n            df=998.0)\n>>> stats.ttest_ind(rvs1, rvs3, equal_var=False)\nTtestResult(statistic=-1.637098448290542,\n            pvalue=0.10202110497954867,\n            df=765.1098655246868)\n\nWhen ``n1 != n2``, the equal variance t-statistic is no longer equal to the\nunequal variance t-statistic:\n\n>>> rvs4 = stats.norm.rvs(loc=5, scale=20, size=100, random_state=rng)\n>>> stats.ttest_ind(rvs1, rvs4)\nTtestResult(statistic=-1.9481646859513422,\n            pvalue=0.05186270935842703,\n            df=598.0)\n>>> stats.ttest_ind(rvs1, rvs4, equal_var=False)\nTtestResult(statistic=-1.3146566100751664,\n            pvalue=0.1913495266513811,\n            df=110.41349083985212)\n\nT-test with different means, variance, and n:\n\n>>> rvs5 = stats.norm.rvs(loc=8, scale=20, size=100, random_state=rng)\n>>> stats.ttest_ind(rvs1, rvs5)\nTtestResult(statistic=-2.8415950600298774,\n            pvalue=0.0046418707568707885,\n            df=598.0)\n>>> stats.ttest_ind(rvs1, rvs5, equal_var=False)\nTtestResult(statistic=-1.8686598649188084,\n            pvalue=0.06434714193919686,\n            df=109.32167496550137)\n\nWhen performing a permutation test, more permutations typically yields\nmore accurate results. Use a ``np.random.Generator`` to ensure\nreproducibility:\n\n>>> stats.ttest_ind(rvs1, rvs5, permutations=10000,\n...                 random_state=rng)\nTtestResult(statistic=-2.8415950600298774,\n            pvalue=0.0052994700529947,\n            df=nan)\n\nTake these two samples, one of which has an extreme tail.\n\n>>> a = (56, 128.6, 12, 123.8, 64.34, 78, 763.3)\n>>> b = (1.1, 2.9, 4.2)\n\nUse the `trim` keyword to perform a trimmed (Yuen) t-test. For example,\nusing 20% trimming, ``trim=.2``, the test will reduce the impact of one\n(``np.floor(trim*len(a))``) element from each tail of sample `a`. It will\nhave no effect on sample `b` because ``np.floor(trim*len(b))`` is 0.\n\n>>> stats.ttest_ind(a, b, trim=.2)\nTtestResult(statistic=3.4463884028073513,\n            pvalue=0.01369338726499547,\n            df=6.0)\n",
    "scipy.stats.ttest_ind_from_stats": "\n    T-test for means of two independent samples from descriptive statistics.\n\n    This is a test for the null hypothesis that two independent\n    samples have identical average (expected) values.\n\n    Parameters\n    ----------\n    mean1 : array_like\n        The mean(s) of sample 1.\n    std1 : array_like\n        The corrected sample standard deviation of sample 1 (i.e. ``ddof=1``).\n    nobs1 : array_like\n        The number(s) of observations of sample 1.\n    mean2 : array_like\n        The mean(s) of sample 2.\n    std2 : array_like\n        The corrected sample standard deviation of sample 2 (i.e. ``ddof=1``).\n    nobs2 : array_like\n        The number(s) of observations of sample 2.\n    equal_var : bool, optional\n        If True (default), perform a standard independent 2 sample test\n        that assumes equal population variances [1]_.\n        If False, perform Welch's t-test, which does not assume equal\n        population variance [2]_.\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Defines the alternative hypothesis.\n        The following options are available (default is 'two-sided'):\n\n        * 'two-sided': the means of the distributions are unequal.\n        * 'less': the mean of the first distribution is less than the\n          mean of the second distribution.\n        * 'greater': the mean of the first distribution is greater than the\n          mean of the second distribution.\n\n        .. versionadded:: 1.6.0\n\n    Returns\n    -------\n    statistic : float or array\n        The calculated t-statistics.\n    pvalue : float or array\n        The two-tailed p-value.\n\n    See Also\n    --------\n    scipy.stats.ttest_ind\n\n    Notes\n    -----\n    The statistic is calculated as ``(mean1 - mean2)/se``, where ``se`` is the\n    standard error. Therefore, the statistic will be positive when `mean1` is\n    greater than `mean2` and negative when `mean1` is less than `mean2`.\n\n    This method does not check whether any of the elements of `std1` or `std2`\n    are negative. If any elements of the `std1` or `std2` parameters are\n    negative in a call to this method, this method will return the same result\n    as if it were passed ``numpy.abs(std1)`` and ``numpy.abs(std2)``,\n    respectively, instead; no exceptions or warnings will be emitted.\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/T-test#Independent_two-sample_t-test\n\n    .. [2] https://en.wikipedia.org/wiki/Welch%27s_t-test\n\n    Examples\n    --------\n    Suppose we have the summary data for two samples, as follows (with the\n    Sample Variance being the corrected sample variance)::\n\n                         Sample   Sample\n                   Size   Mean   Variance\n        Sample 1    13    15.0     87.5\n        Sample 2    11    12.0     39.0\n\n    Apply the t-test to this data (with the assumption that the population\n    variances are equal):\n\n    >>> import numpy as np\n    >>> from scipy.stats import ttest_ind_from_stats\n    >>> ttest_ind_from_stats(mean1=15.0, std1=np.sqrt(87.5), nobs1=13,\n    ...                      mean2=12.0, std2=np.sqrt(39.0), nobs2=11)\n    Ttest_indResult(statistic=0.9051358093310269, pvalue=0.3751996797581487)\n\n    For comparison, here is the data from which those summary statistics\n    were taken.  With this data, we can compute the same result using\n    `scipy.stats.ttest_ind`:\n\n    >>> a = np.array([1, 3, 4, 6, 11, 13, 15, 19, 22, 24, 25, 26, 26])\n    >>> b = np.array([2, 4, 6, 9, 11, 13, 14, 15, 18, 19, 21])\n    >>> from scipy.stats import ttest_ind\n    >>> ttest_ind(a, b)\n    TtestResult(statistic=0.905135809331027,\n                pvalue=0.3751996797581486,\n                df=22.0)\n\n    Suppose we instead have binary data and would like to apply a t-test to\n    compare the proportion of 1s in two independent groups::\n\n                          Number of    Sample     Sample\n                    Size    ones        Mean     Variance\n        Sample 1    150      30         0.2        0.161073\n        Sample 2    200      45         0.225      0.175251\n\n    The sample mean :math:`\\hat{p}` is the proportion of ones in the sample\n    and the variance for a binary observation is estimated by\n    :math:`\\hat{p}(1-\\hat{p})`.\n\n    >>> ttest_ind_from_stats(mean1=0.2, std1=np.sqrt(0.161073), nobs1=150,\n    ...                      mean2=0.225, std2=np.sqrt(0.175251), nobs2=200)\n    Ttest_indResult(statistic=-0.5627187905196761, pvalue=0.5739887114209541)\n\n    For comparison, we could compute the t statistic and p-value using\n    arrays of 0s and 1s and `scipy.stat.ttest_ind`, as above.\n\n    >>> group1 = np.array([1]*30 + [0]*(150-30))\n    >>> group2 = np.array([1]*45 + [0]*(200-45))\n    >>> ttest_ind(group1, group2)\n    TtestResult(statistic=-0.5627179589855622,\n                pvalue=0.573989277115258,\n                df=348.0)\n\n    ",
    "scipy.stats.ttest_rel": "    \n\n\nCalculate the t-test on TWO RELATED samples of scores, a and b.\n\nThis is a test for the null hypothesis that two related or\nrepeated samples have identical average (expected) values.\n\nParameters\n----------\na, b : array_like\n    The arrays must have the same shape.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nalternative : {'two-sided', 'less', 'greater'}, optional\n    Defines the alternative hypothesis.\n    The following options are available (default is 'two-sided'):\n    \n    * 'two-sided': the means of the distributions underlying the samples\n      are unequal.\n    * 'less': the mean of the distribution underlying the first sample\n      is less than the mean of the distribution underlying the second\n      sample.\n    * 'greater': the mean of the distribution underlying the first\n      sample is greater than the mean of the distribution underlying\n      the second sample.\n    \n    .. versionadded:: 1.6.0\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nresult : `~scipy.stats._result_classes.TtestResult`\n    An object with the following attributes:\n    \n    statistic : float or array\n        The t-statistic.\n    pvalue : float or array\n        The p-value associated with the given alternative.\n    df : float or array\n        The number of degrees of freedom used in calculation of the\n        t-statistic; this is one less than the size of the sample\n        (``a.shape[axis]``).\n    \n        .. versionadded:: 1.10.0\n    \n    The object also has the following method:\n    \n    confidence_interval(confidence_level=0.95)\n        Computes a confidence interval around the difference in\n        population means for the given confidence level.\n        The confidence interval is returned in a ``namedtuple`` with\n        fields `low` and `high`.\n    \n        .. versionadded:: 1.10.0\n\nNotes\n-----\nExamples for use are scores of the same set of student in\ndifferent exams, or repeated sampling from the same units. The\ntest measures whether the average score differs significantly\nacross samples (e.g. exams). If we observe a large p-value, for\nexample greater than 0.05 or 0.1 then we cannot reject the null\nhypothesis of identical average scores. If the p-value is smaller\nthan the threshold, e.g. 1%, 5% or 10%, then we reject the null\nhypothesis of equal averages. Small p-values are associated with\nlarge t-statistics.\n\nThe t-statistic is calculated as ``np.mean(a - b)/se``, where ``se`` is the\nstandard error. Therefore, the t-statistic will be positive when the sample\nmean of ``a - b`` is greater than zero and negative when the sample mean of\n``a - b`` is less than zero.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\nhttps://en.wikipedia.org/wiki/T-test#Dependent_t-test_for_paired_samples\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy import stats\n>>> rng = np.random.default_rng()\n\n>>> rvs1 = stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n>>> rvs2 = (stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n...         + stats.norm.rvs(scale=0.2, size=500, random_state=rng))\n>>> stats.ttest_rel(rvs1, rvs2)\nTtestResult(statistic=-0.4549717054410304, pvalue=0.6493274702088672, df=499)\n>>> rvs3 = (stats.norm.rvs(loc=8, scale=10, size=500, random_state=rng)\n...         + stats.norm.rvs(scale=0.2, size=500, random_state=rng))\n>>> stats.ttest_rel(rvs1, rvs3)\nTtestResult(statistic=-5.879467544540889, pvalue=7.540777129099917e-09, df=499)\n",
    "scipy.stats.tukey_hsd": "Perform Tukey's HSD test for equality of means over multiple treatments.\n\n    Tukey's honestly significant difference (HSD) test performs pairwise\n    comparison of means for a set of samples. Whereas ANOVA (e.g. `f_oneway`)\n    assesses whether the true means underlying each sample are identical,\n    Tukey's HSD is a post hoc test used to compare the mean of each sample\n    to the mean of each other sample.\n\n    The null hypothesis is that the distributions underlying the samples all\n    have the same mean. The test statistic, which is computed for every\n    possible pairing of samples, is simply the difference between the sample\n    means. For each pair, the p-value is the probability under the null\n    hypothesis (and other assumptions; see notes) of observing such an extreme\n    value of the statistic, considering that many pairwise comparisons are\n    being performed. Confidence intervals for the difference between each pair\n    of means are also available.\n\n    Parameters\n    ----------\n    sample1, sample2, ... : array_like\n        The sample measurements for each group. There must be at least\n        two arguments.\n\n    Returns\n    -------\n    result : `~scipy.stats._result_classes.TukeyHSDResult` instance\n        The return value is an object with the following attributes:\n\n        statistic : float ndarray\n            The computed statistic of the test for each comparison. The element\n            at index ``(i, j)`` is the statistic for the comparison between\n            groups ``i`` and ``j``.\n        pvalue : float ndarray\n            The computed p-value of the test for each comparison. The element\n            at index ``(i, j)`` is the p-value for the comparison between\n            groups ``i`` and ``j``.\n\n        The object has the following methods:\n\n        confidence_interval(confidence_level=0.95):\n            Compute the confidence interval for the specified confidence level.\n\n    See Also\n    --------\n    dunnett : performs comparison of means against a control group.\n\n    Notes\n    -----\n    The use of this test relies on several assumptions.\n\n    1. The observations are independent within and among groups.\n    2. The observations within each group are normally distributed.\n    3. The distributions from which the samples are drawn have the same finite\n       variance.\n\n    The original formulation of the test was for samples of equal size [6]_.\n    In case of unequal sample sizes, the test uses the Tukey-Kramer method\n    [4]_.\n\n    References\n    ----------\n    .. [1] NIST/SEMATECH e-Handbook of Statistical Methods, \"7.4.7.1. Tukey's\n           Method.\"\n           https://www.itl.nist.gov/div898/handbook/prc/section4/prc471.htm,\n           28 November 2020.\n    .. [2] Abdi, Herve & Williams, Lynne. (2021). \"Tukey's Honestly Significant\n           Difference (HSD) Test.\"\n           https://personal.utdallas.edu/~herve/abdi-HSD2010-pretty.pdf\n    .. [3] \"One-Way ANOVA Using SAS PROC ANOVA & PROC GLM.\" SAS\n           Tutorials, 2007, www.stattutorials.com/SAS/TUTORIAL-PROC-GLM.htm.\n    .. [4] Kramer, Clyde Young. \"Extension of Multiple Range Tests to Group\n           Means with Unequal Numbers of Replications.\" Biometrics, vol. 12,\n           no. 3, 1956, pp. 307-310. JSTOR, www.jstor.org/stable/3001469.\n           Accessed 25 May 2021.\n    .. [5] NIST/SEMATECH e-Handbook of Statistical Methods, \"7.4.3.3.\n           The ANOVA table and tests of hypotheses about means\"\n           https://www.itl.nist.gov/div898/handbook/prc/section4/prc433.htm,\n           2 June 2021.\n    .. [6] Tukey, John W. \"Comparing Individual Means in the Analysis of\n           Variance.\" Biometrics, vol. 5, no. 2, 1949, pp. 99-114. JSTOR,\n           www.jstor.org/stable/3001913. Accessed 14 June 2021.\n\n\n    Examples\n    --------\n    Here are some data comparing the time to relief of three brands of\n    headache medicine, reported in minutes. Data adapted from [3]_.\n\n    >>> import numpy as np\n    >>> from scipy.stats import tukey_hsd\n    >>> group0 = [24.5, 23.5, 26.4, 27.1, 29.9]\n    >>> group1 = [28.4, 34.2, 29.5, 32.2, 30.1]\n    >>> group2 = [26.1, 28.3, 24.3, 26.2, 27.8]\n\n    We would like to see if the means between any of the groups are\n    significantly different. First, visually examine a box and whisker plot.\n\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    >>> ax.boxplot([group0, group1, group2])\n    >>> ax.set_xticklabels([\"group0\", \"group1\", \"group2\"]) # doctest: +SKIP\n    >>> ax.set_ylabel(\"mean\") # doctest: +SKIP\n    >>> plt.show()\n\n    From the box and whisker plot, we can see overlap in the interquartile\n    ranges group 1 to group 2 and group 3, but we can apply the ``tukey_hsd``\n    test to determine if the difference between means is significant. We\n    set a significance level of .05 to reject the null hypothesis.\n\n    >>> res = tukey_hsd(group0, group1, group2)\n    >>> print(res)\n    Tukey's HSD Pairwise Group Comparisons (95.0% Confidence Interval)\n    Comparison  Statistic  p-value   Lower CI   Upper CI\n    (0 - 1)     -4.600      0.014     -8.249     -0.951\n    (0 - 2)     -0.260      0.980     -3.909      3.389\n    (1 - 0)      4.600      0.014      0.951      8.249\n    (1 - 2)      4.340      0.020      0.691      7.989\n    (2 - 0)      0.260      0.980     -3.389      3.909\n    (2 - 1)     -4.340      0.020     -7.989     -0.691\n\n    The null hypothesis is that each group has the same mean. The p-value for\n    comparisons between ``group0`` and ``group1`` as well as ``group1`` and\n    ``group2`` do not exceed .05, so we reject the null hypothesis that they\n    have the same means. The p-value of the comparison between ``group0``\n    and ``group2`` exceeds .05, so we accept the null hypothesis that there\n    is not a significant difference between their means.\n\n    We can also compute the confidence interval associated with our chosen\n    confidence level.\n\n    >>> group0 = [24.5, 23.5, 26.4, 27.1, 29.9]\n    >>> group1 = [28.4, 34.2, 29.5, 32.2, 30.1]\n    >>> group2 = [26.1, 28.3, 24.3, 26.2, 27.8]\n    >>> result = tukey_hsd(group0, group1, group2)\n    >>> conf = res.confidence_interval(confidence_level=.99)\n    >>> for ((i, j), l) in np.ndenumerate(conf.low):\n    ...     # filter out self comparisons\n    ...     if i != j:\n    ...         h = conf.high[i,j]\n    ...         print(f\"({i} - {j}) {l:>6.3f} {h:>6.3f}\")\n    (0 - 1) -9.480  0.280\n    (0 - 2) -5.140  4.620\n    (1 - 0) -0.280  9.480\n    (1 - 2) -0.540  9.220\n    (2 - 0) -4.620  5.140\n    (2 - 1) -9.220  0.540\n    ",
    "scipy.stats.tukeylambda": "A Tukey-Lamdba continuous random variable.\n\n    As an instance of the `rv_continuous` class, `tukeylambda` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(lam, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, lam, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, lam, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, lam, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, lam, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, lam, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, lam, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, lam, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, lam, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, lam, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(lam, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(lam, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(lam,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(lam, loc=0, scale=1)\n        Median of the distribution.\n    mean(lam, loc=0, scale=1)\n        Mean of the distribution.\n    var(lam, loc=0, scale=1)\n        Variance of the distribution.\n    std(lam, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, lam, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    A flexible distribution, able to represent and interpolate between the\n    following distributions:\n\n    - Cauchy                (:math:`lambda = -1`)\n    - logistic              (:math:`lambda = 0`)\n    - approx Normal         (:math:`lambda = 0.14`)\n    - uniform from -1 to 1  (:math:`lambda = 1`)\n\n    `tukeylambda` takes a real number :math:`lambda` (denoted ``lam``\n    in the implementation) as a shape parameter.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``tukeylambda.pdf(x, lam, loc, scale)`` is identically\n    equivalent to ``tukeylambda.pdf(y, lam) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import tukeylambda\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> lam = 3.13\n    >>> mean, var, skew, kurt = tukeylambda.stats(lam, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(tukeylambda.ppf(0.01, lam),\n    ...                 tukeylambda.ppf(0.99, lam), 100)\n    >>> ax.plot(x, tukeylambda.pdf(x, lam),\n    ...        'r-', lw=5, alpha=0.6, label='tukeylambda pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = tukeylambda(lam)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = tukeylambda.ppf([0.001, 0.5, 0.999], lam)\n    >>> np.allclose([0.001, 0.5, 0.999], tukeylambda.cdf(vals, lam))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = tukeylambda.rvs(lam, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.tvar": "    \n\n\nCompute the trimmed variance.\n\nThis function computes the sample variance of an array of values,\nwhile ignoring values which are outside of given `limits`.\n\nParameters\n----------\na : array_like\n    Array of values.\nlimits : None or (lower limit, upper limit), optional\n    Values in the input array less than the lower limit or greater than the\n    upper limit will be ignored. When limits is None, then all values are\n    used. Either of the limit values in the tuple can also be None\n    representing a half-open interval.  The default value is None.\ninclusive : (bool, bool), optional\n    A tuple consisting of the (lower flag, upper flag).  These flags\n    determine whether values exactly equal to the lower or upper limits\n    are included.  The default value is (True, True).\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nddof : int, optional\n    Delta degrees of freedom.  Default is 1.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\ntvar : float\n    Trimmed variance.\n\nNotes\n-----\n`tvar` computes the unbiased sample variance, i.e. it uses a correction\nfactor ``n / (n - 1)``.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy import stats\n>>> x = np.arange(20)\n>>> stats.tvar(x)\n35.0\n>>> stats.tvar(x, (3,17))\n20.0\n",
    "scipy.stats.uniform": "A uniform continuous random variable.\n\n    In the standard form, the distribution is uniform on ``[0, 1]``. Using\n    the parameters ``loc`` and ``scale``, one obtains the uniform distribution\n    on ``[loc, loc + scale]``.\n\n    As an instance of the `rv_continuous` class, `uniform` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import uniform\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    \n    >>> mean, var, skew, kurt = uniform.stats(moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(uniform.ppf(0.01),\n    ...                 uniform.ppf(0.99), 100)\n    >>> ax.plot(x, uniform.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='uniform pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = uniform()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = uniform.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], uniform.cdf(vals))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = uniform.rvs(size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.uniform_direction": "A vector-valued uniform direction.\n\n    Return a random direction (unit vector). The `dim` keyword specifies\n    the dimensionality of the space.\n\n    Methods\n    -------\n    rvs(dim=None, size=1, random_state=None)\n        Draw random directions.\n\n    Parameters\n    ----------\n    dim : scalar\n        Dimension of directions.\n    seed : {None, int, `numpy.random.Generator`,\n            `numpy.random.RandomState`}, optional\n\n        Used for drawing random variates.\n        If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used, seeded\n        with seed.\n        If `seed` is already a ``RandomState`` or ``Generator`` instance,\n        then that object is used.\n        Default is `None`.\n\n    Notes\n    -----\n    This distribution generates unit vectors uniformly distributed on\n    the surface of a hypersphere. These can be interpreted as random\n    directions.\n    For example, if `dim` is 3, 3D vectors from the surface of :math:`S^2`\n    will be sampled.\n\n    References\n    ----------\n    .. [1] Marsaglia, G. (1972). \"Choosing a Point from the Surface of a\n           Sphere\". Annals of Mathematical Statistics. 43 (2): 645-646.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import uniform_direction\n    >>> x = uniform_direction.rvs(3)\n    >>> np.linalg.norm(x)\n    1.\n\n    This generates one random direction, a vector on the surface of\n    :math:`S^2`.\n\n    Alternatively, the object may be called (as a function) to return a frozen\n    distribution with fixed `dim` parameter. Here,\n    we create a `uniform_direction` with ``dim=3`` and draw 5 observations.\n    The samples are then arranged in an array of shape 5x3.\n\n    >>> rng = np.random.default_rng()\n    >>> uniform_sphere_dist = uniform_direction(3)\n    >>> unit_vectors = uniform_sphere_dist.rvs(5, random_state=rng)\n    >>> unit_vectors\n    array([[ 0.56688642, -0.1332634 , -0.81294566],\n           [-0.427126  , -0.74779278,  0.50830044],\n           [ 0.3793989 ,  0.92346629,  0.05715323],\n           [ 0.36428383, -0.92449076, -0.11231259],\n           [-0.27733285,  0.94410968, -0.17816678]])\n    ",
    "scipy.stats.unitary_group": "A matrix-valued U(N) random variable.\n\n    Return a random unitary matrix.\n\n    The `dim` keyword specifies the dimension N.\n\n    Methods\n    -------\n    rvs(dim=None, size=1, random_state=None)\n        Draw random samples from U(N).\n\n    Parameters\n    ----------\n    dim : scalar\n        Dimension of matrices, must be greater than 1.\n    seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n        Used for drawing random variates.\n        If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used, seeded\n        with seed.\n        If `seed` is already a ``RandomState`` or ``Generator`` instance,\n        then that object is used.\n        Default is `None`.\n\n    Notes\n    -----\n    This class is similar to `ortho_group`.\n\n    References\n    ----------\n    .. [1] F. Mezzadri, \"How to generate random matrices from the classical\n           compact groups\", :arXiv:`math-ph/0609050v2`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import unitary_group\n    >>> x = unitary_group.rvs(3)\n\n    >>> np.dot(x, x.conj().T)\n    array([[  1.00000000e+00,   1.13231364e-17,  -2.86852790e-16],\n           [  1.13231364e-17,   1.00000000e+00,  -1.46845020e-16],\n           [ -2.86852790e-16,  -1.46845020e-16,   1.00000000e+00]])\n\n    This generates one random matrix from U(3). The dot product confirms that\n    it is unitary up to machine precision.\n\n    Alternatively, the object may be called (as a function) to fix the `dim`\n    parameter, return a \"frozen\" unitary_group random variable:\n\n    >>> rv = unitary_group(5)\n\n    See Also\n    --------\n    ortho_group\n\n    ",
    "scipy.stats.variation": "    \n\n\nCompute the coefficient of variation.\n\nThe coefficient of variation is the standard deviation divided by the\nmean.  This function is equivalent to::\n\n    np.std(x, axis=axis, ddof=ddof) / np.mean(x)\n\nThe default for ``ddof`` is 0, but many definitions of the coefficient\nof variation use the square root of the unbiased sample variance\nfor the sample standard deviation, which corresponds to ``ddof=1``.\n\nThe function does not take the absolute value of the mean of the data,\nso the return value is negative if the mean is negative.\n\nParameters\n----------\na : array_like\n    Input array.\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nddof : int, optional\n    Gives the \"Delta Degrees Of Freedom\" used when computing the\n    standard deviation.  The divisor used in the calculation of the\n    standard deviation is ``N - ddof``, where ``N`` is the number of\n    elements.  `ddof` must be less than ``N``; if it isn't, the result\n    will be ``nan`` or ``inf``, depending on ``N`` and the values in\n    the array.  By default `ddof` is zero for backwards compatibility,\n    but it is recommended to use ``ddof=1`` to ensure that the sample\n    standard deviation is computed as the square root of the unbiased\n    sample variance.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nvariation : ndarray\n    The calculated variation along the requested axis.\n\nNotes\n-----\nThere are several edge cases that are handled without generating a\nwarning:\n\n* If both the mean and the standard deviation are zero, ``nan``\n  is returned.\n* If the mean is zero and the standard deviation is nonzero, ``inf``\n  is returned.\n* If the input has length zero (either because the array has zero\n  length, or all the input values are ``nan`` and ``nan_policy`` is\n  ``'omit'``), ``nan`` is returned.\n* If the input contains ``inf``, ``nan`` is returned.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n   Probability and Statistics Tables and Formulae. Chapman & Hall: New\n   York. 2000.\n\nExamples\n--------\n>>> import numpy as np\n>>> from scipy.stats import variation\n>>> variation([1, 2, 3, 4, 5], ddof=1)\n0.5270462766947299\n\nCompute the variation along a given dimension of an array that contains\na few ``nan`` values:\n\n>>> x = np.array([[  10.0, np.nan, 11.0, 19.0, 23.0, 29.0, 98.0],\n...               [  29.0,   30.0, 32.0, 33.0, 35.0, 56.0, 57.0],\n...               [np.nan, np.nan, 12.0, 13.0, 16.0, 16.0, 17.0]])\n>>> variation(x, axis=1, ddof=1, nan_policy='omit')\narray([1.05109361, 0.31428986, 0.146483  ])\n",
    "scipy.stats.vonmises": "A Von Mises continuous random variable.\n\n    As an instance of the `rv_continuous` class, `vonmises` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(kappa, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, kappa, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, kappa, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, kappa, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, kappa, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, kappa, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, kappa, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, kappa, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, kappa, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, kappa, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(kappa, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(kappa, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(kappa,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(kappa, loc=0, scale=1)\n        Median of the distribution.\n    mean(kappa, loc=0, scale=1)\n        Mean of the distribution.\n    var(kappa, loc=0, scale=1)\n        Variance of the distribution.\n    std(kappa, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, kappa, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    scipy.stats.vonmises_fisher : Von-Mises Fisher distribution on a\n                                  hypersphere\n\n    Notes\n    -----\n    The probability density function for `vonmises` and `vonmises_line` is:\n\n    .. math::\n\n        f(x, \\kappa) = \\frac{ \\exp(\\kappa \\cos(x)) }{ 2 \\pi I_0(\\kappa) }\n\n    for :math:`-\\pi \\le x \\le \\pi`, :math:`\\kappa \\ge 0`. :math:`I_0` is the\n    modified Bessel function of order zero (`scipy.special.i0`).\n\n    `vonmises` is a circular distribution which does not restrict the\n    distribution to a fixed interval. Currently, there is no circular\n    distribution framework in SciPy. The ``cdf`` is implemented such that\n    ``cdf(x + 2*np.pi) == cdf(x) + 1``.\n\n    `vonmises_line` is the same distribution, defined on :math:`[-\\pi, \\pi]`\n    on the real line. This is a regular (i.e. non-circular) distribution.\n\n    Note about distribution parameters: `vonmises` and `vonmises_line` take\n    ``kappa`` as a shape parameter (concentration) and ``loc`` as the location\n    (circular mean). A ``scale`` parameter is accepted but does not have any\n    effect.\n\n    Examples\n    --------\n    Import the necessary modules.\n\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy.stats import vonmises\n\n    Define distribution parameters.\n\n    >>> loc = 0.5 * np.pi  # circular mean\n    >>> kappa = 1  # concentration\n\n    Compute the probability density at ``x=0`` via the ``pdf`` method.\n\n    >>> vonmises.pdf(0, loc=loc, kappa=kappa)\n    0.12570826359722018\n\n    Verify that the percentile function ``ppf`` inverts the cumulative\n    distribution function ``cdf`` up to floating point accuracy.\n\n    >>> x = 1\n    >>> cdf_value = vonmises.cdf(x, loc=loc, kappa=kappa)\n    >>> ppf_value = vonmises.ppf(cdf_value, loc=loc, kappa=kappa)\n    >>> x, cdf_value, ppf_value\n    (1, 0.31489339900904967, 1.0000000000000004)\n\n    Draw 1000 random variates by calling the ``rvs`` method.\n\n    >>> sample_size = 1000\n    >>> sample = vonmises(loc=loc, kappa=kappa).rvs(sample_size)\n\n    Plot the von Mises density on a Cartesian and polar grid to emphasize\n    that it is a circular distribution.\n\n    >>> fig = plt.figure(figsize=(12, 6))\n    >>> left = plt.subplot(121)\n    >>> right = plt.subplot(122, projection='polar')\n    >>> x = np.linspace(-np.pi, np.pi, 500)\n    >>> vonmises_pdf = vonmises.pdf(x, loc=loc, kappa=kappa)\n    >>> ticks = [0, 0.15, 0.3]\n\n    The left image contains the Cartesian plot.\n\n    >>> left.plot(x, vonmises_pdf)\n    >>> left.set_yticks(ticks)\n    >>> number_of_bins = int(np.sqrt(sample_size))\n    >>> left.hist(sample, density=True, bins=number_of_bins)\n    >>> left.set_title(\"Cartesian plot\")\n    >>> left.set_xlim(-np.pi, np.pi)\n    >>> left.grid(True)\n\n    The right image contains the polar plot.\n\n    >>> right.plot(x, vonmises_pdf, label=\"PDF\")\n    >>> right.set_yticks(ticks)\n    >>> right.hist(sample, density=True, bins=number_of_bins,\n    ...            label=\"Histogram\")\n    >>> right.set_title(\"Polar plot\")\n    >>> right.legend(bbox_to_anchor=(0.15, 1.06))\n\n    ",
    "scipy.stats.vonmises_fisher": "A von Mises-Fisher variable.\n\n    The `mu` keyword specifies the mean direction vector. The `kappa` keyword\n    specifies the concentration parameter.\n\n    Methods\n    -------\n    pdf(x, mu=None, kappa=1)\n        Probability density function.\n    logpdf(x, mu=None, kappa=1)\n        Log of the probability density function.\n    rvs(mu=None, kappa=1, size=1, random_state=None)\n        Draw random samples from a von Mises-Fisher distribution.\n    entropy(mu=None, kappa=1)\n        Compute the differential entropy of the von Mises-Fisher distribution.\n    fit(data)\n        Fit a von Mises-Fisher distribution to data.\n\n    Parameters\n    ----------\n    mu : array_like\n        Mean direction of the distribution. Must be a one-dimensional unit\n        vector of norm 1.\n    kappa : float\n        Concentration parameter. Must be positive.\n    seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n        Used for drawing random variates.\n        If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used, seeded\n        with seed.\n        If `seed` is already a ``RandomState`` or ``Generator`` instance,\n        then that object is used.\n        Default is `None`.\n\n    See Also\n    --------\n    scipy.stats.vonmises : Von-Mises Fisher distribution in 2D on a circle\n    uniform_direction : uniform distribution on the surface of a hypersphere\n\n    Notes\n    -----\n    The von Mises-Fisher distribution is a directional distribution on the\n    surface of the unit hypersphere. The probability density\n    function of a unit vector :math:`\\mathbf{x}` is\n\n    .. math::\n\n        f(\\mathbf{x}) = \\frac{\\kappa^{d/2-1}}{(2\\pi)^{d/2}I_{d/2-1}(\\kappa)}\n               \\exp\\left(\\kappa \\mathbf{\\mu}^T\\mathbf{x}\\right),\n\n    where :math:`\\mathbf{\\mu}` is the mean direction, :math:`\\kappa` the\n    concentration parameter, :math:`d` the dimension and :math:`I` the\n    modified Bessel function of the first kind. As :math:`\\mu` represents\n    a direction, it must be a unit vector or in other words, a point\n    on the hypersphere: :math:`\\mathbf{\\mu}\\in S^{d-1}`. :math:`\\kappa` is a\n    concentration parameter, which means that it must be positive\n    (:math:`\\kappa>0`) and that the distribution becomes more narrow with\n    increasing :math:`\\kappa`. In that sense, the reciprocal value\n    :math:`1/\\kappa` resembles the variance parameter of the normal\n    distribution.\n\n    The von Mises-Fisher distribution often serves as an analogue of the\n    normal distribution on the sphere. Intuitively, for unit vectors, a\n    useful distance measure is given by the angle :math:`\\alpha` between\n    them. This is exactly what the scalar product\n    :math:`\\mathbf{\\mu}^T\\mathbf{x}=\\cos(\\alpha)` in the\n    von Mises-Fisher probability density function describes: the angle\n    between the mean direction :math:`\\mathbf{\\mu}` and the vector\n    :math:`\\mathbf{x}`. The larger the angle between them, the smaller the\n    probability to observe :math:`\\mathbf{x}` for this particular mean\n    direction :math:`\\mathbf{\\mu}`.\n\n    In dimensions 2 and 3, specialized algorithms are used for fast sampling\n    [2]_, [3]_. For dimensions of 4 or higher the rejection sampling algorithm\n    described in [4]_ is utilized. This implementation is partially based on\n    the geomstats package [5]_, [6]_.\n\n    .. versionadded:: 1.11\n\n    References\n    ----------\n    .. [1] Von Mises-Fisher distribution, Wikipedia,\n           https://en.wikipedia.org/wiki/Von_Mises%E2%80%93Fisher_distribution\n    .. [2] Mardia, K., and Jupp, P. Directional statistics. Wiley, 2000.\n    .. [3] J. Wenzel. Numerically stable sampling of the von Mises Fisher\n           distribution on S2.\n           https://www.mitsuba-renderer.org/~wenzel/files/vmf.pdf\n    .. [4] Wood, A. Simulation of the von mises fisher distribution.\n           Communications in statistics-simulation and computation 23,\n           1 (1994), 157-164. https://doi.org/10.1080/03610919408813161\n    .. [5] geomstats, Github. MIT License. Accessed: 06.01.2023.\n           https://github.com/geomstats/geomstats\n    .. [6] Miolane, N. et al. Geomstats:  A Python Package for Riemannian\n           Geometry in Machine Learning. Journal of Machine Learning Research\n           21 (2020). http://jmlr.org/papers/v21/19-027.html\n\n    Examples\n    --------\n    **Visualization of the probability density**\n\n    Plot the probability density in three dimensions for increasing\n    concentration parameter. The density is calculated by the ``pdf``\n    method.\n\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy.stats import vonmises_fisher\n    >>> from matplotlib.colors import Normalize\n    >>> n_grid = 100\n    >>> u = np.linspace(0, np.pi, n_grid)\n    >>> v = np.linspace(0, 2 * np.pi, n_grid)\n    >>> u_grid, v_grid = np.meshgrid(u, v)\n    >>> vertices = np.stack([np.cos(v_grid) * np.sin(u_grid),\n    ...                      np.sin(v_grid) * np.sin(u_grid),\n    ...                      np.cos(u_grid)],\n    ...                     axis=2)\n    >>> x = np.outer(np.cos(v), np.sin(u))\n    >>> y = np.outer(np.sin(v), np.sin(u))\n    >>> z = np.outer(np.ones_like(u), np.cos(u))\n    >>> def plot_vmf_density(ax, x, y, z, vertices, mu, kappa):\n    ...     vmf = vonmises_fisher(mu, kappa)\n    ...     pdf_values = vmf.pdf(vertices)\n    ...     pdfnorm = Normalize(vmin=pdf_values.min(), vmax=pdf_values.max())\n    ...     ax.plot_surface(x, y, z, rstride=1, cstride=1,\n    ...                     facecolors=plt.cm.viridis(pdfnorm(pdf_values)),\n    ...                     linewidth=0)\n    ...     ax.set_aspect('equal')\n    ...     ax.view_init(azim=-130, elev=0)\n    ...     ax.axis('off')\n    ...     ax.set_title(rf\"$\\kappa={kappa}$\")\n    >>> fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(9, 4),\n    ...                          subplot_kw={\"projection\": \"3d\"})\n    >>> left, middle, right = axes\n    >>> mu = np.array([-np.sqrt(0.5), -np.sqrt(0.5), 0])\n    >>> plot_vmf_density(left, x, y, z, vertices, mu, 5)\n    >>> plot_vmf_density(middle, x, y, z, vertices, mu, 20)\n    >>> plot_vmf_density(right, x, y, z, vertices, mu, 100)\n    >>> plt.subplots_adjust(top=1, bottom=0.0, left=0.0, right=1.0, wspace=0.)\n    >>> plt.show()\n\n    As we increase the concentration parameter, the points are getting more\n    clustered together around the mean direction.\n\n    **Sampling**\n\n    Draw 5 samples from the distribution using the ``rvs`` method resulting\n    in a 5x3 array.\n\n    >>> rng = np.random.default_rng()\n    >>> mu = np.array([0, 0, 1])\n    >>> samples = vonmises_fisher(mu, 20).rvs(5, random_state=rng)\n    >>> samples\n    array([[ 0.3884594 , -0.32482588,  0.86231516],\n           [ 0.00611366, -0.09878289,  0.99509023],\n           [-0.04154772, -0.01637135,  0.99900239],\n           [-0.14613735,  0.12553507,  0.98126695],\n           [-0.04429884, -0.23474054,  0.97104814]])\n\n    These samples are unit vectors on the sphere :math:`S^2`. To verify,\n    let us calculate their euclidean norms:\n\n    >>> np.linalg.norm(samples, axis=1)\n    array([1., 1., 1., 1., 1.])\n\n    Plot 20 observations drawn from the von Mises-Fisher distribution for\n    increasing concentration parameter :math:`\\kappa`. The red dot highlights\n    the mean direction :math:`\\mu`.\n\n    >>> def plot_vmf_samples(ax, x, y, z, mu, kappa):\n    ...     vmf = vonmises_fisher(mu, kappa)\n    ...     samples = vmf.rvs(20)\n    ...     ax.plot_surface(x, y, z, rstride=1, cstride=1, linewidth=0,\n    ...                     alpha=0.2)\n    ...     ax.scatter(samples[:, 0], samples[:, 1], samples[:, 2], c='k', s=5)\n    ...     ax.scatter(mu[0], mu[1], mu[2], c='r', s=30)\n    ...     ax.set_aspect('equal')\n    ...     ax.view_init(azim=-130, elev=0)\n    ...     ax.axis('off')\n    ...     ax.set_title(rf\"$\\kappa={kappa}$\")\n    >>> mu = np.array([-np.sqrt(0.5), -np.sqrt(0.5), 0])\n    >>> fig, axes = plt.subplots(nrows=1, ncols=3,\n    ...                          subplot_kw={\"projection\": \"3d\"},\n    ...                          figsize=(9, 4))\n    >>> left, middle, right = axes\n    >>> plot_vmf_samples(left, x, y, z, mu, 5)\n    >>> plot_vmf_samples(middle, x, y, z, mu, 20)\n    >>> plot_vmf_samples(right, x, y, z, mu, 100)\n    >>> plt.subplots_adjust(top=1, bottom=0.0, left=0.0,\n    ...                     right=1.0, wspace=0.)\n    >>> plt.show()\n\n    The plots show that with increasing concentration :math:`\\kappa` the\n    resulting samples are centered more closely around the mean direction.\n\n    **Fitting the distribution parameters**\n\n    The distribution can be fitted to data using the ``fit`` method returning\n    the estimated parameters. As a toy example let's fit the distribution to\n    samples drawn from a known von Mises-Fisher distribution.\n\n    >>> mu, kappa = np.array([0, 0, 1]), 20\n    >>> samples = vonmises_fisher(mu, kappa).rvs(1000, random_state=rng)\n    >>> mu_fit, kappa_fit = vonmises_fisher.fit(samples)\n    >>> mu_fit, kappa_fit\n    (array([0.01126519, 0.01044501, 0.99988199]), 19.306398751730995)\n\n    We see that the estimated parameters `mu_fit` and `kappa_fit` are\n    very close to the ground truth parameters.\n\n    ",
    "scipy.stats.vonmises_line": "A Von Mises continuous random variable.\n\n    As an instance of the `rv_continuous` class, `vonmises_line` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(kappa, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, kappa, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, kappa, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, kappa, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, kappa, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, kappa, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, kappa, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, kappa, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, kappa, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, kappa, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(kappa, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(kappa, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(kappa,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(kappa, loc=0, scale=1)\n        Median of the distribution.\n    mean(kappa, loc=0, scale=1)\n        Mean of the distribution.\n    var(kappa, loc=0, scale=1)\n        Variance of the distribution.\n    std(kappa, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, kappa, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    scipy.stats.vonmises_fisher : Von-Mises Fisher distribution on a\n                                  hypersphere\n\n    Notes\n    -----\n    The probability density function for `vonmises` and `vonmises_line` is:\n\n    .. math::\n\n        f(x, \\kappa) = \\frac{ \\exp(\\kappa \\cos(x)) }{ 2 \\pi I_0(\\kappa) }\n\n    for :math:`-\\pi \\le x \\le \\pi`, :math:`\\kappa \\ge 0`. :math:`I_0` is the\n    modified Bessel function of order zero (`scipy.special.i0`).\n\n    `vonmises` is a circular distribution which does not restrict the\n    distribution to a fixed interval. Currently, there is no circular\n    distribution framework in SciPy. The ``cdf`` is implemented such that\n    ``cdf(x + 2*np.pi) == cdf(x) + 1``.\n\n    `vonmises_line` is the same distribution, defined on :math:`[-\\pi, \\pi]`\n    on the real line. This is a regular (i.e. non-circular) distribution.\n\n    Note about distribution parameters: `vonmises` and `vonmises_line` take\n    ``kappa`` as a shape parameter (concentration) and ``loc`` as the location\n    (circular mean). A ``scale`` parameter is accepted but does not have any\n    effect.\n\n    Examples\n    --------\n    Import the necessary modules.\n\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy.stats import vonmises\n\n    Define distribution parameters.\n\n    >>> loc = 0.5 * np.pi  # circular mean\n    >>> kappa = 1  # concentration\n\n    Compute the probability density at ``x=0`` via the ``pdf`` method.\n\n    >>> vonmises.pdf(0, loc=loc, kappa=kappa)\n    0.12570826359722018\n\n    Verify that the percentile function ``ppf`` inverts the cumulative\n    distribution function ``cdf`` up to floating point accuracy.\n\n    >>> x = 1\n    >>> cdf_value = vonmises.cdf(x, loc=loc, kappa=kappa)\n    >>> ppf_value = vonmises.ppf(cdf_value, loc=loc, kappa=kappa)\n    >>> x, cdf_value, ppf_value\n    (1, 0.31489339900904967, 1.0000000000000004)\n\n    Draw 1000 random variates by calling the ``rvs`` method.\n\n    >>> sample_size = 1000\n    >>> sample = vonmises(loc=loc, kappa=kappa).rvs(sample_size)\n\n    Plot the von Mises density on a Cartesian and polar grid to emphasize\n    that it is a circular distribution.\n\n    >>> fig = plt.figure(figsize=(12, 6))\n    >>> left = plt.subplot(121)\n    >>> right = plt.subplot(122, projection='polar')\n    >>> x = np.linspace(-np.pi, np.pi, 500)\n    >>> vonmises_pdf = vonmises.pdf(x, loc=loc, kappa=kappa)\n    >>> ticks = [0, 0.15, 0.3]\n\n    The left image contains the Cartesian plot.\n\n    >>> left.plot(x, vonmises_pdf)\n    >>> left.set_yticks(ticks)\n    >>> number_of_bins = int(np.sqrt(sample_size))\n    >>> left.hist(sample, density=True, bins=number_of_bins)\n    >>> left.set_title(\"Cartesian plot\")\n    >>> left.set_xlim(-np.pi, np.pi)\n    >>> left.grid(True)\n\n    The right image contains the polar plot.\n\n    >>> right.plot(x, vonmises_pdf, label=\"PDF\")\n    >>> right.set_yticks(ticks)\n    >>> right.hist(sample, density=True, bins=number_of_bins,\n    ...            label=\"Histogram\")\n    >>> right.set_title(\"Polar plot\")\n    >>> right.legend(bbox_to_anchor=(0.15, 1.06))\n\n    ",
    "scipy.stats.wald": "A Wald continuous random variable.\n\n    As an instance of the `rv_continuous` class, `wald` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(loc=0, scale=1)\n        Median of the distribution.\n    mean(loc=0, scale=1)\n        Mean of the distribution.\n    var(loc=0, scale=1)\n        Variance of the distribution.\n    std(loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `wald` is:\n\n    .. math::\n\n        f(x) = \\frac{1}{\\sqrt{2\\pi x^3}} \\exp(- \\frac{ (x-1)^2 }{ 2x })\n\n    for :math:`x >= 0`.\n\n    `wald` is a special case of `invgauss` with ``mu=1``.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``wald.pdf(x, loc, scale)`` is identically\n    equivalent to ``wald.pdf(y) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import wald\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    \n    >>> mean, var, skew, kurt = wald.stats(moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(wald.ppf(0.01),\n    ...                 wald.ppf(0.99), 100)\n    >>> ax.plot(x, wald.pdf(x),\n    ...        'r-', lw=5, alpha=0.6, label='wald pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = wald()\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = wald.ppf([0.001, 0.5, 0.999])\n    >>> np.allclose([0.001, 0.5, 0.999], wald.cdf(vals))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = wald.rvs(size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n    ",
    "scipy.stats.wasserstein_distance": "\n    Compute the Wasserstein-1 distance between two 1D discrete distributions.\n\n    The Wasserstein distance, also called the Earth mover's distance or the\n    optimal transport distance, is a similarity metric between two probability\n    distributions [1]_. In the discrete case, the Wasserstein distance can be\n    understood as the cost of an optimal transport plan to convert one\n    distribution into the other. The cost is calculated as the product of the\n    amount of probability mass being moved and the distance it is being moved.\n    A brief and intuitive introduction can be found at [2]_.\n\n    .. versionadded:: 1.0.0\n\n    Parameters\n    ----------\n    u_values : 1d array_like\n        A sample from a probability distribution or the support (set of all\n        possible values) of a probability distribution. Each element is an\n        observation or possible value.\n\n    v_values : 1d array_like\n        A sample from or the support of a second distribution.\n\n    u_weights, v_weights : 1d array_like, optional\n        Weights or counts corresponding with the sample or probability masses\n        corresponding with the support values. Sum of elements must be positive\n        and finite. If unspecified, each value is assigned the same weight.\n\n    Returns\n    -------\n    distance : float\n        The computed distance between the distributions.\n\n    Notes\n    -----\n    Given two 1D probability mass functions, :math:`u` and :math:`v`, the first\n    Wasserstein distance between the distributions is:\n\n    .. math::\n\n        l_1 (u, v) = \\inf_{\\pi \\in \\Gamma (u, v)} \\int_{\\mathbb{R} \\times\n        \\mathbb{R}} |x-y| \\mathrm{d} \\pi (x, y)\n\n    where :math:`\\Gamma (u, v)` is the set of (probability) distributions on\n    :math:`\\mathbb{R} \\times \\mathbb{R}` whose marginals are :math:`u` and\n    :math:`v` on the first and second factors respectively. For a given value\n    :math:`x`, :math:`u(x)` gives the probabilty of :math:`u` at position\n    :math:`x`, and the same for :math:`v(x)`.\n\n    If :math:`U` and :math:`V` are the respective CDFs of :math:`u` and\n    :math:`v`, this distance also equals to:\n\n    .. math::\n\n        l_1(u, v) = \\int_{-\\infty}^{+\\infty} |U-V|\n\n    See [3]_ for a proof of the equivalence of both definitions.\n\n    The input distributions can be empirical, therefore coming from samples\n    whose values are effectively inputs of the function, or they can be seen as\n    generalized functions, in which case they are weighted sums of Dirac delta\n    functions located at the specified values.\n\n    References\n    ----------\n    .. [1] \"Wasserstein metric\", https://en.wikipedia.org/wiki/Wasserstein_metric\n    .. [2] Lili Weng, \"What is Wasserstein distance?\", Lil'log,\n           https://lilianweng.github.io/posts/2017-08-20-gan/#what-is-wasserstein-distance.\n    .. [3] Ramdas, Garcia, Cuturi \"On Wasserstein Two Sample Testing and Related\n           Families of Nonparametric Tests\" (2015). :arXiv:`1509.02237`.\n\n    See Also\n    --------\n    wasserstein_distance_nd: Compute the Wasserstein-1 distance between two N-D\n        discrete distributions.\n\n    Examples\n    --------\n    >>> from scipy.stats import wasserstein_distance\n    >>> wasserstein_distance([0, 1, 3], [5, 6, 8])\n    5.0\n    >>> wasserstein_distance([0, 1], [0, 1], [3, 1], [2, 2])\n    0.25\n    >>> wasserstein_distance([3.4, 3.9, 7.5, 7.8], [4.5, 1.4],\n    ...                      [1.4, 0.9, 3.1, 7.2], [3.2, 3.5])\n    4.0781331438047861\n\n    ",
    "scipy.stats.wasserstein_distance_nd": "\n    Compute the Wasserstein-1 distance between two N-D discrete distributions.\n\n    The Wasserstein distance, also called the Earth mover's distance or the\n    optimal transport distance, is a similarity metric between two probability\n    distributions [1]_. In the discrete case, the Wasserstein distance can be\n    understood as the cost of an optimal transport plan to convert one\n    distribution into the other. The cost is calculated as the product of the\n    amount of probability mass being moved and the distance it is being moved.\n    A brief and intuitive introduction can be found at [2]_.\n\n    .. versionadded:: 1.13.0\n\n    Parameters\n    ----------\n    u_values : 2d array_like\n        A sample from a probability distribution or the support (set of all\n        possible values) of a probability distribution. Each element along\n        axis 0 is an observation or possible value, and axis 1 represents the\n        dimensionality of the distribution; i.e., each row is a vector\n        observation or possible value.\n\n    v_values : 2d array_like\n        A sample from or the support of a second distribution.\n\n    u_weights, v_weights : 1d array_like, optional\n        Weights or counts corresponding with the sample or probability masses\n        corresponding with the support values. Sum of elements must be positive\n        and finite. If unspecified, each value is assigned the same weight.\n\n    Returns\n    -------\n    distance : float\n        The computed distance between the distributions.\n\n    Notes\n    -----\n    Given two probability mass functions, :math:`u`\n    and :math:`v`, the first Wasserstein distance between the distributions\n    using the Euclidean norm is:\n\n    .. math::\n\n        l_1 (u, v) = \\inf_{\\pi \\in \\Gamma (u, v)} \\int \\| x-y \\|_2 \\mathrm{d} \\pi (x, y)\n\n    where :math:`\\Gamma (u, v)` is the set of (probability) distributions on\n    :math:`\\mathbb{R}^n \\times \\mathbb{R}^n` whose marginals are :math:`u` and\n    :math:`v` on the first and second factors respectively. For a given value\n    :math:`x`, :math:`u(x)` gives the probabilty of :math:`u` at position\n    :math:`x`, and the same for :math:`v(x)`.\n\n    This is also called the optimal transport problem or the Monge problem.\n    Let the finite point sets :math:`\\{x_i\\}` and :math:`\\{y_j\\}` denote\n    the support set of probability mass function :math:`u` and :math:`v`\n    respectively. The Monge problem can be expressed as follows,\n\n    Let :math:`\\Gamma` denote the transport plan, :math:`D` denote the\n    distance matrix and,\n\n    .. math::\n\n        x = \\text{vec}(\\Gamma)          \\\\\n        c = \\text{vec}(D)               \\\\\n        b = \\begin{bmatrix}\n                u\\\\\n                v\\\\\n            \\end{bmatrix}\n\n    The :math:`\\text{vec}()` function denotes the Vectorization function\n    that transforms a matrix into a column vector by vertically stacking\n    the columns of the matrix.\n    The tranport plan :math:`\\Gamma` is a matrix :math:`[\\gamma_{ij}]` in\n    which :math:`\\gamma_{ij}` is a positive value representing the amount of\n    probability mass transported from :math:`u(x_i)` to :math:`v(y_i)`.\n    Summing over the rows of :math:`\\Gamma` should give the source distribution\n    :math:`u` : :math:`\\sum_j \\gamma_{ij} = u(x_i)` holds for all :math:`i`\n    and summing over the columns of :math:`\\Gamma` should give the target\n    distribution :math:`v`: :math:`\\sum_i \\gamma_{ij} = v(y_j)` holds for all\n    :math:`j`.\n    The distance matrix :math:`D` is a matrix :math:`[d_{ij}]`, in which\n    :math:`d_{ij} = d(x_i, y_j)`.\n\n    Given :math:`\\Gamma`, :math:`D`, :math:`b`, the Monge problem can be\n    tranformed into a linear programming problem by\n    taking :math:`A x = b` as constraints and :math:`z = c^T x` as minimization\n    target (sum of costs) , where matrix :math:`A` has the form\n\n    .. math::\n\n        \\begin{array} {rrrr|rrrr|r|rrrr}\n            1 & 1 & \\dots & 1 & 0 & 0 & \\dots & 0 & \\dots & 0 & 0 & \\dots &\n                0 \\cr\n            0 & 0 & \\dots & 0 & 1 & 1 & \\dots & 1 & \\dots & 0 & 0 &\\dots &\n                0 \\cr\n            \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\ddots\n                & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots  \\cr\n            0 & 0 & \\dots & 0 & 0 & 0 & \\dots & 0 & \\dots & 1 & 1 & \\dots &\n                1 \\cr \\hline\n\n            1 & 0 & \\dots & 0 & 1 & 0 & \\dots & \\dots & \\dots & 1 & 0 & \\dots &\n                0 \\cr\n            0 & 1 & \\dots & 0 & 0 & 1 & \\dots & \\dots & \\dots & 0 & 1 & \\dots &\n                0 \\cr\n            \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\ddots &\n                \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\cr\n            0 & 0 & \\dots & 1 & 0 & 0 & \\dots & 1 & \\dots & 0 & 0 & \\dots & 1\n        \\end{array}\n\n    By solving the dual form of the above linear programming problem (with\n    solution :math:`y^*`), the Wasserstein distance :math:`l_1 (u, v)` can\n    be computed as :math:`b^T y^*`.\n\n    The above solution is inspired by Vincent Herrmann's blog [3]_ . For a\n    more thorough explanation, see [4]_ .\n\n    The input distributions can be empirical, therefore coming from samples\n    whose values are effectively inputs of the function, or they can be seen as\n    generalized functions, in which case they are weighted sums of Dirac delta\n    functions located at the specified values.\n\n    References\n    ----------\n    .. [1] \"Wasserstein metric\",\n           https://en.wikipedia.org/wiki/Wasserstein_metric\n    .. [2] Lili Weng, \"What is Wasserstein distance?\", Lil'log,\n           https://lilianweng.github.io/posts/2017-08-20-gan/#what-is-wasserstein-distance.\n    .. [3] Hermann, Vincent. \"Wasserstein GAN and the Kantorovich-Rubinstein\n           Duality\". https://vincentherrmann.github.io/blog/wasserstein/.\n    .. [4] Peyr\u00e9, Gabriel, and Marco Cuturi. \"Computational optimal\n           transport.\" Center for Research in Economics and Statistics\n           Working Papers 2017-86 (2017).\n\n    See Also\n    --------\n    wasserstein_distance: Compute the Wasserstein-1 distance between two\n        1D discrete distributions.\n\n    Examples\n    --------\n    Compute the Wasserstein distance between two three-dimensional samples,\n    each with two observations.\n\n    >>> from scipy.stats import wasserstein_distance_nd\n    >>> wasserstein_distance_nd([[0, 2, 3], [1, 2, 5]], [[3, 2, 3], [4, 2, 5]])\n    3.0\n\n    Compute the Wasserstein distance between two two-dimensional distributions\n    with three and two weighted observations, respectively.\n\n    >>> wasserstein_distance_nd([[0, 2.75], [2, 209.3], [0, 0]],\n    ...                      [[0.2, 0.322], [4.5, 25.1808]],\n    ...                      [0.4, 5.2, 0.114], [0.8, 1.5])\n    174.15840245217169\n    ",
    "scipy.stats.weibull_max": "Weibull maximum continuous random variable.\n\n    The Weibull Maximum Extreme Value distribution, from extreme value theory\n    (Fisher-Gnedenko theorem), is the limiting distribution of rescaled\n    maximum of iid random variables. This is the distribution of -X\n    if X is from the `weibull_min` function.\n\n    As an instance of the `rv_continuous` class, `weibull_max` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    weibull_min\n\n    Notes\n    -----\n    The probability density function for `weibull_max` is:\n\n    .. math::\n\n        f(x, c) = c (-x)^{c-1} \\exp(-(-x)^c)\n\n    for :math:`x < 0`, :math:`c > 0`.\n\n    `weibull_max` takes ``c`` as a shape parameter for :math:`c`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``weibull_max.pdf(x, c, loc, scale)`` is identically\n    equivalent to ``weibull_max.pdf(y, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    https://en.wikipedia.org/wiki/Weibull_distribution\n\n    https://en.wikipedia.org/wiki/Fisher-Tippett-Gnedenko_theorem\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import weibull_max\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c = 2.87\n    >>> mean, var, skew, kurt = weibull_max.stats(c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(weibull_max.ppf(0.01, c),\n    ...                 weibull_max.ppf(0.99, c), 100)\n    >>> ax.plot(x, weibull_max.pdf(x, c),\n    ...        'r-', lw=5, alpha=0.6, label='weibull_max pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = weibull_max(c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = weibull_max.ppf([0.001, 0.5, 0.999], c)\n    >>> np.allclose([0.001, 0.5, 0.999], weibull_max.cdf(vals, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = weibull_max.rvs(c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.weibull_min": "Weibull minimum continuous random variable.\n\n    The Weibull Minimum Extreme Value distribution, from extreme value theory\n    (Fisher-Gnedenko theorem), is also often simply called the Weibull\n    distribution. It arises as the limiting distribution of the rescaled\n    minimum of iid random variables.\n\n    As an instance of the `rv_continuous` class, `weibull_min` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    weibull_max, numpy.random.Generator.weibull, exponweib\n\n    Notes\n    -----\n    The probability density function for `weibull_min` is:\n\n    .. math::\n\n        f(x, c) = c x^{c-1} \\exp(-x^c)\n\n    for :math:`x > 0`, :math:`c > 0`.\n\n    `weibull_min` takes ``c`` as a shape parameter for :math:`c`.\n    (named :math:`k` in Wikipedia article and :math:`a` in\n    ``numpy.random.weibull``).  Special shape values are :math:`c=1` and\n    :math:`c=2` where Weibull distribution reduces to the `expon` and\n    `rayleigh` distributions respectively.\n\n    Suppose ``X`` is an exponentially distributed random variable with\n    scale ``s``. Then ``Y = X**k`` is `weibull_min` distributed with shape\n    ``c = 1/k`` and scale ``s**k``.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``weibull_min.pdf(x, c, loc, scale)`` is identically\n    equivalent to ``weibull_min.pdf(y, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    References\n    ----------\n    https://en.wikipedia.org/wiki/Weibull_distribution\n\n    https://en.wikipedia.org/wiki/Fisher-Tippett-Gnedenko_theorem\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import weibull_min\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c = 1.79\n    >>> mean, var, skew, kurt = weibull_min.stats(c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(weibull_min.ppf(0.01, c),\n    ...                 weibull_min.ppf(0.99, c), 100)\n    >>> ax.plot(x, weibull_min.pdf(x, c),\n    ...        'r-', lw=5, alpha=0.6, label='weibull_min pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = weibull_min(c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = weibull_min.ppf([0.001, 0.5, 0.999], c)\n    >>> np.allclose([0.001, 0.5, 0.999], weibull_min.cdf(vals, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = weibull_min.rvs(c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.weightedtau": "Compute a weighted version of Kendall's :math:`\\tau`.\n\n    The weighted :math:`\\tau` is a weighted version of Kendall's\n    :math:`\\tau` in which exchanges of high weight are more influential than\n    exchanges of low weight. The default parameters compute the additive\n    hyperbolic version of the index, :math:`\\tau_\\mathrm h`, which has\n    been shown to provide the best balance between important and\n    unimportant elements [1]_.\n\n    The weighting is defined by means of a rank array, which assigns a\n    nonnegative rank to each element (higher importance ranks being\n    associated with smaller values, e.g., 0 is the highest possible rank),\n    and a weigher function, which assigns a weight based on the rank to\n    each element. The weight of an exchange is then the sum or the product\n    of the weights of the ranks of the exchanged elements. The default\n    parameters compute :math:`\\tau_\\mathrm h`: an exchange between\n    elements with rank :math:`r` and :math:`s` (starting from zero) has\n    weight :math:`1/(r+1) + 1/(s+1)`.\n\n    Specifying a rank array is meaningful only if you have in mind an\n    external criterion of importance. If, as it usually happens, you do\n    not have in mind a specific rank, the weighted :math:`\\tau` is\n    defined by averaging the values obtained using the decreasing\n    lexicographical rank by (`x`, `y`) and by (`y`, `x`). This is the\n    behavior with default parameters. Note that the convention used\n    here for ranking (lower values imply higher importance) is opposite\n    to that used by other SciPy statistical functions.\n\n    Parameters\n    ----------\n    x, y : array_like\n        Arrays of scores, of the same shape. If arrays are not 1-D, they will\n        be flattened to 1-D.\n    rank : array_like of ints or bool, optional\n        A nonnegative rank assigned to each element. If it is None, the\n        decreasing lexicographical rank by (`x`, `y`) will be used: elements of\n        higher rank will be those with larger `x`-values, using `y`-values to\n        break ties (in particular, swapping `x` and `y` will give a different\n        result). If it is False, the element indices will be used\n        directly as ranks. The default is True, in which case this\n        function returns the average of the values obtained using the\n        decreasing lexicographical rank by (`x`, `y`) and by (`y`, `x`).\n    weigher : callable, optional\n        The weigher function. Must map nonnegative integers (zero\n        representing the most important element) to a nonnegative weight.\n        The default, None, provides hyperbolic weighing, that is,\n        rank :math:`r` is mapped to weight :math:`1/(r+1)`.\n    additive : bool, optional\n        If True, the weight of an exchange is computed by adding the\n        weights of the ranks of the exchanged elements; otherwise, the weights\n        are multiplied. The default is True.\n\n    Returns\n    -------\n    res: SignificanceResult\n        An object containing attributes:\n\n        statistic : float\n           The weighted :math:`\\tau` correlation index.\n        pvalue : float\n           Presently ``np.nan``, as the null distribution of the statistic is\n           unknown (even in the additive hyperbolic case).\n\n    See Also\n    --------\n    kendalltau : Calculates Kendall's tau.\n    spearmanr : Calculates a Spearman rank-order correlation coefficient.\n    theilslopes : Computes the Theil-Sen estimator for a set of points (x, y).\n\n    Notes\n    -----\n    This function uses an :math:`O(n \\log n)`, mergesort-based algorithm\n    [1]_ that is a weighted extension of Knight's algorithm for Kendall's\n    :math:`\\tau` [2]_. It can compute Shieh's weighted :math:`\\tau` [3]_\n    between rankings without ties (i.e., permutations) by setting\n    `additive` and `rank` to False, as the definition given in [1]_ is a\n    generalization of Shieh's.\n\n    NaNs are considered the smallest possible score.\n\n    .. versionadded:: 0.19.0\n\n    References\n    ----------\n    .. [1] Sebastiano Vigna, \"A weighted correlation index for rankings with\n           ties\", Proceedings of the 24th international conference on World\n           Wide Web, pp. 1166-1176, ACM, 2015.\n    .. [2] W.R. Knight, \"A Computer Method for Calculating Kendall's Tau with\n           Ungrouped Data\", Journal of the American Statistical Association,\n           Vol. 61, No. 314, Part 1, pp. 436-439, 1966.\n    .. [3] Grace S. Shieh. \"A weighted Kendall's tau statistic\", Statistics &\n           Probability Letters, Vol. 39, No. 1, pp. 17-24, 1998.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> x = [12, 2, 1, 12, 2]\n    >>> y = [1, 4, 7, 1, 0]\n    >>> res = stats.weightedtau(x, y)\n    >>> res.statistic\n    -0.56694968153682723\n    >>> res.pvalue\n    nan\n    >>> res = stats.weightedtau(x, y, additive=False)\n    >>> res.statistic\n    -0.62205716951801038\n\n    NaNs are considered the smallest possible score:\n\n    >>> x = [12, 2, 1, 12, 2]\n    >>> y = [1, 4, 7, 1, np.nan]\n    >>> res = stats.weightedtau(x, y)\n    >>> res.statistic\n    -0.56694968153682723\n\n    This is exactly Kendall's tau:\n\n    >>> x = [12, 2, 1, 12, 2]\n    >>> y = [1, 4, 7, 1, 0]\n    >>> res = stats.weightedtau(x, y, weigher=lambda x: 1)\n    >>> res.statistic\n    -0.47140452079103173\n\n    >>> x = [12, 2, 1, 12, 2]\n    >>> y = [1, 4, 7, 1, 0]\n    >>> stats.weightedtau(x, y, rank=None)\n    SignificanceResult(statistic=-0.4157652301037516, pvalue=nan)\n    >>> stats.weightedtau(y, x, rank=None)\n    SignificanceResult(statistic=-0.7181341329699028, pvalue=nan)\n\n    ",
    "scipy.stats.wilcoxon": "    \n\n\nCalculate the Wilcoxon signed-rank test.\n\nThe Wilcoxon signed-rank test tests the null hypothesis that two\nrelated paired samples come from the same distribution. In particular,\nit tests whether the distribution of the differences ``x - y`` is symmetric\nabout zero. It is a non-parametric version of the paired T-test.\n\nParameters\n----------\nx : array_like\n    Either the first set of measurements (in which case ``y`` is the second\n    set of measurements), or the differences between two sets of\n    measurements (in which case ``y`` is not to be specified.)  Must be\n    one-dimensional.\ny : array_like, optional\n    Either the second set of measurements (if ``x`` is the first set of\n    measurements), or not specified (if ``x`` is the differences between\n    two sets of measurements.)  Must be one-dimensional.\n    \n    .. warning::\n        When `y` is provided, `wilcoxon` calculates the test statistic\n        based on the ranks of the absolute values of ``d = x - y``.\n        Roundoff error in the subtraction can result in elements of ``d``\n        being assigned different ranks even when they would be tied with\n        exact arithmetic. Rather than passing `x` and `y` separately,\n        consider computing the difference ``x - y``, rounding as needed to\n        ensure that only truly unique elements are numerically distinct,\n        and passing the result as `x`, leaving `y` at the default (None).\nzero_method : {\"wilcox\", \"pratt\", \"zsplit\"}, optional\n    There are different conventions for handling pairs of observations\n    with equal values (\"zero-differences\", or \"zeros\").\n    \n    * \"wilcox\": Discards all zero-differences (default); see [4]_.\n    * \"pratt\": Includes zero-differences in the ranking process,\n      but drops the ranks of the zeros (more conservative); see [3]_.\n      In this case, the normal approximation is adjusted as in [5]_.\n    * \"zsplit\": Includes zero-differences in the ranking process and\n      splits the zero rank between positive and negative ones.\ncorrection : bool, optional\n    If True, apply continuity correction by adjusting the Wilcoxon rank\n    statistic by 0.5 towards the mean value when computing the\n    z-statistic if a normal approximation is used.  Default is False.\nalternative : {\"two-sided\", \"greater\", \"less\"}, optional\n    Defines the alternative hypothesis. Default is 'two-sided'.\n    In the following, let ``d`` represent the difference between the paired\n    samples: ``d = x - y`` if both ``x`` and ``y`` are provided, or\n    ``d = x`` otherwise.\n    \n    * 'two-sided': the distribution underlying ``d`` is not symmetric\n      about zero.\n    * 'less': the distribution underlying ``d`` is stochastically less\n      than a distribution symmetric about zero.\n    * 'greater': the distribution underlying ``d`` is stochastically\n      greater than a distribution symmetric about zero.\nmethod : {\"auto\", \"exact\", \"approx\"} or `PermutationMethod` instance, optional\n    Method to calculate the p-value, see Notes. Default is \"auto\".\naxis : int or None, default: 0\n    If an int, the axis of the input along which to compute the statistic.\n    The statistic of each axis-slice (e.g. row) of the input will appear in a\n    corresponding element of the output.\n    If ``None``, the input will be raveled before computing the statistic.\nnan_policy : {'propagate', 'omit', 'raise'}\n    Defines how to handle input NaNs.\n    \n    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n      which the  statistic is computed, the corresponding entry of the output\n      will be NaN.\n    - ``omit``: NaNs will be omitted when performing the calculation.\n      If insufficient data remains in the axis slice along which the\n      statistic is computed, the corresponding entry of the output will be\n      NaN.\n    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\nkeepdims : bool, default: False\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\nReturns\n-------\nAn object with the following attributes.\nstatistic : array_like\n    If `alternative` is \"two-sided\", the sum of the ranks of the\n    differences above or below zero, whichever is smaller.\n    Otherwise the sum of the ranks of the differences above zero.\npvalue : array_like\n    The p-value for the test depending on `alternative` and `method`.\nzstatistic : array_like\n    When ``method = 'approx'``, this is the normalized z-statistic::\n    \n        z = (T - mn - d) / se\n    \n    where ``T`` is `statistic` as defined above, ``mn`` is the mean of the\n    distribution under the null hypothesis, ``d`` is a continuity\n    correction, and ``se`` is the standard error.\n    When ``method != 'approx'``, this attribute is not available.\n\nSee Also\n--------\n\n:func:`kruskal`, :func:`mannwhitneyu`\n    ..\n\nNotes\n-----\nIn the following, let ``d`` represent the difference between the paired\nsamples: ``d = x - y`` if both ``x`` and ``y`` are provided, or ``d = x``\notherwise. Assume that all elements of ``d`` are independent and\nidentically distributed observations, and all are distinct and nonzero.\n\n- When ``len(d)`` is sufficiently large, the null distribution of the\n  normalized test statistic (`zstatistic` above) is approximately normal,\n  and ``method = 'approx'`` can be used to compute the p-value.\n\n- When ``len(d)`` is small, the normal approximation may not be accurate,\n  and ``method='exact'`` is preferred (at the cost of additional\n  execution time).\n\n- The default, ``method='auto'``, selects between the two: when\n  ``len(d) <= 50`` and there are no zeros, the exact method is used;\n  otherwise, the approximate method is used.\n\nThe presence of \"ties\" (i.e. not all elements of ``d`` are unique) or\n\"zeros\" (i.e. elements of ``d`` are zero) changes the null distribution\nof the test statistic, and ``method='exact'`` no longer calculates\nthe exact p-value. If ``method='approx'``, the z-statistic is adjusted\nfor more accurate comparison against the standard normal, but still,\nfor finite sample sizes, the standard normal is only an approximation of\nthe true null distribution of the z-statistic. For such situations, the\n`method` parameter also accepts instances `PermutationMethod`. In this\ncase, the p-value is computed using `permutation_test` with the provided\nconfiguration options and other appropriate settings.\n\nBeginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\ncode) are converted to ``np.ndarray`` before the calculation is performed. In\nthis case, the output will be a scalar or ``np.ndarray`` of appropriate shape\nrather than a 2D ``np.matrix``. Similarly, while masked elements of masked\narrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\nmasked array with ``mask=False``.\n\nReferences\n----------\n.. [1] https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test\n.. [2] Conover, W.J., Practical Nonparametric Statistics, 1971.\n.. [3] Pratt, J.W., Remarks on Zeros and Ties in the Wilcoxon Signed\n   Rank Procedures, Journal of the American Statistical Association,\n   Vol. 54, 1959, pp. 655-667. :doi:`10.1080/01621459.1959.10501526`\n.. [4] Wilcoxon, F., Individual Comparisons by Ranking Methods,\n   Biometrics Bulletin, Vol. 1, 1945, pp. 80-83. :doi:`10.2307/3001968`\n.. [5] Cureton, E.E., The Normal Approximation to the Signed-Rank\n   Sampling Distribution When Zero Differences are Present,\n   Journal of the American Statistical Association, Vol. 62, 1967,\n   pp. 1068-1069. :doi:`10.1080/01621459.1967.10500917`\n\nExamples\n--------\nIn [4]_, the differences in height between cross- and self-fertilized\ncorn plants is given as follows:\n\n>>> d = [6, 8, 14, 16, 23, 24, 28, 29, 41, -48, 49, 56, 60, -67, 75]\n\nCross-fertilized plants appear to be higher. To test the null\nhypothesis that there is no height difference, we can apply the\ntwo-sided test:\n\n>>> from scipy.stats import wilcoxon\n>>> res = wilcoxon(d)\n>>> res.statistic, res.pvalue\n(24.0, 0.041259765625)\n\nHence, we would reject the null hypothesis at a confidence level of 5%,\nconcluding that there is a difference in height between the groups.\nTo confirm that the median of the differences can be assumed to be\npositive, we use:\n\n>>> res = wilcoxon(d, alternative='greater')\n>>> res.statistic, res.pvalue\n(96.0, 0.0206298828125)\n\nThis shows that the null hypothesis that the median is negative can be\nrejected at a confidence level of 5% in favor of the alternative that\nthe median is greater than zero. The p-values above are exact. Using the\nnormal approximation gives very similar values:\n\n>>> res = wilcoxon(d, method='approx')\n>>> res.statistic, res.pvalue\n(24.0, 0.04088813291185591)\n\nNote that the statistic changed to 96 in the one-sided case (the sum\nof ranks of positive differences) whereas it is 24 in the two-sided\ncase (the minimum of sum of ranks above and below zero).\n\nIn the example above, the differences in height between paired plants are\nprovided to `wilcoxon` directly. Alternatively, `wilcoxon` accepts two\nsamples of equal length, calculates the differences between paired\nelements, then performs the test. Consider the samples ``x`` and ``y``:\n\n>>> import numpy as np\n>>> x = np.array([0.5, 0.825, 0.375, 0.5])\n>>> y = np.array([0.525, 0.775, 0.325, 0.55])\n>>> res = wilcoxon(x, y, alternative='greater')\n>>> res\nWilcoxonResult(statistic=5.0, pvalue=0.5625)\n\nNote that had we calculated the differences by hand, the test would have\nproduced different results:\n\n>>> d = [-0.025, 0.05, 0.05, -0.05]\n>>> ref = wilcoxon(d, alternative='greater')\n>>> ref\nWilcoxonResult(statistic=6.0, pvalue=0.4375)\n\nThe substantial difference is due to roundoff error in the results of\n``x-y``:\n\n>>> d - (x-y)\narray([2.08166817e-17, 6.93889390e-17, 1.38777878e-17, 4.16333634e-17])\n\nEven though we expected all the elements of ``(x-y)[1:]`` to have the same\nmagnitude ``0.05``, they have slightly different magnitudes in practice,\nand therefore are assigned different ranks in the test. Before performing\nthe test, consider calculating ``d`` and adjusting it as necessary to\nensure that theoretically identically values are not numerically distinct.\nFor example:\n\n>>> d2 = np.around(x - y, decimals=3)\n>>> wilcoxon(d2, alternative='greater')\nWilcoxonResult(statistic=6.0, pvalue=0.4375)\n",
    "scipy.stats.wishart": "A Wishart random variable.\n\n    The `df` keyword specifies the degrees of freedom. The `scale` keyword\n    specifies the scale matrix, which must be symmetric and positive definite.\n    In this context, the scale matrix is often interpreted in terms of a\n    multivariate normal precision matrix (the inverse of the covariance\n    matrix). These arguments must satisfy the relationship\n    ``df > scale.ndim - 1``, but see notes on using the `rvs` method with\n    ``df < scale.ndim``.\n\n    Methods\n    -------\n    pdf(x, df, scale)\n        Probability density function.\n    logpdf(x, df, scale)\n        Log of the probability density function.\n    rvs(df, scale, size=1, random_state=None)\n        Draw random samples from a Wishart distribution.\n    entropy()\n        Compute the differential entropy of the Wishart distribution.\n\n    Parameters\n    ----------\n    df : int\n        Degrees of freedom, must be greater than or equal to dimension of the\n        scale matrix\n    scale : array_like\n        Symmetric positive definite scale matrix of the distribution\n    seed : {None, int, np.random.RandomState, np.random.Generator}, optional\n        Used for drawing random variates.\n        If `seed` is `None`, the `~np.random.RandomState` singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used, seeded\n        with seed.\n        If `seed` is already a ``RandomState`` or ``Generator`` instance,\n        then that object is used.\n        Default is `None`.\n\n    Raises\n    ------\n    scipy.linalg.LinAlgError\n        If the scale matrix `scale` is not positive definite.\n\n    See Also\n    --------\n    invwishart, chi2\n\n    Notes\n    -----\n    \n\n    The scale matrix `scale` must be a symmetric positive definite\n    matrix. Singular matrices, including the symmetric positive semi-definite\n    case, are not supported. Symmetry is not checked; only the lower triangular\n    portion is used.\n\n    The Wishart distribution is often denoted\n\n    .. math::\n\n        W_p(\\nu, \\Sigma)\n\n    where :math:`\\nu` is the degrees of freedom and :math:`\\Sigma` is the\n    :math:`p \\times p` scale matrix.\n\n    The probability density function for `wishart` has support over positive\n    definite matrices :math:`S`; if :math:`S \\sim W_p(\\nu, \\Sigma)`, then\n    its PDF is given by:\n\n    .. math::\n\n        f(S) = \\frac{|S|^{\\frac{\\nu - p - 1}{2}}}{2^{ \\frac{\\nu p}{2} }\n               |\\Sigma|^\\frac{\\nu}{2} \\Gamma_p \\left ( \\frac{\\nu}{2} \\right )}\n               \\exp\\left( -tr(\\Sigma^{-1} S) / 2 \\right)\n\n    If :math:`S \\sim W_p(\\nu, \\Sigma)` (Wishart) then\n    :math:`S^{-1} \\sim W_p^{-1}(\\nu, \\Sigma^{-1})` (inverse Wishart).\n\n    If the scale matrix is 1-dimensional and equal to one, then the Wishart\n    distribution :math:`W_1(\\nu, 1)` collapses to the :math:`\\chi^2(\\nu)`\n    distribution.\n\n    The algorithm [2]_ implemented by the `rvs` method may\n    produce numerically singular matrices with :math:`p - 1 < \\nu < p`; the\n    user may wish to check for this condition and generate replacement samples\n    as necessary.\n\n\n    .. versionadded:: 0.16.0\n\n    References\n    ----------\n    .. [1] M.L. Eaton, \"Multivariate Statistics: A Vector Space Approach\",\n           Wiley, 1983.\n    .. [2] W.B. Smith and R.R. Hocking, \"Algorithm AS 53: Wishart Variate\n           Generator\", Applied Statistics, vol. 21, pp. 341-345, 1972.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy.stats import wishart, chi2\n    >>> x = np.linspace(1e-5, 8, 100)\n    >>> w = wishart.pdf(x, df=3, scale=1); w[:5]\n    array([ 0.00126156,  0.10892176,  0.14793434,  0.17400548,  0.1929669 ])\n    >>> c = chi2.pdf(x, 3); c[:5]\n    array([ 0.00126156,  0.10892176,  0.14793434,  0.17400548,  0.1929669 ])\n    >>> plt.plot(x, w)\n    >>> plt.show()\n\n    The input quantiles can be any shape of array, as long as the last\n    axis labels the components.\n\n    Alternatively, the object may be called (as a function) to fix the degrees\n    of freedom and scale parameters, returning a \"frozen\" Wishart random\n    variable:\n\n    >>> rv = wishart(df=1, scale=1)\n    >>> # Frozen object with the same methods but holding the given\n    >>> # degrees of freedom and scale fixed.\n\n    ",
    "scipy.stats.wrapcauchy": "A wrapped Cauchy continuous random variable.\n\n    As an instance of the `rv_continuous` class, `wrapcauchy` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(c, loc=0, scale=1, size=1, random_state=None)\n        Random variates.\n    pdf(x, c, loc=0, scale=1)\n        Probability density function.\n    logpdf(x, c, loc=0, scale=1)\n        Log of the probability density function.\n    cdf(x, c, loc=0, scale=1)\n        Cumulative distribution function.\n    logcdf(x, c, loc=0, scale=1)\n        Log of the cumulative distribution function.\n    sf(x, c, loc=0, scale=1)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(x, c, loc=0, scale=1)\n        Log of the survival function.\n    ppf(q, c, loc=0, scale=1)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, c, loc=0, scale=1)\n        Inverse survival function (inverse of ``sf``).\n    moment(order, c, loc=0, scale=1)\n        Non-central moment of the specified order.\n    stats(c, loc=0, scale=1, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(c, loc=0, scale=1)\n        (Differential) entropy of the RV.\n    fit(data)\n        Parameter estimates for generic data.\n        See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n        keyword arguments.\n    expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(c, loc=0, scale=1)\n        Median of the distribution.\n    mean(c, loc=0, scale=1)\n        Mean of the distribution.\n    var(c, loc=0, scale=1)\n        Variance of the distribution.\n    std(c, loc=0, scale=1)\n        Standard deviation of the distribution.\n    interval(confidence, c, loc=0, scale=1)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n    The probability density function for `wrapcauchy` is:\n\n    .. math::\n\n        f(x, c) = \\frac{1-c^2}{2\\pi (1+c^2 - 2c \\cos(x))}\n\n    for :math:`0 \\le x \\le 2\\pi`, :math:`0 < c < 1`.\n\n    `wrapcauchy` takes ``c`` as a shape parameter for :math:`c`.\n\n    The probability density above is defined in the \"standardized\" form. To shift\n    and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n    Specifically, ``wrapcauchy.pdf(x, c, loc, scale)`` is identically\n    equivalent to ``wrapcauchy.pdf(y, c) / scale`` with\n    ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n    does not make it a \"noncentral\" distribution; noncentral generalizations of\n    some distributions are available in separate classes.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import wrapcauchy\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> c = 0.0311\n    >>> mean, var, skew, kurt = wrapcauchy.stats(c, moments='mvsk')\n    \n    Display the probability density function (``pdf``):\n    \n    >>> x = np.linspace(wrapcauchy.ppf(0.01, c),\n    ...                 wrapcauchy.ppf(0.99, c), 100)\n    >>> ax.plot(x, wrapcauchy.pdf(x, c),\n    ...        'r-', lw=5, alpha=0.6, label='wrapcauchy pdf')\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape, location and scale parameters. This returns a \"frozen\"\n    RV object holding the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pdf``:\n    \n    >>> rv = wrapcauchy(c)\n    >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> vals = wrapcauchy.ppf([0.001, 0.5, 0.999], c)\n    >>> np.allclose([0.001, 0.5, 0.999], wrapcauchy.cdf(vals, c))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = wrapcauchy.rvs(c, size=1000)\n    \n    And compare the histogram:\n    \n    >>> ax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n    >>> ax.set_xlim([x[0], x[-1]])\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n\n    ",
    "scipy.stats.yeojohnson": "Return a dataset transformed by a Yeo-Johnson power transformation.\n\n    Parameters\n    ----------\n    x : ndarray\n        Input array.  Should be 1-dimensional.\n    lmbda : float, optional\n        If ``lmbda`` is ``None``, find the lambda that maximizes the\n        log-likelihood function and return it as the second output argument.\n        Otherwise the transformation is done for the given value.\n\n    Returns\n    -------\n    yeojohnson: ndarray\n        Yeo-Johnson power transformed array.\n    maxlog : float, optional\n        If the `lmbda` parameter is None, the second returned argument is\n        the lambda that maximizes the log-likelihood function.\n\n    See Also\n    --------\n    probplot, yeojohnson_normplot, yeojohnson_normmax, yeojohnson_llf, boxcox\n\n    Notes\n    -----\n    The Yeo-Johnson transform is given by::\n\n        y = ((x + 1)**lmbda - 1) / lmbda,                for x >= 0, lmbda != 0\n            log(x + 1),                                  for x >= 0, lmbda = 0\n            -((-x + 1)**(2 - lmbda) - 1) / (2 - lmbda),  for x < 0, lmbda != 2\n            -log(-x + 1),                                for x < 0, lmbda = 2\n\n    Unlike `boxcox`, `yeojohnson` does not require the input data to be\n    positive.\n\n    .. versionadded:: 1.2.0\n\n\n    References\n    ----------\n    I. Yeo and R.A. Johnson, \"A New Family of Power Transformations to\n    Improve Normality or Symmetry\", Biometrika 87.4 (2000):\n\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n\n    We generate some random variates from a non-normal distribution and make a\n    probability plot for it, to show it is non-normal in the tails:\n\n    >>> fig = plt.figure()\n    >>> ax1 = fig.add_subplot(211)\n    >>> x = stats.loggamma.rvs(5, size=500) + 5\n    >>> prob = stats.probplot(x, dist=stats.norm, plot=ax1)\n    >>> ax1.set_xlabel('')\n    >>> ax1.set_title('Probplot against normal distribution')\n\n    We now use `yeojohnson` to transform the data so it's closest to normal:\n\n    >>> ax2 = fig.add_subplot(212)\n    >>> xt, lmbda = stats.yeojohnson(x)\n    >>> prob = stats.probplot(xt, dist=stats.norm, plot=ax2)\n    >>> ax2.set_title('Probplot after Yeo-Johnson transformation')\n\n    >>> plt.show()\n\n    ",
    "scipy.stats.yeojohnson_llf": "The yeojohnson log-likelihood function.\n\n    Parameters\n    ----------\n    lmb : scalar\n        Parameter for Yeo-Johnson transformation. See `yeojohnson` for\n        details.\n    data : array_like\n        Data to calculate Yeo-Johnson log-likelihood for. If `data` is\n        multi-dimensional, the log-likelihood is calculated along the first\n        axis.\n\n    Returns\n    -------\n    llf : float\n        Yeo-Johnson log-likelihood of `data` given `lmb`.\n\n    See Also\n    --------\n    yeojohnson, probplot, yeojohnson_normplot, yeojohnson_normmax\n\n    Notes\n    -----\n    The Yeo-Johnson log-likelihood function is defined here as\n\n    .. math::\n\n        llf = -N/2 \\log(\\hat{\\sigma}^2) + (\\lambda - 1)\n              \\sum_i \\text{ sign }(x_i)\\log(|x_i| + 1)\n\n    where :math:`\\hat{\\sigma}^2` is estimated variance of the Yeo-Johnson\n    transformed input data ``x``.\n\n    .. versionadded:: 1.2.0\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n    >>> from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\n    Generate some random variates and calculate Yeo-Johnson log-likelihood\n    values for them for a range of ``lmbda`` values:\n\n    >>> x = stats.loggamma.rvs(5, loc=10, size=1000)\n    >>> lmbdas = np.linspace(-2, 10)\n    >>> llf = np.zeros(lmbdas.shape, dtype=float)\n    >>> for ii, lmbda in enumerate(lmbdas):\n    ...     llf[ii] = stats.yeojohnson_llf(lmbda, x)\n\n    Also find the optimal lmbda value with `yeojohnson`:\n\n    >>> x_most_normal, lmbda_optimal = stats.yeojohnson(x)\n\n    Plot the log-likelihood as function of lmbda.  Add the optimal lmbda as a\n    horizontal line to check that that's really the optimum:\n\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> ax.plot(lmbdas, llf, 'b.-')\n    >>> ax.axhline(stats.yeojohnson_llf(lmbda_optimal, x), color='r')\n    >>> ax.set_xlabel('lmbda parameter')\n    >>> ax.set_ylabel('Yeo-Johnson log-likelihood')\n\n    Now add some probability plots to show that where the log-likelihood is\n    maximized the data transformed with `yeojohnson` looks closest to normal:\n\n    >>> locs = [3, 10, 4]  # 'lower left', 'center', 'lower right'\n    >>> for lmbda, loc in zip([-1, lmbda_optimal, 9], locs):\n    ...     xt = stats.yeojohnson(x, lmbda=lmbda)\n    ...     (osm, osr), (slope, intercept, r_sq) = stats.probplot(xt)\n    ...     ax_inset = inset_axes(ax, width=\"20%\", height=\"20%\", loc=loc)\n    ...     ax_inset.plot(osm, osr, 'c.', osm, slope*osm + intercept, 'k-')\n    ...     ax_inset.set_xticklabels([])\n    ...     ax_inset.set_yticklabels([])\n    ...     ax_inset.set_title(r'$\\lambda=%1.2f$' % lmbda)\n\n    >>> plt.show()\n\n    ",
    "scipy.stats.yeojohnson_normmax": "Compute optimal Yeo-Johnson transform parameter.\n\n    Compute optimal Yeo-Johnson transform parameter for input data, using\n    maximum likelihood estimation.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array.\n    brack : 2-tuple, optional\n        The starting interval for a downhill bracket search with\n        `optimize.brent`. Note that this is in most cases not critical; the\n        final result is allowed to be outside this bracket. If None,\n        `optimize.fminbound` is used with bounds that avoid overflow.\n\n    Returns\n    -------\n    maxlog : float\n        The optimal transform parameter found.\n\n    See Also\n    --------\n    yeojohnson, yeojohnson_llf, yeojohnson_normplot\n\n    Notes\n    -----\n    .. versionadded:: 1.2.0\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n\n    Generate some data and determine optimal ``lmbda``\n\n    >>> rng = np.random.default_rng()\n    >>> x = stats.loggamma.rvs(5, size=30, random_state=rng) + 5\n    >>> lmax = stats.yeojohnson_normmax(x)\n\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> prob = stats.yeojohnson_normplot(x, -10, 10, plot=ax)\n    >>> ax.axvline(lmax, color='r')\n\n    >>> plt.show()\n\n    ",
    "scipy.stats.yeojohnson_normplot": "Compute parameters for a Yeo-Johnson normality plot, optionally show it.\n\n    A Yeo-Johnson normality plot shows graphically what the best\n    transformation parameter is to use in `yeojohnson` to obtain a\n    distribution that is close to normal.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array.\n    la, lb : scalar\n        The lower and upper bounds for the ``lmbda`` values to pass to\n        `yeojohnson` for Yeo-Johnson transformations. These are also the\n        limits of the horizontal axis of the plot if that is generated.\n    plot : object, optional\n        If given, plots the quantiles and least squares fit.\n        `plot` is an object that has to have methods \"plot\" and \"text\".\n        The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n        or a custom object with the same methods.\n        Default is None, which means that no plot is created.\n    N : int, optional\n        Number of points on the horizontal axis (equally distributed from\n        `la` to `lb`).\n\n    Returns\n    -------\n    lmbdas : ndarray\n        The ``lmbda`` values for which a Yeo-Johnson transform was done.\n    ppcc : ndarray\n        Probability Plot Correlelation Coefficient, as obtained from `probplot`\n        when fitting the Box-Cox transformed input `x` against a normal\n        distribution.\n\n    See Also\n    --------\n    probplot, yeojohnson, yeojohnson_normmax, yeojohnson_llf, ppcc_max\n\n    Notes\n    -----\n    Even if `plot` is given, the figure is not shown or saved by\n    `boxcox_normplot`; ``plt.show()`` or ``plt.savefig('figname.png')``\n    should be used after calling `probplot`.\n\n    .. versionadded:: 1.2.0\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n\n    Generate some non-normally distributed data, and create a Yeo-Johnson plot:\n\n    >>> x = stats.loggamma.rvs(5, size=500) + 5\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> prob = stats.yeojohnson_normplot(x, -20, 20, plot=ax)\n\n    Determine and plot the optimal ``lmbda`` to transform ``x`` and plot it in\n    the same plot:\n\n    >>> _, maxlog = stats.yeojohnson(x)\n    >>> ax.axvline(maxlog, color='r')\n\n    >>> plt.show()\n\n    ",
    "scipy.stats.yulesimon": "A Yule-Simon discrete random variable.\n\n    As an instance of the `rv_discrete` class, `yulesimon` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(alpha, loc=0, size=1, random_state=None)\n        Random variates.\n    pmf(k, alpha, loc=0)\n        Probability mass function.\n    logpmf(k, alpha, loc=0)\n        Log of the probability mass function.\n    cdf(k, alpha, loc=0)\n        Cumulative distribution function.\n    logcdf(k, alpha, loc=0)\n        Log of the cumulative distribution function.\n    sf(k, alpha, loc=0)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(k, alpha, loc=0)\n        Log of the survival function.\n    ppf(q, alpha, loc=0)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, alpha, loc=0)\n        Inverse survival function (inverse of ``sf``).\n    stats(alpha, loc=0, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(alpha, loc=0)\n        (Differential) entropy of the RV.\n    expect(func, args=(alpha,), loc=0, lb=None, ub=None, conditional=False)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(alpha, loc=0)\n        Median of the distribution.\n    mean(alpha, loc=0)\n        Mean of the distribution.\n    var(alpha, loc=0)\n        Variance of the distribution.\n    std(alpha, loc=0)\n        Standard deviation of the distribution.\n    interval(confidence, alpha, loc=0)\n        Confidence interval with equal areas around the median.\n\n    Notes\n    -----\n\n    The probability mass function for the `yulesimon` is:\n\n    .. math::\n\n        f(k) =  \\alpha B(k, \\alpha+1)\n\n    for :math:`k=1,2,3,...`, where :math:`\\alpha>0`.\n    Here :math:`B` refers to the `scipy.special.beta` function.\n\n    The sampling of random variates is based on pg 553, Section 6.3 of [1]_.\n    Our notation maps to the referenced logic via :math:`\\alpha=a-1`.\n\n    For details see the wikipedia entry [2]_.\n\n    References\n    ----------\n    .. [1] Devroye, Luc. \"Non-uniform Random Variate Generation\",\n         (1986) Springer, New York.\n\n    .. [2] https://en.wikipedia.org/wiki/Yule-Simon_distribution\n\n    The probability mass function above is defined in the \"standardized\" form.\n    To shift distribution use the ``loc`` parameter.\n    Specifically, ``yulesimon.pmf(k, alpha, loc)`` is identically\n    equivalent to ``yulesimon.pmf(k - loc, alpha)``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import yulesimon\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> alpha = 11\n    >>> mean, var, skew, kurt = yulesimon.stats(alpha, moments='mvsk')\n    \n    Display the probability mass function (``pmf``):\n    \n    >>> x = np.arange(yulesimon.ppf(0.01, alpha),\n    ...               yulesimon.ppf(0.99, alpha))\n    >>> ax.plot(x, yulesimon.pmf(x, alpha), 'bo', ms=8, label='yulesimon pmf')\n    >>> ax.vlines(x, 0, yulesimon.pmf(x, alpha), colors='b', lw=5, alpha=0.5)\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape and location. This returns a \"frozen\" RV object holding\n    the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pmf``:\n    \n    >>> rv = yulesimon(alpha)\n    >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n    ...         label='frozen pmf')\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> prob = yulesimon.cdf(x, alpha)\n    >>> np.allclose(x, yulesimon.ppf(prob, alpha))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = yulesimon.rvs(alpha, size=1000)\n\n    ",
    "scipy.stats.zipf": "A Zipf (Zeta) discrete random variable.\n\n    As an instance of the `rv_discrete` class, `zipf` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, loc=0, size=1, random_state=None)\n        Random variates.\n    pmf(k, a, loc=0)\n        Probability mass function.\n    logpmf(k, a, loc=0)\n        Log of the probability mass function.\n    cdf(k, a, loc=0)\n        Cumulative distribution function.\n    logcdf(k, a, loc=0)\n        Log of the cumulative distribution function.\n    sf(k, a, loc=0)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(k, a, loc=0)\n        Log of the survival function.\n    ppf(q, a, loc=0)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, loc=0)\n        Inverse survival function (inverse of ``sf``).\n    stats(a, loc=0, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, loc=0)\n        (Differential) entropy of the RV.\n    expect(func, args=(a,), loc=0, lb=None, ub=None, conditional=False)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, loc=0)\n        Median of the distribution.\n    mean(a, loc=0)\n        Mean of the distribution.\n    var(a, loc=0)\n        Variance of the distribution.\n    std(a, loc=0)\n        Standard deviation of the distribution.\n    interval(confidence, a, loc=0)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    zipfian\n\n    Notes\n    -----\n    The probability mass function for `zipf` is:\n\n    .. math::\n\n        f(k, a) = \\frac{1}{\\zeta(a) k^a}\n\n    for :math:`k \\ge 1`, :math:`a > 1`.\n\n    `zipf` takes :math:`a > 1` as shape parameter. :math:`\\zeta` is the\n    Riemann zeta function (`scipy.special.zeta`)\n\n    The Zipf distribution is also known as the zeta distribution, which is\n    a special case of the Zipfian distribution (`zipfian`).\n\n    The probability mass function above is defined in the \"standardized\" form.\n    To shift distribution use the ``loc`` parameter.\n    Specifically, ``zipf.pmf(k, a, loc)`` is identically\n    equivalent to ``zipf.pmf(k - loc, a)``.\n\n    References\n    ----------\n    .. [1] \"Zeta Distribution\", Wikipedia,\n           https://en.wikipedia.org/wiki/Zeta_distribution\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import zipf\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a = 6.6\n    >>> mean, var, skew, kurt = zipf.stats(a, moments='mvsk')\n    \n    Display the probability mass function (``pmf``):\n    \n    >>> x = np.arange(zipf.ppf(0.01, a),\n    ...               zipf.ppf(0.99, a))\n    >>> ax.plot(x, zipf.pmf(x, a), 'bo', ms=8, label='zipf pmf')\n    >>> ax.vlines(x, 0, zipf.pmf(x, a), colors='b', lw=5, alpha=0.5)\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape and location. This returns a \"frozen\" RV object holding\n    the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pmf``:\n    \n    >>> rv = zipf(a)\n    >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n    ...         label='frozen pmf')\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> prob = zipf.cdf(x, a)\n    >>> np.allclose(x, zipf.ppf(prob, a))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = zipf.rvs(a, size=1000)\n\n    Confirm that `zipf` is the large `n` limit of `zipfian`.\n\n    >>> import numpy as np\n    >>> from scipy.stats import zipf, zipfian\n    >>> k = np.arange(11)\n    >>> np.allclose(zipf.pmf(k, a), zipfian.pmf(k, a, n=10000000))\n    True\n\n    ",
    "scipy.stats.zipfian": "A Zipfian discrete random variable.\n\n    As an instance of the `rv_discrete` class, `zipfian` object inherits from it\n    a collection of generic methods (see below for the full list),\n    and completes them with details specific for this particular distribution.\n    \n    Methods\n    -------\n    rvs(a, n, loc=0, size=1, random_state=None)\n        Random variates.\n    pmf(k, a, n, loc=0)\n        Probability mass function.\n    logpmf(k, a, n, loc=0)\n        Log of the probability mass function.\n    cdf(k, a, n, loc=0)\n        Cumulative distribution function.\n    logcdf(k, a, n, loc=0)\n        Log of the cumulative distribution function.\n    sf(k, a, n, loc=0)\n        Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n    logsf(k, a, n, loc=0)\n        Log of the survival function.\n    ppf(q, a, n, loc=0)\n        Percent point function (inverse of ``cdf`` --- percentiles).\n    isf(q, a, n, loc=0)\n        Inverse survival function (inverse of ``sf``).\n    stats(a, n, loc=0, moments='mv')\n        Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n    entropy(a, n, loc=0)\n        (Differential) entropy of the RV.\n    expect(func, args=(a, n), loc=0, lb=None, ub=None, conditional=False)\n        Expected value of a function (of one argument) with respect to the distribution.\n    median(a, n, loc=0)\n        Median of the distribution.\n    mean(a, n, loc=0)\n        Mean of the distribution.\n    var(a, n, loc=0)\n        Variance of the distribution.\n    std(a, n, loc=0)\n        Standard deviation of the distribution.\n    interval(confidence, a, n, loc=0)\n        Confidence interval with equal areas around the median.\n\n    See Also\n    --------\n    zipf\n\n    Notes\n    -----\n    The probability mass function for `zipfian` is:\n\n    .. math::\n\n        f(k, a, n) = \\frac{1}{H_{n,a} k^a}\n\n    for :math:`k \\in \\{1, 2, \\dots, n-1, n\\}`, :math:`a \\ge 0`,\n    :math:`n \\in \\{1, 2, 3, \\dots\\}`.\n\n    `zipfian` takes :math:`a` and :math:`n` as shape parameters.\n    :math:`H_{n,a}` is the :math:`n`:sup:`th` generalized harmonic\n    number of order :math:`a`.\n\n    The Zipfian distribution reduces to the Zipf (zeta) distribution as\n    :math:`n \\rightarrow \\infty`.\n\n    The probability mass function above is defined in the \"standardized\" form.\n    To shift distribution use the ``loc`` parameter.\n    Specifically, ``zipfian.pmf(k, a, n, loc)`` is identically\n    equivalent to ``zipfian.pmf(k - loc, a, n)``.\n\n    References\n    ----------\n    .. [1] \"Zipf's Law\", Wikipedia, https://en.wikipedia.org/wiki/Zipf's_law\n    .. [2] Larry Leemis, \"Zipf Distribution\", Univariate Distribution\n           Relationships. http://www.math.wm.edu/~leemis/chart/UDR/PDFs/Zipf.pdf\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from scipy.stats import zipfian\n    >>> import matplotlib.pyplot as plt\n    >>> fig, ax = plt.subplots(1, 1)\n    \n    Calculate the first four moments:\n    \n    >>> a, n = 1.25, 10\n    >>> mean, var, skew, kurt = zipfian.stats(a, n, moments='mvsk')\n    \n    Display the probability mass function (``pmf``):\n    \n    >>> x = np.arange(zipfian.ppf(0.01, a, n),\n    ...               zipfian.ppf(0.99, a, n))\n    >>> ax.plot(x, zipfian.pmf(x, a, n), 'bo', ms=8, label='zipfian pmf')\n    >>> ax.vlines(x, 0, zipfian.pmf(x, a, n), colors='b', lw=5, alpha=0.5)\n    \n    Alternatively, the distribution object can be called (as a function)\n    to fix the shape and location. This returns a \"frozen\" RV object holding\n    the given parameters fixed.\n    \n    Freeze the distribution and display the frozen ``pmf``:\n    \n    >>> rv = zipfian(a, n)\n    >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n    ...         label='frozen pmf')\n    >>> ax.legend(loc='best', frameon=False)\n    >>> plt.show()\n    \n    Check accuracy of ``cdf`` and ``ppf``:\n    \n    >>> prob = zipfian.cdf(x, a, n)\n    >>> np.allclose(x, zipfian.ppf(prob, a, n))\n    True\n    \n    Generate random numbers:\n    \n    >>> r = zipfian.rvs(a, n, size=1000)\n\n    Confirm that `zipfian` reduces to `zipf` for large `n`, `a > 1`.\n\n    >>> import numpy as np\n    >>> from scipy.stats import zipf, zipfian\n    >>> k = np.arange(11)\n    >>> np.allclose(zipfian.pmf(k, a=3.5, n=10000000), zipf.pmf(k, a=3.5))\n    True\n\n    ",
    "scipy.stats.zmap": "\n    Calculate the relative z-scores.\n\n    Return an array of z-scores, i.e., scores that are standardized to\n    zero mean and unit variance, where mean and variance are calculated\n    from the comparison array.\n\n    Parameters\n    ----------\n    scores : array_like\n        The input for which z-scores are calculated.\n    compare : array_like\n        The input from which the mean and standard deviation of the\n        normalization are taken; assumed to have the same dimension as\n        `scores`.\n    axis : int or None, optional\n        Axis over which mean and variance of `compare` are calculated.\n        Default is 0. If None, compute over the whole array `scores`.\n    ddof : int, optional\n        Degrees of freedom correction in the calculation of the\n        standard deviation. Default is 0.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle the occurrence of nans in `compare`.\n        'propagate' returns nan, 'raise' raises an exception, 'omit'\n        performs the calculations ignoring nan values. Default is\n        'propagate'. Note that when the value is 'omit', nans in `scores`\n        also propagate to the output, but they do not affect the z-scores\n        computed for the non-nan values.\n\n    Returns\n    -------\n    zscore : array_like\n        Z-scores, in the same shape as `scores`.\n\n    Notes\n    -----\n    This function preserves ndarray subclasses, and works also with\n    matrices and masked arrays (it uses `asanyarray` instead of\n    `asarray` for parameters).\n\n    Examples\n    --------\n    >>> from scipy.stats import zmap\n    >>> a = [0.5, 2.0, 2.5, 3]\n    >>> b = [0, 1, 2, 3, 4]\n    >>> zmap(a, b)\n    array([-1.06066017,  0.        ,  0.35355339,  0.70710678])\n\n    ",
    "scipy.stats.zscore": "\n    Compute the z score.\n\n    Compute the z score of each value in the sample, relative to the\n    sample mean and standard deviation.\n\n    Parameters\n    ----------\n    a : array_like\n        An array like object containing the sample data.\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over\n        the whole array `a`.\n    ddof : int, optional\n        Degrees of freedom correction in the calculation of the\n        standard deviation. Default is 0.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan. 'propagate' returns nan,\n        'raise' throws an error, 'omit' performs the calculations ignoring nan\n        values. Default is 'propagate'.  Note that when the value is 'omit',\n        nans in the input also propagate to the output, but they do not affect\n        the z-scores computed for the non-nan values.\n\n    Returns\n    -------\n    zscore : array_like\n        The z-scores, standardized by mean and standard deviation of\n        input array `a`.\n\n    See Also\n    --------\n    numpy.mean : Arithmetic average\n    numpy.std : Arithmetic standard deviation\n    scipy.stats.gzscore : Geometric standard score\n\n    Notes\n    -----\n    This function preserves ndarray subclasses, and works also with\n    matrices and masked arrays (it uses `asanyarray` instead of\n    `asarray` for parameters).\n\n    References\n    ----------\n    .. [1] \"Standard score\", *Wikipedia*,\n           https://en.wikipedia.org/wiki/Standard_score.\n    .. [2] Huck, S. W., Cross, T. L., Clark, S. B, \"Overcoming misconceptions\n           about Z-scores\", Teaching Statistics, vol. 8, pp. 38-40, 1986\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> a = np.array([ 0.7972,  0.0767,  0.4383,  0.7866,  0.8091,\n    ...                0.1954,  0.6307,  0.6599,  0.1065,  0.0508])\n    >>> from scipy import stats\n    >>> stats.zscore(a)\n    array([ 1.1273, -1.247 , -0.0552,  1.0923,  1.1664, -0.8559,  0.5786,\n            0.6748, -1.1488, -1.3324])\n\n    Computing along a specified axis, using n-1 degrees of freedom\n    (``ddof=1``) to calculate the standard deviation:\n\n    >>> b = np.array([[ 0.3148,  0.0478,  0.6243,  0.4608],\n    ...               [ 0.7149,  0.0775,  0.6072,  0.9656],\n    ...               [ 0.6341,  0.1403,  0.9759,  0.4064],\n    ...               [ 0.5918,  0.6948,  0.904 ,  0.3721],\n    ...               [ 0.0921,  0.2481,  0.1188,  0.1366]])\n    >>> stats.zscore(b, axis=1, ddof=1)\n    array([[-0.19264823, -1.28415119,  1.07259584,  0.40420358],\n           [ 0.33048416, -1.37380874,  0.04251374,  1.00081084],\n           [ 0.26796377, -1.12598418,  1.23283094, -0.37481053],\n           [-0.22095197,  0.24468594,  1.19042819, -1.21416216],\n           [-0.82780366,  1.4457416 , -0.43867764, -0.1792603 ]])\n\n    An example with `nan_policy='omit'`:\n\n    >>> x = np.array([[25.11, 30.10, np.nan, 32.02, 43.15],\n    ...               [14.95, 16.06, 121.25, 94.35, 29.81]])\n    >>> stats.zscore(x, axis=1, nan_policy='omit')\n    array([[-1.13490897, -0.37830299,         nan, -0.08718406,  1.60039602],\n           [-0.91611681, -0.89090508,  1.4983032 ,  0.88731639, -0.5785977 ]])\n    ",
    "stattools": "Statistical learning and inference algorithms implemented in pure Python.",
    "statsmodels.test": "\n    Run the test suite\n\n    Parameters\n    ----------\n    extra_args : list[str]\n        List of argument to pass to pytest when running the test suite. The\n        default is ['--tb=short', '--disable-pytest-warnings'].\n    exit : bool\n        Flag indicating whether the test runner should exit when finished.\n\n    Returns\n    -------\n    int\n        The status code from the test run if exit is False.\n    ",
    "arch.arch_model": "\n    Initialization of common ARCH model specifications\n\n    Parameters\n    ----------\n    y : ndarray, Series, None\n        The dependent variable\n    x : ndarray, DataFrame, optional\n        Exogenous regressors.  Ignored if model does not permit exogenous\n        regressors.\n    mean : str, optional\n        Name of the mean model.  Currently supported options are: 'Constant',\n        'Zero', 'LS', 'AR', 'ARX', 'HAR' and  'HARX'\n    lags : int or list[int], optional\n        Either a scalar integer value indicating lag length or a list of\n        integers specifying lag locations.\n    vol : str, optional\n        Name of the volatility model.  Currently supported options are:\n        'GARCH' (default), 'ARCH', 'EGARCH', 'FIGARCH', 'APARCH' and 'HARCH'\n    p : int, optional\n        Lag order of the symmetric innovation\n    o : int, optional\n        Lag order of the asymmetric innovation\n    q : int, optional\n        Lag order of lagged volatility or equivalent\n    power : float, optional\n        Power to use with GARCH and related models\n    dist : int, optional\n        Name of the error distribution.  Currently supported options are:\n\n            * Normal: 'normal', 'gaussian' (default)\n            * Students's t: 't', 'studentst'\n            * Skewed Student's t: 'skewstudent', 'skewt'\n            * Generalized Error Distribution: 'ged', 'generalized error\"\n\n    hold_back : int\n        Number of observations at the start of the sample to exclude when\n        estimating model parameters.  Used when comparing models with different\n        lag lengths to estimate on the common sample.\n    rescale : bool\n        Flag indicating whether to automatically rescale data if the scale\n        of the data is likely to produce convergence issues when estimating\n        model parameters. If False, the model is estimated on the data without\n        transformation.  If True, than y is rescaled and the new scale is\n        reported in the estimation results.\n\n    Returns\n    -------\n    model : ARCHModel\n        Configured ARCH model\n\n    Examples\n    --------\n    >>> import datetime as dt\n    >>> import pandas_datareader.data as web\n    >>> djia = web.get_data_fred('DJIA')\n    >>> returns = 100 * djia['DJIA'].pct_change().dropna()\n\n    A basic GARCH(1,1) with a constant mean can be constructed using only\n    the return data\n\n    >>> from arch.univariate import arch_model\n    >>> am = arch_model(returns)\n\n    Alternative mean and volatility processes can be directly specified\n\n    >>> am = arch_model(returns, mean='AR', lags=2, vol='harch', p=[1, 5, 22])\n\n    This example demonstrates the construction of a zero mean process\n    with a TARCH volatility process and Student t error distribution\n\n    >>> am = arch_model(returns, mean='zero', p=1, o=1, q=1,\n    ...                 power=1.0, dist='StudentsT')\n\n    Notes\n    -----\n    Input that are not relevant for a particular specification, such as `lags`\n    when `mean='zero'`, are silently ignored.\n    ",
    "arch.test": "\n    Test runner that allows testing of installed package.\n\n    Exists with test status code upon completion.\n\n    Parameters\n    ----------\n    extra_args : {str, list[str]}, default None\n        Extra arguments to pass to pytest. Default options are --tb=short\n        and --disable-pytest-warnings. Providing extra_args overwrites the\n        defaults with the user-provided arguments.\n    ",
    "pandas_datareader.DataReader": "\n    Imports data from a number of online sources.\n\n    Currently supports Google Finance, St. Louis FED (FRED),\n    and Kenneth French's data library, among others.\n\n    Parameters\n    ----------\n    name : str or list of strs\n        the name of the dataset. Some data sources (IEX, fred) will\n        accept a list of names.\n    data_source: {str, None}\n        the data source (\"iex\", \"fred\", \"ff\")\n    start : string, int, date, datetime, Timestamp\n        left boundary for range (defaults to 1/1/2010)\n    end : string, int, date, datetime, Timestamp\n        right boundary for range (defaults to today)\n    retry_count : {int, 3}\n        Number of times to retry query request.\n    pause : {numeric, 0.001}\n        Time, in seconds, to pause between consecutive queries of chunks. If\n        single value given for symbol, represents the pause between retries.\n    session : Session, default None\n        requests.sessions.Session instance to be used\n    api_key : (str, None)\n        Optional parameter to specify an API key for certain data sources.\n\n    Examples\n    ----------\n    # Data from Google Finance\n    aapl = DataReader(\"AAPL\", \"iex\")\n\n    # Price and volume data from IEX\n    tops = DataReader([\"GS\", \"AAPL\"], \"iex-tops\")\n    # Top of book executions from IEX\n    gs = DataReader(\"GS\", \"iex-last\")\n    # Real-time depth of book data from IEX\n    gs = DataReader(\"GS\", \"iex-book\")\n\n    # Data from FRED\n    vix = DataReader(\"VIXCLS\", \"fred\")\n\n    # Data from Fama/French\n    ff = DataReader(\"F-F_Research_Data_Factors\", \"famafrench\")\n    ff = DataReader(\"F-F_Research_Data_Factors_weekly\", \"famafrench\")\n    ff = DataReader(\"6_Portfolios_2x3\", \"famafrench\")\n    ff = DataReader(\"F-F_ST_Reversal_Factor\", \"famafrench\")\n    ",
    "pandas_datareader.get_components_yahoo": "\n    Returns DataFrame containing list of component information for\n    index represented in idx_sym from yahoo. Includes component symbol\n    (ticker), exchange, and name.\n\n    Parameters\n    ----------\n    idx_sym : str\n        Stock index symbol\n        Examples:\n        '^DJI' (Dow Jones Industrial Average)\n        '^NYA' (NYSE Composite)\n        '^IXIC' (NASDAQ Composite)\n\n        See: http://finance.yahoo.com/indices for other index symbols\n\n    Returns\n    -------\n    idx_df : DataFrame\n    ",
    "pandas_datareader.get_dailysummary_iex": "\n    Returns a summary of daily market volume statistics. Without parameters,\n    this will return the most recent trading session by default.\n\n    Parameters\n    ----------\n    start : string, int, date, datetime, Timestamp\n        The beginning of the date range.\n    end : string, int, date, datetime, Timestamp\n        The end of the date range.\n\n    Reference: https://www.iextrading.com/developer/docs/#historical-daily\n\n    :return: DataFrame\n    ",
    "pandas_datareader.get_iex_book": "\n    Returns an array of dictionaries with depth of book data from IEX for up to\n    10 securities at a time. Returns a dictionary of the bid and ask books.\n\n    Parameters\n    ----------\n    symbols : str, List[str]\n        A string or list of strings of valid tickers\n    service : str\n        One of:\n\n        - 'book': Live depth of book data\n        - 'op-halt-status': Checks to see if the exchange has instituted a halt\n        - 'security-event': Denotes individual security related event\n        - 'ssr-status': Short Sale Price Test restrictions, per reg 201 of SHO\n        - 'system-event': Relays current feed status (i.e. market open)\n        - 'trades': Retrieves recent executions, trade size/price and flags\n        - 'trade-breaks': Lists execution breaks for the current trading session\n        - 'trading-status': Returns status and cause codes for securities\n\n    Returns\n    -------\n    DataFrame\n    ",
    "pandas_datareader.get_iex_symbols": "\n    Returns a list of all equity symbols available for trading on IEX. Accepts\n    no additional parameters.\n\n    Reference: https://www.iextrading.com/developer/docs/#symbols\n\n    :return: DataFrame\n    ",
    "pandas_datareader.get_markets_iex": "\n    Returns near-real time volume data across markets segregated by tape\n    and including a percentage of overall volume during the session\n\n    This endpoint does not accept any parameters.\n\n    Reference: https://www.iextrading.com/developer/docs/#markets\n\n    Returns\n    -------\n    DataFrame\n    ",
    "pandas_datareader.get_nasdaq_symbols": "\n    Get the list of all available equity symbols from Nasdaq.\n\n    Returns\n    -------\n    nasdaq_tickers : pandas.DataFrame\n        DataFrame with company tickers, names, and other properties.\n    ",
    "pandas_datareader.get_recent_iex": "\n    Returns market volume and trade routing statistics for recent sessions.\n    Also reports IEX's relative market share, lit share volume and a boolean\n    halfday indicator.\n\n    Reference: https://www.iextrading.com/developer/docs/#recent\n\n    Returns\n    -------\n    DataFrame\n    ",
    "pandas_datareader.get_records_iex": "\n    Returns the record value, record date, recent value, and 30-day average for\n    market volume, # of symbols traded, # of routed trades and notional value.\n    This function accepts no additional parameters.\n\n    Reference: https://www.iextrading.com/developer/docs/#records\n\n    :return: DataFrame\n    ",
    "pandas_datareader.get_summary_iex": "\n    Returns an aggregated monthly summary of market volume and a variety of\n    related metrics for trades by lot size, security market cap, and venue.\n    In the absence of parameters, this will return month-to-date statistics.\n    For ranges spanning multiple months, this will return one row per month.\n\n    start : string, int, date, datetime, Timestamp\n        A datetime object - the beginning of the date range.\n    end : string, int, date, datetime, Timestamp\n        A datetime object - the end of the date range.\n\n    Returns\n    -------\n    DataFrame\n    ",
    "pandas_datareader.test": "\n    Run the test suite\n\n    Parameters\n    ----------\n    extra_args : {str, List[str]}\n        A string or list of strings to pass to pytest. Default is\n        [\"--only-stable\", \"--skip-requires-api-key\"]\n    ",
    "yfinance.EquityQuery": "\n    The `EquityQuery` class constructs filters for stocks based on specific criteria such as region, sector, exchange, and peer group.\n\n    The queries support operators: `GT` (greater than), `LT` (less than), `BTWN` (between), `EQ` (equals), and logical operators `AND` and `OR` for combining multiple conditions.\n\n    Example:\n        Screen for stocks where the end-of-day price is greater than 3.\n        \n        .. code-block:: python\n\n            gt = yf.EquityQuery('gt', ['eodprice', 3])\n\n        Screen for stocks where the average daily volume over the last 3 months is less than a very large number.\n\n        .. code-block:: python\n\n            lt = yf.EquityQuery('lt', ['avgdailyvol3m', 99999999999])\n\n        Screen for stocks where the intraday market cap is between 0 and 100 million.\n\n        .. code-block:: python\n\n            btwn = yf.EquityQuery('btwn', ['intradaymarketcap', 0, 100000000])\n\n        Screen for stocks in the Technology sector.\n\n        .. code-block:: python\n\n            eq = yf.EquityQuery('eq', ['sector', 'Technology'])\n\n        Combine queries using AND/OR.\n\n        .. code-block:: python\n\n            qt = yf.EquityQuery('and', [gt, lt])\n            qf = yf.EquityQuery('or', [qt, btwn, eq])\n    ",
    "yfinance.Industry": "\n    Represents an industry within a sector.\n    ",
    "yfinance.Screener": "\n    The `Screener` class is used to execute the queries and return the filtered results.\n\n    The Screener class provides methods to set and manipulate the body of a screener request,\n    fetch and parse the screener results, and access predefined screener bodies.\n    ",
    "yfinance.Sector": "\n    Represents a financial market sector and allows retrieval of sector-related data \n    such as top ETFs, top mutual funds, and industry data.\n    ",
    "yfinance.download": "\n    Download yahoo tickers\n    :Parameters:\n        tickers : str, list\n            List of tickers to download\n        period : str\n            Valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n            Either Use period parameter or use start and end\n        interval : str\n            Valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n            Intraday data cannot extend last 60 days\n        start: str\n            Download start date string (YYYY-MM-DD) or _datetime, inclusive.\n            Default is 99 years ago\n            E.g. for start=\"2020-01-01\", the first data point will be on \"2020-01-01\"\n        end: str\n            Download end date string (YYYY-MM-DD) or _datetime, exclusive.\n            Default is now\n            E.g. for end=\"2023-01-01\", the last data point will be on \"2022-12-31\"\n        group_by : str\n            Group by 'ticker' or 'column' (default)\n        prepost : bool\n            Include Pre and Post market data in results?\n            Default is False\n        auto_adjust: bool\n            Adjust all OHLC automatically? Default is True\n        repair: bool\n            Detect currency unit 100x mixups and attempt repair\n            Default is False\n        keepna: bool\n            Keep NaN rows returned by Yahoo?\n            Default is False\n        actions: bool\n            Download dividend + stock splits data. Default is False\n        threads: bool / int\n            How many threads to use for mass downloading. Default is True\n        ignore_tz: bool\n            When combining from different timezones, ignore that part of datetime.\n            Default depends on interval. Intraday = False. Day+ = True.\n        proxy: str\n            Optional. Proxy server URL scheme. Default is None\n        rounding: bool\n            Optional. Round values to 2 decimal places?\n        timeout: None or float\n            If not None stops waiting for a response after given number of\n            seconds. (Can also be a fraction of a second e.g. 0.01)\n        session: None or Session\n            Optional. Pass your own session object to be used for all requests\n        multi_level_index: bool\n            Optional. Always return a MultiIndex DataFrame? Default is True\n    ",
    "talib.ACOS": " ACOS(real)\n\n    Vector Trigonometric ACos (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.AD": " AD(high, low, close, volume)\n\n    Chaikin A/D Line (Volume Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close', 'volume']\n    Outputs:\n        real\n    ",
    "talib.ADD": " ADD(real0, real1)\n\n    Vector Arithmetic Add (Math Operators)\n\n    Inputs:\n        real0: (any ndarray)\n        real1: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.ADOSC": " ADOSC(high, low, close, volume[, fastperiod=?, slowperiod=?])\n\n    Chaikin A/D Oscillator (Volume Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close', 'volume']\n    Parameters:\n        fastperiod: 3\n        slowperiod: 10\n    Outputs:\n        real\n    ",
    "talib.ADX": " ADX(high, low, close[, timeperiod=?])\n\n    Average Directional Movement Index (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.ADXR": " ADXR(high, low, close[, timeperiod=?])\n\n    Average Directional Movement Index Rating (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.APO": " APO(real[, fastperiod=?, slowperiod=?, matype=?])\n\n    Absolute Price Oscillator (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        fastperiod: 12\n        slowperiod: 26\n        matype: 0 (Simple Moving Average)\n    Outputs:\n        real\n    ",
    "talib.AROON": " AROON(high, low[, timeperiod=?])\n\n    Aroon (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        aroondown\n        aroonup\n    ",
    "talib.AROONOSC": " AROONOSC(high, low[, timeperiod=?])\n\n    Aroon Oscillator (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.ASIN": " ASIN(real)\n\n    Vector Trigonometric ASin (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.ATAN": " ATAN(real)\n\n    Vector Trigonometric ATan (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.ATR": " ATR(high, low, close[, timeperiod=?])\n\n    Average True Range (Volatility Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.AVGPRICE": " AVGPRICE(open, high, low, close)\n\n    Average Price (Price Transform)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        real\n    ",
    "talib.BBANDS": " BBANDS(real[, timeperiod=?, nbdevup=?, nbdevdn=?, matype=?])\n\n    Bollinger Bands (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 5\n        nbdevup: 2.0\n        nbdevdn: 2.0\n        matype: 0 (Simple Moving Average)\n    Outputs:\n        upperband\n        middleband\n        lowerband\n    ",
    "talib.BETA": " BETA(real0, real1[, timeperiod=?])\n\n    Beta (Statistic Functions)\n\n    Inputs:\n        real0: (any ndarray)\n        real1: (any ndarray)\n    Parameters:\n        timeperiod: 5\n    Outputs:\n        real\n    ",
    "talib.BOP": " BOP(open, high, low, close)\n\n    Balance Of Power (Momentum Indicators)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        real\n    ",
    "talib.CCI": " CCI(high, low, close[, timeperiod=?])\n\n    Commodity Channel Index (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.CDL2CROWS": " CDL2CROWS(open, high, low, close)\n\n    Two Crows (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDL3BLACKCROWS": " CDL3BLACKCROWS(open, high, low, close)\n\n    Three Black Crows (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDL3INSIDE": " CDL3INSIDE(open, high, low, close)\n\n    Three Inside Up/Down (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDL3LINESTRIKE": " CDL3LINESTRIKE(open, high, low, close)\n\n    Three-Line Strike  (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDL3OUTSIDE": " CDL3OUTSIDE(open, high, low, close)\n\n    Three Outside Up/Down (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDL3STARSINSOUTH": " CDL3STARSINSOUTH(open, high, low, close)\n\n    Three Stars In The South (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDL3WHITESOLDIERS": " CDL3WHITESOLDIERS(open, high, low, close)\n\n    Three Advancing White Soldiers (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLABANDONEDBABY": " CDLABANDONEDBABY(open, high, low, close[, penetration=?])\n\n    Abandoned Baby (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Parameters:\n        penetration: 0.3\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLADVANCEBLOCK": " CDLADVANCEBLOCK(open, high, low, close)\n\n    Advance Block (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLBELTHOLD": " CDLBELTHOLD(open, high, low, close)\n\n    Belt-hold (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLBREAKAWAY": " CDLBREAKAWAY(open, high, low, close)\n\n    Breakaway (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLCLOSINGMARUBOZU": " CDLCLOSINGMARUBOZU(open, high, low, close)\n\n    Closing Marubozu (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLCONCEALBABYSWALL": " CDLCONCEALBABYSWALL(open, high, low, close)\n\n    Concealing Baby Swallow (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLCOUNTERATTACK": " CDLCOUNTERATTACK(open, high, low, close)\n\n    Counterattack (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLDARKCLOUDCOVER": " CDLDARKCLOUDCOVER(open, high, low, close[, penetration=?])\n\n    Dark Cloud Cover (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Parameters:\n        penetration: 0.5\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLDOJI": " CDLDOJI(open, high, low, close)\n\n    Doji (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLDOJISTAR": " CDLDOJISTAR(open, high, low, close)\n\n    Doji Star (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLDRAGONFLYDOJI": " CDLDRAGONFLYDOJI(open, high, low, close)\n\n    Dragonfly Doji (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLENGULFING": " CDLENGULFING(open, high, low, close)\n\n    Engulfing Pattern (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLEVENINGDOJISTAR": " CDLEVENINGDOJISTAR(open, high, low, close[, penetration=?])\n\n    Evening Doji Star (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Parameters:\n        penetration: 0.3\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLEVENINGSTAR": " CDLEVENINGSTAR(open, high, low, close[, penetration=?])\n\n    Evening Star (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Parameters:\n        penetration: 0.3\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLGAPSIDESIDEWHITE": " CDLGAPSIDESIDEWHITE(open, high, low, close)\n\n    Up/Down-gap side-by-side white lines (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLGRAVESTONEDOJI": " CDLGRAVESTONEDOJI(open, high, low, close)\n\n    Gravestone Doji (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLHAMMER": " CDLHAMMER(open, high, low, close)\n\n    Hammer (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLHANGINGMAN": " CDLHANGINGMAN(open, high, low, close)\n\n    Hanging Man (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLHARAMI": " CDLHARAMI(open, high, low, close)\n\n    Harami Pattern (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLHARAMICROSS": " CDLHARAMICROSS(open, high, low, close)\n\n    Harami Cross Pattern (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLHIGHWAVE": " CDLHIGHWAVE(open, high, low, close)\n\n    High-Wave Candle (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLHIKKAKE": " CDLHIKKAKE(open, high, low, close)\n\n    Hikkake Pattern (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLHIKKAKEMOD": " CDLHIKKAKEMOD(open, high, low, close)\n\n    Modified Hikkake Pattern (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLHOMINGPIGEON": " CDLHOMINGPIGEON(open, high, low, close)\n\n    Homing Pigeon (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLIDENTICAL3CROWS": " CDLIDENTICAL3CROWS(open, high, low, close)\n\n    Identical Three Crows (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLINNECK": " CDLINNECK(open, high, low, close)\n\n    In-Neck Pattern (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLINVERTEDHAMMER": " CDLINVERTEDHAMMER(open, high, low, close)\n\n    Inverted Hammer (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLKICKING": " CDLKICKING(open, high, low, close)\n\n    Kicking (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLKICKINGBYLENGTH": " CDLKICKINGBYLENGTH(open, high, low, close)\n\n    Kicking - bull/bear determined by the longer marubozu (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLLADDERBOTTOM": " CDLLADDERBOTTOM(open, high, low, close)\n\n    Ladder Bottom (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLLONGLEGGEDDOJI": " CDLLONGLEGGEDDOJI(open, high, low, close)\n\n    Long Legged Doji (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLLONGLINE": " CDLLONGLINE(open, high, low, close)\n\n    Long Line Candle (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLMARUBOZU": " CDLMARUBOZU(open, high, low, close)\n\n    Marubozu (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLMATCHINGLOW": " CDLMATCHINGLOW(open, high, low, close)\n\n    Matching Low (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLMATHOLD": " CDLMATHOLD(open, high, low, close[, penetration=?])\n\n    Mat Hold (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Parameters:\n        penetration: 0.5\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLMORNINGDOJISTAR": " CDLMORNINGDOJISTAR(open, high, low, close[, penetration=?])\n\n    Morning Doji Star (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Parameters:\n        penetration: 0.3\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLMORNINGSTAR": " CDLMORNINGSTAR(open, high, low, close[, penetration=?])\n\n    Morning Star (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Parameters:\n        penetration: 0.3\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLONNECK": " CDLONNECK(open, high, low, close)\n\n    On-Neck Pattern (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLPIERCING": " CDLPIERCING(open, high, low, close)\n\n    Piercing Pattern (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLRICKSHAWMAN": " CDLRICKSHAWMAN(open, high, low, close)\n\n    Rickshaw Man (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLRISEFALL3METHODS": " CDLRISEFALL3METHODS(open, high, low, close)\n\n    Rising/Falling Three Methods (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLSEPARATINGLINES": " CDLSEPARATINGLINES(open, high, low, close)\n\n    Separating Lines (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLSHOOTINGSTAR": " CDLSHOOTINGSTAR(open, high, low, close)\n\n    Shooting Star (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLSHORTLINE": " CDLSHORTLINE(open, high, low, close)\n\n    Short Line Candle (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLSPINNINGTOP": " CDLSPINNINGTOP(open, high, low, close)\n\n    Spinning Top (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLSTALLEDPATTERN": " CDLSTALLEDPATTERN(open, high, low, close)\n\n    Stalled Pattern (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLSTICKSANDWICH": " CDLSTICKSANDWICH(open, high, low, close)\n\n    Stick Sandwich (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLTAKURI": " CDLTAKURI(open, high, low, close)\n\n    Takuri (Dragonfly Doji with very long lower shadow) (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLTASUKIGAP": " CDLTASUKIGAP(open, high, low, close)\n\n    Tasuki Gap (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLTHRUSTING": " CDLTHRUSTING(open, high, low, close)\n\n    Thrusting Pattern (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLTRISTAR": " CDLTRISTAR(open, high, low, close)\n\n    Tristar Pattern (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLUNIQUE3RIVER": " CDLUNIQUE3RIVER(open, high, low, close)\n\n    Unique 3 River (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLUPSIDEGAP2CROWS": " CDLUPSIDEGAP2CROWS(open, high, low, close)\n\n    Upside Gap Two Crows (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CDLXSIDEGAP3METHODS": " CDLXSIDEGAP3METHODS(open, high, low, close)\n\n    Upside/Downside Gap Three Methods (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.CEIL": " CEIL(real)\n\n    Vector Ceil (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.CMO": " CMO(real[, timeperiod=?])\n\n    Chande Momentum Oscillator (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.CORREL": " CORREL(real0, real1[, timeperiod=?])\n\n    Pearson's Correlation Coefficient (r) (Statistic Functions)\n\n    Inputs:\n        real0: (any ndarray)\n        real1: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.COS": " COS(real)\n\n    Vector Trigonometric Cos (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.COSH": " COSH(real)\n\n    Vector Trigonometric Cosh (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.DEMA": " DEMA(real[, timeperiod=?])\n\n    Double Exponential Moving Average (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.DIV": " DIV(real0, real1)\n\n    Vector Arithmetic Div (Math Operators)\n\n    Inputs:\n        real0: (any ndarray)\n        real1: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.DX": " DX(high, low, close[, timeperiod=?])\n\n    Directional Movement Index (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.EMA": " EMA(real[, timeperiod=?])\n\n    Exponential Moving Average (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.EXP": " EXP(real)\n\n    Vector Arithmetic Exp (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.FLOOR": " FLOOR(real)\n\n    Vector Floor (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.HT_DCPERIOD": " HT_DCPERIOD(real)\n\n    Hilbert Transform - Dominant Cycle Period (Cycle Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.HT_DCPHASE": " HT_DCPHASE(real)\n\n    Hilbert Transform - Dominant Cycle Phase (Cycle Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.HT_PHASOR": " HT_PHASOR(real)\n\n    Hilbert Transform - Phasor Components (Cycle Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        inphase\n        quadrature\n    ",
    "talib.HT_SINE": " HT_SINE(real)\n\n    Hilbert Transform - SineWave (Cycle Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        sine\n        leadsine\n    ",
    "talib.HT_TRENDLINE": " HT_TRENDLINE(real)\n\n    Hilbert Transform - Instantaneous Trendline (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.HT_TRENDMODE": " HT_TRENDMODE(real)\n\n    Hilbert Transform - Trend vs Cycle Mode (Cycle Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.KAMA": " KAMA(real[, timeperiod=?])\n\n    Kaufman Adaptive Moving Average (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.LINEARREG": " LINEARREG(real[, timeperiod=?])\n\n    Linear Regression (Statistic Functions)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.LINEARREG_ANGLE": " LINEARREG_ANGLE(real[, timeperiod=?])\n\n    Linear Regression Angle (Statistic Functions)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.LINEARREG_INTERCEPT": " LINEARREG_INTERCEPT(real[, timeperiod=?])\n\n    Linear Regression Intercept (Statistic Functions)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.LINEARREG_SLOPE": " LINEARREG_SLOPE(real[, timeperiod=?])\n\n    Linear Regression Slope (Statistic Functions)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.LN": " LN(real)\n\n    Vector Log Natural (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.LOG10": " LOG10(real)\n\n    Vector Log10 (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.MA": " MA(real[, timeperiod=?, matype=?])\n\n    Moving average (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n        matype: 0 (Simple Moving Average)\n    Outputs:\n        real\n    ",
    "talib.MACD": " MACD(real[, fastperiod=?, slowperiod=?, signalperiod=?])\n\n    Moving Average Convergence/Divergence (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        fastperiod: 12\n        slowperiod: 26\n        signalperiod: 9\n    Outputs:\n        macd\n        macdsignal\n        macdhist\n    ",
    "talib.MACDEXT": " MACDEXT(real[, fastperiod=?, fastmatype=?, slowperiod=?, slowmatype=?, signalperiod=?, signalmatype=?])\n\n    MACD with controllable MA type (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        fastperiod: 12\n        fastmatype: 0\n        slowperiod: 26\n        slowmatype: 0\n        signalperiod: 9\n        signalmatype: 0\n    Outputs:\n        macd\n        macdsignal\n        macdhist\n    ",
    "talib.MACDFIX": " MACDFIX(real[, signalperiod=?])\n\n    Moving Average Convergence/Divergence Fix 12/26 (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        signalperiod: 9\n    Outputs:\n        macd\n        macdsignal\n        macdhist\n    ",
    "talib.MAMA": " MAMA(real[, fastlimit=?, slowlimit=?])\n\n    MESA Adaptive Moving Average (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        fastlimit: 0.5\n        slowlimit: 0.05\n    Outputs:\n        mama\n        fama\n    ",
    "talib.MAVP": " MAVP(real, periods[, minperiod=?, maxperiod=?, matype=?])\n\n    Moving average with variable period (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n        periods: (any ndarray)\n    Parameters:\n        minperiod: 2\n        maxperiod: 30\n        matype: 0 (Simple Moving Average)\n    Outputs:\n        real\n    ",
    "talib.MAX": " MAX(real[, timeperiod=?])\n\n    Highest value over a specified period (Math Operators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.MAXINDEX": " MAXINDEX(real[, timeperiod=?])\n\n    Index of highest value over a specified period (Math Operators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.MEDPRICE": " MEDPRICE(high, low)\n\n    Median Price (Price Transform)\n\n    Inputs:\n        prices: ['high', 'low']\n    Outputs:\n        real\n    ",
    "talib.MFI": " MFI(high, low, close, volume[, timeperiod=?])\n\n    Money Flow Index (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close', 'volume']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.MIDPOINT": " MIDPOINT(real[, timeperiod=?])\n\n    MidPoint over period (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.MIDPRICE": " MIDPRICE(high, low[, timeperiod=?])\n\n    Midpoint Price over period (Overlap Studies)\n\n    Inputs:\n        prices: ['high', 'low']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.MIN": " MIN(real[, timeperiod=?])\n\n    Lowest value over a specified period (Math Operators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.MININDEX": " MININDEX(real[, timeperiod=?])\n\n    Index of lowest value over a specified period (Math Operators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.MINMAX": " MINMAX(real[, timeperiod=?])\n\n    Lowest and highest values over a specified period (Math Operators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        min\n        max\n    ",
    "talib.MINMAXINDEX": " MINMAXINDEX(real[, timeperiod=?])\n\n    Indexes of lowest and highest values over a specified period (Math Operators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        minidx\n        maxidx\n    ",
    "talib.MINUS_DI": " MINUS_DI(high, low, close[, timeperiod=?])\n\n    Minus Directional Indicator (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.MINUS_DM": " MINUS_DM(high, low[, timeperiod=?])\n\n    Minus Directional Movement (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.MOM": " MOM(real[, timeperiod=?])\n\n    Momentum (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 10\n    Outputs:\n        real\n    ",
    "talib.MULT": " MULT(real0, real1)\n\n    Vector Arithmetic Mult (Math Operators)\n\n    Inputs:\n        real0: (any ndarray)\n        real1: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.NATR": " NATR(high, low, close[, timeperiod=?])\n\n    Normalized Average True Range (Volatility Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.OBV": " OBV(real, volume)\n\n    On Balance Volume (Volume Indicators)\n\n    Inputs:\n        real: (any ndarray)\n        prices: ['volume']\n    Outputs:\n        real\n    ",
    "talib.PLUS_DI": " PLUS_DI(high, low, close[, timeperiod=?])\n\n    Plus Directional Indicator (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.PLUS_DM": " PLUS_DM(high, low[, timeperiod=?])\n\n    Plus Directional Movement (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.PPO": " PPO(real[, fastperiod=?, slowperiod=?, matype=?])\n\n    Percentage Price Oscillator (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        fastperiod: 12\n        slowperiod: 26\n        matype: 0 (Simple Moving Average)\n    Outputs:\n        real\n    ",
    "talib.ROC": " ROC(real[, timeperiod=?])\n\n    Rate of change : ((real/prevPrice)-1)*100 (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 10\n    Outputs:\n        real\n    ",
    "talib.ROCP": " ROCP(real[, timeperiod=?])\n\n    Rate of change Percentage: (real-prevPrice)/prevPrice (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 10\n    Outputs:\n        real\n    ",
    "talib.ROCR": " ROCR(real[, timeperiod=?])\n\n    Rate of change ratio: (real/prevPrice) (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 10\n    Outputs:\n        real\n    ",
    "talib.ROCR100": " ROCR100(real[, timeperiod=?])\n\n    Rate of change ratio 100 scale: (real/prevPrice)*100 (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 10\n    Outputs:\n        real\n    ",
    "talib.RSI": " RSI(real[, timeperiod=?])\n\n    Relative Strength Index (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.SAR": " SAR(high, low[, acceleration=?, maximum=?])\n\n    Parabolic SAR (Overlap Studies)\n\n    Inputs:\n        prices: ['high', 'low']\n    Parameters:\n        acceleration: 0.02\n        maximum: 0.2\n    Outputs:\n        real\n    ",
    "talib.SAREXT": " SAREXT(high, low[, startvalue=?, offsetonreverse=?, accelerationinitlong=?, accelerationlong=?, accelerationmaxlong=?, accelerationinitshort=?, accelerationshort=?, accelerationmaxshort=?])\n\n    Parabolic SAR - Extended (Overlap Studies)\n\n    Inputs:\n        prices: ['high', 'low']\n    Parameters:\n        startvalue: 0.0\n        offsetonreverse: 0.0\n        accelerationinitlong: 0.02\n        accelerationlong: 0.02\n        accelerationmaxlong: 0.2\n        accelerationinitshort: 0.02\n        accelerationshort: 0.02\n        accelerationmaxshort: 0.2\n    Outputs:\n        real\n    ",
    "talib.SIN": " SIN(real)\n\n    Vector Trigonometric Sin (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.SINH": " SINH(real)\n\n    Vector Trigonometric Sinh (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.SMA": " SMA(real[, timeperiod=?])\n\n    Simple Moving Average (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.SQRT": " SQRT(real)\n\n    Vector Square Root (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.STDDEV": " STDDEV(real[, timeperiod=?, nbdev=?])\n\n    Standard Deviation (Statistic Functions)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 5\n        nbdev: 1.0\n    Outputs:\n        real\n    ",
    "talib.STOCH": " STOCH(high, low, close[, fastk_period=?, slowk_period=?, slowk_matype=?, slowd_period=?, slowd_matype=?])\n\n    Stochastic (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        fastk_period: 5\n        slowk_period: 3\n        slowk_matype: 0\n        slowd_period: 3\n        slowd_matype: 0\n    Outputs:\n        slowk\n        slowd\n    ",
    "talib.STOCHF": " STOCHF(high, low, close[, fastk_period=?, fastd_period=?, fastd_matype=?])\n\n    Stochastic Fast (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        fastk_period: 5\n        fastd_period: 3\n        fastd_matype: 0\n    Outputs:\n        fastk\n        fastd\n    ",
    "talib.STOCHRSI": " STOCHRSI(real[, timeperiod=?, fastk_period=?, fastd_period=?, fastd_matype=?])\n\n    Stochastic Relative Strength Index (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 14\n        fastk_period: 5\n        fastd_period: 3\n        fastd_matype: 0\n    Outputs:\n        fastk\n        fastd\n    ",
    "talib.SUB": " SUB(real0, real1)\n\n    Vector Arithmetic Subtraction (Math Operators)\n\n    Inputs:\n        real0: (any ndarray)\n        real1: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.SUM": " SUM(real[, timeperiod=?])\n\n    Summation (Math Operators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.T3": " T3(real[, timeperiod=?, vfactor=?])\n\n    Triple Exponential Moving Average (T3) (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 5\n        vfactor: 0.7\n    Outputs:\n        real\n    ",
    "talib.TAN": " TAN(real)\n\n    Vector Trigonometric Tan (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.TANH": " TANH(real)\n\n    Vector Trigonometric Tanh (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.TEMA": " TEMA(real[, timeperiod=?])\n\n    Triple Exponential Moving Average (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.TRANGE": " TRANGE(high, low, close)\n\n    True Range (Volatility Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Outputs:\n        real\n    ",
    "talib.TRIMA": " TRIMA(real[, timeperiod=?])\n\n    Triangular Moving Average (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.TRIX": " TRIX(real[, timeperiod=?])\n\n    1-day Rate-Of-Change (ROC) of a Triple Smooth EMA (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.TSF": " TSF(real[, timeperiod=?])\n\n    Time Series Forecast (Statistic Functions)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.TYPPRICE": " TYPPRICE(high, low, close)\n\n    Typical Price (Price Transform)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Outputs:\n        real\n    ",
    "talib.ULTOSC": " ULTOSC(high, low, close[, timeperiod1=?, timeperiod2=?, timeperiod3=?])\n\n    Ultimate Oscillator (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        timeperiod1: 7\n        timeperiod2: 14\n        timeperiod3: 28\n    Outputs:\n        real\n    ",
    "talib.VAR": " VAR(real[, timeperiod=?, nbdev=?])\n\n    Variance (Statistic Functions)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 5\n        nbdev: 1.0\n    Outputs:\n        real\n    ",
    "talib.WCLPRICE": " WCLPRICE(high, low, close)\n\n    Weighted Close Price (Price Transform)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Outputs:\n        real\n    ",
    "talib.WILLR": " WILLR(high, low, close[, timeperiod=?])\n\n    Williams' %R (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.WMA": " WMA(real[, timeperiod=?])\n\n    Weighted Moving Average (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib._pd_Series": "\n    One-dimensional ndarray with axis labels (including time series).\n\n    Labels need not be unique but must be a hashable type. The object\n    supports both integer- and label-based indexing and provides a host of\n    methods for performing operations involving the index. Statistical\n    methods from ndarray have been overridden to automatically exclude\n    missing data (currently represented as NaN).\n\n    Operations between Series (+, -, /, \\*, \\*\\*) align values based on their\n    associated index values-- they need not be the same length. The result\n    index will be the sorted union of the two indexes.\n\n    Parameters\n    ----------\n    data : array-like, Iterable, dict, or scalar value\n        Contains data stored in Series. If data is a dict, argument order is\n        maintained.\n    index : array-like or Index (1d)\n        Values must be hashable and have the same length as `data`.\n        Non-unique index values are allowed. Will default to\n        RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n        and index is None, then the keys in the data are used as the index. If the\n        index is not None, the resulting Series is reindexed with the index values.\n    dtype : str, numpy.dtype, or ExtensionDtype, optional\n        Data type for the output Series. If not specified, this will be\n        inferred from `data`.\n        See the :ref:`user guide <basics.dtypes>` for more usages.\n    name : Hashable, default None\n        The name to give to the Series.\n    copy : bool, default False\n        Copy input data. Only affects Series or 1d ndarray input. See examples.\n\n    Notes\n    -----\n    Please reference the :ref:`User Guide <basics.series>` for more information.\n\n    Examples\n    --------\n    Constructing Series from a dictionary with an Index specified\n\n    >>> d = {'a': 1, 'b': 2, 'c': 3}\n    >>> ser = pd.Series(data=d, index=['a', 'b', 'c'])\n    >>> ser\n    a   1\n    b   2\n    c   3\n    dtype: int64\n\n    The keys of the dictionary match with the Index values, hence the Index\n    values have no effect.\n\n    >>> d = {'a': 1, 'b': 2, 'c': 3}\n    >>> ser = pd.Series(data=d, index=['x', 'y', 'z'])\n    >>> ser\n    x   NaN\n    y   NaN\n    z   NaN\n    dtype: float64\n\n    Note that the Index is first build with the keys from the dictionary.\n    After this the Series is reindexed with the given Index values, hence we\n    get all NaN as a result.\n\n    Constructing Series from a list with `copy=False`.\n\n    >>> r = [1, 2]\n    >>> ser = pd.Series(r, copy=False)\n    >>> ser.iloc[0] = 999\n    >>> r\n    [1, 2]\n    >>> ser\n    0    999\n    1      2\n    dtype: int64\n\n    Due to input data type the Series has a `copy` of\n    the original data even though `copy=False`, so\n    the data is unchanged.\n\n    Constructing Series from a 1d ndarray with `copy=False`.\n\n    >>> r = np.array([1, 2])\n    >>> ser = pd.Series(r, copy=False)\n    >>> ser.iloc[0] = 999\n    >>> r\n    array([999,   2])\n    >>> ser\n    0    999\n    1      2\n    dtype: int64\n\n    Due to input data type the Series has a `view` on\n    the original data, so\n    the data is changed as well.\n    ",
    "talib.chain": "chain(*iterables) --> chain object\n\nReturn a chain object whose .__next__() method returns elements from the\nfirst iterable until it is exhausted, then elements from the next\niterable, until all of the iterables are exhausted.",
    "talib.get_function_groups": "\n    Returns a dict with keys of function-group names and values of lists\n    of function names ie {'group_names': ['function_names']}\n    ",
    "talib.get_functions": "\n    Returns a list of all the functions supported by TALIB\n    ",
    "talib.stream_ACOS": " ACOS(real)\n\n    Vector Trigonometric ACos (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.stream_AD": " AD(high, low, close, volume)\n\n    Chaikin A/D Line (Volume Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close', 'volume']\n    Outputs:\n        real\n    ",
    "talib.stream_ADD": " ADD(real0, real1)\n\n    Vector Arithmetic Add (Math Operators)\n\n    Inputs:\n        real0: (any ndarray)\n        real1: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.stream_ADOSC": " ADOSC(high, low, close, volume[, fastperiod=?, slowperiod=?])\n\n    Chaikin A/D Oscillator (Volume Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close', 'volume']\n    Parameters:\n        fastperiod: 3\n        slowperiod: 10\n    Outputs:\n        real\n    ",
    "talib.stream_ADX": " ADX(high, low, close[, timeperiod=?])\n\n    Average Directional Movement Index (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.stream_ADXR": " ADXR(high, low, close[, timeperiod=?])\n\n    Average Directional Movement Index Rating (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.stream_APO": " APO(real[, fastperiod=?, slowperiod=?, matype=?])\n\n    Absolute Price Oscillator (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        fastperiod: 12\n        slowperiod: 26\n        matype: 0 (Simple Moving Average)\n    Outputs:\n        real\n    ",
    "talib.stream_AROON": " AROON(high, low[, timeperiod=?])\n\n    Aroon (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        aroondown\n        aroonup\n    ",
    "talib.stream_AROONOSC": " AROONOSC(high, low[, timeperiod=?])\n\n    Aroon Oscillator (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.stream_ASIN": " ASIN(real)\n\n    Vector Trigonometric ASin (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.stream_ATAN": " ATAN(real)\n\n    Vector Trigonometric ATan (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.stream_ATR": " ATR(high, low, close[, timeperiod=?])\n\n    Average True Range (Volatility Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.stream_AVGPRICE": " AVGPRICE(open, high, low, close)\n\n    Average Price (Price Transform)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        real\n    ",
    "talib.stream_BBANDS": " BBANDS(real[, timeperiod=?, nbdevup=?, nbdevdn=?, matype=?])\n\n    Bollinger Bands (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 5\n        nbdevup: 2.0\n        nbdevdn: 2.0\n        matype: 0 (Simple Moving Average)\n    Outputs:\n        upperband\n        middleband\n        lowerband\n    ",
    "talib.stream_BETA": " BETA(real0, real1[, timeperiod=?])\n\n    Beta (Statistic Functions)\n\n    Inputs:\n        real0: (any ndarray)\n        real1: (any ndarray)\n    Parameters:\n        timeperiod: 5\n    Outputs:\n        real\n    ",
    "talib.stream_BOP": " BOP(open, high, low, close)\n\n    Balance Of Power (Momentum Indicators)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        real\n    ",
    "talib.stream_CCI": " CCI(high, low, close[, timeperiod=?])\n\n    Commodity Channel Index (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.stream_CDL2CROWS": " CDL2CROWS(open, high, low, close)\n\n    Two Crows (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDL3BLACKCROWS": " CDL3BLACKCROWS(open, high, low, close)\n\n    Three Black Crows (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDL3INSIDE": " CDL3INSIDE(open, high, low, close)\n\n    Three Inside Up/Down (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDL3LINESTRIKE": " CDL3LINESTRIKE(open, high, low, close)\n\n    Three-Line Strike  (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDL3OUTSIDE": " CDL3OUTSIDE(open, high, low, close)\n\n    Three Outside Up/Down (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDL3STARSINSOUTH": " CDL3STARSINSOUTH(open, high, low, close)\n\n    Three Stars In The South (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDL3WHITESOLDIERS": " CDL3WHITESOLDIERS(open, high, low, close)\n\n    Three Advancing White Soldiers (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLABANDONEDBABY": " CDLABANDONEDBABY(open, high, low, close[, penetration=?])\n\n    Abandoned Baby (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Parameters:\n        penetration: 0.3\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLADVANCEBLOCK": " CDLADVANCEBLOCK(open, high, low, close)\n\n    Advance Block (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLBELTHOLD": " CDLBELTHOLD(open, high, low, close)\n\n    Belt-hold (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLBREAKAWAY": " CDLBREAKAWAY(open, high, low, close)\n\n    Breakaway (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLCLOSINGMARUBOZU": " CDLCLOSINGMARUBOZU(open, high, low, close)\n\n    Closing Marubozu (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLCONCEALBABYSWALL": " CDLCONCEALBABYSWALL(open, high, low, close)\n\n    Concealing Baby Swallow (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLCOUNTERATTACK": " CDLCOUNTERATTACK(open, high, low, close)\n\n    Counterattack (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLDARKCLOUDCOVER": " CDLDARKCLOUDCOVER(open, high, low, close[, penetration=?])\n\n    Dark Cloud Cover (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Parameters:\n        penetration: 0.5\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLDOJI": " CDLDOJI(open, high, low, close)\n\n    Doji (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLDOJISTAR": " CDLDOJISTAR(open, high, low, close)\n\n    Doji Star (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLDRAGONFLYDOJI": " CDLDRAGONFLYDOJI(open, high, low, close)\n\n    Dragonfly Doji (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLENGULFING": " CDLENGULFING(open, high, low, close)\n\n    Engulfing Pattern (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLEVENINGDOJISTAR": " CDLEVENINGDOJISTAR(open, high, low, close[, penetration=?])\n\n    Evening Doji Star (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Parameters:\n        penetration: 0.3\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLEVENINGSTAR": " CDLEVENINGSTAR(open, high, low, close[, penetration=?])\n\n    Evening Star (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Parameters:\n        penetration: 0.3\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLGAPSIDESIDEWHITE": " CDLGAPSIDESIDEWHITE(open, high, low, close)\n\n    Up/Down-gap side-by-side white lines (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLGRAVESTONEDOJI": " CDLGRAVESTONEDOJI(open, high, low, close)\n\n    Gravestone Doji (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLHAMMER": " CDLHAMMER(open, high, low, close)\n\n    Hammer (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLHANGINGMAN": " CDLHANGINGMAN(open, high, low, close)\n\n    Hanging Man (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLHARAMI": " CDLHARAMI(open, high, low, close)\n\n    Harami Pattern (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLHARAMICROSS": " CDLHARAMICROSS(open, high, low, close)\n\n    Harami Cross Pattern (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLHIGHWAVE": " CDLHIGHWAVE(open, high, low, close)\n\n    High-Wave Candle (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLHIKKAKE": " CDLHIKKAKE(open, high, low, close)\n\n    Hikkake Pattern (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLHIKKAKEMOD": " CDLHIKKAKEMOD(open, high, low, close)\n\n    Modified Hikkake Pattern (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLHOMINGPIGEON": " CDLHOMINGPIGEON(open, high, low, close)\n\n    Homing Pigeon (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLIDENTICAL3CROWS": " CDLIDENTICAL3CROWS(open, high, low, close)\n\n    Identical Three Crows (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLINNECK": " CDLINNECK(open, high, low, close)\n\n    In-Neck Pattern (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLINVERTEDHAMMER": " CDLINVERTEDHAMMER(open, high, low, close)\n\n    Inverted Hammer (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLKICKING": " CDLKICKING(open, high, low, close)\n\n    Kicking (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLKICKINGBYLENGTH": " CDLKICKINGBYLENGTH(open, high, low, close)\n\n    Kicking - bull/bear determined by the longer marubozu (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLLADDERBOTTOM": " CDLLADDERBOTTOM(open, high, low, close)\n\n    Ladder Bottom (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLLONGLEGGEDDOJI": " CDLLONGLEGGEDDOJI(open, high, low, close)\n\n    Long Legged Doji (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLLONGLINE": " CDLLONGLINE(open, high, low, close)\n\n    Long Line Candle (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLMARUBOZU": " CDLMARUBOZU(open, high, low, close)\n\n    Marubozu (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLMATCHINGLOW": " CDLMATCHINGLOW(open, high, low, close)\n\n    Matching Low (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLMATHOLD": " CDLMATHOLD(open, high, low, close[, penetration=?])\n\n    Mat Hold (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Parameters:\n        penetration: 0.5\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLMORNINGDOJISTAR": " CDLMORNINGDOJISTAR(open, high, low, close[, penetration=?])\n\n    Morning Doji Star (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Parameters:\n        penetration: 0.3\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLMORNINGSTAR": " CDLMORNINGSTAR(open, high, low, close[, penetration=?])\n\n    Morning Star (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Parameters:\n        penetration: 0.3\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLONNECK": " CDLONNECK(open, high, low, close)\n\n    On-Neck Pattern (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLPIERCING": " CDLPIERCING(open, high, low, close)\n\n    Piercing Pattern (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLRICKSHAWMAN": " CDLRICKSHAWMAN(open, high, low, close)\n\n    Rickshaw Man (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLRISEFALL3METHODS": " CDLRISEFALL3METHODS(open, high, low, close)\n\n    Rising/Falling Three Methods (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLSEPARATINGLINES": " CDLSEPARATINGLINES(open, high, low, close)\n\n    Separating Lines (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLSHOOTINGSTAR": " CDLSHOOTINGSTAR(open, high, low, close)\n\n    Shooting Star (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLSHORTLINE": " CDLSHORTLINE(open, high, low, close)\n\n    Short Line Candle (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLSPINNINGTOP": " CDLSPINNINGTOP(open, high, low, close)\n\n    Spinning Top (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLSTALLEDPATTERN": " CDLSTALLEDPATTERN(open, high, low, close)\n\n    Stalled Pattern (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLSTICKSANDWICH": " CDLSTICKSANDWICH(open, high, low, close)\n\n    Stick Sandwich (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLTAKURI": " CDLTAKURI(open, high, low, close)\n\n    Takuri (Dragonfly Doji with very long lower shadow) (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLTASUKIGAP": " CDLTASUKIGAP(open, high, low, close)\n\n    Tasuki Gap (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLTHRUSTING": " CDLTHRUSTING(open, high, low, close)\n\n    Thrusting Pattern (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLTRISTAR": " CDLTRISTAR(open, high, low, close)\n\n    Tristar Pattern (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLUNIQUE3RIVER": " CDLUNIQUE3RIVER(open, high, low, close)\n\n    Unique 3 River (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLUPSIDEGAP2CROWS": " CDLUPSIDEGAP2CROWS(open, high, low, close)\n\n    Upside Gap Two Crows (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CDLXSIDEGAP3METHODS": " CDLXSIDEGAP3METHODS(open, high, low, close)\n\n    Upside/Downside Gap Three Methods (Pattern Recognition)\n\n    Inputs:\n        prices: ['open', 'high', 'low', 'close']\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_CEIL": " CEIL(real)\n\n    Vector Ceil (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.stream_CMO": " CMO(real[, timeperiod=?])\n\n    Chande Momentum Oscillator (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.stream_CORREL": " CORREL(real0, real1[, timeperiod=?])\n\n    Pearson's Correlation Coefficient (r) (Statistic Functions)\n\n    Inputs:\n        real0: (any ndarray)\n        real1: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.stream_COS": " COS(real)\n\n    Vector Trigonometric Cos (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.stream_COSH": " COSH(real)\n\n    Vector Trigonometric Cosh (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.stream_DEMA": " DEMA(real[, timeperiod=?])\n\n    Double Exponential Moving Average (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.stream_DIV": " DIV(real0, real1)\n\n    Vector Arithmetic Div (Math Operators)\n\n    Inputs:\n        real0: (any ndarray)\n        real1: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.stream_DX": " DX(high, low, close[, timeperiod=?])\n\n    Directional Movement Index (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.stream_EMA": " EMA(real[, timeperiod=?])\n\n    Exponential Moving Average (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.stream_EXP": " EXP(real)\n\n    Vector Arithmetic Exp (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.stream_FLOOR": " FLOOR(real)\n\n    Vector Floor (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.stream_HT_DCPERIOD": " HT_DCPERIOD(real)\n\n    Hilbert Transform - Dominant Cycle Period (Cycle Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.stream_HT_DCPHASE": " HT_DCPHASE(real)\n\n    Hilbert Transform - Dominant Cycle Phase (Cycle Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.stream_HT_PHASOR": " HT_PHASOR(real)\n\n    Hilbert Transform - Phasor Components (Cycle Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        inphase\n        quadrature\n    ",
    "talib.stream_HT_SINE": " HT_SINE(real)\n\n    Hilbert Transform - SineWave (Cycle Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        sine\n        leadsine\n    ",
    "talib.stream_HT_TRENDLINE": " HT_TRENDLINE(real)\n\n    Hilbert Transform - Instantaneous Trendline (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.stream_HT_TRENDMODE": " HT_TRENDMODE(real)\n\n    Hilbert Transform - Trend vs Cycle Mode (Cycle Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_KAMA": " KAMA(real[, timeperiod=?])\n\n    Kaufman Adaptive Moving Average (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.stream_LINEARREG": " LINEARREG(real[, timeperiod=?])\n\n    Linear Regression (Statistic Functions)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.stream_LINEARREG_ANGLE": " LINEARREG_ANGLE(real[, timeperiod=?])\n\n    Linear Regression Angle (Statistic Functions)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.stream_LINEARREG_INTERCEPT": " LINEARREG_INTERCEPT(real[, timeperiod=?])\n\n    Linear Regression Intercept (Statistic Functions)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.stream_LINEARREG_SLOPE": " LINEARREG_SLOPE(real[, timeperiod=?])\n\n    Linear Regression Slope (Statistic Functions)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.stream_LN": " LN(real)\n\n    Vector Log Natural (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.stream_LOG10": " LOG10(real)\n\n    Vector Log10 (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.stream_MA": " MA(real[, timeperiod=?, matype=?])\n\n    Moving average (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n        matype: 0 (Simple Moving Average)\n    Outputs:\n        real\n    ",
    "talib.stream_MACD": " MACD(real[, fastperiod=?, slowperiod=?, signalperiod=?])\n\n    Moving Average Convergence/Divergence (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        fastperiod: 12\n        slowperiod: 26\n        signalperiod: 9\n    Outputs:\n        macd\n        macdsignal\n        macdhist\n    ",
    "talib.stream_MACDEXT": " MACDEXT(real[, fastperiod=?, fastmatype=?, slowperiod=?, slowmatype=?, signalperiod=?, signalmatype=?])\n\n    MACD with controllable MA type (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        fastperiod: 12\n        fastmatype: 0\n        slowperiod: 26\n        slowmatype: 0\n        signalperiod: 9\n        signalmatype: 0\n    Outputs:\n        macd\n        macdsignal\n        macdhist\n    ",
    "talib.stream_MACDFIX": " MACDFIX(real[, signalperiod=?])\n\n    Moving Average Convergence/Divergence Fix 12/26 (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        signalperiod: 9\n    Outputs:\n        macd\n        macdsignal\n        macdhist\n    ",
    "talib.stream_MAMA": " MAMA(real[, fastlimit=?, slowlimit=?])\n\n    MESA Adaptive Moving Average (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        fastlimit: 0.5\n        slowlimit: 0.05\n    Outputs:\n        mama\n        fama\n    ",
    "talib.stream_MAVP": " MAVP(real, periods[, minperiod=?, maxperiod=?, matype=?])\n\n    Moving average with variable period (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n        periods: (any ndarray)\n    Parameters:\n        minperiod: 2\n        maxperiod: 30\n        matype: 0 (Simple Moving Average)\n    Outputs:\n        real\n    ",
    "talib.stream_MAX": " MAX(real[, timeperiod=?])\n\n    Highest value over a specified period (Math Operators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.stream_MAXINDEX": " MAXINDEX(real[, timeperiod=?])\n\n    Index of highest value over a specified period (Math Operators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_MEDPRICE": " MEDPRICE(high, low)\n\n    Median Price (Price Transform)\n\n    Inputs:\n        prices: ['high', 'low']\n    Outputs:\n        real\n    ",
    "talib.stream_MFI": " MFI(high, low, close, volume[, timeperiod=?])\n\n    Money Flow Index (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close', 'volume']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.stream_MIDPOINT": " MIDPOINT(real[, timeperiod=?])\n\n    MidPoint over period (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.stream_MIDPRICE": " MIDPRICE(high, low[, timeperiod=?])\n\n    Midpoint Price over period (Overlap Studies)\n\n    Inputs:\n        prices: ['high', 'low']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.stream_MIN": " MIN(real[, timeperiod=?])\n\n    Lowest value over a specified period (Math Operators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.stream_MININDEX": " MININDEX(real[, timeperiod=?])\n\n    Index of lowest value over a specified period (Math Operators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        integer (values are -100, 0 or 100)\n    ",
    "talib.stream_MINMAX": " MINMAX(real[, timeperiod=?])\n\n    Lowest and highest values over a specified period (Math Operators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        min\n        max\n    ",
    "talib.stream_MINMAXINDEX": " MINMAXINDEX(real[, timeperiod=?])\n\n    Indexes of lowest and highest values over a specified period (Math Operators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        minidx\n        maxidx\n    ",
    "talib.stream_MINUS_DI": " MINUS_DI(high, low, close[, timeperiod=?])\n\n    Minus Directional Indicator (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.stream_MINUS_DM": " MINUS_DM(high, low[, timeperiod=?])\n\n    Minus Directional Movement (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.stream_MOM": " MOM(real[, timeperiod=?])\n\n    Momentum (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 10\n    Outputs:\n        real\n    ",
    "talib.stream_MULT": " MULT(real0, real1)\n\n    Vector Arithmetic Mult (Math Operators)\n\n    Inputs:\n        real0: (any ndarray)\n        real1: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.stream_NATR": " NATR(high, low, close[, timeperiod=?])\n\n    Normalized Average True Range (Volatility Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.stream_OBV": " OBV(real, volume)\n\n    On Balance Volume (Volume Indicators)\n\n    Inputs:\n        real: (any ndarray)\n        prices: ['volume']\n    Outputs:\n        real\n    ",
    "talib.stream_PLUS_DI": " PLUS_DI(high, low, close[, timeperiod=?])\n\n    Plus Directional Indicator (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.stream_PLUS_DM": " PLUS_DM(high, low[, timeperiod=?])\n\n    Plus Directional Movement (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.stream_PPO": " PPO(real[, fastperiod=?, slowperiod=?, matype=?])\n\n    Percentage Price Oscillator (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        fastperiod: 12\n        slowperiod: 26\n        matype: 0 (Simple Moving Average)\n    Outputs:\n        real\n    ",
    "talib.stream_ROC": " ROC(real[, timeperiod=?])\n\n    Rate of change : ((real/prevPrice)-1)*100 (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 10\n    Outputs:\n        real\n    ",
    "talib.stream_ROCP": " ROCP(real[, timeperiod=?])\n\n    Rate of change Percentage: (real-prevPrice)/prevPrice (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 10\n    Outputs:\n        real\n    ",
    "talib.stream_ROCR": " ROCR(real[, timeperiod=?])\n\n    Rate of change ratio: (real/prevPrice) (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 10\n    Outputs:\n        real\n    ",
    "talib.stream_ROCR100": " ROCR100(real[, timeperiod=?])\n\n    Rate of change ratio 100 scale: (real/prevPrice)*100 (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 10\n    Outputs:\n        real\n    ",
    "talib.stream_RSI": " RSI(real[, timeperiod=?])\n\n    Relative Strength Index (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.stream_SAR": " SAR(high, low[, acceleration=?, maximum=?])\n\n    Parabolic SAR (Overlap Studies)\n\n    Inputs:\n        prices: ['high', 'low']\n    Parameters:\n        acceleration: 0.02\n        maximum: 0.2\n    Outputs:\n        real\n    ",
    "talib.stream_SAREXT": " SAREXT(high, low[, startvalue=?, offsetonreverse=?, accelerationinitlong=?, accelerationlong=?, accelerationmaxlong=?, accelerationinitshort=?, accelerationshort=?, accelerationmaxshort=?])\n\n    Parabolic SAR - Extended (Overlap Studies)\n\n    Inputs:\n        prices: ['high', 'low']\n    Parameters:\n        startvalue: 0.0\n        offsetonreverse: 0.0\n        accelerationinitlong: 0.02\n        accelerationlong: 0.02\n        accelerationmaxlong: 0.2\n        accelerationinitshort: 0.02\n        accelerationshort: 0.02\n        accelerationmaxshort: 0.2\n    Outputs:\n        real\n    ",
    "talib.stream_SIN": " SIN(real)\n\n    Vector Trigonometric Sin (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.stream_SINH": " SINH(real)\n\n    Vector Trigonometric Sinh (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.stream_SMA": " SMA(real[, timeperiod=?])\n\n    Simple Moving Average (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.stream_SQRT": " SQRT(real)\n\n    Vector Square Root (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.stream_STDDEV": " STDDEV(real[, timeperiod=?, nbdev=?])\n\n    Standard Deviation (Statistic Functions)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 5\n        nbdev: 1.0\n    Outputs:\n        real\n    ",
    "talib.stream_STOCH": " STOCH(high, low, close[, fastk_period=?, slowk_period=?, slowk_matype=?, slowd_period=?, slowd_matype=?])\n\n    Stochastic (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        fastk_period: 5\n        slowk_period: 3\n        slowk_matype: 0\n        slowd_period: 3\n        slowd_matype: 0\n    Outputs:\n        slowk\n        slowd\n    ",
    "talib.stream_STOCHF": " STOCHF(high, low, close[, fastk_period=?, fastd_period=?, fastd_matype=?])\n\n    Stochastic Fast (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        fastk_period: 5\n        fastd_period: 3\n        fastd_matype: 0\n    Outputs:\n        fastk\n        fastd\n    ",
    "talib.stream_STOCHRSI": " STOCHRSI(real[, timeperiod=?, fastk_period=?, fastd_period=?, fastd_matype=?])\n\n    Stochastic Relative Strength Index (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 14\n        fastk_period: 5\n        fastd_period: 3\n        fastd_matype: 0\n    Outputs:\n        fastk\n        fastd\n    ",
    "talib.stream_SUB": " SUB(real0, real1)\n\n    Vector Arithmetic Subtraction (Math Operators)\n\n    Inputs:\n        real0: (any ndarray)\n        real1: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.stream_SUM": " SUM(real[, timeperiod=?])\n\n    Summation (Math Operators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.stream_T3": " T3(real[, timeperiod=?, vfactor=?])\n\n    Triple Exponential Moving Average (T3) (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 5\n        vfactor: 0.7\n    Outputs:\n        real\n    ",
    "talib.stream_TAN": " TAN(real)\n\n    Vector Trigonometric Tan (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.stream_TANH": " TANH(real)\n\n    Vector Trigonometric Tanh (Math Transform)\n\n    Inputs:\n        real: (any ndarray)\n    Outputs:\n        real\n    ",
    "talib.stream_TEMA": " TEMA(real[, timeperiod=?])\n\n    Triple Exponential Moving Average (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.stream_TRANGE": " TRANGE(high, low, close)\n\n    True Range (Volatility Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Outputs:\n        real\n    ",
    "talib.stream_TRIMA": " TRIMA(real[, timeperiod=?])\n\n    Triangular Moving Average (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.stream_TRIX": " TRIX(real[, timeperiod=?])\n\n    1-day Rate-Of-Change (ROC) of a Triple Smooth EMA (Momentum Indicators)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.stream_TSF": " TSF(real[, timeperiod=?])\n\n    Time Series Forecast (Statistic Functions)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.stream_TYPPRICE": " TYPPRICE(high, low, close)\n\n    Typical Price (Price Transform)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Outputs:\n        real\n    ",
    "talib.stream_ULTOSC": " ULTOSC(high, low, close[, timeperiod1=?, timeperiod2=?, timeperiod3=?])\n\n    Ultimate Oscillator (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        timeperiod1: 7\n        timeperiod2: 14\n        timeperiod3: 28\n    Outputs:\n        real\n    ",
    "talib.stream_VAR": " VAR(real[, timeperiod=?, nbdev=?])\n\n    Variance (Statistic Functions)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 5\n        nbdev: 1.0\n    Outputs:\n        real\n    ",
    "talib.stream_WCLPRICE": " WCLPRICE(high, low, close)\n\n    Weighted Close Price (Price Transform)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Outputs:\n        real\n    ",
    "talib.stream_WILLR": " WILLR(high, low, close[, timeperiod=?])\n\n    Williams' %R (Momentum Indicators)\n\n    Inputs:\n        prices: ['high', 'low', 'close']\n    Parameters:\n        timeperiod: 14\n    Outputs:\n        real\n    ",
    "talib.stream_WMA": " WMA(real[, timeperiod=?])\n\n    Weighted Moving Average (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.wrapped_func": " WMA(real[, timeperiod=?])\n\n    Weighted Moving Average (Overlap Studies)\n\n    Inputs:\n        real: (any ndarray)\n    Parameters:\n        timeperiod: 30\n    Outputs:\n        real\n    ",
    "talib.wraps": "Decorator factory to apply update_wrapper() to a wrapper function\n\n       Returns a decorator that invokes update_wrapper() with the decorated\n       function as the wrapper argument and the arguments to wraps() as the\n       remaining arguments. Default arguments are as for update_wrapper().\n       This is a convenience function to simplify applying partial() to\n       update_wrapper().\n    ",
    "QuantLib": "\n Copyright (C) 2000, 2001, 2002, 2003 RiskMap srl\n\n This file is part of QuantLib, a free-software/open-source library\n for financial quantitative analysts and developers - http://quantlib.org/\n\n QuantLib is free software: you can redistribute it and/or modify it\n under the terms of the QuantLib license.  You should have received a\n copy of the license along with this program; if not, please email\n <quantlib-dev@lists.sf.net>. The license is also available online at\n <http://quantlib.org/license.shtml>.\n\n This program is distributed in the hope that it will be useful, but WITHOUT\n ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n FOR A PARTICULAR PURPOSE.  See the license for more details.\n",
    "QuantLib.AEDCurrency": "Proxy of C++ QuantLib::AEDCurrency class.",
    "QuantLib.AOACurrency": "Proxy of C++ QuantLib::AOACurrency class.",
    "QuantLib.ARSCurrency": "Proxy of C++ QuantLib::ARSCurrency class.",
    "QuantLib.ASX": "Proxy of C++ ASX class.",
    "QuantLib.ATSCurrency": "Proxy of C++ QuantLib::ATSCurrency class.",
    "QuantLib.AUCPI": "Proxy of C++ AUCPI class.",
    "QuantLib.AUDCurrency": "Proxy of C++ QuantLib::AUDCurrency class.",
    "QuantLib.AUDLibor": "Proxy of C++ AUDLibor class.",
    "QuantLib.AbcdFunction": "Proxy of C++ AbcdFunction class.",
    "QuantLib.AbcdMathFunction": "Proxy of C++ AbcdMathFunction class.",
    "QuantLib.AbcdVol": "Proxy of C++ AbcdVol class.",
    "QuantLib.Actual360": "Proxy of C++ QuantLib::Actual360 class.",
    "QuantLib.Actual364": "Proxy of C++ QuantLib::Actual364 class.",
    "QuantLib.Actual36525": "Proxy of C++ QuantLib::Actual36525 class.",
    "QuantLib.Actual365Fixed": "Proxy of C++ QuantLib::Actual365Fixed class.",
    "QuantLib.Actual366": "Proxy of C++ QuantLib::Actual366 class.",
    "QuantLib.ActualActual": "Proxy of C++ QuantLib::ActualActual class.",
    "QuantLib.AmericanExercise": "Proxy of C++ AmericanExercise class.",
    "QuantLib.AmortizingCmsRateBond": "Proxy of C++ AmortizingCmsRateBond class.",
    "QuantLib.AmortizingFixedRateBond": "Proxy of C++ AmortizingFixedRateBond class.",
    "QuantLib.AmortizingFloatingRateBond": "Proxy of C++ AmortizingFloatingRateBond class.",
    "QuantLib.AmortizingPayment": "Proxy of C++ AmortizingPayment class.",
    "QuantLib.AnalyticAmericanMargrabeEngine": "Proxy of C++ AnalyticAmericanMargrabeEngine class.",
    "QuantLib.AnalyticBSMHullWhiteEngine": "Proxy of C++ AnalyticBSMHullWhiteEngine class.",
    "QuantLib.AnalyticBarrierEngine": "Proxy of C++ AnalyticBarrierEngine class.",
    "QuantLib.AnalyticBinaryBarrierEngine": "Proxy of C++ AnalyticBinaryBarrierEngine class.",
    "QuantLib.AnalyticCEVEngine": "Proxy of C++ AnalyticCEVEngine class.",
    "QuantLib.AnalyticCapFloorEngine": "Proxy of C++ AnalyticCapFloorEngine class.",
    "QuantLib.AnalyticCliquetEngine": "Proxy of C++ AnalyticCliquetEngine class.",
    "QuantLib.AnalyticComplexChooserEngine": "Proxy of C++ AnalyticComplexChooserEngine class.",
    "QuantLib.AnalyticCompoundOptionEngine": "Proxy of C++ AnalyticCompoundOptionEngine class.",
    "QuantLib.AnalyticContinuousFixedLookbackEngine": "Proxy of C++ AnalyticContinuousFixedLookbackEngine class.",
    "QuantLib.AnalyticContinuousFloatingLookbackEngine": "Proxy of C++ AnalyticContinuousFloatingLookbackEngine class.",
    "QuantLib.AnalyticContinuousGeometricAveragePriceAsianEngine": "Proxy of C++ AnalyticContinuousGeometricAveragePriceAsianEngine class.",
    "QuantLib.AnalyticContinuousGeometricAveragePriceAsianHestonEngine": "Proxy of C++ AnalyticContinuousGeometricAveragePriceAsianHestonEngine class.",
    "QuantLib.AnalyticContinuousPartialFixedLookbackEngine": "Proxy of C++ AnalyticContinuousPartialFixedLookbackEngine class.",
    "QuantLib.AnalyticContinuousPartialFloatingLookbackEngine": "Proxy of C++ AnalyticContinuousPartialFloatingLookbackEngine class.",
    "QuantLib.AnalyticDigitalAmericanEngine": "Proxy of C++ AnalyticDigitalAmericanEngine class.",
    "QuantLib.AnalyticDigitalAmericanKOEngine": "Proxy of C++ AnalyticDigitalAmericanKOEngine class.",
    "QuantLib.AnalyticDiscreteGeometricAveragePriceAsianEngine": "Proxy of C++ AnalyticDiscreteGeometricAveragePriceAsianEngine class.",
    "QuantLib.AnalyticDiscreteGeometricAveragePriceAsianHestonEngine": "Proxy of C++ AnalyticDiscreteGeometricAveragePriceAsianHestonEngine class.",
    "QuantLib.AnalyticDiscreteGeometricAverageStrikeAsianEngine": "Proxy of C++ AnalyticDiscreteGeometricAverageStrikeAsianEngine class.",
    "QuantLib.AnalyticDividendEuropeanEngine": "Proxy of C++ AnalyticDividendEuropeanEngine class.",
    "QuantLib.AnalyticDoubleBarrierBinaryEngine": "Proxy of C++ AnalyticDoubleBarrierBinaryEngine class.",
    "QuantLib.AnalyticDoubleBarrierEngine": "Double barrier engine implementing Ikeda-Kunitomo series.",
    "QuantLib.AnalyticEuropeanEngine": "Proxy of C++ AnalyticEuropeanEngine class.",
    "QuantLib.AnalyticEuropeanMargrabeEngine": "Proxy of C++ AnalyticEuropeanMargrabeEngine class.",
    "QuantLib.AnalyticGJRGARCHEngine": "Proxy of C++ AnalyticGJRGARCHEngine class.",
    "QuantLib.AnalyticH1HWEngine": "Proxy of C++ AnalyticH1HWEngine class.",
    "QuantLib.AnalyticHaganPricer": "Proxy of C++ AnalyticHaganPricer class.",
    "QuantLib.AnalyticHestonEngine": "Proxy of C++ AnalyticHestonEngine class.",
    "QuantLib.AnalyticHestonEngine_Integration": "Proxy of C++ AnalyticHestonEngine::Integration class.",
    "QuantLib.AnalyticHestonEngine_OptimalAlpha": "Proxy of C++ AnalyticHestonEngine::OptimalAlpha class.",
    "QuantLib.AnalyticHestonForwardEuropeanEngine": "Proxy of C++ AnalyticHestonForwardEuropeanEngine class.",
    "QuantLib.AnalyticHestonHullWhiteEngine": "Proxy of C++ AnalyticHestonHullWhiteEngine class.",
    "QuantLib.AnalyticPTDHestonEngine": "Proxy of C++ AnalyticPTDHestonEngine class.",
    "QuantLib.AnalyticPartialTimeBarrierOptionEngine": "Partial Time Barrier Option Engine",
    "QuantLib.AnalyticPerformanceEngine": "Proxy of C++ AnalyticPerformanceEngine class.",
    "QuantLib.AnalyticSimpleChooserEngine": "Proxy of C++ AnalyticSimpleChooserEngine class.",
    "QuantLib.AndreasenHugeLocalVolAdapter": "Proxy of C++ AndreasenHugeLocalVolAdapter class.",
    "QuantLib.AndreasenHugeVolatilityAdapter": "Proxy of C++ AndreasenHugeVolatilityAdapter class.",
    "QuantLib.AndreasenHugeVolatilityInterpl": "Proxy of C++ AndreasenHugeVolatilityInterpl class.",
    "QuantLib.Aonia": "Proxy of C++ Aonia class.",
    "QuantLib.Argentina": "Proxy of C++ QuantLib::Argentina class.",
    "QuantLib.ArithmeticAverageOIS": "Proxy of C++ ArithmeticAverageOIS class.",
    "QuantLib.ArithmeticAveragedOvernightIndexedCouponPricer": "Proxy of C++ ArithmeticAveragedOvernightIndexedCouponPricer class.",
    "QuantLib.ArithmeticOISRateHelper": "Proxy of C++ ArithmeticOISRateHelper class.",
    "QuantLib.Array": "Proxy of C++ Array class.",
    "QuantLib.AssetOrNothingPayoff": "Proxy of C++ AssetOrNothingPayoff class.",
    "QuantLib.AssetSwap": "Proxy of C++ AssetSwap class.",
    "QuantLib.Australia": "Proxy of C++ QuantLib::Australia class.",
    "QuantLib.Austria": "Proxy of C++ QuantLib::Austria class.",
    "QuantLib.Average": "Proxy of C++ Average class.",
    "QuantLib.AverageBasketPayoff": "Proxy of C++ AverageBasketPayoff class.",
    "QuantLib.AveragingRatePricer": "Proxy of C++ AveragingRatePricer class.",
    "QuantLib.BCHCurrency": "Proxy of C++ QuantLib::BCHCurrency class.",
    "QuantLib.BDTCurrency": "Proxy of C++ QuantLib::BDTCurrency class.",
    "QuantLib.BEFCurrency": "Proxy of C++ QuantLib::BEFCurrency class.",
    "QuantLib.BFGS": "Proxy of C++ BFGS class.",
    "QuantLib.BGLCurrency": "Proxy of C++ QuantLib::BGLCurrency class.",
    "QuantLib.BGNCurrency": "Proxy of C++ QuantLib::BGNCurrency class.",
    "QuantLib.BHDCurrency": "Proxy of C++ QuantLib::BHDCurrency class.",
    "QuantLib.BRLCurrency": "Proxy of C++ QuantLib::BRLCurrency class.",
    "QuantLib.BSMRNDCalculator": "Proxy of C++ BSMRNDCalculator class.",
    "QuantLib.BTCCurrency": "Proxy of C++ QuantLib::BTCCurrency class.",
    "QuantLib.BWPCurrency": "Proxy of C++ QuantLib::BWPCurrency class.",
    "QuantLib.BYRCurrency": "Proxy of C++ QuantLib::BYRCurrency class.",
    "QuantLib.BachelierCapFloorEngine": "Proxy of C++ BachelierCapFloorEngine class.",
    "QuantLib.BachelierSwaptionEngine": "Proxy of C++ BachelierSwaptionEngine class.",
    "QuantLib.BachelierYoYInflationCouponPricer": "Proxy of C++ BachelierYoYInflationCouponPricer class.",
    "QuantLib.BackwardFlat": "Proxy of C++ BackwardFlat class.",
    "QuantLib.BackwardFlatInterpolation": "Proxy of C++ SafeBackwardFlatInterpolation class.",
    "QuantLib.BaroneAdesiWhaleyApproximationEngine": "Proxy of C++ BaroneAdesiWhaleyApproximationEngine class.",
    "QuantLib.Barrier": "Proxy of C++ Barrier class.",
    "QuantLib.BarrierOption": "Proxy of C++ BarrierOption class.",
    "QuantLib.BasketOption": "Proxy of C++ BasketOption class.",
    "QuantLib.BasketPayoff": "Proxy of C++ BasketPayoff class.",
    "QuantLib.BatesEngine": "Proxy of C++ BatesEngine class.",
    "QuantLib.BatesModel": "Proxy of C++ BatesModel class.",
    "QuantLib.BatesProcess": "Proxy of C++ BatesProcess class.",
    "QuantLib.Bbsw": "Proxy of C++ Bbsw class.",
    "QuantLib.Bbsw1M": "Proxy of C++ Bbsw1M class.",
    "QuantLib.Bbsw2M": "Proxy of C++ Bbsw2M class.",
    "QuantLib.Bbsw3M": "Proxy of C++ Bbsw3M class.",
    "QuantLib.Bbsw4M": "Proxy of C++ Bbsw4M class.",
    "QuantLib.Bbsw5M": "Proxy of C++ Bbsw5M class.",
    "QuantLib.Bbsw6M": "Proxy of C++ Bbsw6M class.",
    "QuantLib.BermudanExercise": "Proxy of C++ BermudanExercise class.",
    "QuantLib.BespokeCalendar": "Proxy of C++ QuantLib::BespokeCalendar class.",
    "QuantLib.BiCGstab": "Proxy of C++ BiCGstab class.",
    "QuantLib.Bibor": "Proxy of C++ Bibor class.",
    "QuantLib.Bibor1M": "Proxy of C++ Bibor1M class.",
    "QuantLib.Bibor1Y": "Proxy of C++ Bibor1Y class.",
    "QuantLib.Bibor2M": "Proxy of C++ Bibor2M class.",
    "QuantLib.Bibor3M": "Proxy of C++ Bibor3M class.",
    "QuantLib.Bibor6M": "Proxy of C++ Bibor6M class.",
    "QuantLib.Bibor9M": "Proxy of C++ Bibor9M class.",
    "QuantLib.BiborSW": "Proxy of C++ BiborSW class.",
    "QuantLib.Bicubic": "Proxy of C++ Bicubic class.",
    "QuantLib.BicubicSpline": "Proxy of C++ SafeBicubicSpline class.",
    "QuantLib.BilinearInterpolation": "Proxy of C++ SafeBilinearInterpolation class.",
    "QuantLib.BinomialCRRBarrierEngine": "\n    Binomial Engine for barrier options.\n    Features different binomial models, selected by the type parameters.\n    Uses Boyle-Lau adjustment for optimize steps and Derman-Kani optimization to speed\n    up convergence.\n    Type values:\n        crr or coxrossrubinstein:        Cox-Ross-Rubinstein model\n        jr  or jarrowrudd:               Jarrow-Rudd model\n        eqp or additiveeqpbinomialtree:  Additive EQP model\n        trigeorgis:                      Trigeorgis model\n        tian:                            Tian model\n        lr  or leisenreimer              Leisen-Reimer model\n        j4  or joshi4:                   Joshi 4th (smoothed) model\n\n    Boyle-Lau adjustment is controlled by parameter max_steps.\n    If max_steps is equal to steps Boyle-Lau is disabled.\n    Il max_steps is 0 (default value), max_steps is calculated by capping it to\n    5*steps when Boyle-Lau would need more than 1000 steps.\n    If max_steps is specified, it would limit binomial steps to this value.\n\n    ",
    "QuantLib.BinomialCRRConvertibleEngine": "Proxy of C++ BinomialConvertibleEngine< CoxRossRubinstein > class.",
    "QuantLib.BinomialCRRDoubleBarrierEngine": "\n    Binomial Engine for double barrier options.\n    Features different binomial models, selected by the type parameters.\n    Uses Derman-Kani optimization to speed up convergence.\n    Type values:\n        crr or coxrossrubinstein:        Cox-Ross-Rubinstein model\n        jr  or jarrowrudd:               Jarrow-Rudd model\n        eqp or additiveeqpbinomialtree:  Additive EQP model\n        trigeorgis:                      Trigeorgis model\n        tian:                            Tian model\n        lr  or leisenreimer              Leisen-Reimer model\n        j4  or joshi4:                   Joshi 4th (smoothed) model\n\n    ",
    "QuantLib.BinomialCRRVanillaEngine": "Proxy of C++ BinomialVanillaEngine< CoxRossRubinstein > class.",
    "QuantLib.BinomialDistribution": "Proxy of C++ BinomialDistribution class.",
    "QuantLib.BinomialEQPBarrierEngine": "\n    Binomial Engine for barrier options.\n    Features different binomial models, selected by the type parameters.\n    Uses Boyle-Lau adjustment for optimize steps and Derman-Kani optimization to speed\n    up convergence.\n    Type values:\n        crr or coxrossrubinstein:        Cox-Ross-Rubinstein model\n        jr  or jarrowrudd:               Jarrow-Rudd model\n        eqp or additiveeqpbinomialtree:  Additive EQP model\n        trigeorgis:                      Trigeorgis model\n        tian:                            Tian model\n        lr  or leisenreimer              Leisen-Reimer model\n        j4  or joshi4:                   Joshi 4th (smoothed) model\n\n    Boyle-Lau adjustment is controlled by parameter max_steps.\n    If max_steps is equal to steps Boyle-Lau is disabled.\n    Il max_steps is 0 (default value), max_steps is calculated by capping it to\n    5*steps when Boyle-Lau would need more than 1000 steps.\n    If max_steps is specified, it would limit binomial steps to this value.\n\n    ",
    "QuantLib.BinomialEQPConvertibleEngine": "Proxy of C++ BinomialConvertibleEngine< AdditiveEQPBinomialTree > class.",
    "QuantLib.BinomialEQPDoubleBarrierEngine": "\n    Binomial Engine for double barrier options.\n    Features different binomial models, selected by the type parameters.\n    Uses Derman-Kani optimization to speed up convergence.\n    Type values:\n        crr or coxrossrubinstein:        Cox-Ross-Rubinstein model\n        jr  or jarrowrudd:               Jarrow-Rudd model\n        eqp or additiveeqpbinomialtree:  Additive EQP model\n        trigeorgis:                      Trigeorgis model\n        tian:                            Tian model\n        lr  or leisenreimer              Leisen-Reimer model\n        j4  or joshi4:                   Joshi 4th (smoothed) model\n\n    ",
    "QuantLib.BinomialEQPVanillaEngine": "Proxy of C++ BinomialVanillaEngine< AdditiveEQPBinomialTree > class.",
    "QuantLib.BinomialJ4BarrierEngine": "\n    Binomial Engine for barrier options.\n    Features different binomial models, selected by the type parameters.\n    Uses Boyle-Lau adjustment for optimize steps and Derman-Kani optimization to speed\n    up convergence.\n    Type values:\n        crr or coxrossrubinstein:        Cox-Ross-Rubinstein model\n        jr  or jarrowrudd:               Jarrow-Rudd model\n        eqp or additiveeqpbinomialtree:  Additive EQP model\n        trigeorgis:                      Trigeorgis model\n        tian:                            Tian model\n        lr  or leisenreimer              Leisen-Reimer model\n        j4  or joshi4:                   Joshi 4th (smoothed) model\n\n    Boyle-Lau adjustment is controlled by parameter max_steps.\n    If max_steps is equal to steps Boyle-Lau is disabled.\n    Il max_steps is 0 (default value), max_steps is calculated by capping it to\n    5*steps when Boyle-Lau would need more than 1000 steps.\n    If max_steps is specified, it would limit binomial steps to this value.\n\n    ",
    "QuantLib.BinomialJ4ConvertibleEngine": "Proxy of C++ BinomialConvertibleEngine< Joshi4 > class.",
    "QuantLib.BinomialJ4DoubleBarrierEngine": "\n    Binomial Engine for double barrier options.\n    Features different binomial models, selected by the type parameters.\n    Uses Derman-Kani optimization to speed up convergence.\n    Type values:\n        crr or coxrossrubinstein:        Cox-Ross-Rubinstein model\n        jr  or jarrowrudd:               Jarrow-Rudd model\n        eqp or additiveeqpbinomialtree:  Additive EQP model\n        trigeorgis:                      Trigeorgis model\n        tian:                            Tian model\n        lr  or leisenreimer              Leisen-Reimer model\n        j4  or joshi4:                   Joshi 4th (smoothed) model\n\n    ",
    "QuantLib.BinomialJ4VanillaEngine": "Proxy of C++ BinomialVanillaEngine< Joshi4 > class.",
    "QuantLib.BinomialJRBarrierEngine": "\n    Binomial Engine for barrier options.\n    Features different binomial models, selected by the type parameters.\n    Uses Boyle-Lau adjustment for optimize steps and Derman-Kani optimization to speed\n    up convergence.\n    Type values:\n        crr or coxrossrubinstein:        Cox-Ross-Rubinstein model\n        jr  or jarrowrudd:               Jarrow-Rudd model\n        eqp or additiveeqpbinomialtree:  Additive EQP model\n        trigeorgis:                      Trigeorgis model\n        tian:                            Tian model\n        lr  or leisenreimer              Leisen-Reimer model\n        j4  or joshi4:                   Joshi 4th (smoothed) model\n\n    Boyle-Lau adjustment is controlled by parameter max_steps.\n    If max_steps is equal to steps Boyle-Lau is disabled.\n    Il max_steps is 0 (default value), max_steps is calculated by capping it to\n    5*steps when Boyle-Lau would need more than 1000 steps.\n    If max_steps is specified, it would limit binomial steps to this value.\n\n    ",
    "QuantLib.BinomialJRConvertibleEngine": "Proxy of C++ BinomialConvertibleEngine< JarrowRudd > class.",
    "QuantLib.BinomialJRDoubleBarrierEngine": "\n    Binomial Engine for double barrier options.\n    Features different binomial models, selected by the type parameters.\n    Uses Derman-Kani optimization to speed up convergence.\n    Type values:\n        crr or coxrossrubinstein:        Cox-Ross-Rubinstein model\n        jr  or jarrowrudd:               Jarrow-Rudd model\n        eqp or additiveeqpbinomialtree:  Additive EQP model\n        trigeorgis:                      Trigeorgis model\n        tian:                            Tian model\n        lr  or leisenreimer              Leisen-Reimer model\n        j4  or joshi4:                   Joshi 4th (smoothed) model\n\n    ",
    "QuantLib.BinomialJRVanillaEngine": "Proxy of C++ BinomialVanillaEngine< JarrowRudd > class.",
    "QuantLib.BinomialLRBarrierEngine": "\n    Binomial Engine for barrier options.\n    Features different binomial models, selected by the type parameters.\n    Uses Boyle-Lau adjustment for optimize steps and Derman-Kani optimization to speed\n    up convergence.\n    Type values:\n        crr or coxrossrubinstein:        Cox-Ross-Rubinstein model\n        jr  or jarrowrudd:               Jarrow-Rudd model\n        eqp or additiveeqpbinomialtree:  Additive EQP model\n        trigeorgis:                      Trigeorgis model\n        tian:                            Tian model\n        lr  or leisenreimer              Leisen-Reimer model\n        j4  or joshi4:                   Joshi 4th (smoothed) model\n\n    Boyle-Lau adjustment is controlled by parameter max_steps.\n    If max_steps is equal to steps Boyle-Lau is disabled.\n    Il max_steps is 0 (default value), max_steps is calculated by capping it to\n    5*steps when Boyle-Lau would need more than 1000 steps.\n    If max_steps is specified, it would limit binomial steps to this value.\n\n    ",
    "QuantLib.BinomialLRConvertibleEngine": "Proxy of C++ BinomialConvertibleEngine< LeisenReimer > class.",
    "QuantLib.BinomialLRDoubleBarrierEngine": "\n    Binomial Engine for double barrier options.\n    Features different binomial models, selected by the type parameters.\n    Uses Derman-Kani optimization to speed up convergence.\n    Type values:\n        crr or coxrossrubinstein:        Cox-Ross-Rubinstein model\n        jr  or jarrowrudd:               Jarrow-Rudd model\n        eqp or additiveeqpbinomialtree:  Additive EQP model\n        trigeorgis:                      Trigeorgis model\n        tian:                            Tian model\n        lr  or leisenreimer              Leisen-Reimer model\n        j4  or joshi4:                   Joshi 4th (smoothed) model\n\n    ",
    "QuantLib.BinomialLRVanillaEngine": "Proxy of C++ BinomialVanillaEngine< LeisenReimer > class.",
    "QuantLib.BinomialTianBarrierEngine": "\n    Binomial Engine for barrier options.\n    Features different binomial models, selected by the type parameters.\n    Uses Boyle-Lau adjustment for optimize steps and Derman-Kani optimization to speed\n    up convergence.\n    Type values:\n        crr or coxrossrubinstein:        Cox-Ross-Rubinstein model\n        jr  or jarrowrudd:               Jarrow-Rudd model\n        eqp or additiveeqpbinomialtree:  Additive EQP model\n        trigeorgis:                      Trigeorgis model\n        tian:                            Tian model\n        lr  or leisenreimer              Leisen-Reimer model\n        j4  or joshi4:                   Joshi 4th (smoothed) model\n\n    Boyle-Lau adjustment is controlled by parameter max_steps.\n    If max_steps is equal to steps Boyle-Lau is disabled.\n    Il max_steps is 0 (default value), max_steps is calculated by capping it to\n    5*steps when Boyle-Lau would need more than 1000 steps.\n    If max_steps is specified, it would limit binomial steps to this value.\n\n    ",
    "QuantLib.BinomialTianConvertibleEngine": "Proxy of C++ BinomialConvertibleEngine< Tian > class.",
    "QuantLib.BinomialTianDoubleBarrierEngine": "\n    Binomial Engine for double barrier options.\n    Features different binomial models, selected by the type parameters.\n    Uses Derman-Kani optimization to speed up convergence.\n    Type values:\n        crr or coxrossrubinstein:        Cox-Ross-Rubinstein model\n        jr  or jarrowrudd:               Jarrow-Rudd model\n        eqp or additiveeqpbinomialtree:  Additive EQP model\n        trigeorgis:                      Trigeorgis model\n        tian:                            Tian model\n        lr  or leisenreimer              Leisen-Reimer model\n        j4  or joshi4:                   Joshi 4th (smoothed) model\n\n    ",
    "QuantLib.BinomialTianVanillaEngine": "Proxy of C++ BinomialVanillaEngine< Tian > class.",
    "QuantLib.BinomialTrigeorgisBarrierEngine": "\n    Binomial Engine for barrier options.\n    Features different binomial models, selected by the type parameters.\n    Uses Boyle-Lau adjustment for optimize steps and Derman-Kani optimization to speed\n    up convergence.\n    Type values:\n        crr or coxrossrubinstein:        Cox-Ross-Rubinstein model\n        jr  or jarrowrudd:               Jarrow-Rudd model\n        eqp or additiveeqpbinomialtree:  Additive EQP model\n        trigeorgis:                      Trigeorgis model\n        tian:                            Tian model\n        lr  or leisenreimer              Leisen-Reimer model\n        j4  or joshi4:                   Joshi 4th (smoothed) model\n\n    Boyle-Lau adjustment is controlled by parameter max_steps.\n    If max_steps is equal to steps Boyle-Lau is disabled.\n    Il max_steps is 0 (default value), max_steps is calculated by capping it to\n    5*steps when Boyle-Lau would need more than 1000 steps.\n    If max_steps is specified, it would limit binomial steps to this value.\n\n    ",
    "QuantLib.BinomialTrigeorgisConvertibleEngine": "Proxy of C++ BinomialConvertibleEngine< Trigeorgis > class.",
    "QuantLib.BinomialTrigeorgisDoubleBarrierEngine": "\n    Binomial Engine for double barrier options.\n    Features different binomial models, selected by the type parameters.\n    Uses Derman-Kani optimization to speed up convergence.\n    Type values:\n        crr or coxrossrubinstein:        Cox-Ross-Rubinstein model\n        jr  or jarrowrudd:               Jarrow-Rudd model\n        eqp or additiveeqpbinomialtree:  Additive EQP model\n        trigeorgis:                      Trigeorgis model\n        tian:                            Tian model\n        lr  or leisenreimer              Leisen-Reimer model\n        j4  or joshi4:                   Joshi 4th (smoothed) model\n\n    ",
    "QuantLib.BinomialTrigeorgisVanillaEngine": "Proxy of C++ BinomialVanillaEngine< Trigeorgis > class.",
    "QuantLib.Bisection": "Proxy of C++ Bisection class.",
    "QuantLib.BivariateCumulativeNormalDistribution": "Proxy of C++ BivariateCumulativeNormalDistribution class.",
    "QuantLib.BivariateCumulativeNormalDistributionDr78": "Proxy of C++ BivariateCumulativeNormalDistributionDr78 class.",
    "QuantLib.BivariateCumulativeNormalDistributionWe04DP": "Proxy of C++ BivariateCumulativeNormalDistributionWe04DP class.",
    "QuantLib.BjerksundStenslandApproximationEngine": "Proxy of C++ BjerksundStenslandApproximationEngine class.",
    "QuantLib.Bkbm": "Proxy of C++ Bkbm class.",
    "QuantLib.Bkbm1M": "Proxy of C++ Bkbm1M class.",
    "QuantLib.Bkbm2M": "Proxy of C++ Bkbm2M class.",
    "QuantLib.Bkbm3M": "Proxy of C++ Bkbm3M class.",
    "QuantLib.Bkbm4M": "Proxy of C++ Bkbm4M class.",
    "QuantLib.Bkbm5M": "Proxy of C++ Bkbm5M class.",
    "QuantLib.Bkbm6M": "Proxy of C++ Bkbm6M class.",
    "QuantLib.BlackCalculator": "Proxy of C++ BlackCalculator class.",
    "QuantLib.BlackCalibrationHelper": "Proxy of C++ BlackCalibrationHelper class.",
    "QuantLib.BlackCalibrationHelperVector": "Proxy of C++ std::vector< ext::shared_ptr< BlackCalibrationHelper > > class.",
    "QuantLib.BlackCallableFixedRateBondEngine": "Proxy of C++ BlackCallableFixedRateBondEngine class.",
    "QuantLib.BlackCapFloorEngine": "Proxy of C++ BlackCapFloorEngine class.",
    "QuantLib.BlackCdsOptionEngine": "Proxy of C++ BlackCdsOptionEngine class.",
    "QuantLib.BlackConstantVol": "Proxy of C++ BlackConstantVol class.",
    "QuantLib.BlackDeltaCalculator": "Proxy of C++ BlackDeltaCalculator class.",
    "QuantLib.BlackIborCouponPricer": "Proxy of C++ BlackIborCouponPricer class.",
    "QuantLib.BlackKarasinski": "Proxy of C++ BlackKarasinski class.",
    "QuantLib.BlackProcess": "Proxy of C++ BlackProcess class.",
    "QuantLib.BlackScholesMertonProcess": "Proxy of C++ BlackScholesMertonProcess class.",
    "QuantLib.BlackScholesProcess": "Proxy of C++ BlackScholesProcess class.",
    "QuantLib.BlackSwaptionEngine": "Proxy of C++ BlackSwaptionEngine class.",
    "QuantLib.BlackVarianceCurve": "Proxy of C++ BlackVarianceCurve class.",
    "QuantLib.BlackVarianceSurface": "Proxy of C++ BlackVarianceSurface class.",
    "QuantLib.BlackVolTermStructure": "Proxy of C++ BlackVolTermStructure class.",
    "QuantLib.BlackVolTermStructureHandle": "Proxy of C++ Handle< BlackVolTermStructure > class.",
    "QuantLib.BlackYoYInflationCouponPricer": "Proxy of C++ BlackYoYInflationCouponPricer class.",
    "QuantLib.Bond": "Proxy of C++ Bond class.",
    "QuantLib.BondForward": "Proxy of C++ BondForward class.",
    "QuantLib.BondFunctions": "Proxy of C++ BondFunctions class.",
    "QuantLib.BondHelper": "Proxy of C++ BondHelper class.",
    "QuantLib.BondHelperVector": "Proxy of C++ std::vector< ext::shared_ptr< BondHelper > > class.",
    "QuantLib.BondPrice": "Proxy of C++ BondPrice class.",
    "QuantLib.BoolVector": "Proxy of C++ std::vector< bool > class.",
    "QuantLib.Botswana": "Proxy of C++ QuantLib::Botswana class.",
    "QuantLib.BoundaryConstraint": "Proxy of C++ BoundaryConstraint class.",
    "QuantLib.BoxMullerKnuthGaussianRng": "Proxy of C++ BoxMullerGaussianRng< KnuthUniformRng > class.",
    "QuantLib.BoxMullerLecuyerGaussianRng": "Proxy of C++ BoxMullerGaussianRng< LecuyerUniformRng > class.",
    "QuantLib.BoxMullerMersenneTwisterGaussianRng": "Proxy of C++ BoxMullerGaussianRng< MersenneTwisterUniformRng > class.",
    "QuantLib.BoxMullerXoshiro256StarStarGaussianRng": "Proxy of C++ BoxMullerGaussianRng< Xoshiro256StarStarUniformRng > class.",
    "QuantLib.Brazil": "Proxy of C++ QuantLib::Brazil class.",
    "QuantLib.Brent": "Proxy of C++ Brent class.",
    "QuantLib.BrownianBridge": "Proxy of C++ BrownianBridge class.",
    "QuantLib.BrownianGenerator": "Proxy of C++ BrownianGenerator class.",
    "QuantLib.BrownianGeneratorFactory": "Proxy of C++ BrownianGeneratorFactory class.",
    "QuantLib.Burley2020SobolBrownianBridgeRsg": "Proxy of C++ Burley2020SobolBrownianBridgeRsg class.",
    "QuantLib.Burley2020SobolRsg": "Proxy of C++ Burley2020SobolRsg class.",
    "QuantLib.Business252": "Proxy of C++ QuantLib::Business252 class.",
    "QuantLib.CADCurrency": "Proxy of C++ QuantLib::CADCurrency class.",
    "QuantLib.CADLibor": "Proxy of C++ CADLibor class.",
    "QuantLib.CADLiborON": "Proxy of C++ CADLiborON class.",
    "QuantLib.CEVRNDCalculator": "Proxy of C++ CEVRNDCalculator class.",
    "QuantLib.CHFCurrency": "Proxy of C++ QuantLib::CHFCurrency class.",
    "QuantLib.CHFLibor": "Proxy of C++ CHFLibor class.",
    "QuantLib.CLFCurrency": "Proxy of C++ QuantLib::CLFCurrency class.",
    "QuantLib.CLPCurrency": "Proxy of C++ QuantLib::CLPCurrency class.",
    "QuantLib.CNHCurrency": "Proxy of C++ QuantLib::CNHCurrency class.",
    "QuantLib.CNYCurrency": "Proxy of C++ QuantLib::CNYCurrency class.",
    "QuantLib.COPCurrency": "Proxy of C++ QuantLib::COPCurrency class.",
    "QuantLib.COSHestonEngine": "Proxy of C++ COSHestonEngine class.",
    "QuantLib.COUCurrency": "Proxy of C++ QuantLib::COUCurrency class.",
    "QuantLib.CPI": "Proxy of C++ CPI class.",
    "QuantLib.CPIBond": "Proxy of C++ CPIBond class.",
    "QuantLib.CPICashFlow": "Proxy of C++ CPICashFlow class.",
    "QuantLib.CPICoupon": "Proxy of C++ CPICoupon class.",
    "QuantLib.CPICouponPricer": "Proxy of C++ CPICouponPricer class.",
    "QuantLib.CPILeg": "CPILeg(DoubleVector nominals, Schedule schedule, ext::shared_ptr< ZeroInflationIndex > const & index, Real baseCPI, Period observationLag, DayCounter paymentDayCounter=DayCounter(), BusinessDayConvention const paymentConvention=Following, DoubleVector fixedRates=std::vector< Real >(), DoubleVector caps=std::vector< Rate >(), DoubleVector floors=std::vector< Rate >(), Period exCouponPeriod=Period(), Calendar exCouponCalendar=Calendar(), BusinessDayConvention exCouponConvention=Unadjusted, bool exCouponEndOfMonth=False, Calendar paymentCalendar=Calendar(), bool growthOnly=True, CPI::InterpolationType observationInterpolation=AsIndex) -> Leg",
    "QuantLib.CPISwap": "Proxy of C++ CPISwap class.",
    "QuantLib.CYPCurrency": "Proxy of C++ QuantLib::CYPCurrency class.",
    "QuantLib.CZKCurrency": "Proxy of C++ QuantLib::CZKCurrency class.",
    "QuantLib.Calendar": "Proxy of C++ Calendar class.",
    "QuantLib.CalendarVector": "Proxy of C++ std::vector< Calendar > class.",
    "QuantLib.CalibratedModel": "Proxy of C++ CalibratedModel class.",
    "QuantLib.CalibratedModelHandle": "Proxy of C++ Handle< CalibratedModel > class.",
    "QuantLib.CalibrationErrorTuple": "Proxy of C++ std::tuple< Real,Real,Real > class.",
    "QuantLib.CalibrationHelper": "Proxy of C++ CalibrationHelper class.",
    "QuantLib.CalibrationHelperVector": "Proxy of C++ std::vector< ext::shared_ptr< CalibrationHelper > > class.",
    "QuantLib.CalibrationPair": "Proxy of C++ std::pair< ext::shared_ptr< VanillaOption >,ext::shared_ptr< Quote > > class.",
    "QuantLib.CalibrationSet": "Proxy of C++ std::vector< std::pair< ext::shared_ptr< VanillaOption >,ext::shared_ptr< Quote > > > class.",
    "QuantLib.Callability": "Proxy of C++ Callability class.",
    "QuantLib.CallabilitySchedule": "Proxy of C++ std::vector< ext::shared_ptr< Callability > > class.",
    "QuantLib.CallableBond": "Proxy of C++ CallableBond class.",
    "QuantLib.CallableFixedRateBond": "Proxy of C++ CallableFixedRateBond class.",
    "QuantLib.CallableZeroCouponBond": "Proxy of C++ CallableZeroCouponBond class.",
    "QuantLib.Canada": "Proxy of C++ QuantLib::Canada class.",
    "QuantLib.Cap": "Proxy of C++ Cap class.",
    "QuantLib.CapFloor": "Proxy of C++ CapFloor class.",
    "QuantLib.CapFloorTermVolCurve": "Proxy of C++ CapFloorTermVolCurve class.",
    "QuantLib.CapFloorTermVolSurface": "Proxy of C++ CapFloorTermVolSurface class.",
    "QuantLib.CapFloorTermVolatilityStructure": "Proxy of C++ CapFloorTermVolatilityStructure class.",
    "QuantLib.CapFloorTermVolatilityStructureHandle": "Proxy of C++ Handle< CapFloorTermVolatilityStructure > class.",
    "QuantLib.CapHelper": "Proxy of C++ CapHelper class.",
    "QuantLib.CappedFlooredCmsCoupon": "Proxy of C++ CappedFlooredCmsCoupon class.",
    "QuantLib.CappedFlooredCmsSpreadCoupon": "Proxy of C++ CappedFlooredCmsSpreadCoupon class.",
    "QuantLib.CappedFlooredCoupon": "Proxy of C++ CappedFlooredCoupon class.",
    "QuantLib.CappedFlooredIborCoupon": "Proxy of C++ CappedFlooredIborCoupon class.",
    "QuantLib.CappedFlooredYoYInflationCoupon": "Proxy of C++ CappedFlooredYoYInflationCoupon class.",
    "QuantLib.CashFlow": "Proxy of C++ CashFlow class.",
    "QuantLib.CashFlows": "Proxy of C++ CashFlows class.",
    "QuantLib.CashOrNothingPayoff": "Proxy of C++ CashOrNothingPayoff class.",
    "QuantLib.Cdor": "Proxy of C++ Cdor class.",
    "QuantLib.CdsOption": "Proxy of C++ CdsOption class.",
    "QuantLib.CeilingTruncation": "Proxy of C++ CeilingTruncation class.",
    "QuantLib.CentralLimitKnuthGaussianRng": "Proxy of C++ CLGaussianRng< KnuthUniformRng > class.",
    "QuantLib.CentralLimitLecuyerGaussianRng": "Proxy of C++ CLGaussianRng< LecuyerUniformRng > class.",
    "QuantLib.CentralLimitMersenneTwisterGaussianRng": "Proxy of C++ CLGaussianRng< MersenneTwisterUniformRng > class.",
    "QuantLib.CentralLimitXoshiro256StarStarGaussianRng": "Proxy of C++ CLGaussianRng< Xoshiro256StarStarUniformRng > class.",
    "QuantLib.ChebyshevInterpolation": "Proxy of C++ ChebyshevInterpolation class.",
    "QuantLib.ChfLiborSwapIsdaFix": "Proxy of C++ ChfLiborSwapIsdaFix class.",
    "QuantLib.Chile": "Proxy of C++ QuantLib::Chile class.",
    "QuantLib.China": "Proxy of C++ QuantLib::China class.",
    "QuantLib.Claim": "Proxy of C++ Claim class.",
    "QuantLib.CliquetOption": "Proxy of C++ CliquetOption class.",
    "QuantLib.ClosestRounding": "Proxy of C++ ClosestRounding class.",
    "QuantLib.CmsCoupon": "Proxy of C++ CmsCoupon class.",
    "QuantLib.CmsCouponPricer": "Proxy of C++ CmsCouponPricer class.",
    "QuantLib.CmsCouponPricerVector": "Proxy of C++ std::vector< ext::shared_ptr< CmsCouponPricer > > class.",
    "QuantLib.CmsLeg": "CmsLeg(DoubleVector nominals, Schedule schedule, ext::shared_ptr< SwapIndex > const & index, DayCounter paymentDayCounter=DayCounter(), BusinessDayConvention const paymentConvention=Following, UnsignedIntVector fixingDays=std::vector< Natural >(), DoubleVector gearings=std::vector< Real >(), DoubleVector spreads=std::vector< Spread >(), DoubleVector caps=std::vector< Rate >(), DoubleVector floors=std::vector< Rate >(), bool isInArrears=False, Period exCouponPeriod=Period(), Calendar exCouponCalendar=Calendar(), BusinessDayConvention const exCouponConvention=Unadjusted, bool exCouponEndOfMonth=False) -> Leg",
    "QuantLib.CmsMarket": "Proxy of C++ CmsMarket class.",
    "QuantLib.CmsMarketCalibration": "Proxy of C++ CmsMarketCalibration class.",
    "QuantLib.CmsRateBond": "Proxy of C++ CmsRateBond class.",
    "QuantLib.CmsSpreadCoupon": "Proxy of C++ CmsSpreadCoupon class.",
    "QuantLib.CmsSpreadCouponPricer": "Proxy of C++ CmsSpreadCouponPricer class.",
    "QuantLib.CmsSpreadLeg": "CmsSpreadLeg(DoubleVector nominals, Schedule schedule, ext::shared_ptr< SwapSpreadIndex > const & index, DayCounter paymentDayCounter=DayCounter(), BusinessDayConvention const paymentConvention=Following, UnsignedIntVector fixingDays=std::vector< Natural >(), DoubleVector gearings=std::vector< Real >(), DoubleVector spreads=std::vector< Spread >(), DoubleVector caps=std::vector< Rate >(), DoubleVector floors=std::vector< Rate >(), bool isInArrears=False) -> Leg",
    "QuantLib.CmsZeroLeg": "CmsZeroLeg(DoubleVector nominals, Schedule schedule, ext::shared_ptr< SwapIndex > const & index, DayCounter paymentDayCounter=DayCounter(), BusinessDayConvention const paymentConvention=Following, UnsignedIntVector fixingDays=std::vector< Natural >(), DoubleVector gearings=std::vector< Real >(), DoubleVector spreads=std::vector< Spread >(), DoubleVector caps=std::vector< Rate >(), DoubleVector floors=std::vector< Rate >(), Period exCouponPeriod=Period(), Calendar exCouponCalendar=Calendar(), BusinessDayConvention const exCouponConvention=Unadjusted, bool exCouponEndOfMonth=False) -> Leg",
    "QuantLib.Collar": "Proxy of C++ Collar class.",
    "QuantLib.ComplexChooserOption": "Proxy of C++ ComplexChooserOption class.",
    "QuantLib.CompositeConstraint": "Proxy of C++ CompositeConstraint class.",
    "QuantLib.CompositeInstrument": "Proxy of C++ CompositeInstrument class.",
    "QuantLib.CompositeQuote": "\n    Proxy of C++ CompositeQuote< BinaryFunction > class.\n    Proxy of C++ CompositeQuote< BinaryFunction > class.\n    ",
    "QuantLib.CompositeZeroYieldStructure": "\n    Proxy of C++ CompositeZeroYieldStructure< BinaryFunction > class.\n    Proxy of C++ CompositeZeroYieldStructure< BinaryFunction > class.\n    ",
    "QuantLib.CompoundOption": "Proxy of C++ CompoundOption class.",
    "QuantLib.CompoundingOvernightIndexedCouponPricer": "Proxy of C++ CompoundingOvernightIndexedCouponPricer class.",
    "QuantLib.CompoundingRatePricer": "Proxy of C++ CompoundingRatePricer class.",
    "QuantLib.Concentrating1dMesher": "Proxy of C++ Concentrating1dMesher class.",
    "QuantLib.Concentrating1dMesherPoint": "Proxy of C++ std::tuple< Real,Real,bool > class.",
    "QuantLib.Concentrating1dMesherPointVector": "Proxy of C++ std::vector< std::tuple< Real,Real,bool > > class.",
    "QuantLib.ConjugateGradient": "Proxy of C++ ConjugateGradient class.",
    "QuantLib.ConstNotionalCrossCurrencyBasisSwapRateHelper": "Proxy of C++ ConstNotionalCrossCurrencyBasisSwapRateHelper class.",
    "QuantLib.ConstantEstimator": "Proxy of C++ ConstantEstimator class.",
    "QuantLib.ConstantOptionletVolatility": "Proxy of C++ ConstantOptionletVolatility class.",
    "QuantLib.ConstantParameter": "Proxy of C++ ConstantParameter class.",
    "QuantLib.ConstantSwaptionVolatility": "Proxy of C++ ConstantSwaptionVolatility class.",
    "QuantLib.ConstantYoYOptionletVolatility": "Proxy of C++ ConstantYoYOptionletVolatility class.",
    "QuantLib.Constraint": "Proxy of C++ Constraint class.",
    "QuantLib.ContinuousArithmeticAsianLevyEngine": "Proxy of C++ ContinuousArithmeticAsianLevyEngine class.",
    "QuantLib.ContinuousAveragingAsianOption": "Proxy of C++ ContinuousAveragingAsianOption class.",
    "QuantLib.ContinuousFixedLookbackOption": "Proxy of C++ ContinuousFixedLookbackOption class.",
    "QuantLib.ContinuousFloatingLookbackOption": "Proxy of C++ ContinuousFloatingLookbackOption class.",
    "QuantLib.ContinuousPartialFixedLookbackOption": "Proxy of C++ ContinuousPartialFixedLookbackOption class.",
    "QuantLib.ContinuousPartialFloatingLookbackOption": "Proxy of C++ ContinuousPartialFloatingLookbackOption class.",
    "QuantLib.ConvertibleFixedCouponBond": "Proxy of C++ ConvertibleFixedCouponBond class.",
    "QuantLib.ConvertibleFloatingRateBond": "Proxy of C++ ConvertibleFloatingRateBond class.",
    "QuantLib.ConvertibleZeroCouponBond": "Proxy of C++ ConvertibleZeroCouponBond class.",
    "QuantLib.ConvexMonotone": "Proxy of C++ ConvexMonotone class.",
    "QuantLib.ConvexMonotoneInterpolation": "Proxy of C++ SafeConvexMonotoneInterpolation class.",
    "QuantLib.Corra": "Proxy of C++ Corra class.",
    "QuantLib.Coupon": "Proxy of C++ Coupon class.",
    "QuantLib.CoxIngersollRoss": "Proxy of C++ CoxIngersollRoss class.",
    "QuantLib.CraigSneydScheme": "Proxy of C++ CraigSneydScheme class.",
    "QuantLib.CrankNicolsonScheme": "Proxy of C++ CrankNicolsonScheme class.",
    "QuantLib.CreditDefaultSwap": "Proxy of C++ CreditDefaultSwap class.",
    "QuantLib.Cubic": "Proxy of C++ Cubic class.",
    "QuantLib.CubicBSplinesFitting": "Proxy of C++ CubicBSplinesFitting class.",
    "QuantLib.CubicInterpolatedSmileSection": "Proxy of C++ InterpolatedSmileSection< Cubic > class.",
    "QuantLib.CubicInterpolation": "Proxy of C++ CubicInterpolation class.",
    "QuantLib.CubicNaturalSpline": "Proxy of C++ SafeCubicNaturalSpline class.",
    "QuantLib.CubicZeroCurve": "Proxy of C++ InterpolatedZeroCurve< Cubic > class.",
    "QuantLib.CumulativeBinomialDistribution": "Proxy of C++ CumulativeBinomialDistribution class.",
    "QuantLib.CumulativeChiSquareDistribution": "Proxy of C++ CumulativeChiSquareDistribution class.",
    "QuantLib.CumulativeGammaDistribution": "Proxy of C++ CumulativeGammaDistribution class.",
    "QuantLib.CumulativeNormalDistribution": "Proxy of C++ CumulativeNormalDistribution class.",
    "QuantLib.CumulativePoissonDistribution": "Proxy of C++ CumulativePoissonDistribution class.",
    "QuantLib.CumulativeStudentDistribution": "Proxy of C++ CumulativeStudentDistribution class.",
    "QuantLib.Currency": "Proxy of C++ Currency class.",
    "QuantLib.CurveState": "Proxy of C++ CurveState class.",
    "QuantLib.CustomRegion": "Proxy of C++ CustomRegion class.",
    "QuantLib.CzechRepublic": "Proxy of C++ QuantLib::CzechRepublic class.",
    "QuantLib.DASHCurrency": "Proxy of C++ QuantLib::DASHCurrency class.",
    "QuantLib.DEMCurrency": "Proxy of C++ QuantLib::DEMCurrency class.",
    "QuantLib.DKKCurrency": "Proxy of C++ QuantLib::DKKCurrency class.",
    "QuantLib.DKKLibor": "Proxy of C++ DKKLibor class.",
    "QuantLib.DMinus": "Proxy of C++ DMinus class.",
    "QuantLib.DPlus": "Proxy of C++ DPlus class.",
    "QuantLib.DPlusDMinus": "Proxy of C++ DPlusDMinus class.",
    "QuantLib.DZero": "Proxy of C++ DZero class.",
    "QuantLib.DailyTenorLibor": "Proxy of C++ DailyTenorLibor class.",
    "QuantLib.Date": "Proxy of C++ Date class.",
    "QuantLib.DateGeneration": "Proxy of C++ DateGeneration class.",
    "QuantLib.DatePair": "Proxy of C++ std::pair< Date,Date > class.",
    "QuantLib.DateParser": "Proxy of C++ DateParser class.",
    "QuantLib.DateVector": "Proxy of C++ std::vector< Date > class.",
    "QuantLib.DatedOISRateHelper": "Proxy of C++ DatedOISRateHelper class.",
    "QuantLib.DayCounter": "Proxy of C++ DayCounter class.",
    "QuantLib.DefaultBoundaryCondition": "Proxy of C++ DefaultBoundaryCondition class.",
    "QuantLib.DefaultDensity": "Proxy of C++ DefaultDensity class.",
    "QuantLib.DefaultDensityCurve": "Proxy of C++ InterpolatedDefaultDensityCurve< Linear > class.",
    "QuantLib.DefaultLogCubic": "Proxy of C++ DefaultLogCubic class.",
    "QuantLib.DefaultProbabilityHelper": "Proxy of C++ DefaultProbabilityHelper class.",
    "QuantLib.DefaultProbabilityHelperVector": "Proxy of C++ std::vector< ext::shared_ptr< DefaultProbabilityHelper > > class.",
    "QuantLib.DefaultProbabilityTermStructure": "Proxy of C++ DefaultProbabilityTermStructure class.",
    "QuantLib.DefaultProbabilityTermStructureHandle": "Proxy of C++ Handle< DefaultProbabilityTermStructure > class.",
    "QuantLib.DeltaVolQuote": "Proxy of C++ DeltaVolQuote class.",
    "QuantLib.DeltaVolQuoteHandle": "Proxy of C++ Handle< DeltaVolQuote > class.",
    "QuantLib.Denmark": "Proxy of C++ QuantLib::Denmark class.",
    "QuantLib.DepositRateHelper": "Proxy of C++ DepositRateHelper class.",
    "QuantLib.DerivedQuote": "\n    Proxy of C++ DerivedQuote< UnaryFunction > class.\n    Proxy of C++ DerivedQuote< UnaryFunction > class.\n    ",
    "QuantLib.Destr": "Proxy of C++ Destr class.",
    "QuantLib.DifferentialEvolution": "Proxy of C++ DifferentialEvolution class.",
    "QuantLib.DirichletBC": "Proxy of C++ DirichletBC class.",
    "QuantLib.Discount": "Proxy of C++ Discount class.",
    "QuantLib.DiscountCurve": "Proxy of C++ InterpolatedDiscountCurve< LogLinear > class.",
    "QuantLib.DiscountingBondEngine": "Proxy of C++ DiscountingBondEngine class.",
    "QuantLib.DiscountingSwapEngine": "Proxy of C++ DiscountingSwapEngine class.",
    "QuantLib.DiscreteAveragingAsianOption": "Proxy of C++ DiscreteAveragingAsianOption class.",
    "QuantLib.Dividend": "Proxy of C++ Dividend class.",
    "QuantLib.DividendSchedule": "Proxy of C++ std::vector< ext::shared_ptr< Dividend > > class.",
    "QuantLib.DoubleBarrier": "Proxy of C++ DoubleBarrier class.",
    "QuantLib.DoubleBarrierOption": "Proxy of C++ DoubleBarrierOption class.",
    "QuantLib.DoublePair": "Proxy of C++ std::pair< double,double > class.",
    "QuantLib.DoublePairVector": "Proxy of C++ std::vector< std::pair< double,double > > class.",
    "QuantLib.DoubleVector": "Proxy of C++ std::vector< double > class.",
    "QuantLib.DoubleVectorVector": "Proxy of C++ std::vector< std::vector< double > > class.",
    "QuantLib.DouglasScheme": "Proxy of C++ DouglasScheme class.",
    "QuantLib.DownRounding": "Proxy of C++ DownRounding class.",
    "QuantLib.Duration": "Proxy of C++ Duration class.",
    "QuantLib.EEKCurrency": "Proxy of C++ QuantLib::EEKCurrency class.",
    "QuantLib.EGPCurrency": "Proxy of C++ QuantLib::EGPCurrency class.",
    "QuantLib.ESPCurrency": "Proxy of C++ QuantLib::ESPCurrency class.",
    "QuantLib.ETBCurrency": "Proxy of C++ QuantLib::ETBCurrency class.",
    "QuantLib.ETCCurrency": "Proxy of C++ QuantLib::ETCCurrency class.",
    "QuantLib.ETHCurrency": "Proxy of C++ QuantLib::ETHCurrency class.",
    "QuantLib.EUHICP": "Proxy of C++ EUHICP class.",
    "QuantLib.EUHICPXT": "Proxy of C++ EUHICPXT class.",
    "QuantLib.EURCurrency": "Proxy of C++ QuantLib::EURCurrency class.",
    "QuantLib.EURLibor": "Proxy of C++ EURLibor class.",
    "QuantLib.EURLibor10M": "Proxy of C++ EURLibor10M class.",
    "QuantLib.EURLibor11M": "Proxy of C++ EURLibor11M class.",
    "QuantLib.EURLibor1M": "Proxy of C++ EURLibor1M class.",
    "QuantLib.EURLibor1Y": "Proxy of C++ EURLibor1Y class.",
    "QuantLib.EURLibor2M": "Proxy of C++ EURLibor2M class.",
    "QuantLib.EURLibor2W": "Proxy of C++ EURLibor2W class.",
    "QuantLib.EURLibor3M": "Proxy of C++ EURLibor3M class.",
    "QuantLib.EURLibor4M": "Proxy of C++ EURLibor4M class.",
    "QuantLib.EURLibor5M": "Proxy of C++ EURLibor5M class.",
    "QuantLib.EURLibor6M": "Proxy of C++ EURLibor6M class.",
    "QuantLib.EURLibor7M": "Proxy of C++ EURLibor7M class.",
    "QuantLib.EURLibor8M": "Proxy of C++ EURLibor8M class.",
    "QuantLib.EURLibor9M": "Proxy of C++ EURLibor9M class.",
    "QuantLib.EURLiborSW": "Proxy of C++ EURLiborSW class.",
    "QuantLib.EndCriteria": "Proxy of C++ EndCriteria class.",
    "QuantLib.Eonia": "Proxy of C++ Eonia class.",
    "QuantLib.EquityCashFlow": "Proxy of C++ EquityCashFlow class.",
    "QuantLib.EquityCashFlowPricer": "Proxy of C++ EquityCashFlowPricer class.",
    "QuantLib.EquityIndex": "Proxy of C++ EquityIndex class.",
    "QuantLib.EquityQuantoCashFlowPricer": "Proxy of C++ EquityQuantoCashFlowPricer class.",
    "QuantLib.EquityTotalReturnSwap": "Proxy of C++ EquityTotalReturnSwap class.",
    "QuantLib.Estr": "Proxy of C++ Estr class.",
    "QuantLib.EurLiborSwapIfrFix": "Proxy of C++ EurLiborSwapIfrFix class.",
    "QuantLib.EurLiborSwapIsdaFixA": "Proxy of C++ EurLiborSwapIsdaFixA class.",
    "QuantLib.EurLiborSwapIsdaFixB": "Proxy of C++ EurLiborSwapIsdaFixB class.",
    "QuantLib.Euribor": "Proxy of C++ Euribor class.",
    "QuantLib.Euribor10M": "Proxy of C++ Euribor10M class.",
    "QuantLib.Euribor11M": "Proxy of C++ Euribor11M class.",
    "QuantLib.Euribor1M": "Proxy of C++ Euribor1M class.",
    "QuantLib.Euribor1W": "Proxy of C++ Euribor1W class.",
    "QuantLib.Euribor1Y": "Proxy of C++ Euribor1Y class.",
    "QuantLib.Euribor2M": "Proxy of C++ Euribor2M class.",
    "QuantLib.Euribor2W": "Proxy of C++ Euribor2W class.",
    "QuantLib.Euribor365": "Proxy of C++ Euribor365 class.",
    "QuantLib.Euribor365_10M": "Proxy of C++ Euribor365_10M class.",
    "QuantLib.Euribor365_11M": "Proxy of C++ Euribor365_11M class.",
    "QuantLib.Euribor365_1M": "Proxy of C++ Euribor365_1M class.",
    "QuantLib.Euribor365_1Y": "Proxy of C++ Euribor365_1Y class.",
    "QuantLib.Euribor365_2M": "Proxy of C++ Euribor365_2M class.",
    "QuantLib.Euribor365_2W": "Proxy of C++ Euribor365_2W class.",
    "QuantLib.Euribor365_3M": "Proxy of C++ Euribor365_3M class.",
    "QuantLib.Euribor365_3W": "Proxy of C++ Euribor365_3W class.",
    "QuantLib.Euribor365_4M": "Proxy of C++ Euribor365_4M class.",
    "QuantLib.Euribor365_5M": "Proxy of C++ Euribor365_5M class.",
    "QuantLib.Euribor365_6M": "Proxy of C++ Euribor365_6M class.",
    "QuantLib.Euribor365_7M": "Proxy of C++ Euribor365_7M class.",
    "QuantLib.Euribor365_8M": "Proxy of C++ Euribor365_8M class.",
    "QuantLib.Euribor365_9M": "Proxy of C++ Euribor365_9M class.",
    "QuantLib.Euribor365_SW": "Proxy of C++ Euribor365_SW class.",
    "QuantLib.Euribor3M": "Proxy of C++ Euribor3M class.",
    "QuantLib.Euribor3W": "Proxy of C++ Euribor3W class.",
    "QuantLib.Euribor4M": "Proxy of C++ Euribor4M class.",
    "QuantLib.Euribor5M": "Proxy of C++ Euribor5M class.",
    "QuantLib.Euribor6M": "Proxy of C++ Euribor6M class.",
    "QuantLib.Euribor7M": "Proxy of C++ Euribor7M class.",
    "QuantLib.Euribor8M": "Proxy of C++ Euribor8M class.",
    "QuantLib.Euribor9M": "Proxy of C++ Euribor9M class.",
    "QuantLib.EuriborSW": "Proxy of C++ EuriborSW class.",
    "QuantLib.EuriborSwapIfrFix": "Proxy of C++ EuriborSwapIfrFix class.",
    "QuantLib.EuriborSwapIsdaFixA": "Proxy of C++ EuriborSwapIsdaFixA class.",
    "QuantLib.EuriborSwapIsdaFixB": "Proxy of C++ EuriborSwapIsdaFixB class.",
    "QuantLib.EuropeanExercise": "Proxy of C++ EuropeanExercise class.",
    "QuantLib.EuropeanOption": "Proxy of C++ EuropeanOption class.",
    "QuantLib.EverestOption": "Proxy of C++ EverestOption class.",
    "QuantLib.EvolutionDescription": "Proxy of C++ EvolutionDescription class.",
    "QuantLib.ExchangeRate": "Proxy of C++ ExchangeRate class.",
    "QuantLib.ExchangeRateManager": "Proxy of C++ ExchangeRateManager class.",
    "QuantLib.Exercise": "Proxy of C++ Exercise class.",
    "QuantLib.ExpSinhIntegral": "Proxy of C++ ExpSinhIntegral class.",
    "QuantLib.ExplicitEulerScheme": "Proxy of C++ ExplicitEulerScheme class.",
    "QuantLib.ExponentialFittingHestonEngine": "Proxy of C++ ExponentialFittingHestonEngine class.",
    "QuantLib.ExponentialForwardCorrelation": "Proxy of C++ ExponentialForwardCorrelation class.",
    "QuantLib.ExponentialJump1dMesher": "Proxy of C++ ExponentialJump1dMesher class.",
    "QuantLib.ExponentialSplinesFitting": "Proxy of C++ ExponentialSplinesFitting class.",
    "QuantLib.ExtOUWithJumpsProcess": "Proxy of C++ ExtOUWithJumpsProcess class.",
    "QuantLib.ExtendedCoxIngersollRoss": "Proxy of C++ ExtendedCoxIngersollRoss class.",
    "QuantLib.ExtendedOrnsteinUhlenbeckProcess": "Proxy of C++ ExtendedOrnsteinUhlenbeckProcess class.",
    "QuantLib.FFTVarianceGammaEngine": "Proxy of C++ FFTVarianceGammaEngine class.",
    "QuantLib.FIMCurrency": "Proxy of C++ QuantLib::FIMCurrency class.",
    "QuantLib.FRFCurrency": "Proxy of C++ QuantLib::FRFCurrency class.",
    "QuantLib.FRHICP": "Proxy of C++ FRHICP class.",
    "QuantLib.FaceValueAccrualClaim": "Proxy of C++ FaceValueAccrualClaim class.",
    "QuantLib.FaceValueClaim": "Proxy of C++ FaceValueClaim class.",
    "QuantLib.FalsePosition": "Proxy of C++ FalsePosition class.",
    "QuantLib.Fd2dBlackScholesVanillaEngine": "Proxy of C++ Fd2dBlackScholesVanillaEngine class.",
    "QuantLib.FdBatesVanillaEngine": "Proxy of C++ FdBatesVanillaEngine class.",
    "QuantLib.FdBlackScholesAsianEngine": "Proxy of C++ FdBlackScholesAsianEngine class.",
    "QuantLib.FdBlackScholesBarrierEngine": "Proxy of C++ FdBlackScholesBarrierEngine class.",
    "QuantLib.FdBlackScholesRebateEngine": "Proxy of C++ FdBlackScholesRebateEngine class.",
    "QuantLib.FdBlackScholesShoutEngine": "Proxy of C++ FdBlackScholesShoutEngine class.",
    "QuantLib.FdBlackScholesVanillaEngine": "Proxy of C++ FdBlackScholesVanillaEngine class.",
    "QuantLib.FdCEVVanillaEngine": "Proxy of C++ FdCEVVanillaEngine class.",
    "QuantLib.FdG2SwaptionEngine": "Proxy of C++ FdG2SwaptionEngine class.",
    "QuantLib.FdHestonBarrierEngine": "Proxy of C++ FdHestonBarrierEngine class.",
    "QuantLib.FdHestonDoubleBarrierEngine": "Proxy of C++ FdHestonDoubleBarrierEngine class.",
    "QuantLib.FdHestonHullWhiteVanillaEngine": "Proxy of C++ FdHestonHullWhiteVanillaEngine class.",
    "QuantLib.FdHestonRebateEngine": "Proxy of C++ FdHestonRebateEngine class.",
    "QuantLib.FdHestonVanillaEngine": "Proxy of C++ FdHestonVanillaEngine class.",
    "QuantLib.FdHullWhiteSwaptionEngine": "Proxy of C++ FdHullWhiteSwaptionEngine class.",
    "QuantLib.FdOrnsteinUhlenbeckVanillaEngine": "Proxy of C++ FdOrnsteinUhlenbeckVanillaEngine class.",
    "QuantLib.FdSabrVanillaEngine": "Proxy of C++ FdSabrVanillaEngine class.",
    "QuantLib.FdSimpleBSSwingEngine": "Proxy of C++ FdSimpleBSSwingEngine class.",
    "QuantLib.FdSimpleExtOUJumpSwingEngine": "Proxy of C++ FdSimpleExtOUJumpSwingEngine class.",
    "QuantLib.Fdm1DimSolver": "Proxy of C++ Fdm1DimSolver class.",
    "QuantLib.Fdm1dMesher": "Proxy of C++ Fdm1dMesher class.",
    "QuantLib.Fdm1dMesherVector": "Proxy of C++ std::vector< ext::shared_ptr< Fdm1dMesher > > class.",
    "QuantLib.Fdm2DimSolver": "Proxy of C++ Fdm2DimSolver class.",
    "QuantLib.Fdm2dBlackScholesOp": "Proxy of C++ Fdm2dBlackScholesOp class.",
    "QuantLib.Fdm2dBlackScholesSolver": "Proxy of C++ Fdm2dBlackScholesSolver class.",
    "QuantLib.Fdm3DimSolver": "Proxy of C++ Fdm3DimSolver class.",
    "QuantLib.Fdm4dimSolver": "Proxy of C++ FdmNdimSolver< 4 > class.",
    "QuantLib.Fdm5dimSolver": "Proxy of C++ FdmNdimSolver< 5 > class.",
    "QuantLib.Fdm6dimSolver": "Proxy of C++ FdmNdimSolver< 6 > class.",
    "QuantLib.FdmAffineG2ModelSwapInnerValue": "Proxy of C++ FdmAffineModelSwapInnerValue< G2 > class.",
    "QuantLib.FdmAffineHullWhiteModelSwapInnerValue": "Proxy of C++ FdmAffineModelSwapInnerValue< HullWhite > class.",
    "QuantLib.FdmAmericanStepCondition": "Proxy of C++ FdmAmericanStepCondition class.",
    "QuantLib.FdmArithmeticAverageCondition": "Proxy of C++ FdmArithmeticAverageCondition class.",
    "QuantLib.FdmBackwardSolver": "Proxy of C++ FdmBackwardSolver class.",
    "QuantLib.FdmBatesOp": "Proxy of C++ FdmBatesOp class.",
    "QuantLib.FdmBermudanStepCondition": "Proxy of C++ FdmBermudanStepCondition class.",
    "QuantLib.FdmBlackScholesFwdOp": "Proxy of C++ FdmBlackScholesFwdOp class.",
    "QuantLib.FdmBlackScholesMesher": "Proxy of C++ FdmBlackScholesMesher class.",
    "QuantLib.FdmBlackScholesOp": "Proxy of C++ FdmBlackScholesOp class.",
    "QuantLib.FdmBoundaryCondition": "Proxy of C++ FdmBoundaryCondition class.",
    "QuantLib.FdmBoundaryConditionSet": "Proxy of C++ std::vector< ext::shared_ptr< FdmBoundaryCondition > > class.",
    "QuantLib.FdmCEV1dMesher": "Proxy of C++ FdmCEV1dMesher class.",
    "QuantLib.FdmCEVOp": "Proxy of C++ FdmCEVOp class.",
    "QuantLib.FdmCellAveragingInnerValue": "Proxy of C++ FdmCellAveragingInnerValue class.",
    "QuantLib.FdmDirichletBoundary": "Proxy of C++ FdmDirichletBoundary class.",
    "QuantLib.FdmDiscountDirichletBoundary": "Proxy of C++ FdmDiscountDirichletBoundary class.",
    "QuantLib.FdmDividendHandler": "Proxy of C++ FdmDividendHandler class.",
    "QuantLib.FdmDupire1dOp": "Proxy of C++ FdmDupire1dOp class.",
    "QuantLib.FdmG2Op": "Proxy of C++ FdmG2Op class.",
    "QuantLib.FdmG2Solver": "Proxy of C++ FdmG2Solver class.",
    "QuantLib.FdmHestonFwdOp": "Proxy of C++ FdmHestonFwdOp class.",
    "QuantLib.FdmHestonGreensFct": "Proxy of C++ FdmHestonGreensFct class.",
    "QuantLib.FdmHestonHullWhiteOp": "Proxy of C++ FdmHestonHullWhiteOp class.",
    "QuantLib.FdmHestonHullWhiteSolver": "Proxy of C++ FdmHestonHullWhiteSolver class.",
    "QuantLib.FdmHestonLocalVolatilityVarianceMesher": "Proxy of C++ FdmHestonLocalVolatilityVarianceMesher class.",
    "QuantLib.FdmHestonOp": "Proxy of C++ FdmHestonOp class.",
    "QuantLib.FdmHestonSolver": "Proxy of C++ FdmHestonSolver class.",
    "QuantLib.FdmHestonVarianceMesher": "Proxy of C++ FdmHestonVarianceMesher class.",
    "QuantLib.FdmHullWhiteOp": "Proxy of C++ FdmHullWhiteOp class.",
    "QuantLib.FdmHullWhiteSolver": "Proxy of C++ FdmHullWhiteSolver class.",
    "QuantLib.FdmIndicesOnBoundary": "Proxy of C++ FdmIndicesOnBoundary class.",
    "QuantLib.FdmInnerValueCalculator": "Proxy of C++ FdmInnerValueCalculator class.",
    "QuantLib.FdmInnerValueCalculatorProxy": "Proxy of C++ FdmInnerValueCalculatorProxy class.",
    "QuantLib.FdmLinearOp": "Proxy of C++ FdmLinearOp class.",
    "QuantLib.FdmLinearOpComposite": "Proxy of C++ FdmLinearOpComposite class.",
    "QuantLib.FdmLinearOpCompositeProxy": "Proxy of C++ FdmLinearOpCompositeProxy class.",
    "QuantLib.FdmLinearOpIterator": "Proxy of C++ FdmLinearOpIterator class.",
    "QuantLib.FdmLinearOpLayout": "Proxy of C++ FdmLinearOpLayout class.",
    "QuantLib.FdmLocalVolFwdOp": "Proxy of C++ FdmLocalVolFwdOp class.",
    "QuantLib.FdmLogBasketInnerValue": "Proxy of C++ FdmLogBasketInnerValue class.",
    "QuantLib.FdmLogInnerValue": "Proxy of C++ FdmLogInnerValue class.",
    "QuantLib.FdmMesher": "Proxy of C++ FdmMesher class.",
    "QuantLib.FdmMesherComposite": "Proxy of C++ FdmMesherComposite class.",
    "QuantLib.FdmOrnsteinUhlenbeckOp": "Proxy of C++ FdmOrnsteinUhlenbeckOp class.",
    "QuantLib.FdmQuantoHelper": "Proxy of C++ FdmQuantoHelper class.",
    "QuantLib.FdmSabrOp": "Proxy of C++ FdmSabrOp class.",
    "QuantLib.FdmSchemeDesc": "Proxy of C++ FdmSchemeDesc class.",
    "QuantLib.FdmSimpleProcess1dMesher": "Proxy of C++ FdmSimpleProcess1dMesher class.",
    "QuantLib.FdmSimpleStorageCondition": "Proxy of C++ FdmSimpleStorageCondition class.",
    "QuantLib.FdmSimpleSwingCondition": "Proxy of C++ FdmSimpleSwingCondition class.",
    "QuantLib.FdmSnapshotCondition": "Proxy of C++ FdmSnapshotCondition class.",
    "QuantLib.FdmSolverDesc": "Proxy of C++ FdmSolverDesc class.",
    "QuantLib.FdmSquareRootFwdOp": "Proxy of C++ FdmSquareRootFwdOp class.",
    "QuantLib.FdmStepCondition": "Proxy of C++ StepCondition< Array > class.",
    "QuantLib.FdmStepConditionComposite": "Proxy of C++ FdmStepConditionComposite class.",
    "QuantLib.FdmStepConditionProxy": "Proxy of C++ FdmStepConditionProxy class.",
    "QuantLib.FdmStepConditionVector": "Proxy of C++ std::vector< ext::shared_ptr< StepCondition< Array > > > class.",
    "QuantLib.FdmTimeDepDirichletBoundary": "Proxy of C++ FdmTimeDepDirichletBoundary class.",
    "QuantLib.FdmZabrOp": "Proxy of C++ FdmZabrOp class.",
    "QuantLib.FdmZeroInnerValue": "Proxy of C++ FdmZeroInnerValue class.",
    "QuantLib.FedFunds": "Proxy of C++ FedFunds class.",
    "QuantLib.Finland": "Proxy of C++ QuantLib::Finland class.",
    "QuantLib.FirstDerivativeOp": "Proxy of C++ FirstDerivativeOp class.",
    "QuantLib.FittedBondDiscountCurve": "Proxy of C++ FittedBondDiscountCurve class.",
    "QuantLib.FittingMethod": "Proxy of C++ FittingMethod class.",
    "QuantLib.FixedDividend": "Proxy of C++ FixedDividend class.",
    "QuantLib.FixedLocalVolSurface": "Proxy of C++ FixedLocalVolSurface class.",
    "QuantLib.FixedRateBond": "Proxy of C++ FixedRateBond class.",
    "QuantLib.FixedRateBondForward": "Proxy of C++ FixedRateBondForward class.",
    "QuantLib.FixedRateBondHelper": "Proxy of C++ FixedRateBondHelper class.",
    "QuantLib.FixedRateCoupon": "Proxy of C++ FixedRateCoupon class.",
    "QuantLib.FixedRateLeg": "FixedRateLeg(Schedule schedule, DayCounter dayCount, DoubleVector nominals, DoubleVector couponRates={}, BusinessDayConvention paymentAdjustment=Following, DayCounter firstPeriodDayCount=DayCounter(), Period exCouponPeriod=Period(), Calendar exCouponCalendar=Calendar(), BusinessDayConvention exCouponConvention=Unadjusted, bool exCouponEndOfMonth=False, Calendar paymentCalendar=Calendar(), Integer paymentLag=0, Compounding compounding=Simple, Frequency compoundingFrequency=Annual, InterestRateVector interestRates={}) -> Leg",
    "QuantLib.FixedVsFloatingSwap": "Proxy of C++ FixedVsFloatingSwap class.",
    "QuantLib.FlatForward": "Proxy of C++ FlatForward class.",
    "QuantLib.FlatHazardRate": "Proxy of C++ FlatHazardRate class.",
    "QuantLib.FlatSmileSection": "Proxy of C++ FlatSmileSection class.",
    "QuantLib.FloatFloatSwap": "Proxy of C++ FloatFloatSwap class.",
    "QuantLib.FloatFloatSwaption": "Proxy of C++ FloatFloatSwaption class.",
    "QuantLib.FloatingRateBond": "Proxy of C++ FloatingRateBond class.",
    "QuantLib.FloatingRateCoupon": "Proxy of C++ FloatingRateCoupon class.",
    "QuantLib.FloatingRateCouponPricer": "Proxy of C++ FloatingRateCouponPricer class.",
    "QuantLib.FloatingTypePayoff": "Proxy of C++ FloatingTypePayoff class.",
    "QuantLib.Floor": "Proxy of C++ Floor class.",
    "QuantLib.FloorTruncation": "Proxy of C++ FloorTruncation class.",
    "QuantLib.Forward": "Proxy of C++ Forward class.",
    "QuantLib.ForwardCurve": "Proxy of C++ InterpolatedForwardCurve< BackwardFlat > class.",
    "QuantLib.ForwardEuropeanEngine": "Proxy of C++ ForwardEuropeanEngine class.",
    "QuantLib.ForwardFlat": "Proxy of C++ ForwardFlat class.",
    "QuantLib.ForwardFlatInterpolation": "Proxy of C++ SafeForwardFlatInterpolation class.",
    "QuantLib.ForwardRate": "Proxy of C++ ForwardRate class.",
    "QuantLib.ForwardRateAgreement": "Proxy of C++ ForwardRateAgreement class.",
    "QuantLib.ForwardSpreadedTermStructure": "Proxy of C++ ForwardSpreadedTermStructure class.",
    "QuantLib.ForwardVanillaOption": "Proxy of C++ ForwardVanillaOption class.",
    "QuantLib.FraRateHelper": "Proxy of C++ FraRateHelper class.",
    "QuantLib.FractionalDividend": "Proxy of C++ FractionalDividend class.",
    "QuantLib.France": "Proxy of C++ QuantLib::France class.",
    "QuantLib.FritschButlandCubic": "Proxy of C++ SafeFritschButlandCubic class.",
    "QuantLib.FritschButlandLogCubic": "Proxy of C++ SafeFritschButlandLogCubic class.",
    "QuantLib.Futures": "Proxy of C++ Futures class.",
    "QuantLib.FuturesRateHelper": "Proxy of C++ FuturesRateHelper class.",
    "QuantLib.FxSwapRateHelper": "Proxy of C++ FxSwapRateHelper class.",
    "QuantLib.G2": "Proxy of C++ G2 class.",
    "QuantLib.G2ForwardProcess": "Proxy of C++ G2ForwardProcess class.",
    "QuantLib.G2Process": "Proxy of C++ G2Process class.",
    "QuantLib.G2SwaptionEngine": "Proxy of C++ G2SwaptionEngine class.",
    "QuantLib.GBPCurrency": "Proxy of C++ QuantLib::GBPCurrency class.",
    "QuantLib.GBPLibor": "Proxy of C++ GBPLibor class.",
    "QuantLib.GBPLiborON": "Proxy of C++ GBPLiborON class.",
    "QuantLib.GBSMRNDCalculator": "Proxy of C++ GBSMRNDCalculator class.",
    "QuantLib.GELCurrency": "Proxy of C++ QuantLib::GELCurrency class.",
    "QuantLib.GFunctionFactory": "Proxy of C++ GFunctionFactory class.",
    "QuantLib.GHSCurrency": "Proxy of C++ QuantLib::GHSCurrency class.",
    "QuantLib.GJRGARCHModel": "Proxy of C++ GJRGARCHModel class.",
    "QuantLib.GJRGARCHProcess": "Proxy of C++ GJRGARCHProcess class.",
    "QuantLib.GMRES": "Proxy of C++ GMRES class.",
    "QuantLib.GRDCurrency": "Proxy of C++ QuantLib::GRDCurrency class.",
    "QuantLib.GammaFunction": "Proxy of C++ GammaFunction class.",
    "QuantLib.GapPayoff": "Proxy of C++ GapPayoff class.",
    "QuantLib.GarmanKlassSigma1": "Proxy of C++ GarmanKlassSigma1 class.",
    "QuantLib.GarmanKlassSigma3": "Proxy of C++ GarmanKlassSigma3 class.",
    "QuantLib.GarmanKlassSigma4": "Proxy of C++ GarmanKlassSigma4 class.",
    "QuantLib.GarmanKlassSigma5": "Proxy of C++ GarmanKlassSigma5 class.",
    "QuantLib.GarmanKlassSigma6": "Proxy of C++ GarmanKlassSigma6 class.",
    "QuantLib.GarmanKohlagenProcess": "Proxy of C++ GarmanKohlagenProcess class.",
    "QuantLib.GaussChebyshev2ndIntegration": "Proxy of C++ GaussChebyshev2ndIntegration class.",
    "QuantLib.GaussChebyshevIntegration": "Proxy of C++ GaussChebyshevIntegration class.",
    "QuantLib.GaussGegenbauerIntegration": "Proxy of C++ GaussGegenbauerIntegration class.",
    "QuantLib.GaussHermiteIntegration": "Proxy of C++ GaussHermiteIntegration class.",
    "QuantLib.GaussHyperbolicIntegration": "Proxy of C++ GaussHyperbolicIntegration class.",
    "QuantLib.GaussJacobiIntegration": "Proxy of C++ GaussJacobiIntegration class.",
    "QuantLib.GaussKronrodAdaptive": "Proxy of C++ GaussKronrodAdaptive class.",
    "QuantLib.GaussKronrodNonAdaptive": "Proxy of C++ GaussKronrodNonAdaptive class.",
    "QuantLib.GaussLaguerreIntegration": "Proxy of C++ GaussLaguerreIntegration class.",
    "QuantLib.GaussLegendreIntegration": "Proxy of C++ GaussLegendreIntegration class.",
    "QuantLib.GaussLobattoIntegral": "Proxy of C++ GaussLobattoIntegral class.",
    "QuantLib.Gaussian1dCapFloorEngine": "Proxy of C++ Gaussian1dCapFloorEngine class.",
    "QuantLib.Gaussian1dFloatFloatSwaptionEngine": "Proxy of C++ Gaussian1dFloatFloatSwaptionEngine class.",
    "QuantLib.Gaussian1dJamshidianSwaptionEngine": "Proxy of C++ Gaussian1dJamshidianSwaptionEngine class.",
    "QuantLib.Gaussian1dModel": "Proxy of C++ Gaussian1dModel class.",
    "QuantLib.Gaussian1dNonstandardSwaptionEngine": "Proxy of C++ Gaussian1dNonstandardSwaptionEngine class.",
    "QuantLib.Gaussian1dSwaptionEngine": "Proxy of C++ Gaussian1dSwaptionEngine class.",
    "QuantLib.GaussianLowDiscrepancySequenceGenerator": "Proxy of C++ GaussianLowDiscrepancySequenceGenerator class.",
    "QuantLib.GaussianMultiPathGenerator": "Proxy of C++ MultiPathGenerator< GaussianRandomSequenceGenerator > class.",
    "QuantLib.GaussianPathGenerator": "Proxy of C++ PathGenerator< GaussianRandomSequenceGenerator > class.",
    "QuantLib.GaussianQuadrature": "Proxy of C++ GaussianQuadrature class.",
    "QuantLib.GaussianRandomGenerator": "Proxy of C++ GaussianRandomGenerator class.",
    "QuantLib.GaussianRandomSequenceGenerator": "Proxy of C++ GaussianRandomSequenceGenerator class.",
    "QuantLib.GaussianSimulatedAnnealing": "Proxy of C++ GaussianSimulatedAnnealing class.",
    "QuantLib.GaussianSobolMultiPathGenerator": "Proxy of C++ MultiPathGenerator< GaussianLowDiscrepancySequenceGenerator > class.",
    "QuantLib.GaussianSobolPathGenerator": "Proxy of C++ PathGenerator< GaussianLowDiscrepancySequenceGenerator > class.",
    "QuantLib.GbpLiborSwapIsdaFix": "Proxy of C++ GbpLiborSwapIsdaFix class.",
    "QuantLib.GeneralizedBlackScholesProcess": "Proxy of C++ GeneralizedBlackScholesProcess class.",
    "QuantLib.GeometricBrownianMotionProcess": "Proxy of C++ GeometricBrownianMotionProcess class.",
    "QuantLib.Germany": "Proxy of C++ QuantLib::Germany class.",
    "QuantLib.GlobalBootstrap": "Proxy of C++ _GlobalBootstrap class.",
    "QuantLib.GlobalLinearSimpleZeroCurve": "Proxy of C++ GlobalLinearSimpleZeroCurve class.",
    "QuantLib.Glued1dMesher": "Proxy of C++ Glued1dMesher class.",
    "QuantLib.GridModelLocalVolSurface": "Proxy of C++ GridModelLocalVolSurface class.",
    "QuantLib.Gsr": "Proxy of C++ Gsr class.",
    "QuantLib.GsrProcess": "Proxy of C++ GsrProcess class.",
    "QuantLib.HKDCurrency": "Proxy of C++ QuantLib::HKDCurrency class.",
    "QuantLib.HRKCurrency": "Proxy of C++ QuantLib::HRKCurrency class.",
    "QuantLib.HUFCurrency": "Proxy of C++ QuantLib::HUFCurrency class.",
    "QuantLib.HaltonRsg": "Proxy of C++ HaltonRsg class.",
    "QuantLib.HazardRate": "Proxy of C++ HazardRate class.",
    "QuantLib.HazardRateCurve": "Proxy of C++ InterpolatedHazardRateCurve< BackwardFlat > class.",
    "QuantLib.HestonBlackVolSurface": "Proxy of C++ HestonBlackVolSurface class.",
    "QuantLib.HestonModel": "Proxy of C++ HestonModel class.",
    "QuantLib.HestonModelHandle": "Proxy of C++ Handle< HestonModel > class.",
    "QuantLib.HestonModelHelper": "Proxy of C++ HestonModelHelper class.",
    "QuantLib.HestonProcess": "Proxy of C++ HestonProcess class.",
    "QuantLib.HestonRNDCalculator": "Proxy of C++ HestonRNDCalculator class.",
    "QuantLib.HestonSLVFDMModel": "Proxy of C++ HestonSLVFDMModel class.",
    "QuantLib.HestonSLVFokkerPlanckFdmParams": "Proxy of C++ HestonSLVFokkerPlanckFdmParams class.",
    "QuantLib.HestonSLVMCModel": "Proxy of C++ HestonSLVMCModel class.",
    "QuantLib.HestonSLVProcess": "Proxy of C++ HestonSLVProcess class.",
    "QuantLib.HimalayaOption": "Proxy of C++ HimalayaOption class.",
    "QuantLib.HongKong": "Proxy of C++ QuantLib::HongKong class.",
    "QuantLib.HullWhite": "Proxy of C++ HullWhite class.",
    "QuantLib.HullWhiteForwardProcess": "Proxy of C++ HullWhiteForwardProcess class.",
    "QuantLib.HullWhiteProcess": "Proxy of C++ HullWhiteProcess class.",
    "QuantLib.HundsdorferScheme": "Proxy of C++ HundsdorferScheme class.",
    "QuantLib.Hungary": "Proxy of C++ QuantLib::Hungary class.",
    "QuantLib.IDRCurrency": "Proxy of C++ QuantLib::IDRCurrency class.",
    "QuantLib.IEPCurrency": "Proxy of C++ QuantLib::IEPCurrency class.",
    "QuantLib.ILSCurrency": "Proxy of C++ QuantLib::ILSCurrency class.",
    "QuantLib.IMM": "Proxy of C++ IMM class.",
    "QuantLib.INRCurrency": "Proxy of C++ QuantLib::INRCurrency class.",
    "QuantLib.IQDCurrency": "Proxy of C++ QuantLib::IQDCurrency class.",
    "QuantLib.IRRCurrency": "Proxy of C++ QuantLib::IRRCurrency class.",
    "QuantLib.ISKCurrency": "Proxy of C++ QuantLib::ISKCurrency class.",
    "QuantLib.ITLCurrency": "Proxy of C++ QuantLib::ITLCurrency class.",
    "QuantLib.IborCoupon": "Proxy of C++ IborCoupon class.",
    "QuantLib.IborCouponPricer": "Proxy of C++ IborCouponPricer class.",
    "QuantLib.IborIborBasisSwapRateHelper": "Proxy of C++ IborIborBasisSwapRateHelper class.",
    "QuantLib.IborIndex": "Proxy of C++ IborIndex class.",
    "QuantLib.IborLeg": "IborLeg(DoubleVector nominals, Schedule schedule, ext::shared_ptr< IborIndex > const & index, DayCounter paymentDayCounter=DayCounter(), BusinessDayConvention const paymentConvention=Following, UnsignedIntVector fixingDays=std::vector< Natural >(), DoubleVector gearings=std::vector< Real >(), DoubleVector spreads=std::vector< Spread >(), DoubleVector caps=std::vector< Rate >(), DoubleVector floors=std::vector< Rate >(), bool isInArrears=False, Period exCouponPeriod=Period(), Calendar exCouponCalendar=Calendar(), BusinessDayConvention exCouponConvention=Unadjusted, bool exCouponEndOfMonth=False, Calendar paymentCalendar=Calendar(), Integer paymentLag=0, ext::optional< bool > withIndexedCoupons=ext::nullopt) -> Leg",
    "QuantLib.Iceland": "Proxy of C++ QuantLib::Iceland class.",
    "QuantLib.ImplicitEulerScheme": "Proxy of C++ ImplicitEulerScheme class.",
    "QuantLib.ImpliedTermStructure": "Proxy of C++ ImpliedTermStructure class.",
    "QuantLib.IncrementalStatistics": "Proxy of C++ IncrementalStatistics class.",
    "QuantLib.Index": "Proxy of C++ Index class.",
    "QuantLib.IndexManager": "Proxy of C++ IndexManager class.",
    "QuantLib.IndexedCashFlow": "Proxy of C++ IndexedCashFlow class.",
    "QuantLib.India": "Proxy of C++ QuantLib::India class.",
    "QuantLib.Indonesia": "Proxy of C++ QuantLib::Indonesia class.",
    "QuantLib.InflationCoupon": "Proxy of C++ InflationCoupon class.",
    "QuantLib.InflationIndex": "Proxy of C++ InflationIndex class.",
    "QuantLib.InflationTermStructure": "Proxy of C++ InflationTermStructure class.",
    "QuantLib.Instrument": "Proxy of C++ Instrument class.",
    "QuantLib.InstrumentVector": "Proxy of C++ std::vector< ext::shared_ptr< Instrument > > class.",
    "QuantLib.IntVector": "Proxy of C++ std::vector< int > class.",
    "QuantLib.IntegralCdsEngine": "Proxy of C++ IntegralCdsEngine class.",
    "QuantLib.IntegralEngine": "Proxy of C++ IntegralEngine class.",
    "QuantLib.InterestRate": "Proxy of C++ InterestRate class.",
    "QuantLib.InterestRateIndex": "Proxy of C++ InterestRateIndex class.",
    "QuantLib.InterestRateVector": "Proxy of C++ std::vector< InterestRate > class.",
    "QuantLib.InterpolatedSwaptionVolatilityCube": "Proxy of C++ InterpolatedSwaptionVolatilityCube class.",
    "QuantLib.InterpolatedYoYInflationOptionletStripper": "Proxy of C++ InterpolatedYoYOptionletStripper< Linear > class.",
    "QuantLib.InterpolatedYoYInflationOptionletVolatilityCurve": "Proxy of C++ InterpolatedYoYOptionletVolatilityCurve< Linear > class.",
    "QuantLib.IntervalPrice": "Proxy of C++ IntervalPrice class.",
    "QuantLib.IntervalPriceTimeSeries": "Proxy of C++ TimeSeries< IntervalPrice > class.",
    "QuantLib.IntervalPriceVector": "Proxy of C++ std::vector< IntervalPrice > class.",
    "QuantLib.InvCumulativeBurley2020SobolGaussianRsg": "Proxy of C++ InverseCumulativeRsg< Burley2020SobolRsg,InverseCumulativeNormal > class.",
    "QuantLib.InvCumulativeHaltonGaussianRsg": "Proxy of C++ InverseCumulativeRsg< HaltonRsg,InverseCumulativeNormal > class.",
    "QuantLib.InvCumulativeKnuthGaussianRng": "Proxy of C++ InverseCumulativeRng< KnuthUniformRng,InverseCumulativeNormal > class.",
    "QuantLib.InvCumulativeKnuthGaussianRsg": "Proxy of C++ InverseCumulativeRsg< RandomSequenceGenerator< KnuthUniformRng >,InverseCumulativeNormal > class.",
    "QuantLib.InvCumulativeLecuyerGaussianRng": "Proxy of C++ InverseCumulativeRng< LecuyerUniformRng,InverseCumulativeNormal > class.",
    "QuantLib.InvCumulativeLecuyerGaussianRsg": "Proxy of C++ InverseCumulativeRsg< RandomSequenceGenerator< LecuyerUniformRng >,InverseCumulativeNormal > class.",
    "QuantLib.InvCumulativeMersenneTwisterGaussianRng": "Proxy of C++ InverseCumulativeRng< MersenneTwisterUniformRng,InverseCumulativeNormal > class.",
    "QuantLib.InvCumulativeMersenneTwisterGaussianRsg": "Proxy of C++ InverseCumulativeRsg< RandomSequenceGenerator< MersenneTwisterUniformRng >,InverseCumulativeNormal > class.",
    "QuantLib.InvCumulativeMersenneTwisterPathGenerator": "Proxy of C++ PathGenerator< InverseCumulativeRsg< RandomSequenceGenerator< MersenneTwisterUniformRng >,InverseCumulativeNormal > > class.",
    "QuantLib.InvCumulativeSobolGaussianRsg": "Proxy of C++ InverseCumulativeRsg< SobolRsg,InverseCumulativeNormal > class.",
    "QuantLib.InvCumulativeXoshiro256StarStarGaussianRng": "Proxy of C++ InverseCumulativeRng< Xoshiro256StarStarUniformRng,InverseCumulativeNormal > class.",
    "QuantLib.InvCumulativeXoshiro256StarStarGaussianRsg": "Proxy of C++ InverseCumulativeRsg< RandomSequenceGenerator< Xoshiro256StarStarUniformRng >,InverseCumulativeNormal > class.",
    "QuantLib.InverseCumulativeNormal": "Proxy of C++ InverseCumulativeNormal class.",
    "QuantLib.InverseCumulativePoisson": "Proxy of C++ InverseCumulativePoisson class.",
    "QuantLib.InverseCumulativeStudent": "Proxy of C++ InverseCumulativeStudent class.",
    "QuantLib.InverseNonCentralCumulativeChiSquareDistribution": "Proxy of C++ InverseNonCentralCumulativeChiSquareDistribution class.",
    "QuantLib.IsdaCdsEngine": "Proxy of C++ IsdaCdsEngine class.",
    "QuantLib.Israel": "Proxy of C++ QuantLib::Israel class.",
    "QuantLib.Italy": "Proxy of C++ QuantLib::Italy class.",
    "QuantLib.IterativeBootstrap": "Proxy of C++ _IterativeBootstrap class.",
    "QuantLib.JODCurrency": "Proxy of C++ QuantLib::JODCurrency class.",
    "QuantLib.JPYCurrency": "Proxy of C++ QuantLib::JPYCurrency class.",
    "QuantLib.JPYLibor": "Proxy of C++ JPYLibor class.",
    "QuantLib.JamshidianSwaptionEngine": "Proxy of C++ JamshidianSwaptionEngine class.",
    "QuantLib.Japan": "Proxy of C++ QuantLib::Japan class.",
    "QuantLib.Jibar": "Proxy of C++ Jibar class.",
    "QuantLib.JointCalendar": "Proxy of C++ QuantLib::JointCalendar class.",
    "QuantLib.JpyLiborSwapIsdaFixAm": "Proxy of C++ JpyLiborSwapIsdaFixAm class.",
    "QuantLib.JpyLiborSwapIsdaFixPm": "Proxy of C++ JpyLiborSwapIsdaFixPm class.",
    "QuantLib.JuQuadraticApproximationEngine": "Proxy of C++ JuQuadraticApproximationEngine class.",
    "QuantLib.KESCurrency": "Proxy of C++ QuantLib::KESCurrency class.",
    "QuantLib.KInterpolatedYoYInflationOptionletVolatilitySurface": "Proxy of C++ KInterpolatedYoYOptionletVolatilitySurface< Linear > class.",
    "QuantLib.KRWCurrency": "Proxy of C++ QuantLib::KRWCurrency class.",
    "QuantLib.KWDCurrency": "Proxy of C++ QuantLib::KWDCurrency class.",
    "QuantLib.KZTCurrency": "Proxy of C++ QuantLib::KZTCurrency class.",
    "QuantLib.KahaleSmileSection": "Proxy of C++ KahaleSmileSection class.",
    "QuantLib.KerkhofSeasonality": "Proxy of C++ KerkhofSeasonality class.",
    "QuantLib.KirkEngine": "Proxy of C++ KirkEngine class.",
    "QuantLib.KirkSpreadOptionEngine": "Proxy of C++ KirkSpreadOptionEngine class.",
    "QuantLib.KlugeExtOUProcess": "Proxy of C++ KlugeExtOUProcess class.",
    "QuantLib.KnuthUniformRng": "Proxy of C++ KnuthUniformRng class.",
    "QuantLib.KnuthUniformRsg": "Proxy of C++ RandomSequenceGenerator< KnuthUniformRng > class.",
    "QuantLib.Kruger": "Proxy of C++ Kruger class.",
    "QuantLib.KrugerCubic": "Proxy of C++ SafeKrugerCubic class.",
    "QuantLib.KrugerLog": "Proxy of C++ KrugerLog class.",
    "QuantLib.KrugerLogCubic": "Proxy of C++ SafeKrugerLogCubic class.",
    "QuantLib.KrugerLogDiscountCurve": "Proxy of C++ InterpolatedDiscountCurve< KrugerLog > class.",
    "QuantLib.KrugerZeroCurve": "Proxy of C++ InterpolatedZeroCurve< Kruger > class.",
    "QuantLib.LKRCurrency": "Proxy of C++ QuantLib::LKRCurrency class.",
    "QuantLib.LMMCurveState": "Proxy of C++ LMMCurveState class.",
    "QuantLib.LMMDriftCalculator": "Proxy of C++ LMMDriftCalculator class.",
    "QuantLib.LTCCurrency": "Proxy of C++ QuantLib::LTCCurrency class.",
    "QuantLib.LTLCurrency": "Proxy of C++ QuantLib::LTLCurrency class.",
    "QuantLib.LUFCurrency": "Proxy of C++ QuantLib::LUFCurrency class.",
    "QuantLib.LVLCurrency": "Proxy of C++ QuantLib::LVLCurrency class.",
    "QuantLib.LagrangeInterpolation": "Proxy of C++ SafeLagrangeInterpolation class.",
    "QuantLib.LastFixingQuote": "Proxy of C++ LastFixingQuote class.",
    "QuantLib.LazyObject": "Proxy of C++ LazyObject class.",
    "QuantLib.LecuyerUniformRng": "Proxy of C++ LecuyerUniformRng class.",
    "QuantLib.LecuyerUniformRsg": "Proxy of C++ RandomSequenceGenerator< LecuyerUniformRng > class.",
    "QuantLib.Leg": "Proxy of C++ std::vector< ext::shared_ptr< CashFlow > > class.",
    "QuantLib.LegVector": "Proxy of C++ std::vector< Leg > class.",
    "QuantLib.LevenbergMarquardt": "Proxy of C++ LevenbergMarquardt class.",
    "QuantLib.Libor": "Proxy of C++ Libor class.",
    "QuantLib.Linear": "Proxy of C++ Linear class.",
    "QuantLib.LinearInterpolatedSmileSection": "Proxy of C++ InterpolatedSmileSection< Linear > class.",
    "QuantLib.LinearInterpolation": "Proxy of C++ SafeLinearInterpolation class.",
    "QuantLib.LinearTsrPricer": "Proxy of C++ LinearTsrPricer class.",
    "QuantLib.LinearTsrPricerSettings": "Proxy of C++ LinearTsrPricer::Settings class.",
    "QuantLib.LocalConstantVol": "Proxy of C++ LocalConstantVol class.",
    "QuantLib.LocalVolRNDCalculator": "Proxy of C++ LocalVolRNDCalculator class.",
    "QuantLib.LocalVolSurface": "Proxy of C++ LocalVolSurface class.",
    "QuantLib.LocalVolTermStructure": "Proxy of C++ LocalVolTermStructure class.",
    "QuantLib.LocalVolTermStructureHandle": "Proxy of C++ Handle< LocalVolTermStructure > class.",
    "QuantLib.LogCubicNaturalSpline": "Proxy of C++ SafeLogCubicNaturalSpline class.",
    "QuantLib.LogCubicZeroCurve": "Proxy of C++ InterpolatedZeroCurve< DefaultLogCubic > class.",
    "QuantLib.LogLinear": "Proxy of C++ LogLinear class.",
    "QuantLib.LogLinearInterpolation": "Proxy of C++ SafeLogLinearInterpolation class.",
    "QuantLib.LogLinearZeroCurve": "Proxy of C++ InterpolatedZeroCurve< LogLinear > class.",
    "QuantLib.LogMixedLinearCubic": "Proxy of C++ LogMixedLinearCubic class.",
    "QuantLib.LogMixedLinearCubicDiscountCurve": "Proxy of C++ InterpolatedDiscountCurve< LogMixedLinearCubic > class.",
    "QuantLib.LogNormalFwdRateIpc": "Proxy of C++ LogNormalFwdRateIpc class.",
    "QuantLib.LogNormalSimulatedAnnealing": "Proxy of C++ LogNormalSimulatedAnnealing class.",
    "QuantLib.LogParabolic": "Proxy of C++ SafeLogParabolic class.",
    "QuantLib.LogParabolicCubic": "Proxy of C++ LogParabolicCubic class.",
    "QuantLib.LogParabolicCubicDiscountCurve": "Proxy of C++ InterpolatedDiscountCurve< LogParabolicCubic > class.",
    "QuantLib.LognormalCmsSpreadPricer": "Proxy of C++ LognormalCmsSpreadPricer class.",
    "QuantLib.LsmBasisSystem": "Proxy of C++ LsmBasisSystem class.",
    "QuantLib.MADCurrency": "Proxy of C++ QuantLib::MADCurrency class.",
    "QuantLib.MCLDAmericanBasketEngine": "Proxy of C++ MCAmericanBasketEngine< LowDiscrepancy > class.",
    "QuantLib.MCLDAmericanEngine": "Proxy of C++ MCAmericanEngine< LowDiscrepancy > class.",
    "QuantLib.MCLDBarrierEngine": "Proxy of C++ MCBarrierEngine< LowDiscrepancy > class.",
    "QuantLib.MCLDDigitalEngine": "Proxy of C++ MCDigitalEngine< LowDiscrepancy > class.",
    "QuantLib.MCLDDiscreteArithmeticAPEngine": "Proxy of C++ MCDiscreteArithmeticAPEngine< LowDiscrepancy > class.",
    "QuantLib.MCLDDiscreteArithmeticAPHestonEngine": "Proxy of C++ MCDiscreteArithmeticAPHestonEngine< LowDiscrepancy > class.",
    "QuantLib.MCLDDiscreteArithmeticASEngine": "Proxy of C++ MCDiscreteArithmeticASEngine< LowDiscrepancy > class.",
    "QuantLib.MCLDDiscreteGeometricAPEngine": "Proxy of C++ MCDiscreteGeometricAPEngine< LowDiscrepancy > class.",
    "QuantLib.MCLDDiscreteGeometricAPHestonEngine": "Proxy of C++ MCDiscreteGeometricAPHestonEngine< LowDiscrepancy > class.",
    "QuantLib.MCLDEuropeanBasketEngine": "Proxy of C++ MCEuropeanBasketEngine< LowDiscrepancy > class.",
    "QuantLib.MCLDEuropeanEngine": "Proxy of C++ MCEuropeanEngine< LowDiscrepancy > class.",
    "QuantLib.MCLDEuropeanGJRGARCHEngine": "Proxy of C++ MCEuropeanGJRGARCHEngine< LowDiscrepancy > class.",
    "QuantLib.MCLDEuropeanHestonEngine": "Proxy of C++ MCEuropeanHestonEngine< LowDiscrepancy > class.",
    "QuantLib.MCLDEverestEngine": "Proxy of C++ MCEverestEngine< LowDiscrepancy > class.",
    "QuantLib.MCLDForwardEuropeanBSEngine": "Proxy of C++ MCForwardEuropeanBSEngine< LowDiscrepancy > class.",
    "QuantLib.MCLDForwardEuropeanHestonEngine": "Proxy of C++ MCForwardEuropeanHestonEngine< LowDiscrepancy > class.",
    "QuantLib.MCLDHimalayaEngine": "Proxy of C++ MCHimalayaEngine< LowDiscrepancy > class.",
    "QuantLib.MCLDPerformanceEngine": "Proxy of C++ MCPerformanceEngine< LowDiscrepancy > class.",
    "QuantLib.MCPRAmericanBasketEngine": "Proxy of C++ MCAmericanBasketEngine< PseudoRandom > class.",
    "QuantLib.MCPRAmericanEngine": "Proxy of C++ MCAmericanEngine< PseudoRandom > class.",
    "QuantLib.MCPRBarrierEngine": "Proxy of C++ MCBarrierEngine< PseudoRandom > class.",
    "QuantLib.MCPRDigitalEngine": "Proxy of C++ MCDigitalEngine< PseudoRandom > class.",
    "QuantLib.MCPRDiscreteArithmeticAPEngine": "Proxy of C++ MCDiscreteArithmeticAPEngine< PseudoRandom > class.",
    "QuantLib.MCPRDiscreteArithmeticAPHestonEngine": "Proxy of C++ MCDiscreteArithmeticAPHestonEngine< PseudoRandom > class.",
    "QuantLib.MCPRDiscreteArithmeticASEngine": "Proxy of C++ MCDiscreteArithmeticASEngine< PseudoRandom > class.",
    "QuantLib.MCPRDiscreteGeometricAPEngine": "Proxy of C++ MCDiscreteGeometricAPEngine< PseudoRandom > class.",
    "QuantLib.MCPRDiscreteGeometricAPHestonEngine": "Proxy of C++ MCDiscreteGeometricAPHestonEngine< PseudoRandom > class.",
    "QuantLib.MCPREuropeanBasketEngine": "Proxy of C++ MCEuropeanBasketEngine< PseudoRandom > class.",
    "QuantLib.MCPREuropeanEngine": "Proxy of C++ MCEuropeanEngine< PseudoRandom > class.",
    "QuantLib.MCPREuropeanGJRGARCHEngine": "Proxy of C++ MCEuropeanGJRGARCHEngine< PseudoRandom > class.",
    "QuantLib.MCPREuropeanHestonEngine": "Proxy of C++ MCEuropeanHestonEngine< PseudoRandom > class.",
    "QuantLib.MCPREverestEngine": "Proxy of C++ MCEverestEngine< PseudoRandom > class.",
    "QuantLib.MCPRForwardEuropeanBSEngine": "Proxy of C++ MCForwardEuropeanBSEngine< PseudoRandom > class.",
    "QuantLib.MCPRForwardEuropeanHestonEngine": "Proxy of C++ MCForwardEuropeanHestonEngine< PseudoRandom > class.",
    "QuantLib.MCPRHimalayaEngine": "Proxy of C++ MCHimalayaEngine< PseudoRandom > class.",
    "QuantLib.MCPRPerformanceEngine": "Proxy of C++ MCPerformanceEngine< PseudoRandom > class.",
    "QuantLib.MTBrownianGenerator": "Proxy of C++ MTBrownianGenerator class.",
    "QuantLib.MTBrownianGeneratorFactory": "Proxy of C++ MTBrownianGeneratorFactory class.",
    "QuantLib.MTLCurrency": "Proxy of C++ QuantLib::MTLCurrency class.",
    "QuantLib.MURCurrency": "Proxy of C++ QuantLib::MURCurrency class.",
    "QuantLib.MXNCurrency": "Proxy of C++ QuantLib::MXNCurrency class.",
    "QuantLib.MXVCurrency": "Proxy of C++ QuantLib::MXVCurrency class.",
    "QuantLib.MYRCurrency": "Proxy of C++ QuantLib::MYRCurrency class.",
    "QuantLib.MargrabeOption": "Proxy of C++ MargrabeOption class.",
    "QuantLib.MarketModel": "Proxy of C++ MarketModel class.",
    "QuantLib.MarketModelEvolver": "Proxy of C++ MarketModelEvolver class.",
    "QuantLib.MarketModelFactory": "Proxy of C++ MarketModelFactory class.",
    "QuantLib.MarkovFunctional": "Proxy of C++ MarkovFunctional class.",
    "QuantLib.MarkovFunctionalSettings": "Proxy of C++ MarkovFunctional::ModelSettings class.",
    "QuantLib.Matrix": "Proxy of C++ Matrix class.",
    "QuantLib.MatrixMultiplicationProxy": "Proxy of C++ MatrixMultiplicationProxy class.",
    "QuantLib.MatrixRow": "Proxy of C++ MatrixRow class.",
    "QuantLib.MaxBasketPayoff": "Proxy of C++ MaxBasketPayoff class.",
    "QuantLib.MersenneTwisterUniformRng": "Proxy of C++ MersenneTwisterUniformRng class.",
    "QuantLib.MersenneTwisterUniformRsg": "Proxy of C++ RandomSequenceGenerator< MersenneTwisterUniformRng > class.",
    "QuantLib.Merton76Process": "Proxy of C++ Merton76Process class.",
    "QuantLib.MethodOfLinesScheme": "Proxy of C++ MethodOfLinesScheme class.",
    "QuantLib.Mexico": "Proxy of C++ QuantLib::Mexico class.",
    "QuantLib.MidPointCdsEngine": "Proxy of C++ MidPointCdsEngine class.",
    "QuantLib.MinBasketPayoff": "Proxy of C++ MinBasketPayoff class.",
    "QuantLib.MirrorGaussianSimulatedAnnealing": "Proxy of C++ MirrorGaussianSimulatedAnnealing class.",
    "QuantLib.MixedInterpolation": "Proxy of C++ MixedInterpolation class.",
    "QuantLib.ModifiedCraigSneydScheme": "Proxy of C++ ModifiedCraigSneydScheme class.",
    "QuantLib.Money": "Proxy of C++ Money class.",
    "QuantLib.MonotonicCubic": "Proxy of C++ MonotonicCubic class.",
    "QuantLib.MonotonicCubicInterpolatedSmileSection": "Proxy of C++ InterpolatedSmileSection< MonotonicCubic > class.",
    "QuantLib.MonotonicCubicNaturalSpline": "Proxy of C++ SafeMonotonicCubicNaturalSpline class.",
    "QuantLib.MonotonicCubicZeroCurve": "Proxy of C++ InterpolatedZeroCurve< MonotonicCubic > class.",
    "QuantLib.MonotonicLogCubic": "Proxy of C++ MonotonicLogCubic class.",
    "QuantLib.MonotonicLogCubicDiscountCurve": "Proxy of C++ InterpolatedDiscountCurve< MonotonicLogCubic > class.",
    "QuantLib.MonotonicLogCubicNaturalSpline": "Proxy of C++ SafeMonotonicLogCubicNaturalSpline class.",
    "QuantLib.MonotonicLogParabolic": "Proxy of C++ SafeMonotonicLogParabolic class.",
    "QuantLib.MonotonicLogParabolicCubic": "Proxy of C++ MonotonicLogParabolicCubic class.",
    "QuantLib.MonotonicLogParabolicCubicDiscountCurve": "Proxy of C++ InterpolatedDiscountCurve< MonotonicLogParabolicCubic > class.",
    "QuantLib.MonotonicParabolic": "Proxy of C++ SafeMonotonicParabolic class.",
    "QuantLib.MonotonicParabolicCubic": "Proxy of C++ MonotonicParabolicCubic class.",
    "QuantLib.MonotonicParabolicCubicZeroCurve": "Proxy of C++ InterpolatedZeroCurve< MonotonicParabolicCubic > class.",
    "QuantLib.MoroInvCumulativeBurley2020SobolGaussianRsg": "Proxy of C++ InverseCumulativeRsg< Burley2020SobolRsg,MoroInverseCumulativeNormal > class.",
    "QuantLib.MoroInvCumulativeHaltonGaussianRsg": "Proxy of C++ InverseCumulativeRsg< HaltonRsg,MoroInverseCumulativeNormal > class.",
    "QuantLib.MoroInvCumulativeKnuthGaussianRng": "Proxy of C++ InverseCumulativeRng< KnuthUniformRng,MoroInverseCumulativeNormal > class.",
    "QuantLib.MoroInvCumulativeKnuthGaussianRsg": "Proxy of C++ InverseCumulativeRsg< RandomSequenceGenerator< KnuthUniformRng >,MoroInverseCumulativeNormal > class.",
    "QuantLib.MoroInvCumulativeLecuyerGaussianRng": "Proxy of C++ InverseCumulativeRng< LecuyerUniformRng,MoroInverseCumulativeNormal > class.",
    "QuantLib.MoroInvCumulativeLecuyerGaussianRsg": "Proxy of C++ InverseCumulativeRsg< RandomSequenceGenerator< LecuyerUniformRng >,MoroInverseCumulativeNormal > class.",
    "QuantLib.MoroInvCumulativeMersenneTwisterGaussianRng": "Proxy of C++ InverseCumulativeRng< MersenneTwisterUniformRng,MoroInverseCumulativeNormal > class.",
    "QuantLib.MoroInvCumulativeMersenneTwisterGaussianRsg": "Proxy of C++ InverseCumulativeRsg< RandomSequenceGenerator< MersenneTwisterUniformRng >,MoroInverseCumulativeNormal > class.",
    "QuantLib.MoroInvCumulativeSobolGaussianRsg": "Proxy of C++ InverseCumulativeRsg< SobolRsg,MoroInverseCumulativeNormal > class.",
    "QuantLib.MoroInvCumulativeXoshiro256StarStarGaussianRng": "Proxy of C++ InverseCumulativeRng< Xoshiro256StarStarUniformRng,MoroInverseCumulativeNormal > class.",
    "QuantLib.MoroInvCumulativeXoshiro256StarStarGaussianRsg": "Proxy of C++ InverseCumulativeRsg< RandomSequenceGenerator< Xoshiro256StarStarUniformRng >,MoroInverseCumulativeNormal > class.",
    "QuantLib.MoroInverseCumulativeNormal": "Proxy of C++ MoroInverseCumulativeNormal class.",
    "QuantLib.Mosprime": "Proxy of C++ Mosprime class.",
    "QuantLib.MtMCrossCurrencyBasisSwapRateHelper": "Proxy of C++ MtMCrossCurrencyBasisSwapRateHelper class.",
    "QuantLib.MultiAssetOption": "Proxy of C++ MultiAssetOption class.",
    "QuantLib.MultiPath": "Proxy of C++ MultiPath class.",
    "QuantLib.MultipleIncrementalStatistics": "Proxy of C++ GenericSequenceStatistics< IncrementalStatistics > class.",
    "QuantLib.MultipleStatistics": "Proxy of C++ GenericSequenceStatistics< Statistics > class.",
    "QuantLib.MultiplicativePriceSeasonality": "Proxy of C++ MultiplicativePriceSeasonality class.",
    "QuantLib.NGNCurrency": "Proxy of C++ QuantLib::NGNCurrency class.",
    "QuantLib.NLGCurrency": "Proxy of C++ QuantLib::NLGCurrency class.",
    "QuantLib.NOKCurrency": "Proxy of C++ QuantLib::NOKCurrency class.",
    "QuantLib.NPRCurrency": "Proxy of C++ QuantLib::NPRCurrency class.",
    "QuantLib.NZDCurrency": "Proxy of C++ QuantLib::NZDCurrency class.",
    "QuantLib.NZDLibor": "Proxy of C++ NZDLibor class.",
    "QuantLib.NaturalCubicDiscountCurve": "Proxy of C++ InterpolatedDiscountCurve< SplineCubic > class.",
    "QuantLib.NaturalCubicZeroCurve": "Proxy of C++ InterpolatedZeroCurve< SplineCubic > class.",
    "QuantLib.NaturalLogCubicDiscountCurve": "Proxy of C++ InterpolatedDiscountCurve< SplineLogCubic > class.",
    "QuantLib.NelsonSiegelFitting": "Proxy of C++ NelsonSiegelFitting class.",
    "QuantLib.NeumannBC": "Proxy of C++ NeumannBC class.",
    "QuantLib.NewZealand": "Proxy of C++ QuantLib::NewZealand class.",
    "QuantLib.Newton": "Proxy of C++ Newton class.",
    "QuantLib.NewtonSafe": "Proxy of C++ NewtonSafe class.",
    "QuantLib.NinePointLinearOp": "Proxy of C++ NinePointLinearOp class.",
    "QuantLib.NoArbSabrInterpolatedSmileSection": "Proxy of C++ NoArbSabrInterpolatedSmileSection class.",
    "QuantLib.NoArbSabrSmileSection": "Proxy of C++ NoArbSabrSmileSection class.",
    "QuantLib.NoConstraint": "Proxy of C++ NoConstraint class.",
    "QuantLib.NoExceptLocalVolSurface": "Proxy of C++ NoExceptLocalVolSurface class.",
    "QuantLib.NodePair": "Proxy of C++ std::pair< Date,double > class.",
    "QuantLib.NodeVector": "Proxy of C++ std::vector< std::pair< Date,double > > class.",
    "QuantLib.NonCentralCumulativeChiSquareDistribution": "Proxy of C++ NonCentralCumulativeChiSquareDistribution class.",
    "QuantLib.NonhomogeneousBoundaryConstraint": "Proxy of C++ NonhomogeneousBoundaryConstraint class.",
    "QuantLib.NonstandardSwap": "Proxy of C++ NonstandardSwap class.",
    "QuantLib.NonstandardSwaption": "Proxy of C++ NonstandardSwaption class.",
    "QuantLib.NormalDistribution": "Proxy of C++ NormalDistribution class.",
    "QuantLib.Norway": "Proxy of C++ QuantLib::Norway class.",
    "QuantLib.NthOrderDerivativeOp": "Proxy of C++ NthOrderDerivativeOp class.",
    "QuantLib.NullCalendar": "Proxy of C++ QuantLib::NullCalendar class.",
    "QuantLib.NullParameter": "Proxy of C++ NullParameter class.",
    "QuantLib.NumericHaganPricer": "Proxy of C++ NumericHaganPricer class.",
    "QuantLib.Nzocr": "Proxy of C++ Nzocr class.",
    "QuantLib.OISRateHelper": "Proxy of C++ OISRateHelper class.",
    "QuantLib.OMRCurrency": "Proxy of C++ QuantLib::OMRCurrency class.",
    "QuantLib.Observable": "Proxy of C++ Observable class.",
    "QuantLib.Observer": "Proxy of C++ PyObserver class.",
    "QuantLib.OneAssetOption": "Proxy of C++ OneAssetOption class.",
    "QuantLib.OneDayCounter": "Proxy of C++ QuantLib::OneDayCounter class.",
    "QuantLib.OneFactorAffineModel": "Proxy of C++ OneFactorAffineModel class.",
    "QuantLib.OptimizationMethod": "Proxy of C++ OptimizationMethod class.",
    "QuantLib.Optimizer": "Proxy of C++ Optimizer class.",
    "QuantLib.Option": "Proxy of C++ Option class.",
    "QuantLib.OptionletStripper1": "Proxy of C++ OptionletStripper1 class.",
    "QuantLib.OptionletVolatilityStructure": "Proxy of C++ OptionletVolatilityStructure class.",
    "QuantLib.OptionletVolatilityStructureHandle": "Proxy of C++ Handle< OptionletVolatilityStructure > class.",
    "QuantLib.OrnsteinUhlenbeckProcess": "Proxy of C++ OrnsteinUhlenbeckProcess class.",
    "QuantLib.OvernightIborBasisSwapRateHelper": "Proxy of C++ OvernightIborBasisSwapRateHelper class.",
    "QuantLib.OvernightIndex": "Proxy of C++ OvernightIndex class.",
    "QuantLib.OvernightIndexFuture": "Proxy of C++ OvernightIndexFuture class.",
    "QuantLib.OvernightIndexFutureRateHelper": "Proxy of C++ OvernightIndexFutureRateHelper class.",
    "QuantLib.OvernightIndexedCoupon": "Proxy of C++ OvernightIndexedCoupon class.",
    "QuantLib.OvernightIndexedSwap": "Proxy of C++ OvernightIndexedSwap class.",
    "QuantLib.OvernightIndexedSwapIndex": "Proxy of C++ OvernightIndexedSwapIndex class.",
    "QuantLib.OvernightLeg": "OvernightLeg(DoubleVector nominals, Schedule schedule, ext::shared_ptr< OvernightIndex > const & index, DayCounter paymentDayCounter=DayCounter(), BusinessDayConvention const paymentConvention=Following, DoubleVector gearings=std::vector< Real >(), DoubleVector spreads=std::vector< Spread >(), bool telescopicValueDates=False, RateAveraging::Type averagingMethod=Compound, Calendar paymentCalendar=Calendar(), Integer paymentLag=0, Natural lookbackDays=Null< Natural >(), Natural lockoutDays=0, bool applyObservationShift=False) -> Leg",
    "QuantLib.PEHCurrency": "Proxy of C++ QuantLib::PEHCurrency class.",
    "QuantLib.PEICurrency": "Proxy of C++ QuantLib::PEICurrency class.",
    "QuantLib.PENCurrency": "Proxy of C++ QuantLib::PENCurrency class.",
    "QuantLib.PHPCurrency": "Proxy of C++ QuantLib::PHPCurrency class.",
    "QuantLib.PKRCurrency": "Proxy of C++ QuantLib::PKRCurrency class.",
    "QuantLib.PLNCurrency": "Proxy of C++ QuantLib::PLNCurrency class.",
    "QuantLib.PTECurrency": "Proxy of C++ QuantLib::PTECurrency class.",
    "QuantLib.PairDoubleVector": "Proxy of C++ std::pair< std::vector< double >,std::vector< double > > class.",
    "QuantLib.Parabolic": "Proxy of C++ SafeParabolic class.",
    "QuantLib.ParabolicCubic": "Proxy of C++ ParabolicCubic class.",
    "QuantLib.ParabolicCubicZeroCurve": "Proxy of C++ InterpolatedZeroCurve< ParabolicCubic > class.",
    "QuantLib.Parameter": "Proxy of C++ Parameter class.",
    "QuantLib.ParkinsonSigma": "Proxy of C++ ParkinsonSigma class.",
    "QuantLib.PartialBarrier": "Proxy of C++ PartialBarrier class.",
    "QuantLib.PartialTimeBarrierOption": "Proxy of C++ PartialTimeBarrierOption class.",
    "QuantLib.Path": "Proxy of C++ Path class.",
    "QuantLib.Payoff": "Proxy of C++ Payoff class.",
    "QuantLib.PercentageStrikePayoff": "Proxy of C++ PercentageStrikePayoff class.",
    "QuantLib.Period": "Proxy of C++ Period class.",
    "QuantLib.PeriodParser": "Proxy of C++ PeriodParser class.",
    "QuantLib.PeriodVector": "Proxy of C++ std::vector< Period > class.",
    "QuantLib.PiecewiseConstantCorrelation": "Proxy of C++ PiecewiseConstantCorrelation class.",
    "QuantLib.PiecewiseConstantParameter": "Proxy of C++ PiecewiseConstantParameter class.",
    "QuantLib.PiecewiseConvexMonotoneForward": "Proxy of C++ PiecewiseConvexMonotoneForward class.",
    "QuantLib.PiecewiseConvexMonotoneZero": "Proxy of C++ PiecewiseConvexMonotoneZero class.",
    "QuantLib.PiecewiseCubicZero": "Proxy of C++ PiecewiseCubicZero class.",
    "QuantLib.PiecewiseFlatForward": "Proxy of C++ PiecewiseFlatForward class.",
    "QuantLib.PiecewiseFlatHazardRate": "Proxy of C++ PiecewiseFlatHazardRate class.",
    "QuantLib.PiecewiseKrugerLogDiscount": "Proxy of C++ PiecewiseKrugerLogDiscount class.",
    "QuantLib.PiecewiseKrugerZero": "Proxy of C++ PiecewiseKrugerZero class.",
    "QuantLib.PiecewiseLinearForward": "Proxy of C++ PiecewiseLinearForward class.",
    "QuantLib.PiecewiseLinearZero": "Proxy of C++ PiecewiseLinearZero class.",
    "QuantLib.PiecewiseLogCubicDiscount": "Proxy of C++ PiecewiseLogCubicDiscount class.",
    "QuantLib.PiecewiseLogLinearDiscount": "Proxy of C++ PiecewiseLogLinearDiscount class.",
    "QuantLib.PiecewiseLogMixedLinearCubicDiscount": "Proxy of C++ PiecewiseLogMixedLinearCubicDiscount class.",
    "QuantLib.PiecewiseLogParabolicCubicDiscount": "Proxy of C++ PiecewiseLogParabolicCubicDiscount class.",
    "QuantLib.PiecewiseMonotonicLogParabolicCubicDiscount": "Proxy of C++ PiecewiseMonotonicLogParabolicCubicDiscount class.",
    "QuantLib.PiecewiseMonotonicParabolicCubicZero": "Proxy of C++ PiecewiseMonotonicParabolicCubicZero class.",
    "QuantLib.PiecewiseNaturalCubicZero": "Proxy of C++ PiecewiseNaturalCubicZero class.",
    "QuantLib.PiecewiseNaturalLogCubicDiscount": "Proxy of C++ PiecewiseNaturalLogCubicDiscount class.",
    "QuantLib.PiecewiseParabolicCubicZero": "Proxy of C++ PiecewiseParabolicCubicZero class.",
    "QuantLib.PiecewiseSplineCubicDiscount": "Proxy of C++ PiecewiseSplineCubicDiscount class.",
    "QuantLib.PiecewiseTimeDependentHestonModel": "Proxy of C++ PiecewiseTimeDependentHestonModel class.",
    "QuantLib.PiecewiseYoYInflation": "Proxy of C++ PiecewiseYoYInflationCurve< Linear > class.",
    "QuantLib.PiecewiseZeroInflation": "Proxy of C++ PiecewiseZeroInflationCurve< Linear > class.",
    "QuantLib.PiecewiseZeroSpreadedTermStructure": "Proxy of C++ PiecewiseZeroSpreadedTermStructure class.",
    "QuantLib.Pillar": "Proxy of C++ Pillar class.",
    "QuantLib.PlainVanillaPayoff": "Proxy of C++ PlainVanillaPayoff class.",
    "QuantLib.PoissonDistribution": "Proxy of C++ PoissonDistribution class.",
    "QuantLib.Poland": "Proxy of C++ QuantLib::Poland class.",
    "QuantLib.Position": "Proxy of C++ Position class.",
    "QuantLib.PositiveConstraint": "Proxy of C++ PositiveConstraint class.",
    "QuantLib.Predefined1dMesher": "Proxy of C++ Predefined1dMesher class.",
    "QuantLib.Pribor": "Proxy of C++ Pribor class.",
    "QuantLib.PricingEngine": "Proxy of C++ PricingEngine class.",
    "QuantLib.ProbabilityBoltzmannDownhill": "Proxy of C++ ProbabilityBoltzmannDownhill class.",
    "QuantLib.Protection": "Proxy of C++ Protection class.",
    "QuantLib.QARCurrency": "Proxy of C++ QuantLib::QARCurrency class.",
    "QuantLib.QdFpAmericanEngine": "Proxy of C++ QdFpAmericanEngine class.",
    "QuantLib.QdFpIterationScheme": "Proxy of C++ QdFpIterationScheme class.",
    "QuantLib.QdFpLegendreScheme": "Proxy of C++ QdFpLegendreScheme class.",
    "QuantLib.QdFpLegendreTanhSinhScheme": "Proxy of C++ QdFpLegendreTanhSinhScheme class.",
    "QuantLib.QdFpTanhSinhIterationScheme": "Proxy of C++ QdFpTanhSinhIterationScheme class.",
    "QuantLib.QdPlusAmericanEngine": "Proxy of C++ QdPlusAmericanEngine class.",
    "QuantLib.QuantoBarrierEngine": "Proxy of C++ QuantoBarrierEngine class.",
    "QuantLib.QuantoBarrierOption": "Proxy of C++ QuantoBarrierOption class.",
    "QuantLib.QuantoDoubleBarrierOption": "Proxy of C++ QuantoDoubleBarrierOption class.",
    "QuantLib.QuantoEuropeanEngine": "Proxy of C++ QuantoEuropeanEngine class.",
    "QuantLib.QuantoForwardEuropeanEngine": "Proxy of C++ QuantoForwardEuropeanEngine class.",
    "QuantLib.QuantoForwardVanillaOption": "Proxy of C++ QuantoForwardVanillaOption class.",
    "QuantLib.QuantoTermStructure": "Proxy of C++ QuantoTermStructure class.",
    "QuantLib.QuantoVanillaOption": "Proxy of C++ QuantoVanillaOption class.",
    "QuantLib.Quote": "Proxy of C++ Quote class.",
    "QuantLib.QuoteHandle": "Proxy of C++ Handle< Quote > class.",
    "QuantLib.QuoteHandleVector": "Proxy of C++ std::vector< Handle< Quote > > class.",
    "QuantLib.QuoteHandleVectorVector": "Proxy of C++ std::vector< std::vector< Handle< Quote > > > class.",
    "QuantLib.QuoteVector": "Proxy of C++ std::vector< ext::shared_ptr< Quote > > class.",
    "QuantLib.QuoteVectorVector": "Proxy of C++ std::vector< std::vector< ext::shared_ptr< Quote > > > class.",
    "QuantLib.ROLCurrency": "Proxy of C++ QuantLib::ROLCurrency class.",
    "QuantLib.RONCurrency": "Proxy of C++ QuantLib::RONCurrency class.",
    "QuantLib.RSDCurrency": "Proxy of C++ QuantLib::RSDCurrency class.",
    "QuantLib.RUBCurrency": "Proxy of C++ QuantLib::RUBCurrency class.",
    "QuantLib.RateAveraging": "Proxy of C++ RateAveraging class.",
    "QuantLib.RateHelper": "Proxy of C++ RateHelper class.",
    "QuantLib.RateHelperVector": "Proxy of C++ std::vector< ext::shared_ptr< RateHelper > > class.",
    "QuantLib.RealTimeSeries": "Proxy of C++ TimeSeries< Real > class.",
    "QuantLib.ReannealingTrivial": "Proxy of C++ ReannealingTrivial class.",
    "QuantLib.RebatedExercise": "Proxy of C++ RebatedExercise class.",
    "QuantLib.Redemption": "Proxy of C++ Redemption class.",
    "QuantLib.Region": "Proxy of C++ Region class.",
    "QuantLib.RelinkableBlackVolTermStructureHandle": "Proxy of C++ RelinkableHandle< BlackVolTermStructure > class.",
    "QuantLib.RelinkableCalibratedModelHandle": "Proxy of C++ RelinkableHandle< CalibratedModel > class.",
    "QuantLib.RelinkableCapFloorTermVolatilityStructureHandle": "Proxy of C++ RelinkableHandle< CapFloorTermVolatilityStructure > class.",
    "QuantLib.RelinkableDefaultProbabilityTermStructureHandle": "Proxy of C++ RelinkableHandle< DefaultProbabilityTermStructure > class.",
    "QuantLib.RelinkableDeltaVolQuoteHandle": "Proxy of C++ RelinkableHandle< DeltaVolQuote > class.",
    "QuantLib.RelinkableLocalVolTermStructureHandle": "Proxy of C++ RelinkableHandle< LocalVolTermStructure > class.",
    "QuantLib.RelinkableOptionletVolatilityStructureHandle": "Proxy of C++ RelinkableHandle< OptionletVolatilityStructure > class.",
    "QuantLib.RelinkableQuoteHandle": "Proxy of C++ RelinkableHandle< Quote > class.",
    "QuantLib.RelinkableQuoteHandleVector": "Proxy of C++ std::vector< RelinkableHandle< Quote > > class.",
    "QuantLib.RelinkableQuoteHandleVectorVector": "Proxy of C++ std::vector< std::vector< RelinkableHandle< Quote > > > class.",
    "QuantLib.RelinkableShortRateModelHandle": "Proxy of C++ RelinkableHandle< ShortRateModel > class.",
    "QuantLib.RelinkableSwaptionVolatilityStructureHandle": "Proxy of C++ RelinkableHandle< SwaptionVolatilityStructure > class.",
    "QuantLib.RelinkableYieldTermStructureHandle": "Proxy of C++ RelinkableHandle< YieldTermStructure > class.",
    "QuantLib.RelinkableYoYInflationTermStructureHandle": "Proxy of C++ RelinkableHandle< YoYInflationTermStructure > class.",
    "QuantLib.RelinkableYoYOptionletVolatilitySurfaceHandle": "Proxy of C++ RelinkableHandle< YoYOptionletVolatilitySurface > class.",
    "QuantLib.RelinkableZeroInflationTermStructureHandle": "Proxy of C++ RelinkableHandle< ZeroInflationTermStructure > class.",
    "QuantLib.RichardsonExtrapolation": "Proxy of C++ RichardsonExtrapolation class.",
    "QuantLib.Ridder": "Proxy of C++ Ridder class.",
    "QuantLib.RiskNeutralDensityCalculator": "Proxy of C++ RiskNeutralDensityCalculator class.",
    "QuantLib.RiskStatistics": "Proxy of C++ RiskStatistics class.",
    "QuantLib.RiskyBondEngine": "Proxy of C++ RiskyBondEngine class.",
    "QuantLib.Robor": "Proxy of C++ Robor class.",
    "QuantLib.Romania": "Proxy of C++ QuantLib::Romania class.",
    "QuantLib.Rounding": "Proxy of C++ Rounding class.",
    "QuantLib.RungeKutta": "Proxy of C++ AdaptiveRungeKutta< Real > class.",
    "QuantLib.Russia": "Proxy of C++ QuantLib::Russia class.",
    "QuantLib.SABRInterpolation": "Proxy of C++ SafeSABRInterpolation class.",
    "QuantLib.SARCurrency": "Proxy of C++ QuantLib::SARCurrency class.",
    "QuantLib.SEKCurrency": "Proxy of C++ QuantLib::SEKCurrency class.",
    "QuantLib.SEKLibor": "Proxy of C++ SEKLibor class.",
    "QuantLib.SGDCurrency": "Proxy of C++ QuantLib::SGDCurrency class.",
    "QuantLib.SITCurrency": "Proxy of C++ QuantLib::SITCurrency class.",
    "QuantLib.SKKCurrency": "Proxy of C++ QuantLib::SKKCurrency class.",
    "QuantLib.SVD": "Proxy of C++ SVD class.",
    "QuantLib.SabrSmileSection": "Proxy of C++ SabrSmileSection class.",
    "QuantLib.SabrSwaptionVolatilityCube": "Proxy of C++ SabrSwaptionVolatilityCube class.",
    "QuantLib.SalvagingAlgorithm": "Proxy of C++ SalvagingAlgorithm class.",
    "QuantLib.SampleArray": "Proxy of C++ Sample< Array > class.",
    "QuantLib.SampleMultiPath": "Proxy of C++ Sample< MultiPath > class.",
    "QuantLib.SampleNumber": "Proxy of C++ Sample< Real > class.",
    "QuantLib.SamplePath": "Proxy of C++ Sample< Path > class.",
    "QuantLib.SampleRealVector": "Proxy of C++ Sample< std::vector< Real > > class.",
    "QuantLib.SampledCurve": "Proxy of C++ SampledCurve class.",
    "QuantLib.SamplerGaussian": "Proxy of C++ SamplerGaussian class.",
    "QuantLib.SamplerLogNormal": "Proxy of C++ SamplerLogNormal class.",
    "QuantLib.SamplerMirrorGaussian": "Proxy of C++ SamplerMirrorGaussian class.",
    "QuantLib.SaudiArabia": "Proxy of C++ QuantLib::SaudiArabia class.",
    "QuantLib.SavedSettings": "Proxy of C++ _SavedSettings class.",
    "QuantLib.Schedule": "Proxy of C++ Schedule class.",
    "QuantLib.Seasonality": "Proxy of C++ Seasonality class.",
    "QuantLib.Secant": "Proxy of C++ Secant class.",
    "QuantLib.SecondDerivativeOp": "Proxy of C++ SecondDerivativeOp class.",
    "QuantLib.SecondOrderMixedDerivativeOp": "Proxy of C++ SecondOrderMixedDerivativeOp class.",
    "QuantLib.SegmentIntegral": "Proxy of C++ SegmentIntegral class.",
    "QuantLib.SequenceStatistics": "Proxy of C++ GenericSequenceStatistics< RiskStatistics > class.",
    "QuantLib.Settings": "Proxy of C++ Settings class.",
    "QuantLib.Settlement": "Proxy of C++ Settlement class.",
    "QuantLib.Shibor": "Proxy of C++ Shibor class.",
    "QuantLib.ShortRateModel": "Proxy of C++ ShortRateModel class.",
    "QuantLib.ShortRateModelHandle": "Proxy of C++ Handle< ShortRateModel > class.",
    "QuantLib.SimpleCashFlow": "Proxy of C++ SimpleCashFlow class.",
    "QuantLib.SimpleChooserOption": "Proxy of C++ SimpleChooserOption class.",
    "QuantLib.SimpleDayCounter": "Proxy of C++ QuantLib::SimpleDayCounter class.",
    "QuantLib.SimplePolynomialFitting": "Proxy of C++ SimplePolynomialFitting class.",
    "QuantLib.SimpleQuote": "Proxy of C++ SimpleQuote class.",
    "QuantLib.Simplex": "Proxy of C++ Simplex class.",
    "QuantLib.SimpsonIntegral": "Proxy of C++ SimpsonIntegral class.",
    "QuantLib.Singapore": "Proxy of C++ QuantLib::Singapore class.",
    "QuantLib.Slovakia": "Proxy of C++ QuantLib::Slovakia class.",
    "QuantLib.SmileSection": "Proxy of C++ SmileSection class.",
    "QuantLib.SmileSectionVector": "Proxy of C++ std::vector< ext::shared_ptr< SmileSection > > class.",
    "QuantLib.SobolBrownianBridgeRsg": "Proxy of C++ SobolBrownianBridgeRsg class.",
    "QuantLib.SobolBrownianGenerator": "Proxy of C++ SobolBrownianGenerator class.",
    "QuantLib.SobolBrownianGeneratorFactory": "Proxy of C++ SobolBrownianGeneratorFactory class.",
    "QuantLib.SobolRsg": "Proxy of C++ SobolRsg class.",
    "QuantLib.Sofr": "Proxy of C++ Sofr class.",
    "QuantLib.SofrFutureRateHelper": "Proxy of C++ SofrFutureRateHelper class.",
    "QuantLib.SoftCallability": "Proxy of C++ SoftCallability class.",
    "QuantLib.Sonia": "Proxy of C++ Sonia class.",
    "QuantLib.SouthAfrica": "Proxy of C++ QuantLib::SouthAfrica class.",
    "QuantLib.SouthKorea": "Proxy of C++ QuantLib::SouthKorea class.",
    "QuantLib.SparseMatrix": "Proxy of C++ SparseMatrix class.",
    "QuantLib.SplineCubic": "Proxy of C++ SplineCubic class.",
    "QuantLib.SplineCubicInterpolatedSmileSection": "Proxy of C++ InterpolatedSmileSection< SplineCubic > class.",
    "QuantLib.SplineLogCubic": "Proxy of C++ SplineLogCubic class.",
    "QuantLib.SpreadBasketPayoff": "Proxy of C++ SpreadBasketPayoff class.",
    "QuantLib.SpreadCdsHelper": "Proxy of C++ SpreadCdsHelper class.",
    "QuantLib.SpreadFittingMethod": "Proxy of C++ SpreadFittingMethod class.",
    "QuantLib.SpreadOption": "Proxy of C++ SpreadOption class.",
    "QuantLib.SpreadedBackwardFlatZeroInterpolatedTermStructure": "Proxy of C++ SpreadedBackwardFlatZeroInterpolatedTermStructure class.",
    "QuantLib.SpreadedCubicZeroInterpolatedTermStructure": "Proxy of C++ SpreadedCubicZeroInterpolatedTermStructure class.",
    "QuantLib.SpreadedKrugerZeroInterpolatedTermStructure": "Proxy of C++ SpreadedKrugerZeroInterpolatedTermStructure class.",
    "QuantLib.SpreadedLinearZeroInterpolatedTermStructure": "Proxy of C++ SpreadedLinearZeroInterpolatedTermStructure class.",
    "QuantLib.SpreadedMonotonicParabolicCubicZeroInterpolatedTermStructure": "Proxy of C++ SpreadedMonotonicParabolicCubicZeroInterpolatedTermStructure class.",
    "QuantLib.SpreadedParabolicCubicZeroInterpolatedTermStructure": "Proxy of C++ SpreadedParabolicCubicZeroInterpolatedTermStructure class.",
    "QuantLib.SpreadedSplineCubicZeroInterpolatedTermStructure": "Proxy of C++ SpreadedSplineCubicZeroInterpolatedTermStructure class.",
    "QuantLib.SpreadedSwaptionVolatility": "Proxy of C++ SpreadedSwaptionVolatility class.",
    "QuantLib.SquareRootProcessRNDCalculator": "Proxy of C++ SquareRootProcessRNDCalculator class.",
    "QuantLib.Statistics": "Proxy of C++ Statistics class.",
    "QuantLib.SteepestDescent": "Proxy of C++ SteepestDescent class.",
    "QuantLib.StochasticProcess": "Proxy of C++ StochasticProcess class.",
    "QuantLib.StochasticProcess1D": "Proxy of C++ StochasticProcess1D class.",
    "QuantLib.StochasticProcess1DVector": "Proxy of C++ std::vector< ext::shared_ptr< StochasticProcess1D > > class.",
    "QuantLib.StochasticProcessArray": "Proxy of C++ StochasticProcessArray class.",
    "QuantLib.StochasticProcessVector": "Proxy of C++ std::vector< ext::shared_ptr< StochasticProcess > > class.",
    "QuantLib.Stock": "Proxy of C++ Stock class.",
    "QuantLib.StrVector": "Proxy of C++ std::vector< std::string > class.",
    "QuantLib.StrikedTypePayoff": "Proxy of C++ StrikedTypePayoff class.",
    "QuantLib.StrippedOptionlet": "Proxy of C++ StrippedOptionlet class.",
    "QuantLib.StrippedOptionletAdapter": "Proxy of C++ StrippedOptionletAdapter class.",
    "QuantLib.StrippedOptionletBase": "Proxy of C++ StrippedOptionletBase class.",
    "QuantLib.StudentDistribution": "Proxy of C++ StudentDistribution class.",
    "QuantLib.StulzEngine": "Proxy of C++ StulzEngine class.",
    "QuantLib.SubPeriodsCoupon": "Proxy of C++ SubPeriodsCoupon class.",
    "QuantLib.SubPeriodsLeg": "SubPeriodsLeg(DoubleVector nominals, Schedule schedule, ext::shared_ptr< IborIndex > const & index, DayCounter paymentDayCounter=DayCounter(), BusinessDayConvention const paymentConvention=Following, Calendar paymentCalendar=Calendar(), Integer paymentLag=0, UnsignedIntVector fixingDays=std::vector< Natural >(), DoubleVector gearings=std::vector< Real >(), DoubleVector couponSpreads=std::vector< Spread >(), DoubleVector rateSpreads=std::vector< Spread >(), Period exCouponPeriod=Period(), Calendar exCouponCalendar=Calendar(), BusinessDayConvention exCouponConvention=Unadjusted, bool exCouponEndOfMonth=False, RateAveraging::Type averagingMethod=Compound) -> Leg",
    "QuantLib.SubPeriodsPricer": "Proxy of C++ SubPeriodsPricer class.",
    "QuantLib.SuoWangDoubleBarrierEngine": "Proxy of C++ SuoWangDoubleBarrierEngine class.",
    "QuantLib.SuperSharePayoff": "Proxy of C++ SuperSharePayoff class.",
    "QuantLib.SurvivalProbabilityCurve": "Proxy of C++ InterpolatedSurvivalProbabilityCurve< Linear > class.",
    "QuantLib.SvenssonFitting": "Proxy of C++ SvenssonFitting class.",
    "QuantLib.SviInterpolatedSmileSection": "Proxy of C++ SviInterpolatedSmileSection class.",
    "QuantLib.SviSmileSection": "Proxy of C++ SviSmileSection class.",
    "QuantLib.Swap": "Proxy of C++ Swap class.",
    "QuantLib.SwapIndex": "Proxy of C++ SwapIndex class.",
    "QuantLib.SwapIndexVector": "Proxy of C++ std::vector< ext::shared_ptr< SwapIndex > > class.",
    "QuantLib.SwapRateHelper": "Proxy of C++ SwapRateHelper class.",
    "QuantLib.SwapSpreadIndex": "Proxy of C++ SwapSpreadIndex class.",
    "QuantLib.Swaption": "Proxy of C++ Swaption class.",
    "QuantLib.SwaptionHelper": "Proxy of C++ SwaptionHelper class.",
    "QuantLib.SwaptionVolatilityCube": "Proxy of C++ SwaptionVolatilityCube class.",
    "QuantLib.SwaptionVolatilityDiscrete": "Proxy of C++ SwaptionVolatilityDiscrete class.",
    "QuantLib.SwaptionVolatilityMatrix": "Proxy of C++ SwaptionVolatilityMatrix class.",
    "QuantLib.SwaptionVolatilityStructure": "Proxy of C++ SwaptionVolatilityStructure class.",
    "QuantLib.SwaptionVolatilityStructureHandle": "Proxy of C++ Handle< SwaptionVolatilityStructure > class.",
    "QuantLib.Sweden": "Proxy of C++ QuantLib::Sweden class.",
    "QuantLib.Swestr": "Proxy of C++ Swestr class.",
    "QuantLib.SwingExercise": "Proxy of C++ SwingExercise class.",
    "QuantLib.Switzerland": "Proxy of C++ QuantLib::Switzerland class.",
    "QuantLib.TARGET": "Proxy of C++ QuantLib::TARGET class.",
    "QuantLib.THBCurrency": "Proxy of C++ QuantLib::THBCurrency class.",
    "QuantLib.THBFIX": "Proxy of C++ THBFIX class.",
    "QuantLib.TNDCurrency": "Proxy of C++ QuantLib::TNDCurrency class.",
    "QuantLib.TRLCurrency": "Proxy of C++ QuantLib::TRLCurrency class.",
    "QuantLib.TRLibor": "Proxy of C++ TRLibor class.",
    "QuantLib.TRYCurrency": "Proxy of C++ QuantLib::TRYCurrency class.",
    "QuantLib.TTDCurrency": "Proxy of C++ QuantLib::TTDCurrency class.",
    "QuantLib.TWDCurrency": "Proxy of C++ QuantLib::TWDCurrency class.",
    "QuantLib.Taiwan": "Proxy of C++ QuantLib::Taiwan class.",
    "QuantLib.TanhSinhIntegral": "Proxy of C++ TanhSinhIntegral class.",
    "QuantLib.TemperatureExponential": "Proxy of C++ TemperatureExponential class.",
    "QuantLib.TermStructure": "Proxy of C++ TermStructure class.",
    "QuantLib.TermStructureConsistentModel": "Proxy of C++ TermStructureConsistentModel class.",
    "QuantLib.Thailand": "Proxy of C++ QuantLib::Thailand class.",
    "QuantLib.Thirty360": "Proxy of C++ QuantLib::Thirty360 class.",
    "QuantLib.Thirty365": "Proxy of C++ QuantLib::Thirty365 class.",
    "QuantLib.Tibor": "Proxy of C++ Tibor class.",
    "QuantLib.TimeBasket": "Proxy of C++ TimeBasket class.",
    "QuantLib.TimeGrid": "Proxy of C++ TimeGrid class.",
    "QuantLib.TimeToDateMap": "Proxy of C++ std::map< Time,Date > class.",
    "QuantLib.Tona": "Proxy of C++ Tona class.",
    "QuantLib.TrapezoidIntegralDefault": "Proxy of C++ TrapezoidIntegral< Default > class.",
    "QuantLib.TrapezoidIntegralMidPoint": "Proxy of C++ TrapezoidIntegral< MidPoint > class.",
    "QuantLib.TreeCallableFixedRateBondEngine": "Proxy of C++ TreeCallableFixedRateBondEngine class.",
    "QuantLib.TreeCapFloorEngine": "Proxy of C++ TreeCapFloorEngine class.",
    "QuantLib.TreeSwaptionEngine": "Proxy of C++ TreeSwaptionEngine class.",
    "QuantLib.TridiagonalOperator": "Proxy of C++ TridiagonalOperator class.",
    "QuantLib.TripleBandLinearOp": "Proxy of C++ TripleBandLinearOp class.",
    "QuantLib.Turkey": "Proxy of C++ QuantLib::Turkey class.",
    "QuantLib.TurnbullWakemanAsianEngine": "Proxy of C++ TurnbullWakemanAsianEngine class.",
    "QuantLib.TypePayoff": "Proxy of C++ TypePayoff class.",
    "QuantLib.UAHCurrency": "Proxy of C++ QuantLib::UAHCurrency class.",
    "QuantLib.UGXCurrency": "Proxy of C++ QuantLib::UGXCurrency class.",
    "QuantLib.UKHICP": "Proxy of C++ UKHICP class.",
    "QuantLib.UKRPI": "Proxy of C++ UKRPI class.",
    "QuantLib.USCPI": "Proxy of C++ USCPI class.",
    "QuantLib.USDCurrency": "Proxy of C++ QuantLib::USDCurrency class.",
    "QuantLib.USDLibor": "Proxy of C++ USDLibor class.",
    "QuantLib.USDLiborON": "Proxy of C++ USDLiborON class.",
    "QuantLib.UYUCurrency": "Proxy of C++ QuantLib::UYUCurrency class.",
    "QuantLib.Ukraine": "Proxy of C++ QuantLib::Ukraine class.",
    "QuantLib.UltimateForwardTermStructure": "Proxy of C++ UltimateForwardTermStructure class.",
    "QuantLib.Uniform1dMesher": "Proxy of C++ Uniform1dMesher class.",
    "QuantLib.UniformLowDiscrepancySequenceGenerator": "Proxy of C++ UniformLowDiscrepancySequenceGenerator class.",
    "QuantLib.UniformRandomGenerator": "Proxy of C++ UniformRandomGenerator class.",
    "QuantLib.UniformRandomSequenceGenerator": "Proxy of C++ UniformRandomSequenceGenerator class.",
    "QuantLib.UnitDisplacedBlackYoYInflationCouponPricer": "Proxy of C++ UnitDisplacedBlackYoYInflationCouponPricer class.",
    "QuantLib.UnitedKingdom": "Proxy of C++ QuantLib::UnitedKingdom class.",
    "QuantLib.UnitedStates": "Proxy of C++ QuantLib::UnitedStates class.",
    "QuantLib.UnsignedIntPair": "Proxy of C++ std::pair< unsigned int,unsigned int > class.",
    "QuantLib.UnsignedIntPairVector": "Proxy of C++ std::vector< std::pair< unsigned int,unsigned int > > class.",
    "QuantLib.UnsignedIntVector": "Proxy of C++ std::vector< unsigned int > class.",
    "QuantLib.UpRounding": "Proxy of C++ UpRounding class.",
    "QuantLib.UpfrontCdsHelper": "Proxy of C++ UpfrontCdsHelper class.",
    "QuantLib.UsdLiborSwapIsdaFixAm": "Proxy of C++ UsdLiborSwapIsdaFixAm class.",
    "QuantLib.UsdLiborSwapIsdaFixPm": "Proxy of C++ UsdLiborSwapIsdaFixPm class.",
    "QuantLib.VEBCurrency": "Proxy of C++ QuantLib::VEBCurrency class.",
    "QuantLib.VNDCurrency": "Proxy of C++ QuantLib::VNDCurrency class.",
    "QuantLib.VanillaForwardPayoff": "Proxy of C++ VanillaForwardPayoff class.",
    "QuantLib.VanillaOption": "Proxy of C++ VanillaOption class.",
    "QuantLib.VanillaSwap": "Proxy of C++ VanillaSwap class.",
    "QuantLib.VanillaSwingOption": "Proxy of C++ VanillaSwingOption class.",
    "QuantLib.VannaVolgaBarrierEngine": "Proxy of C++ VannaVolgaBarrierEngine class.",
    "QuantLib.VannaVolgaIKDoubleBarrierEngine": "Proxy of C++ VannaVolgaDoubleBarrierEngine< AnalyticDoubleBarrierEngine > class.",
    "QuantLib.VannaVolgaWODoubleBarrierEngine": "Proxy of C++ VannaVolgaDoubleBarrierEngine< SuoWangDoubleBarrierEngine > class.",
    "QuantLib.VarianceGammaEngine": "Proxy of C++ VarianceGammaEngine class.",
    "QuantLib.VarianceGammaProcess": "Proxy of C++ VarianceGammaProcess class.",
    "QuantLib.Vasicek": "Proxy of C++ Vasicek class.",
    "QuantLib.VolatilityTermStructure": "Proxy of C++ VolatilityTermStructure class.",
    "QuantLib.WeekendsOnly": "Proxy of C++ QuantLib::WeekendsOnly class.",
    "QuantLib.Wibor": "Proxy of C++ Wibor class.",
    "QuantLib.XOFCurrency": "Proxy of C++ QuantLib::XOFCurrency class.",
    "QuantLib.XRPCurrency": "Proxy of C++ QuantLib::XRPCurrency class.",
    "QuantLib.Xoshiro256StarStarUniformRng": "Proxy of C++ Xoshiro256StarStarUniformRng class.",
    "QuantLib.Xoshiro256StarStarUniformRsg": "Proxy of C++ RandomSequenceGenerator< Xoshiro256StarStarUniformRng > class.",
    "QuantLib.YYEUHICP": "Proxy of C++ YYEUHICP class.",
    "QuantLib.YYEUHICPXT": "Proxy of C++ YYEUHICPXT class.",
    "QuantLib.YYFRHICP": "Proxy of C++ YYFRHICP class.",
    "QuantLib.YYUKRPI": "Proxy of C++ YYUKRPI class.",
    "QuantLib.YYUSCPI": "Proxy of C++ YYUSCPI class.",
    "QuantLib.YYZACPI": "Proxy of C++ YYZACPI class.",
    "QuantLib.YearOnYearInflationSwap": "Proxy of C++ YearOnYearInflationSwap class.",
    "QuantLib.YearOnYearInflationSwapHelper": "Proxy of C++ YearOnYearInflationSwapHelper class.",
    "QuantLib.YieldTermStructure": "Proxy of C++ YieldTermStructure class.",
    "QuantLib.YieldTermStructureHandle": "Proxy of C++ Handle< YieldTermStructure > class.",
    "QuantLib.YoYCapFloorTermPriceSurface": "Proxy of C++ YoYCapFloorTermPriceSurface class.",
    "QuantLib.YoYHelper": "Proxy of C++ BootstrapHelper< YoYInflationTermStructure > class.",
    "QuantLib.YoYHelperVector": "Proxy of C++ std::vector< ext::shared_ptr< BootstrapHelper< YoYInflationTermStructure > > > class.",
    "QuantLib.YoYInflationBachelierCapFloorEngine": "Proxy of C++ YoYInflationBachelierCapFloorEngine class.",
    "QuantLib.YoYInflationBlackCapFloorEngine": "Proxy of C++ YoYInflationBlackCapFloorEngine class.",
    "QuantLib.YoYInflationCap": "Proxy of C++ YoYInflationCap class.",
    "QuantLib.YoYInflationCapFloor": "Proxy of C++ YoYInflationCapFloor class.",
    "QuantLib.YoYInflationCapFloorTermPriceSurface": "Proxy of C++ YoYInflationCapFloorTermPriceSurface class.",
    "QuantLib.YoYInflationCollar": "Proxy of C++ YoYInflationCollar class.",
    "QuantLib.YoYInflationCoupon": "Proxy of C++ YoYInflationCoupon class.",
    "QuantLib.YoYInflationCouponPricer": "Proxy of C++ YoYInflationCouponPricer class.",
    "QuantLib.YoYInflationCurve": "Proxy of C++ InterpolatedYoYInflationCurve< Linear > class.",
    "QuantLib.YoYInflationFloor": "Proxy of C++ YoYInflationFloor class.",
    "QuantLib.YoYInflationIndex": "Proxy of C++ YoYInflationIndex class.",
    "QuantLib.YoYInflationTermStructure": "Proxy of C++ YoYInflationTermStructure class.",
    "QuantLib.YoYInflationTermStructureHandle": "Proxy of C++ Handle< YoYInflationTermStructure > class.",
    "QuantLib.YoYInflationUnitDisplacedBlackCapFloorEngine": "Proxy of C++ YoYInflationUnitDisplacedBlackCapFloorEngine class.",
    "QuantLib.YoYOptionHelper": "Proxy of C++ BootstrapHelper< YoYOptionletVolatilitySurface > class.",
    "QuantLib.YoYOptionHelperVector": "Proxy of C++ std::vector< ext::shared_ptr< BootstrapHelper< YoYOptionletVolatilitySurface > > > class.",
    "QuantLib.YoYOptionletHelper": "Proxy of C++ YoYOptionletHelper class.",
    "QuantLib.YoYOptionletStripper": "Proxy of C++ YoYOptionletStripper class.",
    "QuantLib.YoYOptionletVolatilitySurface": "Proxy of C++ YoYOptionletVolatilitySurface class.",
    "QuantLib.YoYOptionletVolatilitySurfaceHandle": "Proxy of C++ Handle< YoYOptionletVolatilitySurface > class.",
    "QuantLib.ZACPI": "Proxy of C++ ZACPI class.",
    "QuantLib.ZARCurrency": "Proxy of C++ QuantLib::ZARCurrency class.",
    "QuantLib.ZECCurrency": "Proxy of C++ QuantLib::ZECCurrency class.",
    "QuantLib.ZMWCurrency": "Proxy of C++ QuantLib::ZMWCurrency class.",
    "QuantLib.ZabrFullFd": "Proxy of C++ ZabrFullFd class.",
    "QuantLib.ZabrFullFdInterpolatedSmileSection": "Proxy of C++ ZabrInterpolatedSmileSection< ZabrFullFd > class.",
    "QuantLib.ZabrFullFdSmileSection": "Proxy of C++ ZabrSmileSection< ZabrFullFd > class.",
    "QuantLib.ZabrLocalVolatility": "Proxy of C++ ZabrLocalVolatility class.",
    "QuantLib.ZabrLocalVolatilityInterpolatedSmileSection": "Proxy of C++ ZabrInterpolatedSmileSection< ZabrLocalVolatility > class.",
    "QuantLib.ZabrLocalVolatilitySmileSection": "Proxy of C++ ZabrSmileSection< ZabrLocalVolatility > class.",
    "QuantLib.ZabrShortMaturityLognormal": "Proxy of C++ ZabrShortMaturityLognormal class.",
    "QuantLib.ZabrShortMaturityLognormalInterpolatedSmileSection": "Proxy of C++ ZabrInterpolatedSmileSection< ZabrShortMaturityLognormal > class.",
    "QuantLib.ZabrShortMaturityLognormalSmileSection": "Proxy of C++ ZabrSmileSection< ZabrShortMaturityLognormal > class.",
    "QuantLib.ZabrShortMaturityNormal": "Proxy of C++ ZabrShortMaturityNormal class.",
    "QuantLib.ZabrShortMaturityNormalInterpolatedSmileSection": "Proxy of C++ ZabrInterpolatedSmileSection< ZabrShortMaturityNormal > class.",
    "QuantLib.ZabrShortMaturityNormalSmileSection": "Proxy of C++ ZabrSmileSection< ZabrShortMaturityNormal > class.",
    "QuantLib.ZeroCouponBond": "Proxy of C++ ZeroCouponBond class.",
    "QuantLib.ZeroCouponInflationSwap": "Proxy of C++ ZeroCouponInflationSwap class.",
    "QuantLib.ZeroCouponInflationSwapHelper": "Proxy of C++ ZeroCouponInflationSwapHelper class.",
    "QuantLib.ZeroCouponSwap": "Proxy of C++ ZeroCouponSwap class.",
    "QuantLib.ZeroCurve": "Proxy of C++ InterpolatedZeroCurve< Linear > class.",
    "QuantLib.ZeroHelper": "Proxy of C++ BootstrapHelper< ZeroInflationTermStructure > class.",
    "QuantLib.ZeroHelperVector": "Proxy of C++ std::vector< ext::shared_ptr< BootstrapHelper< ZeroInflationTermStructure > > > class.",
    "QuantLib.ZeroInflationCashFlow": "Proxy of C++ ZeroInflationCashFlow class.",
    "QuantLib.ZeroInflationCurve": "Proxy of C++ InterpolatedZeroInflationCurve< Linear > class.",
    "QuantLib.ZeroInflationIndex": "Proxy of C++ ZeroInflationIndex class.",
    "QuantLib.ZeroInflationTermStructure": "Proxy of C++ ZeroInflationTermStructure class.",
    "QuantLib.ZeroInflationTermStructureHandle": "Proxy of C++ Handle< ZeroInflationTermStructure > class.",
    "QuantLib.ZeroSpreadedTermStructure": "Proxy of C++ ZeroSpreadedTermStructure class.",
    "QuantLib.ZeroYield": "Proxy of C++ ZeroYield class.",
    "QuantLib.Zibor": "Proxy of C++ Zibor class.",
    "QuantLib.ZigguratXoshiro256StarStarGaussianRng": "Proxy of C++ ZigguratGaussianRng< Xoshiro256StarStarUniformRng > class.",
    "QuantLib.ZigguratXoshiro256StarStarGaussianRsg": "Proxy of C++ ZigguratXoshiro256StarStarGaussianRsg class.",
    "QuantLib.as_black_helper": "as_black_helper(ext::shared_ptr< CalibrationHelper > const & h) -> ext::shared_ptr< BlackCalibrationHelper >",
    "QuantLib.as_capped_floored_yoy_inflation_coupon": "as_capped_floored_yoy_inflation_coupon(ext::shared_ptr< CashFlow > const & cf) -> ext::shared_ptr< CappedFlooredYoYInflationCoupon >",
    "QuantLib.as_constnotionalcrosscurrencybasisswapratehelper": "as_constnotionalcrosscurrencybasisswapratehelper(ext::shared_ptr< RateHelper > const helper) -> ext::shared_ptr< ConstNotionalCrossCurrencyBasisSwapRateHelper > const",
    "QuantLib.as_coupon": "as_coupon(ext::shared_ptr< CashFlow > const & cf) -> ext::shared_ptr< Coupon >",
    "QuantLib.as_cpi_cashflow": "as_cpi_cashflow(ext::shared_ptr< CashFlow > const & cf) -> ext::shared_ptr< CPICashFlow >",
    "QuantLib.as_cpi_coupon": "as_cpi_coupon(ext::shared_ptr< CashFlow > const & cf) -> ext::shared_ptr< CPICoupon >",
    "QuantLib.as_depositratehelper": "as_depositratehelper(ext::shared_ptr< RateHelper > const helper) -> ext::shared_ptr< DepositRateHelper > const",
    "QuantLib.as_fixed_rate_coupon": "as_fixed_rate_coupon(ext::shared_ptr< CashFlow > const & cf) -> ext::shared_ptr< FixedRateCoupon >",
    "QuantLib.as_floating_rate_coupon": "as_floating_rate_coupon(ext::shared_ptr< CashFlow > const & cf) -> ext::shared_ptr< FloatingRateCoupon >",
    "QuantLib.as_fraratehelper": "as_fraratehelper(ext::shared_ptr< RateHelper > const helper) -> ext::shared_ptr< FraRateHelper > const",
    "QuantLib.as_gsr_process": "as_gsr_process(ext::shared_ptr< StochasticProcess > const & proc) -> ext::shared_ptr< GsrProcess > const",
    "QuantLib.as_iborindex": "as_iborindex(ext::shared_ptr< InterestRateIndex > const & index) -> ext::shared_ptr< IborIndex >",
    "QuantLib.as_indexed_cashflow": "as_indexed_cashflow(ext::shared_ptr< CashFlow > const & cf) -> ext::shared_ptr< IndexedCashFlow >",
    "QuantLib.as_inflation_coupon": "as_inflation_coupon(ext::shared_ptr< CashFlow > const & cf) -> ext::shared_ptr< InflationCoupon >",
    "QuantLib.as_mtmcrosscurrencybasisswapratehelper": "as_mtmcrosscurrencybasisswapratehelper(ext::shared_ptr< RateHelper > const helper) -> ext::shared_ptr< MtMCrossCurrencyBasisSwapRateHelper > const",
    "QuantLib.as_oisratehelper": "as_oisratehelper(ext::shared_ptr< RateHelper > const helper) -> ext::shared_ptr< OISRateHelper > const",
    "QuantLib.as_overnight_indexed_coupon": "as_overnight_indexed_coupon(ext::shared_ptr< CashFlow > const & cf) -> ext::shared_ptr< OvernightIndexedCoupon >",
    "QuantLib.as_overnight_swap_index": "as_overnight_swap_index(ext::shared_ptr< InterestRateIndex > const & index) -> ext::shared_ptr< OvernightIndexedSwap >",
    "QuantLib.as_plain_vanilla_payoff": "as_plain_vanilla_payoff(ext::shared_ptr< Payoff > const & payoff) -> ext::shared_ptr< PlainVanillaPayoff > const",
    "QuantLib.as_sub_periods_coupon": "as_sub_periods_coupon(ext::shared_ptr< CashFlow > const & cf) -> ext::shared_ptr< SubPeriodsCoupon >",
    "QuantLib.as_swap_index": "as_swap_index(ext::shared_ptr< InterestRateIndex > const & index) -> ext::shared_ptr< SwapIndex >",
    "QuantLib.as_swapratehelper": "as_swapratehelper(ext::shared_ptr< RateHelper > const helper) -> ext::shared_ptr< SwapRateHelper > const",
    "QuantLib.as_swaption_helper": "as_swaption_helper(ext::shared_ptr< BlackCalibrationHelper > const & h) -> ext::shared_ptr< SwaptionHelper >",
    "QuantLib.as_yoy_inflation_coupon": "as_yoy_inflation_coupon(ext::shared_ptr< CashFlow > const & cf) -> ext::shared_ptr< YoYInflationCoupon >",
    "QuantLib.as_zero_inflation_cash_flow": "as_zero_inflation_cash_flow(ext::shared_ptr< CashFlow > const & cf) -> ext::shared_ptr< ZeroInflationCashFlow >",
    "QuantLib.as_zero_inflation_index": "as_zero_inflation_index(ext::shared_ptr< Index > const & i) -> ext::shared_ptr< ZeroInflationIndex >",
    "QuantLib.bachelierBlackFormula": "bachelierBlackFormula(Option::Type optionType, Real strike, Real forward, Real stdDev, Real discount=1.0) -> Real",
    "QuantLib.bachelierBlackFormulaAssetItmProbability": "\n    bachelierBlackFormulaAssetItmProbability(Option::Type optionType, Real strike, Real forward, Real stdDev) -> Real\n    bachelierBlackFormulaAssetItmProbability(ext::shared_ptr< PlainVanillaPayoff > const & payoff, Real forward, Real stdDev) -> Real\n    ",
    "QuantLib.bachelierBlackFormulaImpliedVol": "bachelierBlackFormulaImpliedVol(Option::Type optionType, Real strike, Real forward, Real tte, Real bachelierPrice, Real discount=1.0) -> Real",
    "QuantLib.bachelierBlackFormulaImpliedVolChoi": "bachelierBlackFormulaImpliedVolChoi(Option::Type optionType, Real strike, Real forward, Real tte, Real bachelierPrice, Real discount=1.0) -> Real",
    "QuantLib.blackFormula": "blackFormula(Option::Type optionType, Real strike, Real forward, Real stdDev, Real discount=1.0, Real displacement=0.0) -> Real",
    "QuantLib.blackFormulaAssetItmProbability": "\n    blackFormulaAssetItmProbability(Option::Type optionType, Real strike, Real forward, Real stdDev, Real displacement=0.0) -> Real\n    blackFormulaAssetItmProbability(ext::shared_ptr< PlainVanillaPayoff > const & payoff, Real forward, Real stdDev, Real displacement=0.0) -> Real\n    ",
    "QuantLib.blackFormulaCashItmProbability": "\n    blackFormulaCashItmProbability(Option::Type optionType, Real strike, Real forward, Real stdDev, Real displacement=0.0) -> Real\n    blackFormulaCashItmProbability(ext::shared_ptr< PlainVanillaPayoff > const & payoff, Real forward, Real stdDev, Real displacement=0.0) -> Real\n    ",
    "QuantLib.blackFormulaImpliedStdDev": "blackFormulaImpliedStdDev(Option::Type optionType, Real strike, Real forward, Real blackPrice, Real discount=1.0, Real displacement=0.0, Real guess=Null< Real >(), Real accuracy=1.0e-6, Natural maxIterations=100) -> Real",
    "QuantLib.blackFormulaImpliedStdDevLiRS": "\n    blackFormulaImpliedStdDevLiRS(Option::Type optionType, Real strike, Real forward, Real blackPrice, Real discount=1.0, Real displacement=0.0, Real guess=Null< Real >(), Real omega=1.0, Real accuracy=1.0e-6, Natural maxIterations=100) -> Real\n    blackFormulaImpliedStdDevLiRS(ext::shared_ptr< PlainVanillaPayoff > const & payoff, Real forward, Real blackPrice, Real discount=1.0, Real displacement=0.0, Real guess=Null< Real >(), Real omega=1.0, Real accuracy=1.0e-6, Natural maxIterations=100) -> Real\n    ",
    "QuantLib.cdsMaturity": "cdsMaturity(Date tradeDate, Period tenor, DateGeneration::Rule rule) -> Date",
    "QuantLib.checkCompatibility": "checkCompatibility(EvolutionDescription evolution, UnsignedIntVector numeraires)",
    "QuantLib.cleanPriceFromZSpread": "cleanPriceFromZSpread(Bond bond, ext::shared_ptr< YieldTermStructure > const & discountCurve, Spread zSpread, DayCounter dc, Compounding compounding, Frequency freq, Date settlementDate=Date()) -> Real",
    "QuantLib.close": "\n    close(Real x, Real y) -> bool\n    close(Real x, Real y, Size n) -> bool\n    ",
    "QuantLib.close_enough": "\n    close_enough(Real x, Real y) -> bool\n    close_enough(Real x, Real y, Size n) -> bool\n    ",
    "QuantLib.daysBetween": "daysBetween(Date arg1, Date arg2) -> Time",
    "QuantLib.disableTracing": "disableTracing()",
    "QuantLib.enableTracing": "enableTracing()",
    "QuantLib.getCovariance": "getCovariance(Array volatilities, Matrix correlations) -> Matrix",
    "QuantLib.inflationBaseDate": "inflationBaseDate(Date referenceDate, Period observationLag, Frequency frequency, bool indexIsInterpolated) -> Date",
    "QuantLib.inflationPeriod": "inflationPeriod(Date d, Frequency f) -> DatePair",
    "QuantLib.inflationYearFraction": "inflationYearFraction(Frequency f, bool indexIsInterpolated, DayCounter dayCount, Date d1, Date d2) -> Time",
    "QuantLib.inverse": "inverse(Matrix m) -> Matrix",
    "QuantLib.isInMoneyMarketMeasure": "isInMoneyMarketMeasure(EvolutionDescription evolution, UnsignedIntVector numeraires) -> bool",
    "QuantLib.isInMoneyMarketPlusMeasure": "isInMoneyMarketPlusMeasure(EvolutionDescription evolution, UnsignedIntVector numeraires, Size offset=1) -> bool",
    "QuantLib.isInTerminalMeasure": "isInTerminalMeasure(EvolutionDescription evolution, UnsignedIntVector numeraires) -> bool",
    "QuantLib.makeQuoteHandle": "makeQuoteHandle(Real value) -> RelinkableQuoteHandle",
    "QuantLib.moneyMarketMeasure": "moneyMarketMeasure(EvolutionDescription evolution) -> UnsignedIntVector",
    "QuantLib.moneyMarketPlusMeasure": "moneyMarketPlusMeasure(EvolutionDescription evolution, Size offset=1) -> UnsignedIntVector",
    "QuantLib.nullDouble": "nullDouble() -> double",
    "QuantLib.nullInt": "nullInt() -> int",
    "QuantLib.outerProduct": "outerProduct(Array v1, Array v2) -> Matrix",
    "QuantLib.pseudoSqrt": "pseudoSqrt(Matrix m, SalvagingAlgorithm::Type a) -> Matrix",
    "QuantLib.sabrFlochKennedyVolatility": "sabrFlochKennedyVolatility(Rate strike, Rate forward, Time expiryTime, Real alpha, Real beta, Real nu, Real rho) -> Real",
    "QuantLib.sabrVolatility": "sabrVolatility(Rate strike, Rate forward, Time expiryTime, Real alpha, Real beta, Real nu, Real rho, VolatilityType volatilityType=VolatilityType::ShiftedLognormal) -> Real",
    "QuantLib.setCouponPricer": "\n    setCouponPricer(Leg arg1, ext::shared_ptr< FloatingRateCouponPricer > const & arg2)\n    setCouponPricer(Leg arg1, ext::shared_ptr< EquityCashFlowPricer > const & arg2)\n    setCouponPricer(Leg arg1, ext::shared_ptr< YoYInflationCouponPricer > const & arg2)\n    ",
    "QuantLib.shiftedSabrVolatility": "shiftedSabrVolatility(Rate strike, Rate forward, Time expiryTime, Real alpha, Real beta, Real nu, Real rho, Real shift, VolatilityType volatilityType=VolatilityType::ShiftedLognormal) -> Real",
    "QuantLib.simplifyNotificationGraph": "\n    simplifyNotificationGraph(Swap swap, bool unregisterCoupons=False)\n    simplifyNotificationGraph(Bond bond, bool unregisterCoupons=False)\n    ",
    "QuantLib.sinkingNotionals": "sinkingNotionals(Period bondLength, Frequency const & frequency, Rate couponRate, Real initialNotional) -> DoubleVector",
    "QuantLib.sinkingSchedule": "sinkingSchedule(Date startDate, Period bondLength, Frequency const & frequency, Calendar paymentCalendar) -> Schedule",
    "QuantLib.terminalMeasure": "terminalMeasure(EvolutionDescription evolution) -> UnsignedIntVector",
    "QuantLib.transpose": "transpose(Matrix m) -> Matrix",
    "QuantLib.yearFractionToDate": "yearFractionToDate(DayCounter dayCounter, Date referenceDate, Time t) -> Date",
    "QuantLib.yoyInflationLeg": "\n    yoyInflationLeg(Schedule schedule, Calendar calendar, ext::shared_ptr< YoYInflationIndex > const & index, Period observationLag, CPI::InterpolationType interpolation, DoubleVector notionals, DayCounter paymentDayCounter, BusinessDayConvention paymentAdjustment=Following, Natural fixingDays=0, DoubleVector gearings=std::vector< Real >(), DoubleVector spreads=std::vector< Spread >(), DoubleVector caps=std::vector< Rate >(), DoubleVector floors=std::vector< Rate >()) -> Leg\n    yoyInflationLeg(Schedule schedule, Calendar calendar, ext::shared_ptr< YoYInflationIndex > const & index, Period observationLag, DoubleVector notionals, DayCounter paymentDayCounter, BusinessDayConvention paymentAdjustment=Following, Natural fixingDays=0, DoubleVector gearings=std::vector< Real >(), DoubleVector spreads=std::vector< Spread >(), DoubleVector caps=std::vector< Rate >(), DoubleVector floors=std::vector< Rate >()) -> Leg\n    ",
    "empyrical.aggregate_returns": "\n    Aggregates returns by week, month, or year.\n\n    Parameters\n    ----------\n    returns : pd.Series\n       Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    convert_to : str\n        Can be 'weekly', 'monthly', or 'yearly'.\n\n    Returns\n    -------\n    aggregated_returns : pd.Series\n    ",
    "empyrical.alpha": "Calculates annualized alpha.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    factor_returns : pd.Series\n        Daily noncumulative returns of the factor to which beta is\n        computed. Usually a benchmark such as the market.\n        - This is in the same style as returns.\n    risk_free : int, float, optional\n        Constant risk-free return throughout the period. For example, the\n        interest rate on a three month us treasury bill.\n    period : str, optional\n        Defines the periodicity of the 'returns' data for purposes of\n        annualizing. Value ignored if `annualization` parameter is specified.\n        Defaults are::\n\n            'monthly':12\n            'weekly': 52\n            'daily': 252\n\n    annualization : int, optional\n        Used to suppress default values available in `period` to convert\n        returns into annual returns. Value should be the annual frequency of\n        `returns`.\n        - See full explanation in :func:`~empyrical.stats.annual_return`.\n    _beta : float, optional\n        The beta for the given inputs, if already known. Will be calculated\n        internally if not provided.\n    out : array-like, optional\n        Array to use as output buffer.\n        If not passed, a new array will be created.\n\n    Returns\n    -------\n    float\n        Alpha.\n    ",
    "empyrical.alpha_aligned": "Calculates annualized alpha.\n\n    If they are pd.Series, expects returns and factor_returns have already\n    been aligned on their labels.  If np.ndarray, these arguments should have\n    the same shape.\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    factor_returns : pd.Series or np.ndarray\n        Daily noncumulative returns of the factor to which beta is\n        computed. Usually a benchmark such as the market.\n        - This is in the same style as returns.\n    risk_free : int, float, optional\n        Constant risk-free return throughout the period. For example, the\n        interest rate on a three month us treasury bill.\n    period : str, optional\n        Defines the periodicity of the 'returns' data for purposes of\n        annualizing. Value ignored if `annualization` parameter is specified.\n        Defaults are::\n\n            'monthly':12\n            'weekly': 52\n            'daily': 252\n\n    annualization : int, optional\n        Used to suppress default values available in `period` to convert\n        returns into annual returns. Value should be the annual frequency of\n        `returns`.\n        - See full explanation in :func:`~empyrical.stats.annual_return`.\n    _beta : float, optional\n        The beta for the given inputs, if already known. Will be calculated\n        internally if not provided.\n    out : array-like, optional\n        Array to use as output buffer.\n        If not passed, a new array will be created.\n\n    Returns\n    -------\n    alpha : float\n    ",
    "empyrical.alpha_beta": "Calculates annualized alpha and beta.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    factor_returns : pd.Series\n         Daily noncumulative returns of the factor to which beta is\n         computed. Usually a benchmark such as the market.\n         - This is in the same style as returns.\n    risk_free : int, float, optional\n        Constant risk-free return throughout the period. For example, the\n        interest rate on a three month us treasury bill.\n    period : str, optional\n        Defines the periodicity of the 'returns' data for purposes of\n        annualizing. Value ignored if `annualization` parameter is specified.\n        Defaults are::\n\n            'monthly':12\n            'weekly': 52\n            'daily': 252\n\n    annualization : int, optional\n        Used to suppress default values available in `period` to convert\n        returns into annual returns. Value should be the annual frequency of\n        `returns`.\n    out : array-like, optional\n        Array to use as output buffer.\n        If not passed, a new array will be created.\n\n    Returns\n    -------\n    alpha : float\n    beta : float\n    ",
    "empyrical.alpha_beta_aligned": "Calculates annualized alpha and beta.\n\n    If they are pd.Series, expects returns and factor_returns have already\n    been aligned on their labels.  If np.ndarray, these arguments should have\n    the same shape.\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    factor_returns : pd.Series or np.ndarray\n         Daily noncumulative returns of the factor to which beta is\n         computed. Usually a benchmark such as the market.\n         - This is in the same style as returns.\n    risk_free : int, float, optional\n        Constant risk-free return throughout the period. For example, the\n        interest rate on a three month us treasury bill.\n    period : str, optional\n        Defines the periodicity of the 'returns' data for purposes of\n        annualizing. Value ignored if `annualization` parameter is specified.\n        Defaults are::\n\n            'monthly':12\n            'weekly': 52\n            'daily': 252\n\n    annualization : int, optional\n        Used to suppress default values available in `period` to convert\n        returns into annual returns. Value should be the annual frequency of\n        `returns`.\n    out : array-like, optional\n        Array to use as output buffer.\n        If not passed, a new array will be created.\n\n    Returns\n    -------\n    alpha : float\n    beta : float\n    ",
    "empyrical.annual_return": "\n    Determines the mean annual growth rate of returns. This is equivilent\n    to the compound annual growth rate.\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Periodic returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    period : str, optional\n        Defines the periodicity of the 'returns' data for purposes of\n        annualizing. Value ignored if `annualization` parameter is specified.\n        Defaults are::\n\n            'monthly':12\n            'weekly': 52\n            'daily': 252\n\n    annualization : int, optional\n        Used to suppress default values available in `period` to convert\n        returns into annual returns. Value should be the annual frequency of\n        `returns`.\n\n    Returns\n    -------\n    annual_return : float\n        Annual Return as CAGR (Compounded Annual Growth Rate).\n\n    ",
    "empyrical.annual_volatility": "\n    Determines the annual volatility of a strategy.\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Periodic returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    period : str, optional\n        Defines the periodicity of the 'returns' data for purposes of\n        annualizing. Value ignored if `annualization` parameter is specified.\n        Defaults are::\n\n            'monthly':12\n            'weekly': 52\n            'daily': 252\n\n    alpha : float, optional\n        Scaling relation (Levy stability exponent).\n    annualization : int, optional\n        Used to suppress default values available in `period` to convert\n        returns into annual returns. Value should be the annual frequency of\n        `returns`.\n    out : array-like, optional\n        Array to use as output buffer.\n        If not passed, a new array will be created.\n\n    Returns\n    -------\n    annual_volatility : float\n    ",
    "empyrical.batting_average": "\n    Computes the batting average.\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    factor_returns : pd.Series or np.ndarray\n        Noncumulative returns of the factor to which beta is\n        computed. Usually a benchmark such as the market.\n        - This is in the same style as returns.\n    Returns\n    -------\n    batting_average : pd.Series\n        batting average, up market, down market\n    Note\n    ----\n    See https://www.investopedia.com/terms/b/batting-average.asp for\n    more information.\n    ",
    "empyrical.beta": "Calculates beta.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    factor_returns : pd.Series\n         Daily noncumulative returns of the factor to which beta is\n         computed. Usually a benchmark such as the market.\n         - This is in the same style as returns.\n    risk_free : int, float, optional\n        Constant risk-free return throughout the period. For example, the\n        interest rate on a three month us treasury bill.\n    out : array-like, optional\n        Array to use as output buffer.\n        If not passed, a new array will be created.\n\n    Returns\n    -------\n    beta : float\n    ",
    "empyrical.beta_aligned": "Calculates beta.\n\n    If they are pd.Series, expects returns and factor_returns have already\n    been aligned on their labels.  If np.ndarray, these arguments should have\n    the same shape.\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    factor_returns : pd.Series or np.ndarray\n         Daily noncumulative returns of the factor to which beta is\n         computed. Usually a benchmark such as the market.\n         - This is in the same style as returns.\n    risk_free : int, float, optional\n        Constant risk-free return throughout the period. For example, the\n        interest rate on a three month us treasury bill.\n    out : array-like, optional\n        Array to use as output buffer.\n        If not passed, a new array will be created.\n\n    Returns\n    -------\n    beta : float\n        Beta.\n    ",
    "empyrical.beta_fragility_heuristic": "Estimate fragility to drops in beta.\n\n        Parameters\n        ----------\n        returns : pd.Series or np.ndarray\n            Daily returns of the strategy, noncumulative.\n            - See full explanation in :func:`~empyrical.stats.cum_returns`.\n        factor_returns : pd.Series or np.ndarray\n             Daily noncumulative returns of the factor to which beta is\n             computed. Usually a benchmark such as the market.\n             - This is in the same style as returns.\n\n        Returns\n        -------\n        float, np.nan\n            The beta fragility of the strategy.\n\n        Note\n        ----\n        A negative return value indicates potential losses\n        could follow volatility in beta.\n        The magnitude of the negative value indicates the size of\n        the potential loss.\n        seealso::\n        `A New Heuristic Measure of Fragility and\n    Tail Risks: Application to Stress Testing`\n            https://www.imf.org/external/pubs/ft/wp/2012/wp12216.pdf\n            An IMF Working Paper describing the heuristic\n    ",
    "empyrical.beta_fragility_heuristic_aligned": "Estimate fragility to drops in beta\n\n        Parameters\n        ----------\n        returns : pd.Series or np.ndarray\n            Daily returns of the strategy, noncumulative.\n            - See full explanation in :func:`~empyrical.stats.cum_returns`.\n        factor_returns : pd.Series or np.ndarray\n             Daily noncumulative returns of the factor to which beta is\n             computed. Usually a benchmark such as the market.\n             - This is in the same style as returns.\n\n        Returns\n        -------\n        float, np.nan\n            The beta fragility of the strategy.\n\n        Note\n        ----\n        If they are pd.Series, expects returns and factor_returns have already\n        been aligned on their labels.  If np.ndarray, these arguments should\n        have the same shape.\n        seealso::\n        `A New Heuristic Measure of Fragility and\n    Tail Risks: Application to Stress Testing`\n            https://www.imf.org/external/pubs/ft/wp/2012/wp12216.pdf\n            An IMF Working Paper describing the heuristic\n    ",
    "empyrical.cagr": "\n    Compute compound annual growth rate. Alias function for\n    :func:`~empyrical.stats.annual_return`\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    period : str, optional\n        Defines the periodicity of the 'returns' data for purposes of\n        annualizing. Value ignored if `annualization` parameter is specified.\n        Defaults are::\n\n            'monthly':12\n            'weekly': 52\n            'daily': 252\n\n    annualization : int, optional\n        Used to suppress default values available in `period` to convert\n        returns into annual returns. Value should be the annual frequency of\n        `returns`.\n        - See full explanation in :func:`~empyrical.stats.annual_return`.\n\n    Returns\n    -------\n    cagr : float\n        The CAGR value.\n\n    ",
    "empyrical.calmar_ratio": "\n    Determines the Calmar ratio, or drawdown ratio, of a strategy.\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    period : str, optional\n        Defines the periodicity of the 'returns' data for purposes of\n        annualizing. Value ignored if `annualization` parameter is specified.\n        Defaults are::\n\n            'monthly':12\n            'weekly': 52\n            'daily': 252\n\n    annualization : int, optional\n        Used to suppress default values available in `period` to convert\n        returns into annual returns. Value should be the annual frequency of\n        `returns`.\n\n\n    Returns\n    -------\n    calmar_ratio : float\n        Calmar ratio (drawdown ratio) as float. Returns np.nan if there is no\n        calmar ratio.\n\n    Note\n    -----\n    See https://en.wikipedia.org/wiki/Calmar_ratio for more details.\n    ",
    "empyrical.capture": "Compute capture ratio.\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    factor_returns : pd.Series or np.ndarray\n        Noncumulative returns of the factor to which beta is\n        computed. Usually a benchmark such as the market.\n        - This is in the same style as returns.\n    period : str, optional\n        Defines the periodicity of the 'returns' data for purposes of\n        annualizing. Value ignored if `annualization` parameter is specified.\n        Defaults are::\n\n            'monthly':12\n            'weekly': 52\n            'daily': 252\n\n    Returns\n    -------\n    capture_ratio : float\n\n    Note\n    ----\n    See http://www.investopedia.com/terms/u/up-market-capture-ratio.asp for\n    details.\n    ",
    "empyrical.compute_exposures": "\n    Compute daily risk factor exposures.\n\n    Parameters\n    ----------\n    positions: pd.Series\n        A series of holdings as percentages indexed by date and ticker.\n        - Examples:\n            dt          ticker\n            2017-01-01  AAPL      0.417582\n                        TLT       0.010989\n                        XOM       0.571429\n            2017-01-02  AAPL      0.202381\n                        TLT       0.535714\n                        XOM       0.261905\n\n    factor_loadings : pd.DataFrame\n        Factor loadings for all days in the date range, with date and ticker as\n        index, and factors as columns.\n        - Example:\n                               momentum  reversal\n            dt         ticker\n            2017-01-01 AAPL   -1.592914  0.852830\n                       TLT     0.184864  0.895534\n                       XOM     0.993160  1.149353\n            2017-01-02 AAPL   -0.140009 -0.524952\n                       TLT    -1.066978  0.185435\n                       XOM    -1.798401  0.761549\n\n    Returns\n    -------\n    risk_exposures_portfolio : pd.DataFrame\n        df indexed by datetime, with factors as columns\n        - Example:\n                        momentum  reversal\n            dt\n            2017-01-01 -0.238655  0.077123\n            2017-01-02  0.821872  1.520515\n    ",
    "empyrical.conditional_value_at_risk": "\n    Conditional value at risk (CVaR) of a returns stream.\n\n    CVaR measures the expected single-day returns of an asset on that asset's\n    worst performing days, where \"worst-performing\" is defined as falling below\n    ``cutoff`` as a percentile of all daily returns.\n\n    Parameters\n    ----------\n    returns : pandas.Series or 1-D numpy.array\n        Non-cumulative daily returns.\n    cutoff : float, optional\n        Decimal representing the percentage cutoff for the bottom percentile of\n        returns. Defaults to 0.05.\n\n    Returns\n    -------\n    CVaR : float\n        The CVaR value.\n    ",
    "empyrical.cum_returns": "\n    Compute cumulative returns from simple returns.\n\n    Parameters\n    ----------\n    returns : pd.Series, np.ndarray, or pd.DataFrame\n        Returns of the strategy as a percentage, noncumulative.\n         - Time series with decimal returns.\n         - Example::\n\n            2015-07-16   -0.012143\n            2015-07-17    0.045350\n            2015-07-20    0.030957\n            2015-07-21    0.004902\n\n         - Also accepts two dimensional data. In this case, each column is\n           cumulated.\n\n    starting_value : float, optional\n       The starting returns.\n    out : array-like, optional\n        Array to use as output buffer.\n        If not passed, a new array will be created.\n\n    Returns\n    -------\n    cumulative_returns : array-like\n        Series of cumulative returns.\n    ",
    "empyrical.cum_returns_final": "\n    Compute total returns from simple returns.\n\n    Parameters\n    ----------\n    returns : pd.DataFrame, pd.Series, or np.ndarray\n       Noncumulative simple returns of one or more timeseries.\n    starting_value : float, optional\n       The starting returns.\n\n    Returns\n    -------\n    total_returns : pd.Series, np.ndarray, or float\n        If input is 1-dimensional (a Series or 1D numpy array), the result is a\n        scalar.\n\n        If input is 2-dimensional (a DataFrame or 2D numpy array), the result\n        is a 1D array containing cumulative returns for each column of input.\n    ",
    "empyrical.down_alpha_beta": "\n    Computes alpha and beta for periods when the benchmark return is negative.\n\n    Parameters\n    ----------\n    see documentation for `alpha_beta`.\n\n    Returns\n    -------\n    alpha : float\n    beta : float\n    ",
    "empyrical.down_capture": "\n    Compute the capture ratio for periods when the benchmark return is negative\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    factor_returns : pd.Series or np.ndarray\n        Noncumulative returns of the factor to which beta is\n        computed. Usually a benchmark such as the market.\n        - This is in the same style as returns.\n    period : str, optional\n        Defines the periodicity of the 'returns' data for purposes of\n        annualizing. Value ignored if `annualization` parameter is specified.\n        Defaults are::\n\n            'monthly':12\n            'weekly': 52\n            'daily': 252\n\n    Returns\n    -------\n    down_capture : float\n\n    Note\n    ----\n    See http://www.investopedia.com/terms/d/down-market-capture-ratio.asp for\n    more information.\n    ",
    "empyrical.downside_risk": "\n    Determines the downside deviation below a threshold\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray or pd.DataFrame\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    required_return: float / series\n        minimum acceptable return\n    period : str, optional\n        Defines the periodicity of the 'returns' data for purposes of\n        annualizing. Value ignored if `annualization` parameter is specified.\n        Defaults are::\n\n            'monthly':12\n            'weekly': 52\n            'daily': 252\n\n    annualization : int, optional\n        Used to suppress default values available in `period` to convert\n        returns into annual returns. Value should be the annual frequency of\n        `returns`.\n    out : array-like, optional\n        Array to use as output buffer.\n        If not passed, a new array will be created.\n\n    Returns\n    -------\n    downside_deviation : float or pd.Series\n        depends on input type\n        series ==> float\n        DataFrame ==> pd.Series\n\n    Note\n    -----\n    See `<https://www.sunrisecapital.com/wp-content/uploads/2014/06/Futures_\n    Mag_Sortino_0213.pdf>`__ for more details, specifically why using the\n    standard deviation of the negative returns is not correct.\n    ",
    "empyrical.excess_sharpe": "\n    Determines the Excess Sharpe of a strategy.\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    factor_returns: float / series\n        Benchmark return to compare returns against.\n    out : array-like, optional\n        Array to use as output buffer.\n        If not passed, a new array will be created.\n\n    Returns\n    -------\n    excess_sharpe : float\n\n    Note\n    -----\n    The excess Sharpe is a simplified Information Ratio that uses\n    tracking error rather than \"active risk\" as the denominator.\n    ",
    "empyrical.gpd_risk_estimates": "Estimate VaR and ES using the Generalized Pareto Distribution (GPD)\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    var_p : float\n        The percentile to use for estimating the VaR and ES\n\n    Returns\n    -------\n    [threshold, scale_param, shape_param, var_estimate, es_estimate]\n        : list[float]\n        threshold - the threshold use to cut off exception tail losses\n        scale_param - a parameter (often denoted by sigma, capturing the\n            scale, related to variance)\n        shape_param - a parameter (often denoted by xi, capturing the shape\n         or type of the distribution)\n        var_estimate - an estimate for the VaR for the given percentile\n        es_estimate - an estimate for the ES for the given percentile\n\n    Note\n    ----\n    see also::\n    `An Application of Extreme Value Theory for Measuring Risk\n    <https://link.springer.com/article/10.1007/s10614-006-9025-7>`\n        A paper describing how to use the Generalized Pareto\n        Distribution to estimate VaR and ES.\n    ",
    "empyrical.gpd_risk_estimates_aligned": "Estimate VaR and ES using the Generalized Pareto Distribution (GPD)\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    var_p : float\n        The percentile to use for estimating the VaR and ES\n\n    Returns\n    -------\n    [threshold, scale_param, shape_param, var_estimate, es_estimate]\n        : list[float]\n        threshold - the threshold use to cut off exception tail losses\n        scale_param - a parameter (often denoted by sigma, capturing the\n            scale, related to variance)\n        shape_param - a parameter (often denoted by xi, capturing the shape\n        or type of the distribution)\n        var_estimate - an estimate for the VaR for the given percentile\n        es_estimate - an estimate for the ES for the given percentile\n\n    Note\n    ----\n    seealso::\n    `An Application of Extreme Value Theory for Measuring Risk\n    <https://link.springer.com/article/10.1007/s10614-006-9025-7>`\n        A paper describing how to use the Generalized Pareto\n        Distribution to estimate VaR and ES.\n    ",
    "empyrical.max_drawdown": "\n    Determines the maximum drawdown of a strategy.\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    out : array-like, optional\n        Array to use as output buffer.\n        If not passed, a new array will be created.\n\n    Returns\n    -------\n    max_drawdown : float, np.array or pd.Series\n\n    Note\n    -----\n    See https://en.wikipedia.org/wiki/Drawdown_(economics) for more details.\n    ",
    "empyrical.omega_ratio": "Determines the Omega ratio of a strategy.\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    risk_free : int, float\n        Constant risk-free return throughout the period\n    required_return : float, optional\n        Minimum acceptance return of the investor. Threshold over which to\n        consider positive vs negative returns. It will be converted to a\n        value appropriate for the period of the returns. E.g. An annual minimum\n        acceptable return of 100 will translate to a minimum acceptable\n        return of 0.018.\n    annualization : int, optional\n        Factor used to convert the required_return into a daily\n        value. Enter 1 if no time period conversion is necessary.\n\n    Returns\n    -------\n    omega_ratio : float\n\n    Note\n    -----\n    See https://en.wikipedia.org/wiki/Omega_ratio for more details.\n\n    ",
    "empyrical.perf_attrib": "\n    Attributes the performance of a returns stream to a set of risk factors.\n\n    Performance attribution determines how much each risk factor, e.g.,\n    momentum, the technology sector, etc., contributed to total returns, as\n    well as the daily exposure to each of the risk factors. The returns that\n    can be attributed to one of the given risk factors are the\n    `common_returns`, and the returns that _cannot_ be attributed to a risk\n    factor are the `specific_returns`. The `common_returns` and\n    `specific_returns` summed together will always equal the total returns.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Returns for each day in the date range.\n        - Example:\n            2017-01-01   -0.017098\n            2017-01-02    0.002683\n            2017-01-03   -0.008669\n\n    positions: pd.Series\n        Daily holdings in percentages, indexed by date.\n        - Examples:\n            dt          ticker\n            2017-01-01  AAPL      0.417582\n                        TLT       0.010989\n                        XOM       0.571429\n            2017-01-02  AAPL      0.202381\n                        TLT       0.535714\n                        XOM       0.261905\n\n    factor_returns : pd.DataFrame\n        Returns by factor, with date as index and factors as columns\n        - Example:\n                        momentum  reversal\n            2017-01-01  0.002779 -0.005453\n            2017-01-02  0.001096  0.010290\n\n    factor_loadings : pd.DataFrame\n        Factor loadings for all days in the date range, with date and ticker as\n        index, and factors as columns.\n        - Example:\n                               momentum  reversal\n            dt         ticker\n            2017-01-01 AAPL   -1.592914  0.852830\n                       TLT     0.184864  0.895534\n                       XOM     0.993160  1.149353\n            2017-01-02 AAPL   -0.140009 -0.524952\n                       TLT    -1.066978  0.185435\n                       XOM    -1.798401  0.761549\n\n    Returns\n    -------\n    tuple of (risk_exposures_portfolio, perf_attribution)\n\n    risk_exposures_portfolio : pd.DataFrame\n        df indexed by datetime, with factors as columns\n        - Example:\n                        momentum  reversal\n            dt\n            2017-01-01 -0.238655  0.077123\n            2017-01-02  0.821872  1.520515\n\n    perf_attribution : pd.DataFrame\n        df with factors, common returns, and specific returns as columns,\n        and datetimes as index\n        - Example:\n                        momentum  reversal  common_returns  specific_returns\n            dt\n            2017-01-01  0.249087  0.935925        1.185012          1.185012\n            2017-01-02 -0.003194 -0.400786       -0.403980         -0.403980\n\n    Note\n    ----\n    See https://en.wikipedia.org/wiki/Performance_attribution for more details.\n    ",
    "empyrical.roll_alpha": "\n        Computes the alpha measure over a rolling window.\n\n        Parameters\n        ----------\n        lhs : array-like\n            The first array to pass to the rolling alpha.\n        rhs : array-like\n            The second array to pass to the rolling alpha.\n        window : int\n            Size of the rolling window in terms of the periodicity of the data.\n        out : array-like, optional\n            Array to use as output buffer.\n            If not passed, a new array will be created.\n        **kwargs\n            Forwarded to :func:`~empyrical.alpha`.\n\n        Returns\n        -------\n        rolling_alpha : array-like\n            The rolling alpha.\n        ",
    "empyrical.roll_alpha_aligned": "\n        Computes the alpha aligned measure over a rolling window.\n\n        Parameters\n        ----------\n        lhs : array-like\n            The first array to pass to the rolling alpha aligned.\n        rhs : array-like\n            The second array to pass to the rolling alpha aligned.\n        window : int\n            Size of the rolling window in terms of the periodicity of the data.\n        out : array-like, optional\n            Array to use as output buffer.\n            If not passed, a new array will be created.\n        **kwargs\n            Forwarded to :func:`~empyrical.alpha_aligned`.\n\n        Returns\n        -------\n        rolling_alpha_aligned : array-like\n            The rolling alpha aligned.\n        ",
    "empyrical.roll_alpha_beta": "\n    Computes alpha and beta over a rolling window.\n\n    Parameters\n    ----------\n    lhs : array-like\n        The first array to pass to the rolling alpha-beta.\n    rhs : array-like\n        The second array to pass to the rolling alpha-beta.\n    window : int\n        Size of the rolling window in terms of the periodicity of the data.\n    out : array-like, optional\n        Array to use as output buffer.\n        If not passed, a new array will be created.\n    **kwargs\n        Forwarded to :func:`~empyrical.alpha_beta`.\n    ",
    "empyrical.roll_alpha_beta_aligned": "\n        Computes the alpha beta aligned measure over a rolling window.\n\n        Parameters\n        ----------\n        lhs : array-like\n            The first array to pass to the rolling alpha beta aligned.\n        rhs : array-like\n            The second array to pass to the rolling alpha beta aligned.\n        window : int\n            Size of the rolling window in terms of the periodicity of the data.\n        out : array-like, optional\n            Array to use as output buffer.\n            If not passed, a new array will be created.\n        **kwargs\n            Forwarded to :func:`~empyrical.alpha_beta_aligned`.\n\n        Returns\n        -------\n        rolling_alpha_beta_aligned : array-like\n            The rolling alpha beta aligned.\n        ",
    "empyrical.roll_annual_volatility": "\n        Computes the annual volatility measure over a rolling window.\n\n        Parameters\n        ----------\n        arr : array-like\n            The array to compute the rolling annual volatility over.\n        window : int\n            Size of the rolling window in terms of the periodicity of the data.\n        out : array-like, optional\n            Array to use as output buffer.\n            If not passed, a new array will be created.\n        **kwargs\n            Forwarded to :func:`~empyrical.annual_volatility`.\n\n        Returns\n        -------\n        rolling_annual_volatility : array-like\n            The rolling annual volatility.\n        ",
    "empyrical.roll_beta": "\n        Computes the beta measure over a rolling window.\n\n        Parameters\n        ----------\n        lhs : array-like\n            The first array to pass to the rolling beta.\n        rhs : array-like\n            The second array to pass to the rolling beta.\n        window : int\n            Size of the rolling window in terms of the periodicity of the data.\n        out : array-like, optional\n            Array to use as output buffer.\n            If not passed, a new array will be created.\n        **kwargs\n            Forwarded to :func:`~empyrical.beta`.\n\n        Returns\n        -------\n        rolling_beta : array-like\n            The rolling beta.\n        ",
    "empyrical.roll_beta_aligned": "\n        Computes the beta aligned measure over a rolling window.\n\n        Parameters\n        ----------\n        lhs : array-like\n            The first array to pass to the rolling beta aligned.\n        rhs : array-like\n            The second array to pass to the rolling beta aligned.\n        window : int\n            Size of the rolling window in terms of the periodicity of the data.\n        out : array-like, optional\n            Array to use as output buffer.\n            If not passed, a new array will be created.\n        **kwargs\n            Forwarded to :func:`~empyrical.beta_aligned`.\n\n        Returns\n        -------\n        rolling_beta_aligned : array-like\n            The rolling beta aligned.\n        ",
    "empyrical.roll_down_capture": "\n    Computes the down capture measure over a rolling window.\n    see documentation for :func:`~empyrical.stats.down_capture`.\n    (pass all args, kwargs required)\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n\n    factor_returns : pd.Series or np.ndarray\n        Noncumulative returns of the factor to which beta is\n        computed. Usually a benchmark such as the market.\n        - This is in the same style as returns.\n\n    window : int, required\n        Size of the rolling window in terms of the periodicity of the data.\n        - eg window = 60, periodicity=DAILY, represents a rolling 60 day window\n    ",
    "empyrical.roll_max_drawdown": "\n        Computes the max drawdown measure over a rolling window.\n\n        Parameters\n        ----------\n        arr : array-like\n            The array to compute the rolling max drawdown over.\n        window : int\n            Size of the rolling window in terms of the periodicity of the data.\n        out : array-like, optional\n            Array to use as output buffer.\n            If not passed, a new array will be created.\n        **kwargs\n            Forwarded to :func:`~empyrical.max_drawdown`.\n\n        Returns\n        -------\n        rolling_max_drawdown : array-like\n            The rolling max drawdown.\n        ",
    "empyrical.roll_sharpe_ratio": "\n        Computes the sharpe ratio measure over a rolling window.\n\n        Parameters\n        ----------\n        arr : array-like\n            The array to compute the rolling sharpe ratio over.\n        window : int\n            Size of the rolling window in terms of the periodicity of the data.\n        out : array-like, optional\n            Array to use as output buffer.\n            If not passed, a new array will be created.\n        **kwargs\n            Forwarded to :func:`~empyrical.sharpe_ratio`.\n\n        Returns\n        -------\n        rolling_sharpe_ratio : array-like\n            The rolling sharpe ratio.\n        ",
    "empyrical.roll_sortino_ratio": "\n        Computes the sortino ratio measure over a rolling window.\n\n        Parameters\n        ----------\n        arr : array-like\n            The array to compute the rolling sortino ratio over.\n        window : int\n            Size of the rolling window in terms of the periodicity of the data.\n        out : array-like, optional\n            Array to use as output buffer.\n            If not passed, a new array will be created.\n        **kwargs\n            Forwarded to :func:`~empyrical.sortino_ratio`.\n\n        Returns\n        -------\n        rolling_sortino_ratio : array-like\n            The rolling sortino ratio.\n        ",
    "empyrical.roll_up_capture": "\n    Computes the up capture measure over a rolling window.\n    see documentation for :func:`~empyrical.stats.up_capture`.\n    (pass all args, kwargs required)\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n\n    factor_returns : pd.Series or np.ndarray\n        Noncumulative returns of the factor to which beta is\n        computed. Usually a benchmark such as the market.\n        - This is in the same style as returns.\n\n    window : int, required\n        Size of the rolling window in terms of the periodicity of the data.\n        - eg window = 60, periodicity=DAILY, represents a rolling 60 day window\n    ",
    "empyrical.roll_up_down_capture": "\n    Computes the up/down capture measure over a rolling window.\n    see documentation for :func:`~empyrical.stats.up_down_capture`.\n    (pass all args, kwargs required)\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n\n    factor_returns : pd.Series or np.ndarray\n        Noncumulative returns of the factor to which beta is\n        computed. Usually a benchmark such as the market.\n        - This is in the same style as returns.\n\n    window : int, required\n        Size of the rolling window in terms of the periodicity of the data.\n        - eg window = 60, periodicity=DAILY, represents a rolling 60 day window\n    ",
    "empyrical.sharpe_ratio": "\n    Determines the Sharpe ratio of a strategy.\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    risk_free : int, float\n        Constant daily risk-free return throughout the period.\n    period : str, optional\n        Defines the periodicity of the 'returns' data for purposes of\n        annualizing. Value ignored if `annualization` parameter is specified.\n        Defaults are::\n\n            'monthly':12\n            'weekly': 52\n            'daily': 252\n\n    annualization : int, optional\n        Used to suppress default values available in `period` to convert\n        returns into annual returns. Value should be the annual frequency of\n        `returns`.\n    out : array-like, optional\n        Array to use as output buffer.\n        If not passed, a new array will be created.\n\n    Returns\n    -------\n    sharpe_ratio : float\n        nan if insufficient length of returns or if if adjusted returns are 0.\n\n    Note\n    -----\n    See https://en.wikipedia.org/wiki/Sharpe_ratio for more details.\n\n    ",
    "empyrical.simple_returns": "\n    Compute simple returns from a timeseries of prices.\n\n    Parameters\n    ----------\n    prices : pd.Series, pd.DataFrame or np.ndarray\n        Prices of assets in wide-format, with assets as columns,\n        and indexed by datetimes.\n\n    Returns\n    -------\n    returns : array-like\n        Returns of assets in wide-format, with assets as columns,\n        and index coerced to be tz-aware.\n    ",
    "empyrical.sortino_ratio": "\n    Determines the Sortino ratio of a strategy.\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray or pd.DataFrame\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    required_return: float / series\n        minimum acceptable return\n    period : str, optional\n        Defines the periodicity of the 'returns' data for purposes of\n        annualizing. Value ignored if `annualization` parameter is specified.\n        Defaults are::\n\n            'monthly':12\n            'weekly': 52\n            'daily': 252\n\n    annualization : int, optional\n        Used to suppress default values available in `period` to convert\n        returns into annual returns. Value should be the annual frequency of\n        `returns`.\n    _downside_risk : float, optional\n        The downside risk of the given inputs, if known. Will be calculated if\n        not provided.\n    out : array-like, optional\n        Array to use as output buffer.\n        If not passed, a new array will be created.\n\n    Returns\n    -------\n    sortino_ratio : float or pd.Series\n\n        depends on input type\n        series ==> float\n        DataFrame ==> pd.Series\n\n    Note\n    -----\n    See `<https://www.sunrisecapital.com/wp-content/uploads/2014/06/Futures_\n    Mag_Sortino_0213.pdf>`__ for more details.\n\n    ",
    "empyrical.stability_of_timeseries": "Determines R-squared of a linear fit to the cumulative\n    log returns. Computes an ordinary least squares linear fit,\n    and returns R-squared.\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Daily returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n\n    Returns\n    -------\n    float\n        R-squared.\n\n    ",
    "empyrical.tail_ratio": "Determines the ratio between the right (95%) and left tail (5%).\n\n    For example, a ratio of 0.25 means that losses are four times\n    as bad as profits.\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in :func:`~empyrical.stats.cum_returns`.\n\n    Returns\n    -------\n    tail_ratio : float\n    ",
    "empyrical.up_alpha_beta": "\n    Computes alpha and beta for periods when the benchmark return is positive.\n\n    Parameters\n    ----------\n    see documentation for `alpha_beta`.\n\n    Returns\n    -------\n    float\n        Alpha.\n    float\n        Beta.\n    ",
    "empyrical.up_capture": "\n    Compute the capture ratio for periods when the benchmark return is positive\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    factor_returns : pd.Series or np.ndarray\n        Noncumulative returns of the factor to which beta is\n        computed. Usually a benchmark such as the market.\n        - This is in the same style as returns.\n    period : str, optional\n        Defines the periodicity of the 'returns' data for purposes of\n        annualizing. Value ignored if `annualization` parameter is specified.\n        Defaults are::\n\n            'monthly':12\n            'weekly': 52\n            'daily': 252\n\n    Returns\n    -------\n    up_capture : float\n\n    Note\n    ----\n    See http://www.investopedia.com/terms/u/up-market-capture-ratio.asp for\n    more information.\n    ",
    "empyrical.up_down_capture": "\n    Computes the ratio of up_capture to down_capture.\n\n    Parameters\n    ----------\n    returns : pd.Series or np.ndarray\n        Returns of the strategy, noncumulative.\n        - See full explanation in :func:`~empyrical.stats.cum_returns`.\n    factor_returns : pd.Series or np.ndarray\n        Noncumulative returns of the factor to which beta is\n        computed. Usually a benchmark such as the market.\n        - This is in the same style as returns.\n    period : str, optional\n        Defines the periodicity of the 'returns' data for purposes of\n        annualizing. Value ignored if `annualization` parameter is specified.\n        Defaults are::\n\n            'monthly':12\n            'weekly': 52\n            'daily': 252\n\n    Returns\n    -------\n    up_down_capture : float\n        the updown capture ratio\n    ",
    "empyrical.value_at_risk": "\n    Value at risk (VaR) of a returns stream.\n\n    Parameters\n    ----------\n    returns : pandas.Series or 1-D numpy.array\n        Non-cumulative daily returns.\n    cutoff : float, optional\n        Decimal representing the percentage cutoff for the bottom percentile of\n        returns. Defaults to 0.05.\n\n    Returns\n    -------\n    VaR : float\n        The VaR value.\n    ",
    "pyfolio.FuncFormatter": "\n    Use a user-defined function for formatting.\n\n    The function should take in two inputs (a tick value ``x`` and a\n    position ``pos``), and return a string containing the corresponding\n    tick label.\n    ",
    "pyfolio.OrderedDict": "Dictionary that remembers insertion order",
    "pyfolio.axes_style": "\n    Create pyfolio default axes style context.\n\n    Under the hood, calls and returns seaborn.axes_style() with\n    some custom settings. Usually you would use in a with-context.\n\n    Parameters\n    ----------\n    style : str, optional\n        Name of seaborn style.\n    rc : dict, optional\n        Config flags.\n\n    Returns\n    -------\n    seaborn plotting context\n\n    Example\n    -------\n    >>> with pyfolio.plotting.axes_style(style='whitegrid'):\n    >>>    pyfolio.create_full_tear_sheet(..., set_context=False)\n\n    See also\n    --------\n    For more information, see seaborn.plotting_context().\n\n    ",
    "pyfolio.create_capacity_tear_sheet": "\n    Generates a report detailing portfolio size constraints set by\n    least liquid tickers. Plots a \"capacity sweep,\" a curve describing\n    projected sharpe ratio given the slippage penalties that are\n    applied at various capital bases.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n    market_data : pd.DataFrame\n        Daily market_data\n        - DataFrame has a multi-index index, one level is dates and another is\n        market_data contains volume & price, equities as columns\n    liquidation_daily_vol_limit : float\n        Max proportion of a daily bar that can be consumed in the\n        process of liquidating a position in the\n        \"days to liquidation\" analysis.\n    trade_daily_vol_limit : float\n        Flag daily transaction totals that exceed proportion of\n        daily bar.\n    last_n_days : integer\n        Compute max position allocation and dollar volume for only\n        the last N days of the backtest\n    days_to_liquidate_limit : integer\n        Display all tickers with greater max days to liquidation.\n    estimate_intraday: boolean or str, optional\n        Approximate returns for intraday strategies.\n        See description in create_full_tear_sheet.\n    return_fig : boolean, optional\n        If True, returns the figure that was plotted on.\n    ",
    "pyfolio.create_full_tear_sheet": "\n    Generate a number of tear sheets that are useful\n    for analyzing a strategy's performance.\n\n    - Fetches benchmarks if needed.\n    - Creates tear sheets for returns, and significant events.\n        If possible, also creates tear sheets for position analysis\n        and transaction analysis.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - Time series with decimal returns.\n         - Example:\n            2015-07-16    -0.012143\n            2015-07-17    0.045350\n            2015-07-20    0.030957\n            2015-07-21    0.004902\n    positions : pd.DataFrame, optional\n        Daily net position values.\n         - Time series of dollar amount invested in each position and cash.\n         - Days where stocks are not held can be represented by 0 or NaN.\n         - Non-working capital is labelled 'cash'\n         - Example:\n            index         'AAPL'         'MSFT'          cash\n            2004-01-09    13939.3800     -14012.9930     711.5585\n            2004-01-12    14492.6300     -14624.8700     27.1821\n            2004-01-13    -13853.2800    13653.6400      -43.6375\n    transactions : pd.DataFrame, optional\n        Executed trade volumes and fill prices.\n        - One row per trade.\n        - Trades on different names that occur at the\n          same time will have identical indicies.\n        - Example:\n            index                  amount   price    symbol\n            2004-01-09 12:18:01    483      324.12   'AAPL'\n            2004-01-09 12:18:01    122      83.10    'MSFT'\n            2004-01-13 14:12:23    -75      340.43   'AAPL'\n    market_data : pd.DataFrame, optional\n        Daily market_data\n        - DataFrame has a multi-index index, one level is dates and another is\n        market_data contains volume & price, equities as columns\n    slippage : int/float, optional\n        Basis points of slippage to apply to returns before generating\n        tearsheet stats and plots.\n        If a value is provided, slippage parameter sweep\n        plots will be generated from the unadjusted returns.\n        Transactions and positions must also be passed.\n        - See txn.adjust_returns_for_slippage for more details.\n    live_start_date : datetime, optional\n        The point in time when the strategy began live trading,\n        after its backtest period. This datetime should be normalized.\n    hide_positions : bool, optional\n        If True, will not output any symbol names.\n    round_trips: boolean, optional\n        If True, causes the generation of a round trip tear sheet.\n    sector_mappings : dict or pd.Series, optional\n        Security identifier to sector mapping.\n        Security ids as keys, sectors as values.\n    estimate_intraday: boolean or str, optional\n        Instead of using the end-of-day positions, use the point in the day\n        where we have the most $ invested. This will adjust positions to\n        better approximate and represent how an intraday strategy behaves.\n        By default, this is 'infer', and an attempt will be made to detect\n        an intraday strategy. Specifying this value will prevent detection.\n    cone_std : float, or tuple, optional\n        If float, The standard deviation to use for the cone plots.\n        If tuple, Tuple of standard deviation values to use for the cone plots\n         - The cone is a normal distribution with this standard deviation\n             centered around a linear regression.\n    bootstrap : boolean (optional)\n        Whether to perform bootstrap analysis for the performance\n        metrics. Takes a few minutes longer.\n    turnover_denom : str\n        Either AGB or portfolio_value, default AGB.\n        - See full explanation in txn.get_turnover.\n    factor_returns : pd.Dataframe, optional\n        Returns by factor, with date as index and factors as columns\n    factor_loadings : pd.Dataframe, optional\n        Factor loadings for all days in the date range, with date and\n        ticker as index, and factors as columns.\n    pos_in_dollars : boolean, optional\n        indicates whether positions is in dollars\n    header_rows : dict or OrderedDict, optional\n        Extra rows to display at the top of the perf stats table.\n    set_context : boolean, optional\n        If True, set default plotting style context.\n         - See plotting.context().\n    factor_partitions : dict, optional\n        dict specifying how factors should be separated in perf attrib\n        factor returns and risk exposures plots\n        - See create_perf_attrib_tear_sheet().\n    ",
    "pyfolio.create_interesting_times_tear_sheet": "\n    Generate a number of returns plots around interesting points in time,\n    like the flash crash and 9/11.\n\n    Plots: returns around the dotcom bubble burst, Lehmann Brothers' failure,\n    9/11, US downgrade and EU debt crisis, Fukushima meltdown, US housing\n    bubble burst, EZB IR, Great Recession (August 2007, March and September\n    of 2008, Q1 & Q2 2009), flash crash, April and October 2014.\n\n    benchmark_rets must be passed, as it is meaningless to analyze performance\n    during interesting times without some benchmark to refer to.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    benchmark_rets : pd.Series\n        Daily noncumulative returns of the benchmark.\n         - This is in the same style as returns.\n    periods: dict or OrderedDict, optional\n        historical event dates that may have had significant\n        impact on markets\n    legend_loc : plt.legend_loc, optional\n         The legend's location.\n    return_fig : boolean, optional\n        If True, returns the figure that was plotted on.\n    ",
    "pyfolio.create_perf_attrib_tear_sheet": "\n    Generate plots and tables for analyzing a strategy's performance.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Returns for each day in the date range.\n\n    positions: pd.DataFrame\n        Daily holdings (in dollars or percentages), indexed by date.\n        Will be converted to percentages if positions are in dollars.\n        Short positions show up as cash in the 'cash' column.\n\n    factor_returns : pd.DataFrame\n        Returns by factor, with date as index and factors as columns\n\n    factor_loadings : pd.DataFrame\n        Factor loadings for all days in the date range, with date\n        and ticker as index, and factors as columns.\n\n    transactions : pd.DataFrame, optional\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n         - Default is None.\n\n    pos_in_dollars : boolean, optional\n        Flag indicating whether `positions` are in dollars or percentages\n        If True, positions are in dollars.\n\n    factor_partitions : dict\n        dict specifying how factors should be separated in factor returns\n        and risk exposures plots\n        - Example:\n          {'style': ['momentum', 'size', 'value', ...],\n           'sector': ['technology', 'materials', ... ]}\n\n    return_fig : boolean, optional\n        If True, returns the figure that was plotted on.\n    ",
    "pyfolio.create_position_tear_sheet": "\n    Generate a number of plots for analyzing a\n    strategy's positions and holdings.\n\n    - Plots: gross leverage, exposures, top positions, and holdings.\n    - Will also print the top positions held.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    show_and_plot_top_pos : int, optional\n        By default, this is 2, and both prints and plots the\n        top 10 positions.\n        If this is 0, it will only plot; if 1, it will only print.\n    hide_positions : bool, optional\n        If True, will not output any symbol names.\n        Overrides show_and_plot_top_pos to 0 to suppress text output.\n    sector_mappings : dict or pd.Series, optional\n        Security identifier to sector mapping.\n        Security ids as keys, sectors as values.\n    transactions : pd.DataFrame, optional\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n    estimate_intraday: boolean or str, optional\n        Approximate returns for intraday strategies.\n        See description in create_full_tear_sheet.\n    return_fig : boolean, optional\n        If True, returns the figure that was plotted on.\n    ",
    "pyfolio.create_returns_tear_sheet": "\n    Generate a number of plots for analyzing a strategy's returns.\n\n    - Fetches benchmarks, then creates the plots on a single figure.\n    - Plots: rolling returns (with cone), rolling beta, rolling sharpe,\n        rolling Fama-French risk factors, drawdowns, underwater plot, monthly\n        and annual return plots, daily similarity plots,\n        and return quantile box plot.\n    - Will also print the start and end dates of the strategy,\n        performance statistics, drawdown periods, and the return range.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    positions : pd.DataFrame, optional\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame, optional\n        Executed trade volumes and fill prices.\n        - See full explanation in create_full_tear_sheet.\n    live_start_date : datetime, optional\n        The point in time when the strategy began live trading,\n        after its backtest period.\n    cone_std : float, or tuple, optional\n        If float, The standard deviation to use for the cone plots.\n        If tuple, Tuple of standard deviation values to use for the cone plots\n         - The cone is a normal distribution with this standard deviation\n             centered around a linear regression.\n    benchmark_rets : pd.Series, optional\n        Daily noncumulative returns of the benchmark.\n         - This is in the same style as returns.\n    bootstrap : boolean, optional\n        Whether to perform bootstrap analysis for the performance\n        metrics. Takes a few minutes longer.\n    turnover_denom : str, optional\n        Either AGB or portfolio_value, default AGB.\n        - See full explanation in txn.get_turnover.\n    header_rows : dict or OrderedDict, optional\n        Extra rows to display at the top of the perf stats table.\n    return_fig : boolean, optional\n        If True, returns the figure that was plotted on.\n    ",
    "pyfolio.create_round_trip_tear_sheet": "\n    Generate a number of figures and plots describing the duration,\n    frequency, and profitability of trade \"round trips.\"\n    A round trip is started when a new long or short position is\n    opened and is only completed when the number of shares in that\n    position returns to or crosses zero.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n    sector_mappings : dict or pd.Series, optional\n        Security identifier to sector mapping.\n        Security ids as keys, sectors as values.\n    estimate_intraday: boolean or str, optional\n        Approximate returns for intraday strategies.\n        See description in create_full_tear_sheet.\n    return_fig : boolean, optional\n        If True, returns the figure that was plotted on.\n    ",
    "pyfolio.create_simple_tear_sheet": "\n    Simpler version of create_full_tear_sheet; generates summary performance\n    statistics and important plots as a single image.\n\n    - Plots: cumulative returns, rolling beta, rolling Sharpe, underwater,\n        exposure, top 10 holdings, total holdings, long/short holdings,\n        daily turnover, transaction time distribution.\n    - Never accept market_data input (market_data = None)\n    - Never accept sector_mappings input (sector_mappings = None)\n    - Never perform bootstrap analysis (bootstrap = False)\n    - Never hide posistions on top 10 holdings plot (hide_positions = False)\n    - Always use default cone_std (cone_std = (1.0, 1.5, 2.0))\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - Time series with decimal returns.\n         - Example:\n            2015-07-16    -0.012143\n            2015-07-17    0.045350\n            2015-07-20    0.030957\n            2015-07-21    0.004902\n    positions : pd.DataFrame, optional\n        Daily net position values.\n         - Time series of dollar amount invested in each position and cash.\n         - Days where stocks are not held can be represented by 0 or NaN.\n         - Non-working capital is labelled 'cash'\n         - Example:\n            index         'AAPL'         'MSFT'          cash\n            2004-01-09    13939.3800     -14012.9930     711.5585\n            2004-01-12    14492.6300     -14624.8700     27.1821\n            2004-01-13    -13853.2800    13653.6400      -43.6375\n    transactions : pd.DataFrame, optional\n        Executed trade volumes and fill prices.\n        - One row per trade.\n        - Trades on different names that occur at the\n          same time will have identical indicies.\n        - Example:\n            index                  amount   price    symbol\n            2004-01-09 12:18:01    483      324.12   'AAPL'\n            2004-01-09 12:18:01    122      83.10    'MSFT'\n            2004-01-13 14:12:23    -75      340.43   'AAPL'\n    benchmark_rets : pd.Series, optional\n        Daily returns of the benchmark, noncumulative.\n    slippage : int/float, optional\n        Basis points of slippage to apply to returns before generating\n        tearsheet stats and plots.\n        If a value is provided, slippage parameter sweep\n        plots will be generated from the unadjusted returns.\n        Transactions and positions must also be passed.\n        - See txn.adjust_returns_for_slippage for more details.\n    live_start_date : datetime, optional\n        The point in time when the strategy began live trading,\n        after its backtest period. This datetime should be normalized.\n    turnover_denom : str, optional\n        Either AGB or portfolio_value, default AGB.\n        - See full explanation in txn.get_turnover.\n    header_rows : dict or OrderedDict, optional\n        Extra rows to display at the top of the perf stats table.\n    set_context : boolean, optional\n        If True, set default plotting style context.\n    ",
    "pyfolio.create_txn_tear_sheet": "\n    Generate a number of plots for analyzing a strategy's transactions.\n\n    Plots: turnover, daily volume, and a histogram of daily volume.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in create_full_tear_sheet.\n    turnover_denom : str, optional\n        Either AGB or portfolio_value, default AGB.\n        - See full explanation in txn.get_turnover.\n    unadjusted_returns : pd.Series, optional\n        Daily unadjusted returns of the strategy, noncumulative.\n        Will plot additional swippage sweep analysis.\n         - See pyfolio.plotting.plot_swippage_sleep and\n           pyfolio.plotting.plot_slippage_sensitivity\n    estimate_intraday: boolean or str, optional\n        Approximate returns for intraday strategies.\n        See description in create_full_tear_sheet.\n    return_fig : boolean, optional\n        If True, returns the figure that was plotted on.\n    ",
    "pyfolio.customize": "\n    Decorator to set plotting context and axes style during function call.\n    ",
    "pyfolio.display": "Display a Python object in all frontends.\n\n    By default all representations will be computed and sent to the frontends.\n    Frontends can decide which representation is used and how.\n\n    In terminal IPython this will be similar to using :func:`print`, for use in richer\n    frontends see Jupyter notebook examples with rich display logic.\n\n    Parameters\n    ----------\n    *objs : object\n        The Python objects to display.\n    raw : bool, optional\n        Are the objects to be displayed already mimetype-keyed dicts of raw display data,\n        or Python objects that need to be formatted before display? [default: False]\n    include : list, tuple or set, optional\n        A list of format type strings (MIME types) to include in the\n        format data dict. If this is set *only* the format types included\n        in this list will be computed.\n    exclude : list, tuple or set, optional\n        A list of format type strings (MIME types) to exclude in the format\n        data dict. If this is set all format types will be computed,\n        except for those included in this argument.\n    metadata : dict, optional\n        A dictionary of metadata to associate with the output.\n        mime-type keys in this dictionary will be associated with the individual\n        representation formats, if they exist.\n    transient : dict, optional\n        A dictionary of transient data to associate with the output.\n        Data in this dict should not be persisted to files (e.g. notebooks).\n    display_id : str, bool optional\n        Set an id for the display.\n        This id can be used for updating this display area later via update_display.\n        If given as `True`, generate a new `display_id`\n    clear : bool, optional\n        Should the output area be cleared before displaying anything? If True,\n        this will wait for additional output before clearing. [default: False]\n    **kwargs : additional keyword-args, optional\n        Additional keyword-arguments are passed through to the display publisher.\n\n    Returns\n    -------\n    handle: DisplayHandle\n        Returns a handle on updatable displays for use with :func:`update_display`,\n        if `display_id` is given. Returns :any:`None` if no `display_id` is given\n        (default).\n\n    Examples\n    --------\n    >>> class Json(object):\n    ...     def __init__(self, json):\n    ...         self.json = json\n    ...     def _repr_pretty_(self, pp, cycle):\n    ...         import json\n    ...         pp.text(json.dumps(self.json, indent=2))\n    ...     def __repr__(self):\n    ...         return str(self.json)\n    ...\n\n    >>> d = Json({1:2, 3: {4:5}})\n\n    >>> print(d)\n    {1: 2, 3: {4: 5}}\n\n    >>> display(d)\n    {\n      \"1\": 2,\n      \"3\": {\n        \"4\": 5\n      }\n    }\n\n    >>> def int_formatter(integer, pp, cycle):\n    ...     pp.text('I'*integer)\n\n    >>> plain = get_ipython().display_formatter.formatters['text/plain']\n    >>> plain.for_type(int, int_formatter)\n    <function _repr_pprint at 0x...>\n    >>> display(7-5)\n    II\n\n    >>> del plain.type_printers[int]\n    >>> display(7-5)\n    2\n\n    See Also\n    --------\n    :func:`update_display`\n\n    Notes\n    -----\n    In Python, objects can declare their textual representation using the\n    `__repr__` method. IPython expands on this idea and allows objects to declare\n    other, rich representations including:\n\n      - HTML\n      - JSON\n      - PNG\n      - JPEG\n      - SVG\n      - LaTeX\n\n    A single object can declare some or all of these representations; all are\n    handled by IPython's display system.\n\n    The main idea of the first approach is that you have to implement special\n    display methods when you define your class, one for each representation you\n    want to use. Here is a list of the names of the special methods and the\n    values they must return:\n\n      - `_repr_html_`: return raw HTML as a string, or a tuple (see below).\n      - `_repr_json_`: return a JSONable dict, or a tuple (see below).\n      - `_repr_jpeg_`: return raw JPEG data, or a tuple (see below).\n      - `_repr_png_`: return raw PNG data, or a tuple (see below).\n      - `_repr_svg_`: return raw SVG data as a string, or a tuple (see below).\n      - `_repr_latex_`: return LaTeX commands in a string surrounded by \"$\",\n                        or a tuple (see below).\n      - `_repr_mimebundle_`: return a full mimebundle containing the mapping\n                             from all mimetypes to data.\n                             Use this for any mime-type not listed above.\n\n    The above functions may also return the object's metadata alonside the\n    data.  If the metadata is available, the functions will return a tuple\n    containing the data and metadata, in that order.  If there is no metadata\n    available, then the functions will return the data only.\n\n    When you are directly writing your own classes, you can adapt them for\n    display in IPython by following the above approach. But in practice, you\n    often need to work with existing classes that you can't easily modify.\n\n    You can refer to the documentation on integrating with the display system in\n    order to register custom formatters for already existing types\n    (:ref:`integrating_rich_display`).\n\n    .. versionadded:: 5.4 display available without import\n    .. versionadded:: 6.1 display available without import\n\n    Since IPython 5.4 and 6.1 :func:`display` is automatically made available to\n    the user without import. If you are using display in a document that might\n    be used in a pure python context or with older version of IPython, use the\n    following import at the top of your file::\n\n        from IPython.display import display\n\n    ",
    "pyfolio.plot_annual_returns": "\n    Plots a bar graph of returns by year.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_cones": "\n    Plots the upper and lower bounds of an n standard deviation\n    cone of forecasted cumulative returns. Redraws a new cone when\n    cumulative returns fall outside of last cone drawn.\n\n    Parameters\n    ----------\n    name : str\n        Account name to be used as figure title.\n    bounds : pandas.core.frame.DataFrame\n        Contains upper and lower cone boundaries. Column names are\n        strings corresponding to the number of standard devations\n        above (positive) or below (negative) the projected mean\n        cumulative returns.\n    oos_returns : pandas.core.frame.DataFrame\n        Non-cumulative out-of-sample returns.\n    num_samples : int\n        Number of samples to draw from the in-sample daily returns.\n        Each sample will be an array with length num_days.\n        A higher number of samples will generate a more accurate\n        bootstrap cone.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    cone_std : list of int/float\n        Number of standard devations to use in the boundaries of\n        the cone. If multiple values are passed, cone bounds will\n        be generated for each value.\n    random_seed : int\n        Seed for the pseudorandom number generator used by the pandas\n        sample method.\n    num_strikes : int\n        Upper limit for number of cones drawn. Can be anything from 0 to 3.\n\n    Returns\n    -------\n    Returns are either an ax or fig option, but not both. If a\n    matplotlib.Axes instance is passed in as ax, then it will be modified\n    and returned. This allows for users to plot interactively in jupyter\n    notebook. When no ax object is passed in, a matplotlib.figure instance\n    is generated and returned. This figure can then be used to save\n    the plot as an image without viewing it.\n\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    fig : matplotlib.figure\n        The figure instance which contains all the plot elements.\n    ",
    "pyfolio.plot_daily_turnover_hist": "\n    Plots a histogram of daily turnover rates.\n\n    Parameters\n    ----------\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    turnover_denom : str, optional\n        Either AGB or portfolio_value, default AGB.\n        - See full explanation in txn.get_turnover.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to seaborn plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_daily_volume": "\n    Plots trading volume per day vs. date.\n\n    Also displays all-time daily average.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_drawdown_periods": "\n    Plots cumulative returns highlighting top drawdown periods.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    top : int, optional\n        Amount of top drawdowns periods to plot (default 10).\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_drawdown_underwater": "\n    Plots how far underwaterr returns are over time, or plots current\n    drawdown vs. date.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_exposures": "\n    Plots a cake chart of the long and short exposure.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions_alloc : pd.DataFrame\n        Portfolio allocation of positions. See\n        pos.get_percent_alloc.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_gross_leverage": "\n    Plots gross leverage versus date.\n\n    Gross leverage is the sum of long and short exposure per share\n    divided by net asset value.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_holdings": "\n    Plots total amount of stocks with an active position, either short\n    or long. Displays daily total, daily average per month, and\n    all-time daily average.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions : pd.DataFrame, optional\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_long_short_holdings": "\n    Plots total amount of stocks with an active position, breaking out\n    short and long into transparent filled regions.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions : pd.DataFrame, optional\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n\n    ",
    "pyfolio.plot_max_median_position_concentration": "\n    Plots the max and median of long and short position concentrations\n    over the time.\n\n    Parameters\n    ----------\n    positions : pd.DataFrame\n        The positions that the strategy takes over time.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_monthly_returns_dist": "\n    Plots a distribution of monthly returns.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_monthly_returns_heatmap": "\n    Plots a heatmap of returns by month.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to seaborn plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_monthly_returns_timeseries": "\n    Plots monthly returns as a timeseries.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to seaborn plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_perf_stats": "\n    Create box plot of some performance metrics of the strategy.\n    The width of the box whiskers is determined by a bootstrap.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - This is in the same style as returns.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_prob_profit_trade": "\n    Plots a probability distribution for the event of making\n    a profitable trade.\n\n    Parameters\n    ----------\n    round_trips : pd.DataFrame\n        DataFrame with one row per round trip trade.\n        - See full explanation in round_trips.extract_round_trips\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_return_quantiles": "\n    Creates a box plot of daily, weekly, and monthly return\n    distributions.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    live_start_date : datetime, optional\n        The point in time when the strategy began live trading, after\n        its backtest period.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to seaborn plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_returns": "\n    Plots raw returns over time.\n\n    Backtest returns are in green, and out-of-sample (live trading)\n    returns are in red.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    live_start_date : datetime, optional\n        The date when the strategy began live trading, after\n        its backtest period. This date should be normalized.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_rolling_beta": "\n    Plots the rolling 6-month and 12-month beta versus date.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - This is in the same style as returns.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_rolling_returns": "\n    Plots cumulative rolling returns versus some benchmarks'.\n\n    Backtest returns are in green, and out-of-sample (live trading)\n    returns are in red.\n\n    Additionally, a non-parametric cone plot may be added to the\n    out-of-sample returns region.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series, optional\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - This is in the same style as returns.\n    live_start_date : datetime, optional\n        The date when the strategy began live trading, after\n        its backtest period. This date should be normalized.\n    logy : bool, optional\n        Whether to log-scale the y-axis.\n    cone_std : float, or tuple, optional\n        If float, The standard deviation to use for the cone plots.\n        If tuple, Tuple of standard deviation values to use for the cone plots\n         - See timeseries.forecast_cone_bounds for more details.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n    volatility_match : bool, optional\n        Whether to normalize the volatility of the returns to those of the\n        benchmark returns. This helps compare strategies with different\n        volatilities. Requires passing of benchmark_rets.\n    cone_function : function, optional\n        Function to use when generating forecast probability cone.\n        The function signiture must follow the form:\n        def cone(in_sample_returns (pd.Series),\n                 days_to_project_forward (int),\n                 cone_std= (float, or tuple),\n                 starting_value= (int, or float))\n        See timeseries.forecast_cone_bootstrap for an example.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_rolling_sharpe": "\n    Plots the rolling Sharpe ratio versus date.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series, optional\n        Daily noncumulative returns of the benchmark factor for\n        which the benchmark rolling Sharpe is computed. Usually\n        a benchmark such as market returns.\n         - This is in the same style as returns.\n    rolling_window : int, optional\n        The days window over which to compute the sharpe ratio.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_rolling_volatility": "\n    Plots the rolling volatility versus date.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series, optional\n        Daily noncumulative returns of the benchmark factor for which the\n        benchmark rolling volatility is computed. Usually a benchmark such\n        as market returns.\n         - This is in the same style as returns.\n    rolling_window : int, optional\n        The days window over which to compute the volatility.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_round_trip_lifetimes": "\n    Plots timespans and directions of a sample of round trip trades.\n\n    Parameters\n    ----------\n    round_trips : pd.DataFrame\n        DataFrame with one row per round trip trade.\n        - See full explanation in round_trips.extract_round_trips\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_sector_allocations": "\n    Plots the sector exposures of the portfolio over time.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    sector_alloc : pd.DataFrame\n        Portfolio allocation of positions. See pos.get_sector_alloc.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_slippage_sensitivity": "\n    Plots curve relating per-dollar slippage to average annual returns.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Timeseries of portfolio returns to be adjusted for various\n        degrees of slippage.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in tears.create_full_tear_sheet.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to seaborn plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_slippage_sweep": "\n    Plots equity curves at different per-dollar slippage assumptions.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Timeseries of portfolio returns to be adjusted for various\n        degrees of slippage.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in tears.create_full_tear_sheet.\n    slippage_params: tuple\n        Slippage pameters to apply to the return time series (in\n        basis points).\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to seaborn plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_turnover": "\n    Plots turnover vs. date.\n\n    Turnover is the number of shares traded for a period as a fraction\n    of total shares.\n\n    Displays daily total, daily average per month, and all-time daily\n    average.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions : pd.DataFrame\n        Daily net position values.\n         - See full explanation in tears.create_full_tear_sheet.\n    turnover_denom : str, optional\n        Either AGB or portfolio_value, default AGB.\n        - See full explanation in txn.get_turnover.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plot_txn_time_hist": "\n    Plots a histogram of transaction times, binning the times into\n    buckets of a given duration.\n\n    Parameters\n    ----------\n    transactions : pd.DataFrame\n        Prices and amounts of executed trades. One row per trade.\n         - See full explanation in tears.create_full_tear_sheet.\n    bin_minutes : float, optional\n        Sizes of the bins in minutes, defaults to 5 minutes.\n    tz : str, optional\n        Time zone to plot against. Note that if the specified\n        zone does not apply daylight savings, the distribution\n        may be partially offset.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.plotting_context": "\n    Create pyfolio default plotting style context.\n\n    Under the hood, calls and returns seaborn.plotting_context() with\n    some custom settings. Usually you would use in a with-context.\n\n    Parameters\n    ----------\n    context : str, optional\n        Name of seaborn context.\n    font_scale : float, optional\n        Scale font by factor font_scale.\n    rc : dict, optional\n        Config flags.\n        By default, {'lines.linewidth': 1.5}\n        is being used and will be added to any\n        rc passed in, unless explicitly overriden.\n\n    Returns\n    -------\n    seaborn plotting context\n\n    Example\n    -------\n    >>> with pyfolio.plotting.plotting_context(font_scale=2):\n    >>>    pyfolio.create_full_tear_sheet(..., set_context=False)\n\n    See also\n    --------\n    For more information, see seaborn.plotting_context().\n\n    ",
    "pyfolio.show_and_plot_top_positions": "\n    Prints and/or plots the exposures of the top 10 held positions of\n    all time.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    positions_alloc : pd.DataFrame\n        Portfolio allocation of positions. See pos.get_percent_alloc.\n    show_and_plot : int, optional\n        By default, this is 2, and both prints and plots.\n        If this is 0, it will only plot; if 1, it will only print.\n    hide_positions : bool, optional\n        If True, will not output any symbol names.\n    legend_loc : matplotlib.loc, optional\n        The location of the legend on the plot.\n        By default, the legend will display below the plot.\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n    **kwargs, optional\n        Passed to plotting function.\n\n    Returns\n    -------\n    ax : matplotlib.Axes, conditional\n        The axes that were plotted on.\n\n    ",
    "pyfolio.show_perf_stats": "\n    Prints some performance metrics of the strategy.\n\n    - Shows amount of time the strategy has been run in backtest and\n      out-of-sample (in live trading).\n\n    - Shows Omega ratio, max drawdown, Calmar ratio, annual return,\n      stability, Sharpe ratio, annual volatility, alpha, and beta.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    factor_returns : pd.Series, optional\n        Daily noncumulative returns of the benchmark factor to which betas are\n        computed. Usually a benchmark such as market returns.\n         - This is in the same style as returns.\n    positions : pd.DataFrame, optional\n        Daily net position values.\n         - See full explanation in create_full_tear_sheet.\n    transactions : pd.DataFrame, optional\n        Prices and amounts of executed trades. One row per trade.\n        - See full explanation in tears.create_full_tear_sheet\n    turnover_denom : str, optional\n        Either AGB or portfolio_value, default AGB.\n        - See full explanation in txn.get_turnover.\n    live_start_date : datetime, optional\n        The point in time when the strategy began live trading, after\n        its backtest period.\n    bootstrap : boolean, optional\n        Whether to perform bootstrap analysis for the performance\n        metrics.\n         - For more information, see timeseries.perf_stats_bootstrap\n    header_rows : dict or OrderedDict, optional\n        Extra rows to display at the top of the displayed table.\n    ",
    "pyfolio.show_profit_attribution": "\n    Prints the share of total PnL contributed by each\n    traded name.\n\n    Parameters\n    ----------\n    round_trips : pd.DataFrame\n        DataFrame with one row per round trip trade.\n        - See full explanation in round_trips.extract_round_trips\n    ax : matplotlib.Axes, optional\n        Axes upon which to plot.\n\n    Returns\n    -------\n    ax : matplotlib.Axes\n        The axes that were plotted on.\n    ",
    "pyfolio.show_worst_drawdown_periods": "\n    Prints information about the worst drawdown periods.\n\n    Prints peak dates, valley dates, recovery dates, and net\n    drawdowns.\n\n    Parameters\n    ----------\n    returns : pd.Series\n        Daily returns of the strategy, noncumulative.\n         - See full explanation in tears.create_full_tear_sheet.\n    top : int, optional\n        Amount of top drawdowns periods to plot (default 5).\n    ",
    "pyfolio.time": "time() -> floating point number\n\nReturn the current time in seconds since the Epoch.\nFractions of a second may be present if the system clock provides them.",
    "pyfolio.wraps": "Decorator factory to apply update_wrapper() to a wrapper function\n\n       Returns a decorator that invokes update_wrapper() with the decorated\n       function as the wrapper argument and the arguments to wraps() as the\n       remaining arguments. Default arguments are as for update_wrapper().\n       This is a convenience function to simplify applying partial() to\n       update_wrapper().\n    ",
    "zipline.api.EODCancel": "This policy cancels open orders at the end of the day.  For now,\n    Zipline will only apply this policy to minutely simulations.\n\n    Parameters\n    ----------\n    warn_on_cancel : bool, optional\n        Should a warning be raised if this causes an order to be cancelled?\n    ",
    "zipline.api.FixedBasisPointsSlippage": "\n    Model slippage as a fixed percentage difference from historical minutely\n    close price, limiting the size of fills to a fixed percentage of historical\n    minutely volume.\n\n    Orders to buy are filled at::\n\n        historical_price * (1 + (basis_points * 0.0001))\n\n    Orders to sell are filled at::\n\n        historical_price * (1 - (basis_points * 0.0001))\n\n    Fill sizes are capped at::\n\n        historical_volume * volume_limit\n\n    Parameters\n    ----------\n    basis_points : float, optional\n        Number of basis points of slippage to apply for each fill. Default\n        is 5 basis points.\n    volume_limit : float, optional\n        Fraction of trading volume that can be filled each minute. Default is\n        10% of trading volume.\n\n    Notes\n    -----\n    - A basis point is one one-hundredth of a percent.\n    - This class, default-constructed, is zipline's default slippage model for\n      equities.\n    ",
    "zipline.api.FixedSlippage": "Simple model assuming a fixed-size spread for all assets.\n\n    Parameters\n    ----------\n    spread : float, optional\n        Size of the assumed spread for all assets.\n        Orders to buy will be filled at ``close + (spread / 2)``.\n        Orders to sell will be filled at ``close - (spread / 2)``.\n\n    Notes\n    -----\n    This model does not impose limits on the size of fills. An order for an\n    asset will always be filled as soon as any trading activity occurs in the\n    order's asset, even if the size of the order is greater than the historical\n    volume.\n    ",
    "zipline.api.HistoricalRestrictions": "Historical restrictions stored in memory with effective dates for each\n    asset.\n\n    Parameters\n    ----------\n    restrictions : iterable of namedtuple Restriction\n        The restrictions, each defined by an asset, effective date and state\n    ",
    "zipline.api.NeverCancel": "Orders are never automatically canceled.",
    "zipline.api.Restriction": "Restriction(asset, effective_date, state)",
    "zipline.api.StaticRestrictions": "Static restrictions stored in memory that are constant regardless of dt\n    for each asset.\n\n    Parameters\n    ----------\n    restricted_list : iterable of assets\n        The assets to be restricted\n    ",
    "zipline.api.VolumeShareSlippage": "Model slippage as a quadratic function of percentage of historical volume.\n\n    Orders to buy will be filled at::\n\n       price * (1 + price_impact * (volume_share ** 2))\n\n    Orders to sell will be filled at::\n\n       price * (1 - price_impact * (volume_share ** 2))\n\n    where ``price`` is the close price for the bar, and ``volume_share`` is the\n    percentage of minutely volume filled, up to a max of ``volume_limit``.\n\n    Parameters\n    ----------\n    volume_limit : float, optional\n        Maximum percent of historical volume that can fill in each bar. 0.5\n        means 50% of historical volume. 1.0 means 100%. Default is 0.025 (i.e.,\n        2.5%).\n    price_impact : float, optional\n        Scaling coefficient for price impact. Larger values will result in more\n        simulated price impact. Smaller values will result in less simulated\n        price impact. Default is 0.1.\n    ",
    "zipline.api.attach_pipeline": "Register a pipeline to be computed at the start of each day.\n\n        Parameters\n        ----------\n        pipeline : Pipeline\n            The pipeline to have computed.\n        name : str\n            The name of the pipeline.\n        chunks : int or iterator, optional\n            The number of days to compute pipeline results for. Increasing\n            this number will make it longer to get the first results but\n            may improve the total runtime of the simulation. If an iterator\n            is passed, we will run in chunks based on values of the iterator.\n            Default is True.\n        eager : bool, optional\n            Whether or not to compute this pipeline prior to\n            before_trading_start.\n\n        Returns\n        -------\n        pipeline : Pipeline\n            Returns the pipeline that was attached unchanged.\n\n        See Also\n        --------\n        :func:`zipline.api.pipeline_output`\n        ",
    "zipline.api.batch_market_order": "Place a batch market order for multiple assets.\n\n        Parameters\n        ----------\n        share_counts : pd.Series[Asset -> int]\n            Map from asset to number of shares to order for that asset.\n\n        Returns\n        -------\n        order_ids : pd.Index[str]\n            Index of ids for newly-created orders.\n        ",
    "zipline.api.cancel_order": "Cancel an open order.\n\n        Parameters\n        ----------\n        order_param : str or Order\n            The order_id or order object to cancel.\n        ",
    "zipline.api.continuous_future": "Create a specifier for a continuous contract.\n\n        Parameters\n        ----------\n        root_symbol_str : str\n            The root symbol for the future chain.\n\n        offset : int, optional\n            The distance from the primary contract. Default is 0.\n\n        roll_style : str, optional\n            How rolls are determined. Default is 'volume'.\n\n        adjustment : str, optional\n            Method for adjusting lookback prices between rolls. Options are\n            'mul', 'add', and None. Default is 'mul'.\n\n        Returns\n        -------\n        continuous_future : zipline.assets.ContinuousFuture\n            The continuous future specifier.\n        ",
    "zipline.api.date_rules": "Factories for date-based :func:`~zipline.api.schedule_function` rules.\n\n    See Also\n    --------\n    :func:`~zipline.api.schedule_function`\n    ",
    "zipline.api.fetch_csv": "Fetch a csv from a remote url and register the data so that it is\n        queryable from the ``data`` object.\n\n        Parameters\n        ----------\n        url : str\n            The url of the csv file to load.\n        pre_func : callable[pd.DataFrame -> pd.DataFrame], optional\n            A callback to allow preprocessing the raw data returned from\n            fetch_csv before dates are paresed or symbols are mapped.\n        post_func : callable[pd.DataFrame -> pd.DataFrame], optional\n            A callback to allow postprocessing of the data after dates and\n            symbols have been mapped.\n        date_column : str, optional\n            The name of the column in the preprocessed dataframe containing\n            datetime information to map the data.\n        date_format : str, optional\n            The format of the dates in the ``date_column``. If not provided\n            ``fetch_csv`` will attempt to infer the format. For information\n            about the format of this string, see :func:`pandas.read_csv`.\n        timezone : tzinfo or str, optional\n            The timezone for the datetime in the ``date_column``.\n        symbol : str, optional\n            If the data is about a new asset or index then this string will\n            be the name used to identify the values in ``data``. For example,\n            one may use ``fetch_csv`` to load data for VIX, then this field\n            could be the string ``'VIX'``.\n        mask : bool, optional\n            Drop any rows which cannot be symbol mapped.\n        symbol_column : str\n            If the data is attaching some new attribute to each asset then this\n            argument is the name of the column in the preprocessed dataframe\n            containing the symbols. This will be used along with the date\n            information to map the sids in the asset finder.\n        country_code : str, optional\n            Country code to use to disambiguate symbol lookups.\n        **kwargs\n            Forwarded to :func:`pandas.read_csv`.\n\n        Returns\n        -------\n        csv_data_source : zipline.sources.requests_csv.PandasRequestsCSV\n            A requests source that will pull data from the url specified.\n        ",
    "zipline.api.future_symbol": "Lookup a futures contract with a given symbol.\n\n        Parameters\n        ----------\n        symbol : str\n            The symbol of the desired contract.\n\n        Returns\n        -------\n        future : zipline.assets.Future\n            The future that trades with the name ``symbol``.\n\n        Raises\n        ------\n        SymbolNotFound\n            Raised when no contract named 'symbol' is found.\n        ",
    "zipline.api.get_datetime": "Returns the current simulation datetime.\n\n        Parameters\n        ----------\n        tz : tzinfo or str, optional\n            The timezone to return the datetime in. This defaults to utc.\n\n        Returns\n        -------\n        dt : datetime\n            The current simulation datetime converted to ``tz``.\n        ",
    "zipline.api.get_environment": "Query the execution environment.\n\n        Parameters\n        ----------\n        field : {'platform', 'arena', 'data_frequency', 'start', 'end',\n        'capital_base', 'platform', '*'}\n\n        The field to query. The options have the following meanings:\n\n        - arena : str\n          The arena from the simulation parameters. This will normally\n          be ``'backtest'`` but some systems may use this distinguish\n          live trading from backtesting.\n        - data_frequency : {'daily', 'minute'}\n          data_frequency tells the algorithm if it is running with\n          daily data or minute data.\n        - start : datetime\n          The start date for the simulation.\n        - end : datetime\n          The end date for the simulation.\n        - capital_base : float\n          The starting capital for the simulation.\n        -platform : str\n          The platform that the code is running on. By default, this\n          will be the string 'zipline'. This can allow algorithms to\n          know if they are running on the Quantopian platform instead.\n        - * : dict[str -> any]\n          Returns all the fields in a dictionary.\n\n        Returns\n        -------\n        val : any\n            The value for the field queried. See above for more information.\n\n        Raises\n        ------\n        ValueError\n            Raised when ``field`` is not a valid option.\n        ",
    "zipline.api.get_open_orders": "Retrieve all of the current open orders.\n\n        Parameters\n        ----------\n        asset : Asset\n            If passed and not None, return only the open orders for the given\n            asset instead of all open orders.\n\n        Returns\n        -------\n        open_orders : dict[list[Order]] or list[Order]\n            If no asset is passed this will return a dict mapping Assets\n            to a list containing all the open orders for the asset.\n            If an asset is passed then this will return a list of the open\n            orders for this asset.\n        ",
    "zipline.api.get_order": "Lookup an order based on the order id returned from one of the\n        order functions.\n\n        Parameters\n        ----------\n        order_id : str\n            The unique identifier for the order.\n\n        Returns\n        -------\n        order : Order\n            The order object.\n        ",
    "zipline.api.order": "Place an order for a fixed number of shares.\n\n        Parameters\n        ----------\n        asset : Asset\n            The asset to be ordered.\n        amount : int\n            The amount of shares to order. If ``amount`` is positive, this is\n            the number of shares to buy or cover. If ``amount`` is negative,\n            this is the number of shares to sell or short.\n        limit_price : float, optional\n            The limit price for the order.\n        stop_price : float, optional\n            The stop price for the order.\n        style : ExecutionStyle, optional\n            The execution style for the order.\n\n        Returns\n        -------\n        order_id : str or None\n            The unique identifier for this order, or None if no order was\n            placed.\n\n        Notes\n        -----\n        The ``limit_price`` and ``stop_price`` arguments provide shorthands for\n        passing common execution styles. Passing ``limit_price=N`` is\n        equivalent to ``style=LimitOrder(N)``. Similarly, passing\n        ``stop_price=M`` is equivalent to ``style=StopOrder(M)``, and passing\n        ``limit_price=N`` and ``stop_price=M`` is equivalent to\n        ``style=StopLimitOrder(N, M)``. It is an error to pass both a ``style``\n        and ``limit_price`` or ``stop_price``.\n\n        See Also\n        --------\n        :class:`zipline.finance.execution.ExecutionStyle`\n        :func:`zipline.api.order_value`\n        :func:`zipline.api.order_percent`\n        ",
    "zipline.api.order_percent": "Place an order in the specified asset corresponding to the given\n        percent of the current portfolio value.\n\n        Parameters\n        ----------\n        asset : Asset\n            The asset that this order is for.\n        percent : float\n            The percentage of the portfolio value to allocate to ``asset``.\n            This is specified as a decimal, for example: 0.50 means 50%.\n        limit_price : float, optional\n            The limit price for the order.\n        stop_price : float, optional\n            The stop price for the order.\n        style : ExecutionStyle\n            The execution style for the order.\n\n        Returns\n        -------\n        order_id : str\n            The unique identifier for this order.\n\n        Notes\n        -----\n        See :func:`zipline.api.order` for more information about\n        ``limit_price``, ``stop_price``, and ``style``\n\n        See Also\n        --------\n        :class:`zipline.finance.execution.ExecutionStyle`\n        :func:`zipline.api.order`\n        :func:`zipline.api.order_value`\n        ",
    "zipline.api.order_target": "Place an order to adjust a position to a target number of shares. If\n        the position doesn't already exist, this is equivalent to placing a new\n        order. If the position does exist, this is equivalent to placing an\n        order for the difference between the target number of shares and the\n        current number of shares.\n\n        Parameters\n        ----------\n        asset : Asset\n            The asset that this order is for.\n        target : int\n            The desired number of shares of ``asset``.\n        limit_price : float, optional\n            The limit price for the order.\n        stop_price : float, optional\n            The stop price for the order.\n        style : ExecutionStyle\n            The execution style for the order.\n\n        Returns\n        -------\n        order_id : str\n            The unique identifier for this order.\n\n\n        Notes\n        -----\n        ``order_target`` does not take into account any open orders. For\n        example:\n\n        .. code-block:: python\n\n           order_target(sid(0), 10)\n           order_target(sid(0), 10)\n\n        This code will result in 20 shares of ``sid(0)`` because the first\n        call to ``order_target`` will not have been filled when the second\n        ``order_target`` call is made.\n\n        See :func:`zipline.api.order` for more information about\n        ``limit_price``, ``stop_price``, and ``style``\n\n        See Also\n        --------\n        :class:`zipline.finance.execution.ExecutionStyle`\n        :func:`zipline.api.order`\n        :func:`zipline.api.order_target_percent`\n        :func:`zipline.api.order_target_value`\n        ",
    "zipline.api.order_target_percent": "Place an order to adjust a position to a target percent of the\n        current portfolio value. If the position doesn't already exist, this is\n        equivalent to placing a new order. If the position does exist, this is\n        equivalent to placing an order for the difference between the target\n        percent and the current percent.\n\n        Parameters\n        ----------\n        asset : Asset\n            The asset that this order is for.\n        target : float\n            The desired percentage of the portfolio value to allocate to\n            ``asset``. This is specified as a decimal, for example:\n            0.50 means 50%.\n        limit_price : float, optional\n            The limit price for the order.\n        stop_price : float, optional\n            The stop price for the order.\n        style : ExecutionStyle\n            The execution style for the order.\n\n        Returns\n        -------\n        order_id : str\n            The unique identifier for this order.\n\n        Notes\n        -----\n        ``order_target_value`` does not take into account any open orders. For\n        example:\n\n        .. code-block:: python\n\n           order_target_percent(sid(0), 10)\n           order_target_percent(sid(0), 10)\n\n        This code will result in 20% of the portfolio being allocated to sid(0)\n        because the first call to ``order_target_percent`` will not have been\n        filled when the second ``order_target_percent`` call is made.\n\n        See :func:`zipline.api.order` for more information about\n        ``limit_price``, ``stop_price``, and ``style``\n\n        See Also\n        --------\n        :class:`zipline.finance.execution.ExecutionStyle`\n        :func:`zipline.api.order`\n        :func:`zipline.api.order_target`\n        :func:`zipline.api.order_target_value`\n        ",
    "zipline.api.order_target_value": "Place an order to adjust a position to a target value. If\n        the position doesn't already exist, this is equivalent to placing a new\n        order. If the position does exist, this is equivalent to placing an\n        order for the difference between the target value and the\n        current value.\n        If the Asset being ordered is a Future, the 'target value' calculated\n        is actually the target exposure, as Futures have no 'value'.\n\n        Parameters\n        ----------\n        asset : Asset\n            The asset that this order is for.\n        target : float\n            The desired total value of ``asset``.\n        limit_price : float, optional\n            The limit price for the order.\n        stop_price : float, optional\n            The stop price for the order.\n        style : ExecutionStyle\n            The execution style for the order.\n\n        Returns\n        -------\n        order_id : str\n            The unique identifier for this order.\n\n        Notes\n        -----\n        ``order_target_value`` does not take into account any open orders. For\n        example:\n\n        .. code-block:: python\n\n           order_target_value(sid(0), 10)\n           order_target_value(sid(0), 10)\n\n        This code will result in 20 dollars of ``sid(0)`` because the first\n        call to ``order_target_value`` will not have been filled when the\n        second ``order_target_value`` call is made.\n\n        See :func:`zipline.api.order` for more information about\n        ``limit_price``, ``stop_price``, and ``style``\n\n        See Also\n        --------\n        :class:`zipline.finance.execution.ExecutionStyle`\n        :func:`zipline.api.order`\n        :func:`zipline.api.order_target`\n        :func:`zipline.api.order_target_percent`\n        ",
    "zipline.api.order_value": "Place an order for a fixed amount of money.\n\n        Equivalent to ``order(asset, value / data.current(asset, 'price'))``.\n\n        Parameters\n        ----------\n        asset : Asset\n            The asset to be ordered.\n        value : float\n            Amount of value of ``asset`` to be transacted. The number of shares\n            bought or sold will be equal to ``value / current_price``.\n        limit_price : float, optional\n            Limit price for the order.\n        stop_price : float, optional\n            Stop price for the order.\n        style : ExecutionStyle\n            The execution style for the order.\n\n        Returns\n        -------\n        order_id : str\n            The unique identifier for this order.\n\n        Notes\n        -----\n        See :func:`zipline.api.order` for more information about\n        ``limit_price``, ``stop_price``, and ``style``\n\n        See Also\n        --------\n        :class:`zipline.finance.execution.ExecutionStyle`\n        :func:`zipline.api.order`\n        :func:`zipline.api.order_percent`\n        ",
    "zipline.api.pipeline_output": "Get results of the pipeline attached by with name ``name``.\n\n        Parameters\n        ----------\n        name : str\n            Name of the pipeline from which to fetch results.\n\n        Returns\n        -------\n        results : pd.DataFrame\n            DataFrame containing the results of the requested pipeline for\n            the current simulation date.\n\n        Raises\n        ------\n        NoSuchPipeline\n            Raised when no pipeline with the name `name` has been registered.\n\n        See Also\n        --------\n        :func:`zipline.api.attach_pipeline`\n        :meth:`zipline.pipeline.engine.PipelineEngine.run_pipeline`\n        ",
    "zipline.api.record": "Track and record values each day.\n\n        Parameters\n        ----------\n        **kwargs\n            The names and values to record.\n\n        Notes\n        -----\n        These values will appear in the performance packets and the performance\n        dataframe passed to ``analyze`` and returned from\n        :func:`~zipline.run_algorithm`.\n        ",
    "zipline.api.schedule_function": "Schedule a function to be called repeatedly in the future.\n\n        Parameters\n        ----------\n        func : callable\n            The function to execute when the rule is triggered. ``func`` should\n            have the same signature as ``handle_data``.\n        date_rule : zipline.utils.events.EventRule, optional\n            Rule for the dates on which to execute ``func``. If not\n            passed, the function will run every trading day.\n        time_rule : zipline.utils.events.EventRule, optional\n            Rule for the time at which to execute ``func``. If not passed, the\n            function will execute at the end of the first market minute of the\n            day.\n        half_days : bool, optional\n            Should this rule fire on half days? Default is True.\n        calendar : Sentinel, optional\n            Calendar used to compute rules that depend on the trading calendar.\n\n        See Also\n        --------\n        :class:`zipline.api.date_rules`\n        :class:`zipline.api.time_rules`\n        ",
    "zipline.api.set_asset_restrictions": "Set a restriction on which assets can be ordered.\n\n        Parameters\n        ----------\n        restricted_list : Restrictions\n            An object providing information about restricted assets.\n\n        See Also\n        --------\n        zipline.finance.asset_restrictions.Restrictions\n        ",
    "zipline.api.set_benchmark": "Set the benchmark asset.\n\n        Parameters\n        ----------\n        benchmark : zipline.assets.Asset\n            The asset to set as the new benchmark.\n\n        Notes\n        -----\n        Any dividends payed out for that new benchmark asset will be\n        automatically reinvested.\n        ",
    "zipline.api.set_cancel_policy": "Sets the order cancellation policy for the simulation.\n\n        Parameters\n        ----------\n        cancel_policy : CancelPolicy\n            The cancellation policy to use.\n\n        See Also\n        --------\n        :class:`zipline.api.EODCancel`\n        :class:`zipline.api.NeverCancel`\n        ",
    "zipline.api.set_commission": "Sets the commission models for the simulation.\n\n        Parameters\n        ----------\n        us_equities : EquityCommissionModel\n            The commission model to use for trading US equities.\n        us_futures : FutureCommissionModel\n            The commission model to use for trading US futures.\n\n        Notes\n        -----\n        This function can only be called during\n        :func:`~zipline.api.initialize`.\n\n        See Also\n        --------\n        :class:`zipline.finance.commission.PerShare`\n        :class:`zipline.finance.commission.PerTrade`\n        :class:`zipline.finance.commission.PerDollar`\n        ",
    "zipline.api.set_do_not_order_list": "Set a restriction on which assets can be ordered.\n\n        Parameters\n        ----------\n        restricted_list : container[Asset], SecurityList\n            The assets that cannot be ordered.\n        ",
    "zipline.api.set_long_only": "Set a rule specifying that this algorithm cannot take short\n        positions.\n        ",
    "zipline.api.set_max_leverage": "Set a limit on the maximum leverage of the algorithm.\n\n        Parameters\n        ----------\n        max_leverage : float\n            The maximum leverage for the algorithm. If not provided there will\n            be no maximum.\n        ",
    "zipline.api.set_max_order_count": "Set a limit on the number of orders that can be placed in a single\n        day.\n\n        Parameters\n        ----------\n        max_count : int\n            The maximum number of orders that can be placed on any single day.\n        ",
    "zipline.api.set_max_order_size": "Set a limit on the number of shares and/or dollar value of any single\n        order placed for sid.  Limits are treated as absolute values and are\n        enforced at the time that the algo attempts to place an order for sid.\n\n        If an algorithm attempts to place an order that would result in\n        exceeding one of these limits, raise a TradingControlException.\n\n        Parameters\n        ----------\n        asset : Asset, optional\n            If provided, this sets the guard only on positions in the given\n            asset.\n        max_shares : int, optional\n            The maximum number of shares that can be ordered at one time.\n        max_notional : float, optional\n            The maximum value that can be ordered at one time.\n        ",
    "zipline.api.set_max_position_size": "Set a limit on the number of shares and/or dollar value held for the\n        given sid. Limits are treated as absolute values and are enforced at\n        the time that the algo attempts to place an order for sid. This means\n        that it's possible to end up with more than the max number of shares\n        due to splits/dividends, and more than the max notional due to price\n        improvement.\n\n        If an algorithm attempts to place an order that would result in\n        increasing the absolute value of shares/dollar value exceeding one of\n        these limits, raise a TradingControlException.\n\n        Parameters\n        ----------\n        asset : Asset, optional\n            If provided, this sets the guard only on positions in the given\n            asset.\n        max_shares : int, optional\n            The maximum number of shares to hold for an asset.\n        max_notional : float, optional\n            The maximum value to hold for an asset.\n        ",
    "zipline.api.set_min_leverage": "Set a limit on the minimum leverage of the algorithm.\n\n        Parameters\n        ----------\n        min_leverage : float\n            The minimum leverage for the algorithm.\n        grace_period : pd.Timedelta\n            The offset from the start date used to enforce a minimum leverage.\n        ",
    "zipline.api.set_slippage": "Set the slippage models for the simulation.\n\n        Parameters\n        ----------\n        us_equities : EquitySlippageModel\n            The slippage model to use for trading US equities.\n        us_futures : FutureSlippageModel\n            The slippage model to use for trading US futures.\n\n        Notes\n        -----\n        This function can only be called during\n        :func:`~zipline.api.initialize`.\n\n        See Also\n        --------\n        :class:`zipline.finance.slippage.SlippageModel`\n        ",
    "zipline.api.set_symbol_lookup_date": "Set the date for which symbols will be resolved to their assets\n        (symbols may map to different firms or underlying assets at\n        different times)\n\n        Parameters\n        ----------\n        dt : datetime\n            The new symbol lookup date.\n        ",
    "zipline.api.sid": "Lookup an Asset by its unique asset identifier.\n\n        Parameters\n        ----------\n        sid : int\n            The unique integer that identifies an asset.\n\n        Returns\n        -------\n        asset : zipline.assets.Asset\n            The asset with the given ``sid``.\n\n        Raises\n        ------\n        SidsNotFound\n            When a requested ``sid`` does not map to any asset.\n        ",
    "zipline.api.symbol": "Lookup an Equity by its ticker symbol.\n\n        Parameters\n        ----------\n        symbol_str : str\n            The ticker symbol for the equity to lookup.\n        country_code : str or None, optional\n            A country to limit symbol searches to.\n\n        Returns\n        -------\n        equity : zipline.assets.Equity\n            The equity that held the ticker symbol on the current\n            symbol lookup date.\n\n        Raises\n        ------\n        SymbolNotFound\n            Raised when the symbols was not held on the current lookup date.\n\n        See Also\n        --------\n        :func:`zipline.api.set_symbol_lookup_date`\n        ",
    "zipline.api.symbols": "Lookup multuple Equities as a list.\n\n        Parameters\n        ----------\n        *args : iterable[str]\n            The ticker symbols to lookup.\n        country_code : str or None, optional\n            A country to limit symbol searches to.\n\n        Returns\n        -------\n        equities : list[zipline.assets.Equity]\n            The equities that held the given ticker symbols on the current\n            symbol lookup date.\n\n        Raises\n        ------\n        SymbolNotFound\n            Raised when one of the symbols was not held on the current\n            lookup date.\n\n        See Also\n        --------\n        :func:`zipline.api.set_symbol_lookup_date`\n        ",
    "zipline.api.time_rules": "Factories for time-based :func:`~zipline.api.schedule_function` rules.\n\n    See Also\n    --------\n    :func:`~zipline.api.schedule_function`\n    ",
    "sklearn.decomposition": "Matrix decomposition algorithms.\n\nThese include PCA, NMF, ICA, and more. Most of the algorithms of this module can be\nregarded as dimensionality reduction techniques.\n",
    "sklearn.decomposition.DictionaryLearning": "Dictionary learning.\n\n    Finds a dictionary (a set of atoms) that performs well at sparsely\n    encoding the fitted data.\n\n    Solves the optimization problem::\n\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\n                    (U,V)\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\n\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\n    the entry-wise matrix norm which is the sum of the absolute values\n    of all the entries in the matrix.\n\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of dictionary elements to extract. If None, then ``n_components``\n        is set to ``n_features``.\n\n    alpha : float, default=1.0\n        Sparsity controlling parameter.\n\n    max_iter : int, default=1000\n        Maximum number of iterations to perform.\n\n    tol : float, default=1e-8\n        Tolerance for numerical error.\n\n    fit_algorithm : {'lars', 'cd'}, default='lars'\n        * `'lars'`: uses the least angle regression method to solve the lasso\n          problem (:func:`~sklearn.linear_model.lars_path`);\n        * `'cd'`: uses the coordinate descent method to compute the\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\n          faster if the estimated components are sparse.\n\n        .. versionadded:: 0.17\n           *cd* coordinate descent method to improve speed.\n\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp'\n        Algorithm used to transform the data:\n\n        - `'lars'`: uses the least angle regression method\n          (:func:`~sklearn.linear_model.lars_path`);\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\n          will be faster if the estimated components are sparse.\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n          solution.\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\n          the projection ``dictionary * X'``.\n\n        .. versionadded:: 0.17\n           *lasso_cd* coordinate descent method to improve speed.\n\n    transform_n_nonzero_coefs : int, default=None\n        Number of nonzero coefficients to target in each column of the\n        solution. This is only used by `algorithm='lars'` and\n        `algorithm='omp'`. If `None`, then\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\n\n    transform_alpha : float, default=None\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n        penalty applied to the L1 norm.\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\n        threshold below which coefficients will be squashed to zero.\n        If `None`, defaults to `alpha`.\n\n        .. versionchanged:: 1.2\n            When None, default value changed from 1.0 to `alpha`.\n\n    n_jobs : int or None, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    code_init : ndarray of shape (n_samples, n_components), default=None\n        Initial value for the code, for warm restart. Only used if `code_init`\n        and `dict_init` are not None.\n\n    dict_init : ndarray of shape (n_components, n_features), default=None\n        Initial values for the dictionary, for warm restart. Only used if\n        `code_init` and `dict_init` are not None.\n\n    callback : callable, default=None\n        Callable that gets invoked every five iterations.\n\n        .. versionadded:: 1.3\n\n    verbose : bool, default=False\n        To control the verbosity of the procedure.\n\n    split_sign : bool, default=False\n        Whether to split the sparse feature vector into the concatenation of\n        its negative part and its positive part. This can improve the\n        performance of downstream classifiers.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initializing the dictionary when ``dict_init`` is not\n        specified, randomly shuffling the data when ``shuffle`` is set to\n        ``True``, and updating the dictionary. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    positive_code : bool, default=False\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    positive_dict : bool, default=False\n        Whether to enforce positivity when finding the dictionary.\n\n        .. versionadded:: 0.20\n\n    transform_max_iter : int, default=1000\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n        `'lasso_lars'`.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        dictionary atoms extracted from the data\n\n    error_ : array\n        vector of errors at each iteration\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : int\n        Number of iterations run.\n\n    See Also\n    --------\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\n        dictionary learning algorithm.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    SparseCoder : Find a sparse representation of data from a fixed,\n        precomputed dictionary.\n    SparsePCA : Sparse Principal Components Analysis.\n\n    References\n    ----------\n\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\n    for sparse coding (https://www.di.ens.fr/~fbach/mairal_icml09.pdf)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_sparse_coded_signal\n    >>> from sklearn.decomposition import DictionaryLearning\n    >>> X, dictionary, code = make_sparse_coded_signal(\n    ...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\n    ...     random_state=42,\n    ... )\n    >>> dict_learner = DictionaryLearning(\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\n    ...     random_state=42,\n    ... )\n    >>> X_transformed = dict_learner.fit(X).transform(X)\n\n    We can check the level of sparsity of `X_transformed`:\n\n    >>> np.mean(X_transformed == 0)\n    np.float64(0.52...)\n\n    We can compare the average squared euclidean norm of the reconstruction\n    error of the sparse coded signal relative to the squared euclidean norm of\n    the original signal:\n\n    >>> X_hat = X_transformed @ dict_learner.components_\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n    np.float64(0.05...)\n    ",
    "sklearn.decomposition.FactorAnalysis": "Factor Analysis (FA).\n\n    A simple linear generative model with Gaussian latent variables.\n\n    The observations are assumed to be caused by a linear transformation of\n    lower dimensional latent factors and added Gaussian noise.\n    Without loss of generality the factors are distributed according to a\n    Gaussian with zero mean and unit covariance. The noise is also zero mean\n    and has an arbitrary diagonal covariance matrix.\n\n    If we would restrict the model further, by assuming that the Gaussian\n    noise is even isotropic (all diagonal entries are the same) we would obtain\n    :class:`PCA`.\n\n    FactorAnalysis performs a maximum likelihood estimate of the so-called\n    `loading` matrix, the transformation of the latent variables to the\n    observed ones, using SVD based approach.\n\n    Read more in the :ref:`User Guide <FA>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Dimensionality of latent space, the number of components\n        of ``X`` that are obtained after ``transform``.\n        If None, n_components is set to the number of features.\n\n    tol : float, default=1e-2\n        Stopping tolerance for log-likelihood increase.\n\n    copy : bool, default=True\n        Whether to make a copy of X. If ``False``, the input X gets overwritten\n        during fitting.\n\n    max_iter : int, default=1000\n        Maximum number of iterations.\n\n    noise_variance_init : array-like of shape (n_features,), default=None\n        The initial guess of the noise variance for each feature.\n        If None, it defaults to np.ones(n_features).\n\n    svd_method : {'lapack', 'randomized'}, default='randomized'\n        Which SVD method to use. If 'lapack' use standard SVD from\n        scipy.linalg, if 'randomized' use fast ``randomized_svd`` function.\n        Defaults to 'randomized'. For most applications 'randomized' will\n        be sufficiently precise while providing significant speed gains.\n        Accuracy can also be improved by setting higher values for\n        `iterated_power`. If this is not sufficient, for maximum precision\n        you should choose 'lapack'.\n\n    iterated_power : int, default=3\n        Number of iterations for the power method. 3 by default. Only used\n        if ``svd_method`` equals 'randomized'.\n\n    rotation : {'varimax', 'quartimax'}, default=None\n        If not None, apply the indicated rotation. Currently, varimax and\n        quartimax are implemented. See\n        `\"The varimax criterion for analytic rotation in factor analysis\"\n        <https://link.springer.com/article/10.1007%2FBF02289233>`_\n        H. F. Kaiser, 1958.\n\n        .. versionadded:: 0.24\n\n    random_state : int or RandomState instance, default=0\n        Only used when ``svd_method`` equals 'randomized'. Pass an int for\n        reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Components with maximum variance.\n\n    loglike_ : list of shape (n_iterations,)\n        The log likelihood at each iteration.\n\n    noise_variance_ : ndarray of shape (n_features,)\n        The estimated noise variance for each feature.\n\n    n_iter_ : int\n        Number of iterations run.\n\n    mean_ : ndarray of shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    PCA: Principal component analysis is also a latent linear variable model\n        which however assumes equal noise variance for each feature.\n        This extra assumption makes probabilistic PCA faster as it can be\n        computed in closed form.\n    FastICA: Independent component analysis, a latent variable model with\n        non-Gaussian latent variables.\n\n    References\n    ----------\n    - David Barber, Bayesian Reasoning and Machine Learning,\n      Algorithm 21.1.\n\n    - Christopher M. Bishop: Pattern Recognition and Machine Learning,\n      Chapter 12.2.4.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import FactorAnalysis\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = FactorAnalysis(n_components=7, random_state=0)\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)\n    ",
    "sklearn.decomposition.FastICA": "FastICA: a fast algorithm for Independent Component Analysis.\n\n    The implementation is based on [1]_.\n\n    Read more in the :ref:`User Guide <ICA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of components to use. If None is passed, all are used.\n\n    algorithm : {'parallel', 'deflation'}, default='parallel'\n        Specify which algorithm to use for FastICA.\n\n    whiten : str or bool, default='unit-variance'\n        Specify the whitening strategy to use.\n\n        - If 'arbitrary-variance', a whitening with variance\n          arbitrary is used.\n        - If 'unit-variance', the whitening matrix is rescaled to ensure that\n          each recovered source has unit variance.\n        - If False, the data is already considered to be whitened, and no\n          whitening is performed.\n\n        .. versionchanged:: 1.3\n            The default value of `whiten` changed to 'unit-variance' in 1.3.\n\n    fun : {'logcosh', 'exp', 'cube'} or callable, default='logcosh'\n        The functional form of the G function used in the\n        approximation to neg-entropy. Could be either 'logcosh', 'exp',\n        or 'cube'.\n        You can also provide your own function. It should return a tuple\n        containing the value of the function, and of its derivative, in the\n        point. The derivative should be averaged along its last dimension.\n        Example::\n\n            def my_g(x):\n                return x ** 3, (3 * x ** 2).mean(axis=-1)\n\n    fun_args : dict, default=None\n        Arguments to send to the functional form.\n        If empty or None and if fun='logcosh', fun_args will take value\n        {'alpha' : 1.0}.\n\n    max_iter : int, default=200\n        Maximum number of iterations during fit.\n\n    tol : float, default=1e-4\n        A positive scalar giving the tolerance at which the\n        un-mixing matrix is considered to have converged.\n\n    w_init : array-like of shape (n_components, n_components), default=None\n        Initial un-mixing array. If `w_init=None`, then an array of values\n        drawn from a normal distribution is used.\n\n    whiten_solver : {\"eigh\", \"svd\"}, default=\"svd\"\n        The solver to use for whitening.\n\n        - \"svd\" is more stable numerically if the problem is degenerate, and\n          often faster when `n_samples <= n_features`.\n\n        - \"eigh\" is generally more memory efficient when\n          `n_samples >= n_features`, and can be faster when\n          `n_samples >= 50 * n_features`.\n\n        .. versionadded:: 1.2\n\n    random_state : int, RandomState instance or None, default=None\n        Used to initialize ``w_init`` when not specified, with a\n        normal distribution. Pass an int, for reproducible results\n        across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        The linear operator to apply to the data to get the independent\n        sources. This is equal to the unmixing matrix when ``whiten`` is\n        False, and equal to ``np.dot(unmixing_matrix, self.whitening_)`` when\n        ``whiten`` is True.\n\n    mixing_ : ndarray of shape (n_features, n_components)\n        The pseudo-inverse of ``components_``. It is the linear operator\n        that maps independent sources to the data.\n\n    mean_ : ndarray of shape(n_features,)\n        The mean over features. Only set if `self.whiten` is True.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : int\n        If the algorithm is \"deflation\", n_iter is the\n        maximum number of iterations run across all components. Else\n        they are just the number of iterations taken to converge.\n\n    whitening_ : ndarray of shape (n_components, n_features)\n        Only set if whiten is 'True'. This is the pre-whitening matrix\n        that projects data onto the first `n_components` principal components.\n\n    See Also\n    --------\n    PCA : Principal component analysis (PCA).\n    IncrementalPCA : Incremental principal components analysis (IPCA).\n    KernelPCA : Kernel Principal component analysis (KPCA).\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    SparsePCA : Sparse Principal Components Analysis (SparsePCA).\n\n    References\n    ----------\n    .. [1] A. Hyvarinen and E. Oja, Independent Component Analysis:\n           Algorithms and Applications, Neural Networks, 13(4-5), 2000,\n           pp. 411-430.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import FastICA\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = FastICA(n_components=7,\n    ...         random_state=0,\n    ...         whiten='unit-variance')\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)\n    ",
    "sklearn.decomposition.IncrementalPCA": "Incremental principal components analysis (IPCA).\n\n    Linear dimensionality reduction using Singular Value Decomposition of\n    the data, keeping only the most significant singular vectors to\n    project the data to a lower dimensional space. The input data is centered\n    but not scaled for each feature before applying the SVD.\n\n    Depending on the size of the input data, this algorithm can be much more\n    memory efficient than a PCA, and allows sparse input.\n\n    This algorithm has constant memory complexity, on the order\n    of ``batch_size * n_features``, enabling use of np.memmap files without\n    loading the entire file into memory. For sparse matrices, the input\n    is converted to dense in batches (in order to be able to subtract the\n    mean) which avoids storing the entire dense matrix at any one time.\n\n    The computational overhead of each SVD is\n    ``O(batch_size * n_features ** 2)``, but only 2 * batch_size samples\n    remain in memory at a time. There will be ``n_samples / batch_size`` SVD\n    computations to get the principal components, versus 1 large SVD of\n    complexity ``O(n_samples * n_features ** 2)`` for PCA.\n\n    For a usage example, see\n    :ref:`sphx_glr_auto_examples_decomposition_plot_incremental_pca.py`.\n\n    Read more in the :ref:`User Guide <IncrementalPCA>`.\n\n    .. versionadded:: 0.16\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of components to keep. If ``n_components`` is ``None``,\n        then ``n_components`` is set to ``min(n_samples, n_features)``.\n\n    whiten : bool, default=False\n        When True (False by default) the ``components_`` vectors are divided\n        by ``n_samples`` times ``components_`` to ensure uncorrelated outputs\n        with unit component-wise variances.\n\n        Whitening will remove some information from the transformed signal\n        (the relative variance scales of the components) but can sometimes\n        improve the predictive accuracy of the downstream estimators by\n        making data respect some hard-wired assumptions.\n\n    copy : bool, default=True\n        If False, X will be overwritten. ``copy=False`` can be used to\n        save memory but is unsafe for general use.\n\n    batch_size : int, default=None\n        The number of samples to use for each batch. Only used when calling\n        ``fit``. If ``batch_size`` is ``None``, then ``batch_size``\n        is inferred from the data and set to ``5 * n_features``, to provide a\n        balance between approximation accuracy and memory consumption.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Principal axes in feature space, representing the directions of\n        maximum variance in the data. Equivalently, the right singular\n        vectors of the centered input data, parallel to its eigenvectors.\n        The components are sorted by decreasing ``explained_variance_``.\n\n    explained_variance_ : ndarray of shape (n_components,)\n        Variance explained by each of the selected components.\n\n    explained_variance_ratio_ : ndarray of shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n        If all components are stored, the sum of explained variances is equal\n        to 1.0.\n\n    singular_values_ : ndarray of shape (n_components,)\n        The singular values corresponding to each of the selected components.\n        The singular values are equal to the 2-norms of the ``n_components``\n        variables in the lower-dimensional space.\n\n    mean_ : ndarray of shape (n_features,)\n        Per-feature empirical mean, aggregate over calls to ``partial_fit``.\n\n    var_ : ndarray of shape (n_features,)\n        Per-feature empirical variance, aggregate over calls to\n        ``partial_fit``.\n\n    noise_variance_ : float\n        The estimated noise covariance following the Probabilistic PCA model\n        from Tipping and Bishop 1999. See \"Pattern Recognition and\n        Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n        http://www.miketipping.com/papers/met-mppca.pdf.\n\n    n_components_ : int\n        The estimated number of components. Relevant when\n        ``n_components=None``.\n\n    n_samples_seen_ : int\n        The number of samples processed by the estimator. Will be reset on\n        new calls to fit, but increments across ``partial_fit`` calls.\n\n    batch_size_ : int\n        Inferred batch size from ``batch_size``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    PCA : Principal component analysis (PCA).\n    KernelPCA : Kernel Principal component analysis (KPCA).\n    SparsePCA : Sparse Principal Components Analysis (SparsePCA).\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n\n    Notes\n    -----\n    Implements the incremental PCA model from:\n    *D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual\n    Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3,\n    pp. 125-141, May 2008.*\n    See https://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf\n\n    This model is an extension of the Sequential Karhunen-Loeve Transform from:\n    :doi:`A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and\n    its Application to Images, IEEE Transactions on Image Processing, Volume 9,\n    Number 8, pp. 1371-1374, August 2000. <10.1109/83.855432>`\n\n    We have specifically abstained from an optimization used by authors of both\n    papers, a QR decomposition used in specific situations to reduce the\n    algorithmic complexity of the SVD. The source for this technique is\n    *Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5,\n    section 5.4.4, pp 252-253.*. This technique has been omitted because it is\n    advantageous only when decomposing a matrix with ``n_samples`` (rows)\n    >= 5/3 * ``n_features`` (columns), and hurts the readability of the\n    implemented algorithm. This would be a good opportunity for future\n    optimization, if it is deemed necessary.\n\n    References\n    ----------\n    D. Ross, J. Lim, R. Lin, M. Yang. Incremental Learning for Robust Visual\n    Tracking, International Journal of Computer Vision, Volume 77,\n    Issue 1-3, pp. 125-141, May 2008.\n\n    G. Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,\n    Section 5.4.4, pp. 252-253.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import IncrementalPCA\n    >>> from scipy import sparse\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = IncrementalPCA(n_components=7, batch_size=200)\n    >>> # either partially fit on smaller batches of data\n    >>> transformer.partial_fit(X[:100, :])\n    IncrementalPCA(batch_size=200, n_components=7)\n    >>> # or let the fit function itself divide the data into batches\n    >>> X_sparse = sparse.csr_matrix(X)\n    >>> X_transformed = transformer.fit_transform(X_sparse)\n    >>> X_transformed.shape\n    (1797, 7)\n    ",
    "sklearn.decomposition.KernelPCA": "Kernel Principal component analysis (KPCA).\n\n    Non-linear dimensionality reduction through the use of kernels [1]_, see also\n    :ref:`metrics`.\n\n    It uses the :func:`scipy.linalg.eigh` LAPACK implementation of the full SVD\n    or the :func:`scipy.sparse.linalg.eigsh` ARPACK implementation of the\n    truncated SVD, depending on the shape of the input data and the number of\n    components to extract. It can also use a randomized truncated SVD by the\n    method proposed in [3]_, see `eigen_solver`.\n\n    For a usage example and comparison between\n    Principal Components Analysis (PCA) and its kernelized version (KPCA), see\n    :ref:`sphx_glr_auto_examples_decomposition_plot_kernel_pca.py`.\n\n    For a usage example in denoising images using KPCA, see\n    :ref:`sphx_glr_auto_examples_applications_plot_digits_denoising.py`.\n\n    Read more in the :ref:`User Guide <kernel_PCA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of components. If None, all non-zero components are kept.\n\n    kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'cosine', 'precomputed'}             or callable, default='linear'\n        Kernel used for PCA.\n\n    gamma : float, default=None\n        Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other\n        kernels. If ``gamma`` is ``None``, then it is set to ``1/n_features``.\n\n    degree : float, default=3\n        Degree for poly kernels. Ignored by other kernels.\n\n    coef0 : float, default=1\n        Independent term in poly and sigmoid kernels.\n        Ignored by other kernels.\n\n    kernel_params : dict, default=None\n        Parameters (keyword arguments) and\n        values for kernel passed as callable object.\n        Ignored by other kernels.\n\n    alpha : float, default=1.0\n        Hyperparameter of the ridge regression that learns the\n        inverse transform (when fit_inverse_transform=True).\n\n    fit_inverse_transform : bool, default=False\n        Learn the inverse transform for non-precomputed kernels\n        (i.e. learn to find the pre-image of a point). This method is based\n        on [2]_.\n\n    eigen_solver : {'auto', 'dense', 'arpack', 'randomized'},             default='auto'\n        Select eigensolver to use. If `n_components` is much\n        less than the number of training samples, randomized (or arpack to a\n        smaller extent) may be more efficient than the dense eigensolver.\n        Randomized SVD is performed according to the method of Halko et al\n        [3]_.\n\n        auto :\n            the solver is selected by a default policy based on n_samples\n            (the number of training samples) and `n_components`:\n            if the number of components to extract is less than 10 (strict) and\n            the number of samples is more than 200 (strict), the 'arpack'\n            method is enabled. Otherwise the exact full eigenvalue\n            decomposition is computed and optionally truncated afterwards\n            ('dense' method).\n        dense :\n            run exact full eigenvalue decomposition calling the standard\n            LAPACK solver via `scipy.linalg.eigh`, and select the components\n            by postprocessing\n        arpack :\n            run SVD truncated to n_components calling ARPACK solver using\n            `scipy.sparse.linalg.eigsh`. It requires strictly\n            0 < n_components < n_samples\n        randomized :\n            run randomized SVD by the method of Halko et al. [3]_. The current\n            implementation selects eigenvalues based on their module; therefore\n            using this method can lead to unexpected results if the kernel is\n            not positive semi-definite. See also [4]_.\n\n        .. versionchanged:: 1.0\n           `'randomized'` was added.\n\n    tol : float, default=0\n        Convergence tolerance for arpack.\n        If 0, optimal value will be chosen by arpack.\n\n    max_iter : int, default=None\n        Maximum number of iterations for arpack.\n        If None, optimal value will be chosen by arpack.\n\n    iterated_power : int >= 0, or 'auto', default='auto'\n        Number of iterations for the power method computed by\n        svd_solver == 'randomized'. When 'auto', it is set to 7 when\n        `n_components < 0.1 * min(X.shape)`, other it is set to 4.\n\n        .. versionadded:: 1.0\n\n    remove_zero_eig : bool, default=False\n        If True, then all components with zero eigenvalues are removed, so\n        that the number of components in the output may be < n_components\n        (and sometimes even zero due to numerical instability).\n        When n_components is None, this parameter is ignored and components\n        with zero eigenvalues are removed regardless.\n\n    random_state : int, RandomState instance or None, default=None\n        Used when ``eigen_solver`` == 'arpack' or 'randomized'. Pass an int\n        for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n        .. versionadded:: 0.18\n\n    copy_X : bool, default=True\n        If True, input X is copied and stored by the model in the `X_fit_`\n        attribute. If no further changes will be done to X, setting\n        `copy_X=False` saves memory by storing a reference.\n\n        .. versionadded:: 0.18\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.18\n\n    Attributes\n    ----------\n    eigenvalues_ : ndarray of shape (n_components,)\n        Eigenvalues of the centered kernel matrix in decreasing order.\n        If `n_components` and `remove_zero_eig` are not set,\n        then all values are stored.\n\n    eigenvectors_ : ndarray of shape (n_samples, n_components)\n        Eigenvectors of the centered kernel matrix. If `n_components` and\n        `remove_zero_eig` are not set, then all components are stored.\n\n    dual_coef_ : ndarray of shape (n_samples, n_features)\n        Inverse transform matrix. Only available when\n        ``fit_inverse_transform`` is True.\n\n    X_transformed_fit_ : ndarray of shape (n_samples, n_components)\n        Projection of the fitted data on the kernel principal components.\n        Only available when ``fit_inverse_transform`` is True.\n\n    X_fit_ : ndarray of shape (n_samples, n_features)\n        The data used to fit the model. If `copy_X=False`, then `X_fit_` is\n        a reference. This attribute is used for the calls to transform.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    gamma_ : float\n        Kernel coefficient for rbf, poly and sigmoid kernels. When `gamma`\n        is explicitly provided, this is just the same as `gamma`. When `gamma`\n        is `None`, this is the actual value of kernel coefficient.\n\n        .. versionadded:: 1.3\n\n    See Also\n    --------\n    FastICA : A fast algorithm for Independent Component Analysis.\n    IncrementalPCA : Incremental Principal Component Analysis.\n    NMF : Non-Negative Matrix Factorization.\n    PCA : Principal Component Analysis.\n    SparsePCA : Sparse Principal Component Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n\n    References\n    ----------\n    .. [1] `Sch\u00f6lkopf, Bernhard, Alexander Smola, and Klaus-Robert M\u00fcller.\n       \"Kernel principal component analysis.\"\n       International conference on artificial neural networks.\n       Springer, Berlin, Heidelberg, 1997.\n       <https://people.eecs.berkeley.edu/~wainwrig/stat241b/scholkopf_kernel.pdf>`_\n\n    .. [2] `Bak\u0131r, G\u00f6khan H., Jason Weston, and Bernhard Sch\u00f6lkopf.\n       \"Learning to find pre-images.\"\n       Advances in neural information processing systems 16 (2004): 449-456.\n       <https://papers.nips.cc/paper/2003/file/ac1ad983e08ad3304a97e147f522747e-Paper.pdf>`_\n\n    .. [3] :arxiv:`Halko, Nathan, Per-Gunnar Martinsson, and Joel A. Tropp.\n       \"Finding structure with randomness: Probabilistic algorithms for\n       constructing approximate matrix decompositions.\"\n       SIAM review 53.2 (2011): 217-288. <0909.4061>`\n\n    .. [4] `Martinsson, Per-Gunnar, Vladimir Rokhlin, and Mark Tygert.\n       \"A randomized algorithm for the decomposition of matrices.\"\n       Applied and Computational Harmonic Analysis 30.1 (2011): 47-68.\n       <https://www.sciencedirect.com/science/article/pii/S1063520310000242>`_\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import KernelPCA\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = KernelPCA(n_components=7, kernel='linear')\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)\n    ",
    "sklearn.decomposition.LatentDirichletAllocation": "Latent Dirichlet Allocation with online variational Bayes algorithm.\n\n    The implementation is based on [1]_ and [2]_.\n\n    .. versionadded:: 0.17\n\n    Read more in the :ref:`User Guide <LatentDirichletAllocation>`.\n\n    Parameters\n    ----------\n    n_components : int, default=10\n        Number of topics.\n\n        .. versionchanged:: 0.19\n            ``n_topics`` was renamed to ``n_components``\n\n    doc_topic_prior : float, default=None\n        Prior of document topic distribution `theta`. If the value is None,\n        defaults to `1 / n_components`.\n        In [1]_, this is called `alpha`.\n\n    topic_word_prior : float, default=None\n        Prior of topic word distribution `beta`. If the value is None, defaults\n        to `1 / n_components`.\n        In [1]_, this is called `eta`.\n\n    learning_method : {'batch', 'online'}, default='batch'\n        Method used to update `_component`. Only used in :meth:`fit` method.\n        In general, if the data size is large, the online update will be much\n        faster than the batch update.\n\n        Valid options:\n\n        - 'batch': Batch variational Bayes method. Use all training data in each EM\n          update. Old `components_` will be overwritten in each iteration.\n        - 'online': Online variational Bayes method. In each EM update, use mini-batch\n          of training data to update the ``components_`` variable incrementally. The\n          learning rate is controlled by the ``learning_decay`` and the\n          ``learning_offset`` parameters.\n\n        .. versionchanged:: 0.20\n            The default learning method is now ``\"batch\"``.\n\n    learning_decay : float, default=0.7\n        It is a parameter that control learning rate in the online learning\n        method. The value should be set between (0.5, 1.0] to guarantee\n        asymptotic convergence. When the value is 0.0 and batch_size is\n        ``n_samples``, the update method is same as batch learning. In the\n        literature, this is called kappa.\n\n    learning_offset : float, default=10.0\n        A (positive) parameter that downweights early iterations in online\n        learning.  It should be greater than 1.0. In the literature, this is\n        called tau_0.\n\n    max_iter : int, default=10\n        The maximum number of passes over the training data (aka epochs).\n        It only impacts the behavior in the :meth:`fit` method, and not the\n        :meth:`partial_fit` method.\n\n    batch_size : int, default=128\n        Number of documents to use in each EM iteration. Only used in online\n        learning.\n\n    evaluate_every : int, default=-1\n        How often to evaluate perplexity. Only used in `fit` method.\n        set it to 0 or negative number to not evaluate perplexity in\n        training at all. Evaluating perplexity can help you check convergence\n        in training process, but it will also increase total training time.\n        Evaluating perplexity in every iteration might increase training time\n        up to two-fold.\n\n    total_samples : int, default=1e6\n        Total number of documents. Only used in the :meth:`partial_fit` method.\n\n    perp_tol : float, default=1e-1\n        Perplexity tolerance. Only used when ``evaluate_every`` is greater than 0.\n\n    mean_change_tol : float, default=1e-3\n        Stopping tolerance for updating document topic distribution in E-step.\n\n    max_doc_update_iter : int, default=100\n        Max number of iterations for updating document topic distribution in\n        the E-step.\n\n    n_jobs : int, default=None\n        The number of jobs to use in the E-step.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : int, default=0\n        Verbosity level.\n\n    random_state : int, RandomState instance or None, default=None\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Variational parameters for topic word distribution. Since the complete\n        conditional for topic word distribution is a Dirichlet,\n        ``components_[i, j]`` can be viewed as pseudocount that represents the\n        number of times word `j` was assigned to topic `i`.\n        It can also be viewed as distribution over the words for each topic\n        after normalization:\n        ``model.components_ / model.components_.sum(axis=1)[:, np.newaxis]``.\n\n    exp_dirichlet_component_ : ndarray of shape (n_components, n_features)\n        Exponential value of expectation of log topic word distribution.\n        In the literature, this is `exp(E[log(beta)])`.\n\n    n_batch_iter_ : int\n        Number of iterations of the EM step.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : int\n        Number of passes over the dataset.\n\n    bound_ : float\n        Final perplexity score on training set.\n\n    doc_topic_prior_ : float\n        Prior of document topic distribution `theta`. If the value is None,\n        it is `1 / n_components`.\n\n    random_state_ : RandomState instance\n        RandomState instance that is generated either from a seed, the random\n        number generator or by `np.random`.\n\n    topic_word_prior_ : float\n        Prior of topic word distribution `beta`. If the value is None, it is\n        `1 / n_components`.\n\n    See Also\n    --------\n    sklearn.discriminant_analysis.LinearDiscriminantAnalysis:\n        A classifier with a linear decision boundary, generated by fitting\n        class conditional densities to the data and using Bayes' rule.\n\n    References\n    ----------\n    .. [1] \"Online Learning for Latent Dirichlet Allocation\", Matthew D.\n           Hoffman, David M. Blei, Francis Bach, 2010\n           https://github.com/blei-lab/onlineldavb\n\n    .. [2] \"Stochastic Variational Inference\", Matthew D. Hoffman,\n           David M. Blei, Chong Wang, John Paisley, 2013\n\n    Examples\n    --------\n    >>> from sklearn.decomposition import LatentDirichletAllocation\n    >>> from sklearn.datasets import make_multilabel_classification\n    >>> # This produces a feature matrix of token counts, similar to what\n    >>> # CountVectorizer would produce on text.\n    >>> X, _ = make_multilabel_classification(random_state=0)\n    >>> lda = LatentDirichletAllocation(n_components=5,\n    ...     random_state=0)\n    >>> lda.fit(X)\n    LatentDirichletAllocation(...)\n    >>> # get topics for some given samples:\n    >>> lda.transform(X[-2:])\n    array([[0.00360392, 0.25499205, 0.0036211 , 0.64236448, 0.09541846],\n           [0.15297572, 0.00362644, 0.44412786, 0.39568399, 0.003586  ]])\n    ",
    "sklearn.decomposition.MiniBatchDictionaryLearning": "Mini-batch dictionary learning.\n\n    Finds a dictionary (a set of atoms) that performs well at sparsely\n    encoding the fitted data.\n\n    Solves the optimization problem::\n\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\n                    (U,V)\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\n\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\n    the entry-wise matrix norm which is the sum of the absolute values\n    of all the entries in the matrix.\n\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of dictionary elements to extract.\n\n    alpha : float, default=1\n        Sparsity controlling parameter.\n\n    max_iter : int, default=1_000\n        Maximum number of iterations over the complete dataset before\n        stopping independently of any early stopping criterion heuristics.\n\n        .. versionadded:: 1.1\n\n    fit_algorithm : {'lars', 'cd'}, default='lars'\n        The algorithm used:\n\n        - `'lars'`: uses the least angle regression method to solve the lasso\n          problem (`linear_model.lars_path`)\n        - `'cd'`: uses the coordinate descent method to compute the\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\n          the estimated components are sparse.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    batch_size : int, default=256\n        Number of samples in each mini-batch.\n\n        .. versionchanged:: 1.3\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\n\n    shuffle : bool, default=True\n        Whether to shuffle the samples before forming batches.\n\n    dict_init : ndarray of shape (n_components, n_features), default=None\n        Initial value of the dictionary for warm restart scenarios.\n\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp'\n        Algorithm used to transform the data:\n\n        - `'lars'`: uses the least angle regression method\n          (`linear_model.lars_path`);\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\n          if the estimated components are sparse.\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n          solution.\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\n          the projection ``dictionary * X'``.\n\n    transform_n_nonzero_coefs : int, default=None\n        Number of nonzero coefficients to target in each column of the\n        solution. This is only used by `algorithm='lars'` and\n        `algorithm='omp'`. If `None`, then\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\n\n    transform_alpha : float, default=None\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n        penalty applied to the L1 norm.\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\n        threshold below which coefficients will be squashed to zero.\n        If `None`, defaults to `alpha`.\n\n        .. versionchanged:: 1.2\n            When None, default value changed from 1.0 to `alpha`.\n\n    verbose : bool or int, default=False\n        To control the verbosity of the procedure.\n\n    split_sign : bool, default=False\n        Whether to split the sparse feature vector into the concatenation of\n        its negative part and its positive part. This can improve the\n        performance of downstream classifiers.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initializing the dictionary when ``dict_init`` is not\n        specified, randomly shuffling the data when ``shuffle`` is set to\n        ``True``, and updating the dictionary. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    positive_code : bool, default=False\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    positive_dict : bool, default=False\n        Whether to enforce positivity when finding the dictionary.\n\n        .. versionadded:: 0.20\n\n    transform_max_iter : int, default=1000\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n        `'lasso_lars'`.\n\n        .. versionadded:: 0.22\n\n    callback : callable, default=None\n        A callable that gets invoked at the end of each iteration.\n\n        .. versionadded:: 1.1\n\n    tol : float, default=1e-3\n        Control early stopping based on the norm of the differences in the\n        dictionary between 2 steps.\n\n        To disable early stopping based on changes in the dictionary, set\n        `tol` to 0.0.\n\n        .. versionadded:: 1.1\n\n    max_no_improvement : int, default=10\n        Control early stopping based on the consecutive number of mini batches\n        that does not yield an improvement on the smoothed cost function.\n\n        To disable convergence detection based on cost function, set\n        `max_no_improvement` to None.\n\n        .. versionadded:: 1.1\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Components extracted from the data.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : int\n        Number of iterations over the full dataset.\n\n    n_steps_ : int\n        Number of mini-batches processed.\n\n        .. versionadded:: 1.1\n\n    See Also\n    --------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    SparseCoder : Find a sparse representation of data from a fixed,\n        precomputed dictionary.\n    SparsePCA : Sparse Principal Components Analysis.\n\n    References\n    ----------\n\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\n    for sparse coding (https://www.di.ens.fr/~fbach/mairal_icml09.pdf)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_sparse_coded_signal\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\n    >>> X, dictionary, code = make_sparse_coded_signal(\n    ...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\n    ...     random_state=42)\n    >>> dict_learner = MiniBatchDictionaryLearning(\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\n    ...     transform_alpha=0.1, max_iter=20, random_state=42)\n    >>> X_transformed = dict_learner.fit_transform(X)\n\n    We can check the level of sparsity of `X_transformed`:\n\n    >>> np.mean(X_transformed == 0) > 0.5\n    np.True_\n\n    We can compare the average squared euclidean norm of the reconstruction\n    error of the sparse coded signal relative to the squared euclidean norm of\n    the original signal:\n\n    >>> X_hat = X_transformed @ dict_learner.components_\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n    np.float64(0.052...)\n    ",
    "sklearn.decomposition.MiniBatchNMF": "Mini-Batch Non-Negative Matrix Factorization (NMF).\n\n    .. versionadded:: 1.1\n\n    Find two non-negative matrices, i.e. matrices with all non-negative elements,\n    (`W`, `H`) whose product approximates the non-negative matrix `X`. This\n    factorization can be used for example for dimensionality reduction, source\n    separation or topic extraction.\n\n    The objective function is:\n\n    .. math::\n\n        L(W, H) &= 0.5 * ||X - WH||_{loss}^2\n\n                &+ alpha\\_W * l1\\_ratio * n\\_features * ||vec(W)||_1\n\n                &+ alpha\\_H * l1\\_ratio * n\\_samples * ||vec(H)||_1\n\n                &+ 0.5 * alpha\\_W * (1 - l1\\_ratio) * n\\_features * ||W||_{Fro}^2\n\n                &+ 0.5 * alpha\\_H * (1 - l1\\_ratio) * n\\_samples * ||H||_{Fro}^2,\n\n    where :math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm) and\n    :math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm).\n\n    The generic norm :math:`||X - WH||_{loss}^2` may represent\n    the Frobenius norm or another supported beta-divergence loss.\n    The choice between options is controlled by the `beta_loss` parameter.\n\n    The objective function is minimized with an alternating minimization of `W`\n    and `H`.\n\n    Note that the transformed data is named `W` and the components matrix is\n    named `H`. In the NMF literature, the naming convention is usually the opposite\n    since the data matrix `X` is transposed.\n\n    Read more in the :ref:`User Guide <MiniBatchNMF>`.\n\n    Parameters\n    ----------\n    n_components : int or {'auto'} or None, default='auto'\n        Number of components. If `None`, all features are kept.\n        If `n_components='auto'`, the number of components is automatically inferred\n        from W or H shapes.\n\n        .. versionchanged:: 1.4\n            Added `'auto'` value.\n\n        .. versionchanged:: 1.6\n            Default value changed from `None` to `'auto'`.\n\n    init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n        Method used to initialize the procedure.\n        Valid options:\n\n        - `None`: 'nndsvda' if `n_components <= min(n_samples, n_features)`,\n          otherwise random.\n\n        - `'random'`: non-negative random matrices, scaled with:\n          `sqrt(X.mean() / n_components)`\n\n        - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)\n          initialization (better for sparseness).\n\n        - `'nndsvda'`: NNDSVD with zeros filled with the average of X\n          (better when sparsity is not desired).\n\n        - `'nndsvdar'` NNDSVD with zeros filled with small random values\n          (generally faster, less accurate alternative to NNDSVDa\n          for when sparsity is not desired).\n\n        - `'custom'`: Use custom matrices `W` and `H` which must both be provided.\n\n    batch_size : int, default=1024\n        Number of samples in each mini-batch. Large batch sizes\n        give better long-term convergence at the cost of a slower start.\n\n    beta_loss : float or {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'\n        Beta divergence to be minimized, measuring the distance between `X`\n        and the dot product `WH`. Note that values different from 'frobenius'\n        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n        fits. Note that for `beta_loss <= 0` (or 'itakura-saito'), the input\n        matrix `X` cannot contain zeros.\n\n    tol : float, default=1e-4\n        Control early stopping based on the norm of the differences in `H`\n        between 2 steps. To disable early stopping based on changes in `H`, set\n        `tol` to 0.0.\n\n    max_no_improvement : int, default=10\n        Control early stopping based on the consecutive number of mini batches\n        that does not yield an improvement on the smoothed cost function.\n        To disable convergence detection based on cost function, set\n        `max_no_improvement` to None.\n\n    max_iter : int, default=200\n        Maximum number of iterations over the complete dataset before\n        timing out.\n\n    alpha_W : float, default=0.0\n        Constant that multiplies the regularization terms of `W`. Set it to zero\n        (default) to have no regularization on `W`.\n\n    alpha_H : float or \"same\", default=\"same\"\n        Constant that multiplies the regularization terms of `H`. Set it to zero to\n        have no regularization on `H`. If \"same\" (default), it takes the same value as\n        `alpha_W`.\n\n    l1_ratio : float, default=0.0\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\n        (aka Frobenius Norm).\n        For l1_ratio = 1 it is an elementwise L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n    forget_factor : float, default=0.7\n        Amount of rescaling of past information. Its value could be 1 with\n        finite datasets. Choosing values < 1 is recommended with online\n        learning as more recent batches will weight more than past batches.\n\n    fresh_restarts : bool, default=False\n        Whether to completely solve for W at each step. Doing fresh restarts will likely\n        lead to a better solution for a same number of iterations but it is much slower.\n\n    fresh_restarts_max_iter : int, default=30\n        Maximum number of iterations when solving for W at each step. Only used when\n        doing fresh restarts. These iterations may be stopped early based on a small\n        change of W controlled by `tol`.\n\n    transform_max_iter : int, default=None\n        Maximum number of iterations when solving for W at transform time.\n        If None, it defaults to `max_iter`.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initialisation (when ``init`` == 'nndsvdar' or\n        'random'), and in Coordinate Descent. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    verbose : bool, default=False\n        Whether to be verbose.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Factorization matrix, sometimes called 'dictionary'.\n\n    n_components_ : int\n        The number of components. It is same as the `n_components` parameter\n        if it was given. Otherwise, it will be same as the number of\n        features.\n\n    reconstruction_err_ : float\n        Frobenius norm of the matrix difference, or beta-divergence, between\n        the training data `X` and the reconstructed data `WH` from\n        the fitted model.\n\n    n_iter_ : int\n        Actual number of started iterations over the whole dataset.\n\n    n_steps_ : int\n        Number of mini-batches processed.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n    See Also\n    --------\n    NMF : Non-negative matrix factorization.\n    MiniBatchDictionaryLearning : Finds a dictionary that can best be used to represent\n        data using a sparse code.\n\n    References\n    ----------\n    .. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n       factorizations\" <10.1587/transfun.E92.A.708>`\n       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n       of electronics, communications and computer sciences 92.3: 708-721, 2009.\n\n    .. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n       beta-divergence\" <10.1162/NECO_a_00168>`\n       Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\n\n    .. [3] :doi:`\"Online algorithms for nonnegative matrix factorization with the\n       Itakura-Saito divergence\" <10.1109/ASPAA.2011.6082314>`\n       Lefevre, A., Bach, F., Fevotte, C. (2011). WASPA.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import MiniBatchNMF\n    >>> model = MiniBatchNMF(n_components=2, init='random', random_state=0)\n    >>> W = model.fit_transform(X)\n    >>> H = model.components_\n    ",
    "sklearn.decomposition.MiniBatchSparsePCA": "Mini-batch Sparse Principal Components Analysis.\n\n    Finds the set of sparse components that can optimally reconstruct\n    the data.  The amount of sparseness is controllable by the coefficient\n    of the L1 penalty, given by the parameter alpha.\n\n    For an example comparing sparse PCA to PCA, see\n    :ref:`sphx_glr_auto_examples_decomposition_plot_faces_decomposition.py`\n\n    Read more in the :ref:`User Guide <SparsePCA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of sparse atoms to extract. If None, then ``n_components``\n        is set to ``n_features``.\n\n    alpha : int, default=1\n        Sparsity controlling parameter. Higher values lead to sparser\n        components.\n\n    ridge_alpha : float, default=0.01\n        Amount of ridge shrinkage to apply in order to improve\n        conditioning when calling the transform method.\n\n    max_iter : int, default=1_000\n        Maximum number of iterations over the complete dataset before\n        stopping independently of any early stopping criterion heuristics.\n\n        .. versionadded:: 1.2\n\n    callback : callable, default=None\n        Callable that gets invoked every five iterations.\n\n    batch_size : int, default=3\n        The number of features to take in each mini batch.\n\n    verbose : int or bool, default=False\n        Controls the verbosity; the higher, the more messages. Defaults to 0.\n\n    shuffle : bool, default=True\n        Whether to shuffle the data before splitting it in batches.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    method : {'lars', 'cd'}, default='lars'\n        Method to be used for optimization.\n        lars: uses the least angle regression method to solve the lasso problem\n        (linear_model.lars_path)\n        cd: uses the coordinate descent method to compute the\n        Lasso solution (linear_model.Lasso). Lars will be faster if\n        the estimated components are sparse.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for random shuffling when ``shuffle`` is set to ``True``,\n        during online dictionary learning. Pass an int for reproducible results\n        across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    tol : float, default=1e-3\n        Control early stopping based on the norm of the differences in the\n        dictionary between 2 steps.\n\n        To disable early stopping based on changes in the dictionary, set\n        `tol` to 0.0.\n\n        .. versionadded:: 1.1\n\n    max_no_improvement : int or None, default=10\n        Control early stopping based on the consecutive number of mini batches\n        that does not yield an improvement on the smoothed cost function.\n\n        To disable convergence detection based on cost function, set\n        `max_no_improvement` to `None`.\n\n        .. versionadded:: 1.1\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Sparse components extracted from the data.\n\n    n_components_ : int\n        Estimated number of components.\n\n        .. versionadded:: 0.23\n\n    n_iter_ : int\n        Number of iterations run.\n\n    mean_ : ndarray of shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n        Equal to ``X.mean(axis=0)``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    IncrementalPCA : Incremental principal components analysis.\n    PCA : Principal component analysis.\n    SparsePCA : Sparse Principal Components Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.decomposition import MiniBatchSparsePCA\n    >>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)\n    >>> transformer = MiniBatchSparsePCA(n_components=5, batch_size=50,\n    ...                                  max_iter=10, random_state=0)\n    >>> transformer.fit(X)\n    MiniBatchSparsePCA(...)\n    >>> X_transformed = transformer.transform(X)\n    >>> X_transformed.shape\n    (200, 5)\n    >>> # most values in the components_ are zero (sparsity)\n    >>> np.mean(transformer.components_ == 0)\n    np.float64(0.9...)\n    ",
    "sklearn.decomposition.NMF": "Non-Negative Matrix Factorization (NMF).\n\n    Find two non-negative matrices, i.e. matrices with all non-negative elements, (W, H)\n    whose product approximates the non-negative matrix X. This factorization can be used\n    for example for dimensionality reduction, source separation or topic extraction.\n\n    The objective function is:\n\n    .. math::\n\n        L(W, H) &= 0.5 * ||X - WH||_{loss}^2\n\n                &+ alpha\\_W * l1\\_ratio * n\\_features * ||vec(W)||_1\n\n                &+ alpha\\_H * l1\\_ratio * n\\_samples * ||vec(H)||_1\n\n                &+ 0.5 * alpha\\_W * (1 - l1\\_ratio) * n\\_features * ||W||_{Fro}^2\n\n                &+ 0.5 * alpha\\_H * (1 - l1\\_ratio) * n\\_samples * ||H||_{Fro}^2,\n\n    where :math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm) and\n    :math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm).\n\n    The generic norm :math:`||X - WH||_{loss}` may represent\n    the Frobenius norm or another supported beta-divergence loss.\n    The choice between options is controlled by the `beta_loss` parameter.\n\n    The regularization terms are scaled by `n_features` for `W` and by `n_samples` for\n    `H` to keep their impact balanced with respect to one another and to the data fit\n    term as independent as possible of the size `n_samples` of the training set.\n\n    The objective function is minimized with an alternating minimization of W\n    and H.\n\n    Note that the transformed data is named W and the components matrix is named H. In\n    the NMF literature, the naming convention is usually the opposite since the data\n    matrix X is transposed.\n\n    Read more in the :ref:`User Guide <NMF>`.\n\n    Parameters\n    ----------\n    n_components : int or {'auto'} or None, default='auto'\n        Number of components. If `None`, all features are kept.\n        If `n_components='auto'`, the number of components is automatically inferred\n        from W or H shapes.\n\n        .. versionchanged:: 1.4\n            Added `'auto'` value.\n\n        .. versionchanged:: 1.6\n            Default value changed from `None` to `'auto'`.\n\n    init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n        Method used to initialize the procedure.\n        Valid options:\n\n        - `None`: 'nndsvda' if n_components <= min(n_samples, n_features),\n          otherwise random.\n\n        - `'random'`: non-negative random matrices, scaled with:\n          `sqrt(X.mean() / n_components)`\n\n        - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)\n          initialization (better for sparseness)\n\n        - `'nndsvda'`: NNDSVD with zeros filled with the average of X\n          (better when sparsity is not desired)\n\n        - `'nndsvdar'` NNDSVD with zeros filled with small random values\n          (generally faster, less accurate alternative to NNDSVDa\n          for when sparsity is not desired)\n\n        - `'custom'`: Use custom matrices `W` and `H` which must both be provided.\n\n        .. versionchanged:: 1.1\n            When `init=None` and n_components is less than n_samples and n_features\n            defaults to `nndsvda` instead of `nndsvd`.\n\n    solver : {'cd', 'mu'}, default='cd'\n        Numerical solver to use:\n\n        - 'cd' is a Coordinate Descent solver.\n        - 'mu' is a Multiplicative Update solver.\n\n        .. versionadded:: 0.17\n           Coordinate Descent solver.\n\n        .. versionadded:: 0.19\n           Multiplicative Update solver.\n\n    beta_loss : float or {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'\n        Beta divergence to be minimized, measuring the distance between X\n        and the dot product WH. Note that values different from 'frobenius'\n        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n        fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n        matrix X cannot contain zeros. Used only in 'mu' solver.\n\n        .. versionadded:: 0.19\n\n    tol : float, default=1e-4\n        Tolerance of the stopping condition.\n\n    max_iter : int, default=200\n        Maximum number of iterations before timing out.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initialisation (when ``init`` == 'nndsvdar' or\n        'random'), and in Coordinate Descent. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    alpha_W : float, default=0.0\n        Constant that multiplies the regularization terms of `W`. Set it to zero\n        (default) to have no regularization on `W`.\n\n        .. versionadded:: 1.0\n\n    alpha_H : float or \"same\", default=\"same\"\n        Constant that multiplies the regularization terms of `H`. Set it to zero to\n        have no regularization on `H`. If \"same\" (default), it takes the same value as\n        `alpha_W`.\n\n        .. versionadded:: 1.0\n\n    l1_ratio : float, default=0.0\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\n        (aka Frobenius Norm).\n        For l1_ratio = 1 it is an elementwise L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n        .. versionadded:: 0.17\n           Regularization parameter *l1_ratio* used in the Coordinate Descent\n           solver.\n\n    verbose : int, default=0\n        Whether to be verbose.\n\n    shuffle : bool, default=False\n        If true, randomize the order of coordinates in the CD solver.\n\n        .. versionadded:: 0.17\n           *shuffle* parameter used in the Coordinate Descent solver.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Factorization matrix, sometimes called 'dictionary'.\n\n    n_components_ : int\n        The number of components. It is same as the `n_components` parameter\n        if it was given. Otherwise, it will be same as the number of\n        features.\n\n    reconstruction_err_ : float\n        Frobenius norm of the matrix difference, or beta-divergence, between\n        the training data ``X`` and the reconstructed data ``WH`` from\n        the fitted model.\n\n    n_iter_ : int\n        Actual number of iterations.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    PCA : Principal component analysis.\n    SparseCoder : Find a sparse representation of data from a fixed,\n        precomputed dictionary.\n    SparsePCA : Sparse Principal Components Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n\n    References\n    ----------\n    .. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n       factorizations\" <10.1587/transfun.E92.A.708>`\n       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n       of electronics, communications and computer sciences 92.3: 708-721, 2009.\n\n    .. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n       beta-divergence\" <10.1162/NECO_a_00168>`\n       Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import NMF\n    >>> model = NMF(n_components=2, init='random', random_state=0)\n    >>> W = model.fit_transform(X)\n    >>> H = model.components_\n    ",
    "sklearn.decomposition.PCA": "Principal component analysis (PCA).\n\n    Linear dimensionality reduction using Singular Value Decomposition of the\n    data to project it to a lower dimensional space. The input data is centered\n    but not scaled for each feature before applying the SVD.\n\n    It uses the LAPACK implementation of the full SVD or a randomized truncated\n    SVD by the method of Halko et al. 2009, depending on the shape of the input\n    data and the number of components to extract.\n\n    With sparse inputs, the ARPACK implementation of the truncated SVD can be\n    used (i.e. through :func:`scipy.sparse.linalg.svds`). Alternatively, one\n    may consider :class:`TruncatedSVD` where the data are not centered.\n\n    Notice that this class only supports sparse inputs for some solvers such as\n    \"arpack\" and \"covariance_eigh\". See :class:`TruncatedSVD` for an\n    alternative with sparse data.\n\n    For a usage example, see\n    :ref:`sphx_glr_auto_examples_decomposition_plot_pca_iris.py`\n\n    Read more in the :ref:`User Guide <PCA>`.\n\n    Parameters\n    ----------\n    n_components : int, float or 'mle', default=None\n        Number of components to keep.\n        if n_components is not set all components are kept::\n\n            n_components == min(n_samples, n_features)\n\n        If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\n        MLE is used to guess the dimension. Use of ``n_components == 'mle'``\n        will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n\n        If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\n        number of components such that the amount of variance that needs to be\n        explained is greater than the percentage specified by n_components.\n\n        If ``svd_solver == 'arpack'``, the number of components must be\n        strictly less than the minimum of n_features and n_samples.\n\n        Hence, the None case results in::\n\n            n_components == min(n_samples, n_features) - 1\n\n    copy : bool, default=True\n        If False, data passed to fit are overwritten and running\n        fit(X).transform(X) will not yield the expected results,\n        use fit_transform(X) instead.\n\n    whiten : bool, default=False\n        When True (False by default) the `components_` vectors are multiplied\n        by the square root of n_samples and then divided by the singular values\n        to ensure uncorrelated outputs with unit component-wise variances.\n\n        Whitening will remove some information from the transformed signal\n        (the relative variance scales of the components) but can sometime\n        improve the predictive accuracy of the downstream estimators by\n        making their data respect some hard-wired assumptions.\n\n    svd_solver : {'auto', 'full', 'covariance_eigh', 'arpack', 'randomized'},            default='auto'\n        \"auto\" :\n            The solver is selected by a default 'auto' policy is based on `X.shape` and\n            `n_components`: if the input data has fewer than 1000 features and\n            more than 10 times as many samples, then the \"covariance_eigh\"\n            solver is used. Otherwise, if the input data is larger than 500x500\n            and the number of components to extract is lower than 80% of the\n            smallest dimension of the data, then the more efficient\n            \"randomized\" method is selected. Otherwise the exact \"full\" SVD is\n            computed and optionally truncated afterwards.\n        \"full\" :\n            Run exact full SVD calling the standard LAPACK solver via\n            `scipy.linalg.svd` and select the components by postprocessing\n        \"covariance_eigh\" :\n            Precompute the covariance matrix (on centered data), run a\n            classical eigenvalue decomposition on the covariance matrix\n            typically using LAPACK and select the components by postprocessing.\n            This solver is very efficient for n_samples >> n_features and small\n            n_features. It is, however, not tractable otherwise for large\n            n_features (large memory footprint required to materialize the\n            covariance matrix). Also note that compared to the \"full\" solver,\n            this solver effectively doubles the condition number and is\n            therefore less numerical stable (e.g. on input data with a large\n            range of singular values).\n        \"arpack\" :\n            Run SVD truncated to `n_components` calling ARPACK solver via\n            `scipy.sparse.linalg.svds`. It requires strictly\n            `0 < n_components < min(X.shape)`\n        \"randomized\" :\n            Run randomized SVD by the method of Halko et al.\n\n        .. versionadded:: 0.18.0\n\n        .. versionchanged:: 1.5\n            Added the 'covariance_eigh' solver.\n\n    tol : float, default=0.0\n        Tolerance for singular values computed by svd_solver == 'arpack'.\n        Must be of range [0.0, infinity).\n\n        .. versionadded:: 0.18.0\n\n    iterated_power : int or 'auto', default='auto'\n        Number of iterations for the power method computed by\n        svd_solver == 'randomized'.\n        Must be of range [0, infinity).\n\n        .. versionadded:: 0.18.0\n\n    n_oversamples : int, default=10\n        This parameter is only relevant when `svd_solver=\"randomized\"`.\n        It corresponds to the additional number of random vectors to sample the\n        range of `X` so as to ensure proper conditioning. See\n        :func:`~sklearn.utils.extmath.randomized_svd` for more details.\n\n        .. versionadded:: 1.1\n\n    power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n        Power iteration normalizer for randomized SVD solver.\n        Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd`\n        for more details.\n\n        .. versionadded:: 1.1\n\n    random_state : int, RandomState instance or None, default=None\n        Used when the 'arpack' or 'randomized' solvers are used. Pass an int\n        for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n        .. versionadded:: 0.18.0\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Principal axes in feature space, representing the directions of\n        maximum variance in the data. Equivalently, the right singular\n        vectors of the centered input data, parallel to its eigenvectors.\n        The components are sorted by decreasing ``explained_variance_``.\n\n    explained_variance_ : ndarray of shape (n_components,)\n        The amount of variance explained by each of the selected components.\n        The variance estimation uses `n_samples - 1` degrees of freedom.\n\n        Equal to n_components largest eigenvalues\n        of the covariance matrix of X.\n\n        .. versionadded:: 0.18\n\n    explained_variance_ratio_ : ndarray of shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n\n        If ``n_components`` is not set then all components are stored and the\n        sum of the ratios is equal to 1.0.\n\n    singular_values_ : ndarray of shape (n_components,)\n        The singular values corresponding to each of the selected components.\n        The singular values are equal to the 2-norms of the ``n_components``\n        variables in the lower-dimensional space.\n\n        .. versionadded:: 0.19\n\n    mean_ : ndarray of shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n\n        Equal to `X.mean(axis=0)`.\n\n    n_components_ : int\n        The estimated number of components. When n_components is set\n        to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this\n        number is estimated from input data. Otherwise it equals the parameter\n        n_components, or the lesser value of n_features and n_samples\n        if n_components is None.\n\n    n_samples_ : int\n        Number of samples in the training data.\n\n    noise_variance_ : float\n        The estimated noise covariance following the Probabilistic PCA model\n        from Tipping and Bishop 1999. See \"Pattern Recognition and\n        Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n        http://www.miketipping.com/papers/met-mppca.pdf. It is required to\n        compute the estimated data covariance and score samples.\n\n        Equal to the average of (min(n_features, n_samples) - n_components)\n        smallest eigenvalues of the covariance matrix of X.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    KernelPCA : Kernel Principal Component Analysis.\n    SparsePCA : Sparse Principal Component Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n    IncrementalPCA : Incremental Principal Component Analysis.\n\n    References\n    ----------\n    For n_components == 'mle', this class uses the method from:\n    `Minka, T. P.. \"Automatic choice of dimensionality for PCA\".\n    In NIPS, pp. 598-604 <https://tminka.github.io/papers/pca/minka-pca.pdf>`_\n\n    Implements the probabilistic PCA model from:\n    `Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal\n    component analysis\". Journal of the Royal Statistical Society:\n    Series B (Statistical Methodology), 61(3), 611-622.\n    <http://www.miketipping.com/papers/met-mppca.pdf>`_\n    via the score and score_samples methods.\n\n    For svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.\n\n    For svd_solver == 'randomized', see:\n    :doi:`Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n    \"Finding structure with randomness: Probabilistic algorithms for\n    constructing approximate matrix decompositions\".\n    SIAM review, 53(2), 217-288.\n    <10.1137/090771806>`\n    and also\n    :doi:`Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n    \"A randomized algorithm for the decomposition of matrices\".\n    Applied and Computational Harmonic Analysis, 30(1), 47-68.\n    <10.1016/j.acha.2010.02.003>`\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.decomposition import PCA\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> pca = PCA(n_components=2)\n    >>> pca.fit(X)\n    PCA(n_components=2)\n    >>> print(pca.explained_variance_ratio_)\n    [0.9924... 0.0075...]\n    >>> print(pca.singular_values_)\n    [6.30061... 0.54980...]\n\n    >>> pca = PCA(n_components=2, svd_solver='full')\n    >>> pca.fit(X)\n    PCA(n_components=2, svd_solver='full')\n    >>> print(pca.explained_variance_ratio_)\n    [0.9924... 0.00755...]\n    >>> print(pca.singular_values_)\n    [6.30061... 0.54980...]\n\n    >>> pca = PCA(n_components=1, svd_solver='arpack')\n    >>> pca.fit(X)\n    PCA(n_components=1, svd_solver='arpack')\n    >>> print(pca.explained_variance_ratio_)\n    [0.99244...]\n    >>> print(pca.singular_values_)\n    [6.30061...]\n    ",
    "sklearn.decomposition.SparseCoder": "Sparse coding.\n\n    Finds a sparse representation of data against a fixed, precomputed\n    dictionary.\n\n    Each row of the result is the solution to a sparse coding problem.\n    The goal is to find a sparse array `code` such that::\n\n        X ~= code * dictionary\n\n    Read more in the :ref:`User Guide <SparseCoder>`.\n\n    Parameters\n    ----------\n    dictionary : ndarray of shape (n_components, n_features)\n        The dictionary atoms used for sparse coding. Lines are assumed to be\n        normalized to unit norm.\n\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp'\n        Algorithm used to transform the data:\n\n        - `'lars'`: uses the least angle regression method\n          (`linear_model.lars_path`);\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\n          the estimated components are sparse;\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n          solution;\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\n          the projection ``dictionary * X'``.\n\n    transform_n_nonzero_coefs : int, default=None\n        Number of nonzero coefficients to target in each column of the\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n        and is overridden by `alpha` in the `omp` case. If `None`, then\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\n\n    transform_alpha : float, default=None\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n        penalty applied to the L1 norm.\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\n        threshold below which coefficients will be squashed to zero.\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n        the reconstruction error targeted. In this case, it overrides\n        `n_nonzero_coefs`.\n        If `None`, default to 1.\n\n    split_sign : bool, default=False\n        Whether to split the sparse feature vector into the concatenation of\n        its negative part and its positive part. This can improve the\n        performance of downstream classifiers.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    positive_code : bool, default=False\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    transform_max_iter : int, default=1000\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n        `lasso_lars`.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    n_components_ : int\n        Number of atoms.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\n        dictionary learning algorithm.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    SparsePCA : Sparse Principal Components Analysis.\n    sparse_encode : Sparse coding where each row of the result is the solution\n        to a sparse coding problem.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.decomposition import SparseCoder\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\n    >>> dictionary = np.array(\n    ...     [[0, 1, 0],\n    ...      [-1, -1, 2],\n    ...      [1, 1, 1],\n    ...      [0, 1, 1],\n    ...      [0, 2, 1]],\n    ...    dtype=np.float64\n    ... )\n    >>> coder = SparseCoder(\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\n    ...     transform_alpha=1e-10,\n    ... )\n    >>> coder.transform(X)\n    array([[ 0.,  0., -1.,  0.,  0.],\n           [ 0.,  1.,  1.,  0.,  0.]])\n    ",
    "sklearn.decomposition.SparsePCA": "Sparse Principal Components Analysis (SparsePCA).\n\n    Finds the set of sparse components that can optimally reconstruct\n    the data.  The amount of sparseness is controllable by the coefficient\n    of the L1 penalty, given by the parameter alpha.\n\n    Read more in the :ref:`User Guide <SparsePCA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of sparse atoms to extract. If None, then ``n_components``\n        is set to ``n_features``.\n\n    alpha : float, default=1\n        Sparsity controlling parameter. Higher values lead to sparser\n        components.\n\n    ridge_alpha : float, default=0.01\n        Amount of ridge shrinkage to apply in order to improve\n        conditioning when calling the transform method.\n\n    max_iter : int, default=1000\n        Maximum number of iterations to perform.\n\n    tol : float, default=1e-8\n        Tolerance for the stopping condition.\n\n    method : {'lars', 'cd'}, default='lars'\n        Method to be used for optimization.\n        lars: uses the least angle regression method to solve the lasso problem\n        (linear_model.lars_path)\n        cd: uses the coordinate descent method to compute the\n        Lasso solution (linear_model.Lasso). Lars will be faster if\n        the estimated components are sparse.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    U_init : ndarray of shape (n_samples, n_components), default=None\n        Initial values for the loadings for warm restart scenarios. Only used\n        if `U_init` and `V_init` are not None.\n\n    V_init : ndarray of shape (n_components, n_features), default=None\n        Initial values for the components for warm restart scenarios. Only used\n        if `U_init` and `V_init` are not None.\n\n    verbose : int or bool, default=False\n        Controls the verbosity; the higher, the more messages. Defaults to 0.\n\n    random_state : int, RandomState instance or None, default=None\n        Used during dictionary learning. Pass an int for reproducible results\n        across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Sparse components extracted from the data.\n\n    error_ : ndarray\n        Vector of errors at each iteration.\n\n    n_components_ : int\n        Estimated number of components.\n\n        .. versionadded:: 0.23\n\n    n_iter_ : int\n        Number of iterations run.\n\n    mean_ : ndarray of shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n        Equal to ``X.mean(axis=0)``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    PCA : Principal Component Analysis implementation.\n    MiniBatchSparsePCA : Mini batch variant of `SparsePCA` that is faster but less\n        accurate.\n    DictionaryLearning : Generic dictionary learning problem using a sparse code.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.decomposition import SparsePCA\n    >>> X, _ = make_friedman1(n_samples=200, n_features=30, random_state=0)\n    >>> transformer = SparsePCA(n_components=5, random_state=0)\n    >>> transformer.fit(X)\n    SparsePCA(...)\n    >>> X_transformed = transformer.transform(X)\n    >>> X_transformed.shape\n    (200, 5)\n    >>> # most values in the components_ are zero (sparsity)\n    >>> np.mean(transformer.components_ == 0)\n    np.float64(0.9666...)\n    ",
    "sklearn.decomposition.TruncatedSVD": "Dimensionality reduction using truncated SVD (aka LSA).\n\n    This transformer performs linear dimensionality reduction by means of\n    truncated singular value decomposition (SVD). Contrary to PCA, this\n    estimator does not center the data before computing the singular value\n    decomposition. This means it can work with sparse matrices\n    efficiently.\n\n    In particular, truncated SVD works on term count/tf-idf matrices as\n    returned by the vectorizers in :mod:`sklearn.feature_extraction.text`. In\n    that context, it is known as latent semantic analysis (LSA).\n\n    This estimator supports two algorithms: a fast randomized SVD solver, and\n    a \"naive\" algorithm that uses ARPACK as an eigensolver on `X * X.T` or\n    `X.T * X`, whichever is more efficient.\n\n    Read more in the :ref:`User Guide <LSA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=2\n        Desired dimensionality of output data.\n        If algorithm='arpack', must be strictly less than the number of features.\n        If algorithm='randomized', must be less than or equal to the number of features.\n        The default value is useful for visualisation. For LSA, a value of\n        100 is recommended.\n\n    algorithm : {'arpack', 'randomized'}, default='randomized'\n        SVD solver to use. Either \"arpack\" for the ARPACK wrapper in SciPy\n        (scipy.sparse.linalg.svds), or \"randomized\" for the randomized\n        algorithm due to Halko (2009).\n\n    n_iter : int, default=5\n        Number of iterations for randomized SVD solver. Not used by ARPACK. The\n        default is larger than the default in\n        :func:`~sklearn.utils.extmath.randomized_svd` to handle sparse\n        matrices that may have large slowly decaying spectrum.\n\n    n_oversamples : int, default=10\n        Number of oversamples for randomized SVD solver. Not used by ARPACK.\n        See :func:`~sklearn.utils.extmath.randomized_svd` for a complete\n        description.\n\n        .. versionadded:: 1.1\n\n    power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n        Power iteration normalizer for randomized SVD solver.\n        Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd`\n        for more details.\n\n        .. versionadded:: 1.1\n\n    random_state : int, RandomState instance or None, default=None\n        Used during randomized svd. Pass an int for reproducible results across\n        multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    tol : float, default=0.0\n        Tolerance for ARPACK. 0 means machine precision. Ignored by randomized\n        SVD solver.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        The right singular vectors of the input data.\n\n    explained_variance_ : ndarray of shape (n_components,)\n        The variance of the training samples transformed by a projection to\n        each component.\n\n    explained_variance_ratio_ : ndarray of shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n\n    singular_values_ : ndarray of shape (n_components,)\n        The singular values corresponding to each of the selected components.\n        The singular values are equal to the 2-norms of the ``n_components``\n        variables in the lower-dimensional space.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    FactorAnalysis : A simple linear generative model with\n        Gaussian latent variables.\n    IncrementalPCA : Incremental principal components analysis.\n    KernelPCA : Kernel Principal component analysis.\n    NMF : Non-Negative Matrix Factorization.\n    PCA : Principal component analysis.\n\n    Notes\n    -----\n    SVD suffers from a problem called \"sign indeterminacy\", which means the\n    sign of the ``components_`` and the output from transform depend on the\n    algorithm and random state. To work around this, fit instances of this\n    class to data once, then keep the instance around to do transformations.\n\n    References\n    ----------\n    :arxiv:`Halko, et al. (2009). \"Finding structure with randomness:\n    Stochastic algorithms for constructing approximate matrix decompositions\"\n    <0909.4061>`\n\n    Examples\n    --------\n    >>> from sklearn.decomposition import TruncatedSVD\n    >>> from scipy.sparse import csr_matrix\n    >>> import numpy as np\n    >>> np.random.seed(0)\n    >>> X_dense = np.random.rand(100, 100)\n    >>> X_dense[:, 2 * np.arange(50)] = 0\n    >>> X = csr_matrix(X_dense)\n    >>> svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n    >>> svd.fit(X)\n    TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n    >>> print(svd.explained_variance_ratio_)\n    [0.0157... 0.0512... 0.0499... 0.0479... 0.0453...]\n    >>> print(svd.explained_variance_ratio_.sum())\n    0.2102...\n    >>> print(svd.singular_values_)\n    [35.2410...  4.5981...   4.5420...  4.4486...  4.3288...]\n    ",
    "sklearn.decomposition.dict_learning": "Solve a dictionary learning matrix factorization problem.\n\n    Finds the best dictionary and the corresponding sparse code for\n    approximating the data matrix X by solving::\n\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\n                     (U,V)\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\n\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\n    which is the sum of the absolute values of all the entries in the matrix.\n\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data matrix.\n\n    n_components : int\n        Number of dictionary atoms to extract.\n\n    alpha : int or float\n        Sparsity controlling parameter.\n\n    max_iter : int, default=100\n        Maximum number of iterations to perform.\n\n    tol : float, default=1e-8\n        Tolerance for the stopping condition.\n\n    method : {'lars', 'cd'}, default='lars'\n        The method used:\n\n        * `'lars'`: uses the least angle regression method to solve the lasso\n           problem (`linear_model.lars_path`);\n        * `'cd'`: uses the coordinate descent method to compute the\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\n          the estimated components are sparse.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    dict_init : ndarray of shape (n_components, n_features), default=None\n        Initial value for the dictionary for warm restart scenarios. Only used\n        if `code_init` and `dict_init` are not None.\n\n    code_init : ndarray of shape (n_samples, n_components), default=None\n        Initial value for the sparse code for warm restart scenarios. Only used\n        if `code_init` and `dict_init` are not None.\n\n    callback : callable, default=None\n        Callable that gets invoked every five iterations.\n\n    verbose : bool, default=False\n        To control the verbosity of the procedure.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for randomly initializing the dictionary. Pass an int for\n        reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    positive_dict : bool, default=False\n        Whether to enforce positivity when finding the dictionary.\n\n        .. versionadded:: 0.20\n\n    positive_code : bool, default=False\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    method_max_iter : int, default=1000\n        Maximum number of iterations to perform.\n\n        .. versionadded:: 0.22\n\n    Returns\n    -------\n    code : ndarray of shape (n_samples, n_components)\n        The sparse code factor in the matrix factorization.\n\n    dictionary : ndarray of shape (n_components, n_features),\n        The dictionary factor in the matrix factorization.\n\n    errors : array\n        Vector of errors at each iteration.\n\n    n_iter : int\n        Number of iterations run. Returned only if `return_n_iter` is\n        set to True.\n\n    See Also\n    --------\n    dict_learning_online : Solve a dictionary learning matrix factorization\n        problem online.\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchDictionaryLearning : A faster, less accurate version\n        of the dictionary learning algorithm.\n    SparsePCA : Sparse Principal Components Analysis.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_sparse_coded_signal\n    >>> from sklearn.decomposition import dict_learning\n    >>> X, _, _ = make_sparse_coded_signal(\n    ...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\n    ...     random_state=42,\n    ... )\n    >>> U, V, errors = dict_learning(X, n_components=15, alpha=0.1, random_state=42)\n\n    We can check the level of sparsity of `U`:\n\n    >>> np.mean(U == 0)\n    np.float64(0.6...)\n\n    We can compare the average squared euclidean norm of the reconstruction\n    error of the sparse coded signal relative to the squared euclidean norm of\n    the original signal:\n\n    >>> X_hat = U @ V\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n    np.float64(0.01...)\n    ",
    "sklearn.decomposition.dict_learning_online": "Solve a dictionary learning matrix factorization problem online.\n\n    Finds the best dictionary and the corresponding sparse code for\n    approximating the data matrix X by solving::\n\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\n                     (U,V)\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\n\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\n    which is the sum of the absolute values of all the entries in the matrix.\n    This is accomplished by repeatedly iterating over mini-batches by slicing\n    the input data.\n\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data matrix.\n\n    n_components : int or None, default=2\n        Number of dictionary atoms to extract. If None, then ``n_components``\n        is set to ``n_features``.\n\n    alpha : float, default=1\n        Sparsity controlling parameter.\n\n    max_iter : int, default=100\n        Maximum number of iterations over the complete dataset before\n        stopping independently of any early stopping criterion heuristics.\n\n        .. versionadded:: 1.1\n\n    return_code : bool, default=True\n        Whether to also return the code U or just the dictionary `V`.\n\n    dict_init : ndarray of shape (n_components, n_features), default=None\n        Initial values for the dictionary for warm restart scenarios.\n        If `None`, the initial values for the dictionary are created\n        with an SVD decomposition of the data via\n        :func:`~sklearn.utils.extmath.randomized_svd`.\n\n    callback : callable, default=None\n        A callable that gets invoked at the end of each iteration.\n\n    batch_size : int, default=256\n        The number of samples to take in each batch.\n\n        .. versionchanged:: 1.3\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\n\n    verbose : bool, default=False\n        To control the verbosity of the procedure.\n\n    shuffle : bool, default=True\n        Whether to shuffle the data before splitting it in batches.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    method : {'lars', 'cd'}, default='lars'\n        * `'lars'`: uses the least angle regression method to solve the lasso\n          problem (`linear_model.lars_path`);\n        * `'cd'`: uses the coordinate descent method to compute the\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\n          the estimated components are sparse.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initializing the dictionary when ``dict_init`` is not\n        specified, randomly shuffling the data when ``shuffle`` is set to\n        ``True``, and updating the dictionary. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    positive_dict : bool, default=False\n        Whether to enforce positivity when finding the dictionary.\n\n        .. versionadded:: 0.20\n\n    positive_code : bool, default=False\n        Whether to enforce positivity when finding the code.\n\n        .. versionadded:: 0.20\n\n    method_max_iter : int, default=1000\n        Maximum number of iterations to perform when solving the lasso problem.\n\n        .. versionadded:: 0.22\n\n    tol : float, default=1e-3\n        Control early stopping based on the norm of the differences in the\n        dictionary between 2 steps.\n\n        To disable early stopping based on changes in the dictionary, set\n        `tol` to 0.0.\n\n        .. versionadded:: 1.1\n\n    max_no_improvement : int, default=10\n        Control early stopping based on the consecutive number of mini batches\n        that does not yield an improvement on the smoothed cost function.\n\n        To disable convergence detection based on cost function, set\n        `max_no_improvement` to None.\n\n        .. versionadded:: 1.1\n\n    Returns\n    -------\n    code : ndarray of shape (n_samples, n_components),\n        The sparse code (only returned if `return_code=True`).\n\n    dictionary : ndarray of shape (n_components, n_features),\n        The solutions to the dictionary learning problem.\n\n    n_iter : int\n        Number of iterations run. Returned only if `return_n_iter` is\n        set to `True`.\n\n    See Also\n    --------\n    dict_learning : Solve a dictionary learning matrix factorization problem.\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\n        learning algorithm.\n    SparsePCA : Sparse Principal Components Analysis.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.datasets import make_sparse_coded_signal\n    >>> from sklearn.decomposition import dict_learning_online\n    >>> X, _, _ = make_sparse_coded_signal(\n    ...     n_samples=30, n_components=15, n_features=20, n_nonzero_coefs=10,\n    ...     random_state=42,\n    ... )\n    >>> U, V = dict_learning_online(\n    ...     X, n_components=15, alpha=0.2, max_iter=20, batch_size=3, random_state=42\n    ... )\n\n    We can check the level of sparsity of `U`:\n\n    >>> np.mean(U == 0)\n    np.float64(0.53...)\n\n    We can compare the average squared euclidean norm of the reconstruction\n    error of the sparse coded signal relative to the squared euclidean norm of\n    the original signal:\n\n    >>> X_hat = U @ V\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\n    np.float64(0.05...)\n    ",
    "sklearn.decomposition.fastica": "Perform Fast Independent Component Analysis.\n\n    The implementation is based on [1]_.\n\n    Read more in the :ref:`User Guide <ICA>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Training vector, where `n_samples` is the number of samples and\n        `n_features` is the number of features.\n\n    n_components : int, default=None\n        Number of components to use. If None is passed, all are used.\n\n    algorithm : {'parallel', 'deflation'}, default='parallel'\n        Specify which algorithm to use for FastICA.\n\n    whiten : str or bool, default='unit-variance'\n        Specify the whitening strategy to use.\n\n        - If 'arbitrary-variance', a whitening with variance\n          arbitrary is used.\n        - If 'unit-variance', the whitening matrix is rescaled to ensure that\n          each recovered source has unit variance.\n        - If False, the data is already considered to be whitened, and no\n          whitening is performed.\n\n        .. versionchanged:: 1.3\n            The default value of `whiten` changed to 'unit-variance' in 1.3.\n\n    fun : {'logcosh', 'exp', 'cube'} or callable, default='logcosh'\n        The functional form of the G function used in the\n        approximation to neg-entropy. Could be either 'logcosh', 'exp',\n        or 'cube'.\n        You can also provide your own function. It should return a tuple\n        containing the value of the function, and of its derivative, in the\n        point. The derivative should be averaged along its last dimension.\n        Example::\n\n            def my_g(x):\n                return x ** 3, (3 * x ** 2).mean(axis=-1)\n\n    fun_args : dict, default=None\n        Arguments to send to the functional form.\n        If empty or None and if fun='logcosh', fun_args will take value\n        {'alpha' : 1.0}.\n\n    max_iter : int, default=200\n        Maximum number of iterations to perform.\n\n    tol : float, default=1e-4\n        A positive scalar giving the tolerance at which the\n        un-mixing matrix is considered to have converged.\n\n    w_init : ndarray of shape (n_components, n_components), default=None\n        Initial un-mixing array. If `w_init=None`, then an array of values\n        drawn from a normal distribution is used.\n\n    whiten_solver : {\"eigh\", \"svd\"}, default=\"svd\"\n        The solver to use for whitening.\n\n        - \"svd\" is more stable numerically if the problem is degenerate, and\n          often faster when `n_samples <= n_features`.\n\n        - \"eigh\" is generally more memory efficient when\n          `n_samples >= n_features`, and can be faster when\n          `n_samples >= 50 * n_features`.\n\n        .. versionadded:: 1.2\n\n    random_state : int, RandomState instance or None, default=None\n        Used to initialize ``w_init`` when not specified, with a\n        normal distribution. Pass an int, for reproducible results\n        across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    return_X_mean : bool, default=False\n        If True, X_mean is returned too.\n\n    compute_sources : bool, default=True\n        If False, sources are not computed, but only the rotation matrix.\n        This can save memory when working with big data. Defaults to True.\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n    K : ndarray of shape (n_components, n_features) or None\n        If whiten is 'True', K is the pre-whitening matrix that projects data\n        onto the first n_components principal components. If whiten is 'False',\n        K is 'None'.\n\n    W : ndarray of shape (n_components, n_components)\n        The square matrix that unmixes the data after whitening.\n        The mixing matrix is the pseudo-inverse of matrix ``W K``\n        if K is not None, else it is the inverse of W.\n\n    S : ndarray of shape (n_samples, n_components) or None\n        Estimated source matrix.\n\n    X_mean : ndarray of shape (n_features,)\n        The mean over features. Returned only if return_X_mean is True.\n\n    n_iter : int\n        If the algorithm is \"deflation\", n_iter is the\n        maximum number of iterations run across all components. Else\n        they are just the number of iterations taken to converge. This is\n        returned only when return_n_iter is set to `True`.\n\n    Notes\n    -----\n    The data matrix X is considered to be a linear combination of\n    non-Gaussian (independent) components i.e. X = AS where columns of S\n    contain the independent components and A is a linear mixing\n    matrix. In short ICA attempts to `un-mix' the data by estimating an\n    un-mixing matrix W where ``S = W K X.``\n    While FastICA was proposed to estimate as many sources\n    as features, it is possible to estimate less by setting\n    n_components < n_features. It this case K is not a square matrix\n    and the estimated A is the pseudo-inverse of ``W K``.\n\n    This implementation was originally made for data of shape\n    [n_features, n_samples]. Now the input is transposed\n    before the algorithm is applied. This makes it slightly\n    faster for Fortran-ordered input.\n\n    References\n    ----------\n    .. [1] A. Hyvarinen and E. Oja, \"Fast Independent Component Analysis\",\n           Algorithms and Applications, Neural Networks, 13(4-5), 2000,\n           pp. 411-430.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import fastica\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> K, W, S = fastica(X, n_components=7, random_state=0, whiten='unit-variance')\n    >>> K.shape\n    (7, 64)\n    >>> W.shape\n    (7, 7)\n    >>> S.shape\n    (1797, 7)\n    ",
    "sklearn.decomposition.non_negative_factorization": "Compute Non-negative Matrix Factorization (NMF).\n\n    Find two non-negative matrices (W, H) whose product approximates the non-\n    negative matrix X. This factorization can be used for example for\n    dimensionality reduction, source separation or topic extraction.\n\n    The objective function is:\n\n    .. math::\n\n        L(W, H) &= 0.5 * ||X - WH||_{loss}^2\n\n                &+ alpha\\_W * l1\\_ratio * n\\_features * ||vec(W)||_1\n\n                &+ alpha\\_H * l1\\_ratio * n\\_samples * ||vec(H)||_1\n\n                &+ 0.5 * alpha\\_W * (1 - l1\\_ratio) * n\\_features * ||W||_{Fro}^2\n\n                &+ 0.5 * alpha\\_H * (1 - l1\\_ratio) * n\\_samples * ||H||_{Fro}^2,\n\n    where :math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm) and\n    :math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)\n\n    The generic norm :math:`||X - WH||_{loss}^2` may represent\n    the Frobenius norm or another supported beta-divergence loss.\n    The choice between options is controlled by the `beta_loss` parameter.\n\n    The regularization terms are scaled by `n_features` for `W` and by `n_samples` for\n    `H` to keep their impact balanced with respect to one another and to the data fit\n    term as independent as possible of the size `n_samples` of the training set.\n\n    The objective function is minimized with an alternating minimization of W\n    and H. If H is given and update_H=False, it solves for W only.\n\n    Note that the transformed data is named W and the components matrix is named H. In\n    the NMF literature, the naming convention is usually the opposite since the data\n    matrix X is transposed.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Constant matrix.\n\n    W : array-like of shape (n_samples, n_components), default=None\n        If `init='custom'`, it is used as initial guess for the solution.\n        If `update_H=False`, it is initialised as an array of zeros, unless\n        `solver='mu'`, then it is filled with values calculated by\n        `np.sqrt(X.mean() / self._n_components)`.\n        If `None`, uses the initialisation method specified in `init`.\n\n    H : array-like of shape (n_components, n_features), default=None\n        If `init='custom'`, it is used as initial guess for the solution.\n        If `update_H=False`, it is used as a constant, to solve for W only.\n        If `None`, uses the initialisation method specified in `init`.\n\n    n_components : int or {'auto'} or None, default='auto'\n        Number of components. If `None`, all features are kept.\n        If `n_components='auto'`, the number of components is automatically inferred\n        from `W` or `H` shapes.\n\n        .. versionchanged:: 1.4\n            Added `'auto'` value.\n\n        .. versionchanged:: 1.6\n            Default value changed from `None` to `'auto'`.\n\n    init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n        Method used to initialize the procedure.\n\n        Valid options:\n\n        - None: 'nndsvda' if n_components < n_features, otherwise 'random'.\n        - 'random': non-negative random matrices, scaled with:\n          `sqrt(X.mean() / n_components)`\n        - 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)\n          initialization (better for sparseness)\n        - 'nndsvda': NNDSVD with zeros filled with the average of X\n          (better when sparsity is not desired)\n        - 'nndsvdar': NNDSVD with zeros filled with small random values\n          (generally faster, less accurate alternative to NNDSVDa\n          for when sparsity is not desired)\n        - 'custom': If `update_H=True`, use custom matrices W and H which must both\n          be provided. If `update_H=False`, then only custom matrix H is used.\n\n        .. versionchanged:: 0.23\n            The default value of `init` changed from 'random' to None in 0.23.\n\n        .. versionchanged:: 1.1\n            When `init=None` and n_components is less than n_samples and n_features\n            defaults to `nndsvda` instead of `nndsvd`.\n\n    update_H : bool, default=True\n        Set to True, both W and H will be estimated from initial guesses.\n        Set to False, only W will be estimated.\n\n    solver : {'cd', 'mu'}, default='cd'\n        Numerical solver to use:\n\n        - 'cd' is a Coordinate Descent solver that uses Fast Hierarchical\n          Alternating Least Squares (Fast HALS).\n        - 'mu' is a Multiplicative Update solver.\n\n        .. versionadded:: 0.17\n           Coordinate Descent solver.\n\n        .. versionadded:: 0.19\n           Multiplicative Update solver.\n\n    beta_loss : float or {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'\n        Beta divergence to be minimized, measuring the distance between X\n        and the dot product WH. Note that values different from 'frobenius'\n        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n        fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n        matrix X cannot contain zeros. Used only in 'mu' solver.\n\n        .. versionadded:: 0.19\n\n    tol : float, default=1e-4\n        Tolerance of the stopping condition.\n\n    max_iter : int, default=200\n        Maximum number of iterations before timing out.\n\n    alpha_W : float, default=0.0\n        Constant that multiplies the regularization terms of `W`. Set it to zero\n        (default) to have no regularization on `W`.\n\n        .. versionadded:: 1.0\n\n    alpha_H : float or \"same\", default=\"same\"\n        Constant that multiplies the regularization terms of `H`. Set it to zero to\n        have no regularization on `H`. If \"same\" (default), it takes the same value as\n        `alpha_W`.\n\n        .. versionadded:: 1.0\n\n    l1_ratio : float, default=0.0\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\n        (aka Frobenius Norm).\n        For l1_ratio = 1 it is an elementwise L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for NMF initialisation (when ``init`` == 'nndsvdar' or\n        'random'), and in Coordinate Descent. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    verbose : int, default=0\n        The verbosity level.\n\n    shuffle : bool, default=False\n        If true, randomize the order of coordinates in the CD solver.\n\n    Returns\n    -------\n    W : ndarray of shape (n_samples, n_components)\n        Solution to the non-negative least squares problem.\n\n    H : ndarray of shape (n_components, n_features)\n        Solution to the non-negative least squares problem.\n\n    n_iter : int\n        Actual number of iterations.\n\n    References\n    ----------\n    .. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n       factorizations\" <10.1587/transfun.E92.A.708>`\n       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n       of electronics, communications and computer sciences 92.3: 708-721, 2009.\n\n    .. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n       beta-divergence\" <10.1162/NECO_a_00168>`\n       Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import non_negative_factorization\n    >>> W, H, n_iter = non_negative_factorization(\n    ...     X, n_components=2, init='random', random_state=0)\n    ",
    "sklearn.decomposition.randomized_svd": "Compute a truncated randomized SVD.\n\n    This method solves the fixed-rank approximation problem described in [1]_\n    (problem (1.5), p5).\n\n    Refer to\n    :ref:`sphx_glr_auto_examples_applications_wikipedia_principal_eigenvector.py`\n    for a typical example where the power iteration algorithm is used to rank web pages.\n    This algorithm is also known to be used as a building block in Google's PageRank\n    algorithm.\n\n    Parameters\n    ----------\n    M : {ndarray, sparse matrix}\n        Matrix to decompose.\n\n    n_components : int\n        Number of singular values and vectors to extract.\n\n    n_oversamples : int, default=10\n        Additional number of random vectors to sample the range of `M` so as\n        to ensure proper conditioning. The total number of random vectors\n        used to find the range of `M` is `n_components + n_oversamples`. Smaller\n        number can improve speed but can negatively impact the quality of\n        approximation of singular vectors and singular values. Users might wish\n        to increase this parameter up to `2*k - n_components` where k is the\n        effective rank, for large matrices, noisy problems, matrices with\n        slowly decaying spectrums, or to increase precision accuracy. See [1]_\n        (pages 5, 23 and 26).\n\n    n_iter : int or 'auto', default='auto'\n        Number of power iterations. It can be used to deal with very noisy\n        problems. When 'auto', it is set to 4, unless `n_components` is small\n        (< .1 * min(X.shape)) in which case `n_iter` is set to 7.\n        This improves precision with few components. Note that in general\n        users should rather increase `n_oversamples` before increasing `n_iter`\n        as the principle of the randomized method is to avoid usage of these\n        more costly power iterations steps. When `n_components` is equal\n        or greater to the effective matrix rank and the spectrum does not\n        present a slow decay, `n_iter=0` or `1` should even work fine in theory\n        (see [1]_ page 9).\n\n        .. versionchanged:: 0.18\n\n    power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n        Whether the power iterations are normalized with step-by-step\n        QR factorization (the slowest but most accurate), 'none'\n        (the fastest but numerically unstable when `n_iter` is large, e.g.\n        typically 5 or larger), or 'LU' factorization (numerically stable\n        but can lose slightly in accuracy). The 'auto' mode applies no\n        normalization if `n_iter` <= 2 and switches to LU otherwise.\n\n        .. versionadded:: 0.18\n\n    transpose : bool or 'auto', default='auto'\n        Whether the algorithm should be applied to M.T instead of M. The\n        result should approximately be the same. The 'auto' mode will\n        trigger the transposition if M.shape[1] > M.shape[0] since this\n        implementation of randomized SVD tend to be a little faster in that\n        case.\n\n        .. versionchanged:: 0.18\n\n    flip_sign : bool, default=True\n        The output of a singular value decomposition is only unique up to a\n        permutation of the signs of the singular vectors. If `flip_sign` is\n        set to `True`, the sign ambiguity is resolved by making the largest\n        loadings for each component in the left singular vectors positive.\n\n    random_state : int, RandomState instance or None, default='warn'\n        The seed of the pseudo random number generator to use when\n        shuffling the data, i.e. getting the random vectors to initialize\n        the algorithm. Pass an int for reproducible results across multiple\n        function calls. See :term:`Glossary <random_state>`.\n\n        .. versionchanged:: 1.2\n            The default value changed from 0 to None.\n\n    svd_lapack_driver : {\"gesdd\", \"gesvd\"}, default=\"gesdd\"\n        Whether to use the more efficient divide-and-conquer approach\n        (`\"gesdd\"`) or more general rectangular approach (`\"gesvd\"`) to compute\n        the SVD of the matrix B, which is the projection of M into a low\n        dimensional subspace, as described in [1]_.\n\n        .. versionadded:: 1.2\n\n    Returns\n    -------\n    u : ndarray of shape (n_samples, n_components)\n        Unitary matrix having left singular vectors with signs flipped as columns.\n    s : ndarray of shape (n_components,)\n        The singular values, sorted in non-increasing order.\n    vh : ndarray of shape (n_components, n_features)\n        Unitary matrix having right singular vectors with signs flipped as rows.\n\n    Notes\n    -----\n    This algorithm finds a (usually very good) approximate truncated\n    singular value decomposition using randomization to speed up the\n    computations. It is particularly fast on large matrices on which\n    you wish to extract only a small number of components. In order to\n    obtain further speed up, `n_iter` can be set <=2 (at the cost of\n    loss of precision). To increase the precision it is recommended to\n    increase `n_oversamples`, up to `2*k-n_components` where k is the\n    effective rank. Usually, `n_components` is chosen to be greater than k\n    so increasing `n_oversamples` up to `n_components` should be enough.\n\n    References\n    ----------\n    .. [1] :arxiv:`\"Finding structure with randomness:\n      Stochastic algorithms for constructing approximate matrix decompositions\"\n      <0909.4061>`\n      Halko, et al. (2009)\n\n    .. [2] A randomized algorithm for the decomposition of matrices\n      Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert\n\n    .. [3] An implementation of a randomized algorithm for principal component\n      analysis A. Szlam et al. 2014\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.utils.extmath import randomized_svd\n    >>> a = np.array([[1, 2, 3, 5],\n    ...               [3, 4, 5, 6],\n    ...               [7, 8, 9, 10]])\n    >>> U, s, Vh = randomized_svd(a, n_components=2, random_state=0)\n    >>> U.shape, s.shape, Vh.shape\n    ((3, 2), (2,), (2, 4))\n    ",
    "sklearn.decomposition.sparse_encode": "Sparse coding.\n\n    Each row of the result is the solution to a sparse coding problem.\n    The goal is to find a sparse array `code` such that::\n\n        X ~= code * dictionary\n\n    Read more in the :ref:`User Guide <SparseCoder>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Data matrix.\n\n    dictionary : array-like of shape (n_components, n_features)\n        The dictionary matrix against which to solve the sparse coding of\n        the data. Some of the algorithms assume normalized rows for meaningful\n        output.\n\n    gram : array-like of shape (n_components, n_components), default=None\n        Precomputed Gram matrix, `dictionary * dictionary'`.\n\n    cov : array-like of shape (n_components, n_samples), default=None\n        Precomputed covariance, `dictionary' * X`.\n\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'},             default='lasso_lars'\n        The algorithm used:\n\n        * `'lars'`: uses the least angle regression method\n          (`linear_model.lars_path`);\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\n          the estimated components are sparse;\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\n          solution;\n        * `'threshold'`: squashes to zero all coefficients less than\n          regularization from the projection `dictionary * data'`.\n\n    n_nonzero_coefs : int, default=None\n        Number of nonzero coefficients to target in each column of the\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\n        and is overridden by `alpha` in the `omp` case. If `None`, then\n        `n_nonzero_coefs=int(n_features / 10)`.\n\n    alpha : float, default=None\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\n        penalty applied to the L1 norm.\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\n        threshold below which coefficients will be squashed to zero.\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\n        the reconstruction error targeted. In this case, it overrides\n        `n_nonzero_coefs`.\n        If `None`, default to 1.\n\n    copy_cov : bool, default=True\n        Whether to copy the precomputed covariance matrix; if `False`, it may\n        be overwritten.\n\n    init : ndarray of shape (n_samples, n_components), default=None\n        Initialization value of the sparse codes. Only used if\n        `algorithm='lasso_cd'`.\n\n    max_iter : int, default=1000\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n        `'lasso_lars'`.\n\n    n_jobs : int, default=None\n        Number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    check_input : bool, default=True\n        If `False`, the input arrays X and dictionary will not be checked.\n\n    verbose : int, default=0\n        Controls the verbosity; the higher, the more messages.\n\n    positive : bool, default=False\n        Whether to enforce positivity when finding the encoding.\n\n        .. versionadded:: 0.20\n\n    Returns\n    -------\n    code : ndarray of shape (n_samples, n_components)\n        The sparse codes.\n\n    See Also\n    --------\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\n        path using LARS algorithm.\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\n        dictionary.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.decomposition import sparse_encode\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\n    >>> dictionary = np.array(\n    ...     [[0, 1, 0],\n    ...      [-1, -1, 2],\n    ...      [1, 1, 1],\n    ...      [0, 1, 1],\n    ...      [0, 2, 1]],\n    ...    dtype=np.float64\n    ... )\n    >>> sparse_encode(X, dictionary, alpha=1e-10)\n    array([[ 0.,  0., -1.,  0.,  0.],\n           [ 0.,  1.,  1.,  0.,  0.]])\n    ",
    "sklearn.manifold": "Data embedding techniques.",
    "sklearn.manifold.Isomap": "Isomap Embedding.\n\n    Non-linear dimensionality reduction through Isometric Mapping\n\n    Read more in the :ref:`User Guide <isomap>`.\n\n    Parameters\n    ----------\n    n_neighbors : int or None, default=5\n        Number of neighbors to consider for each point. If `n_neighbors` is an int,\n        then `radius` must be `None`.\n\n    radius : float or None, default=None\n        Limiting distance of neighbors to return. If `radius` is a float,\n        then `n_neighbors` must be set to `None`.\n\n        .. versionadded:: 1.1\n\n    n_components : int, default=2\n        Number of coordinates for the manifold.\n\n    eigen_solver : {'auto', 'arpack', 'dense'}, default='auto'\n        'auto' : Attempt to choose the most efficient solver\n        for the given problem.\n\n        'arpack' : Use Arnoldi decomposition to find the eigenvalues\n        and eigenvectors.\n\n        'dense' : Use a direct solver (i.e. LAPACK)\n        for the eigenvalue decomposition.\n\n    tol : float, default=0\n        Convergence tolerance passed to arpack or lobpcg.\n        not used if eigen_solver == 'dense'.\n\n    max_iter : int, default=None\n        Maximum number of iterations for the arpack solver.\n        not used if eigen_solver == 'dense'.\n\n    path_method : {'auto', 'FW', 'D'}, default='auto'\n        Method to use in finding shortest path.\n\n        'auto' : attempt to choose the best algorithm automatically.\n\n        'FW' : Floyd-Warshall algorithm.\n\n        'D' : Dijkstra's algorithm.\n\n    neighbors_algorithm : {'auto', 'brute', 'kd_tree', 'ball_tree'},                           default='auto'\n        Algorithm to use for nearest neighbors search,\n        passed to neighbors.NearestNeighbors instance.\n\n    n_jobs : int or None, default=None\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    metric : str, or callable, default=\"minkowski\"\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n        its metric parameter.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square. X may be a :term:`Glossary <sparse graph>`.\n\n        .. versionadded:: 0.22\n\n    p : float, default=2\n        Parameter for the Minkowski metric from\n        sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n        equivalent to using manhattan_distance (l1), and euclidean_distance\n        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n        .. versionadded:: 0.22\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    embedding_ : array-like, shape (n_samples, n_components)\n        Stores the embedding vectors.\n\n    kernel_pca_ : object\n        :class:`~sklearn.decomposition.KernelPCA` object used to implement the\n        embedding.\n\n    nbrs_ : sklearn.neighbors.NearestNeighbors instance\n        Stores nearest neighbors instance, including BallTree or KDtree\n        if applicable.\n\n    dist_matrix_ : array-like, shape (n_samples, n_samples)\n        Stores the geodesic distance matrix of training data.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    sklearn.decomposition.PCA : Principal component analysis that is a linear\n        dimensionality reduction method.\n    sklearn.decomposition.KernelPCA : Non-linear dimensionality reduction using\n        kernels and PCA.\n    MDS : Manifold learning using multidimensional scaling.\n    TSNE : T-distributed Stochastic Neighbor Embedding.\n    LocallyLinearEmbedding : Manifold learning using Locally Linear Embedding.\n    SpectralEmbedding : Spectral embedding for non-linear dimensionality.\n\n    References\n    ----------\n\n    .. [1] Tenenbaum, J.B.; De Silva, V.; & Langford, J.C. A global geometric\n           framework for nonlinear dimensionality reduction. Science 290 (5500)\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.manifold import Isomap\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> X.shape\n    (1797, 64)\n    >>> embedding = Isomap(n_components=2)\n    >>> X_transformed = embedding.fit_transform(X[:100])\n    >>> X_transformed.shape\n    (100, 2)\n    ",
    "sklearn.manifold.LocallyLinearEmbedding": "Locally Linear Embedding.\n\n    Read more in the :ref:`User Guide <locally_linear_embedding>`.\n\n    Parameters\n    ----------\n    n_neighbors : int, default=5\n        Number of neighbors to consider for each point.\n\n    n_components : int, default=2\n        Number of coordinates for the manifold.\n\n    reg : float, default=1e-3\n        Regularization constant, multiplies the trace of the local covariance\n        matrix of the distances.\n\n    eigen_solver : {'auto', 'arpack', 'dense'}, default='auto'\n        The solver used to compute the eigenvectors. The available options are:\n\n        - `'auto'` : algorithm will attempt to choose the best method for input\n          data.\n        - `'arpack'` : use arnoldi iteration in shift-invert mode. For this\n          method, M may be a dense matrix, sparse matrix, or general linear\n          operator.\n        - `'dense'`  : use standard dense matrix operations for the eigenvalue\n          decomposition. For this method, M must be an array or matrix type.\n          This method should be avoided for large problems.\n\n        .. warning::\n           ARPACK can be unstable for some problems.  It is best to try several\n           random seeds in order to check results.\n\n    tol : float, default=1e-6\n        Tolerance for 'arpack' method\n        Not used if eigen_solver=='dense'.\n\n    max_iter : int, default=100\n        Maximum number of iterations for the arpack solver.\n        Not used if eigen_solver=='dense'.\n\n    method : {'standard', 'hessian', 'modified', 'ltsa'}, default='standard'\n        - `standard`: use the standard locally linear embedding algorithm. see\n          reference [1]_\n        - `hessian`: use the Hessian eigenmap method. This method requires\n          ``n_neighbors > n_components * (1 + (n_components + 1) / 2``. see\n          reference [2]_\n        - `modified`: use the modified locally linear embedding algorithm.\n          see reference [3]_\n        - `ltsa`: use local tangent space alignment algorithm. see\n          reference [4]_\n\n    hessian_tol : float, default=1e-4\n        Tolerance for Hessian eigenmapping method.\n        Only used if ``method == 'hessian'``.\n\n    modified_tol : float, default=1e-12\n        Tolerance for modified LLE method.\n        Only used if ``method == 'modified'``.\n\n    neighbors_algorithm : {'auto', 'brute', 'kd_tree', 'ball_tree'},                           default='auto'\n        Algorithm to use for nearest neighbors search, passed to\n        :class:`~sklearn.neighbors.NearestNeighbors` instance.\n\n    random_state : int, RandomState instance, default=None\n        Determines the random number generator when\n        ``eigen_solver`` == 'arpack'. Pass an int for reproducible results\n        across multiple function calls. See :term:`Glossary <random_state>`.\n\n    n_jobs : int or None, default=None\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    embedding_ : array-like, shape [n_samples, n_components]\n        Stores the embedding vectors\n\n    reconstruction_error_ : float\n        Reconstruction error associated with `embedding_`\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    nbrs_ : NearestNeighbors object\n        Stores nearest neighbors instance, including BallTree or KDtree\n        if applicable.\n\n    See Also\n    --------\n    SpectralEmbedding : Spectral embedding for non-linear dimensionality\n        reduction.\n    TSNE : Distributed Stochastic Neighbor Embedding.\n\n    References\n    ----------\n\n    .. [1] Roweis, S. & Saul, L. Nonlinear dimensionality reduction\n        by locally linear embedding.  Science 290:2323 (2000).\n    .. [2] Donoho, D. & Grimes, C. Hessian eigenmaps: Locally\n        linear embedding techniques for high-dimensional data.\n        Proc Natl Acad Sci U S A.  100:5591 (2003).\n    .. [3] `Zhang, Z. & Wang, J. MLLE: Modified Locally Linear\n        Embedding Using Multiple Weights.\n        <https://citeseerx.ist.psu.edu/doc_view/pid/0b060fdbd92cbcc66b383bcaa9ba5e5e624d7ee3>`_\n    .. [4] Zhang, Z. & Zha, H. Principal manifolds and nonlinear\n        dimensionality reduction via tangent space alignment.\n        Journal of Shanghai Univ.  8:406 (2004)\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.manifold import LocallyLinearEmbedding\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> X.shape\n    (1797, 64)\n    >>> embedding = LocallyLinearEmbedding(n_components=2)\n    >>> X_transformed = embedding.fit_transform(X[:100])\n    >>> X_transformed.shape\n    (100, 2)\n    ",
    "sklearn.manifold.MDS": "Multidimensional scaling.\n\n    Read more in the :ref:`User Guide <multidimensional_scaling>`.\n\n    Parameters\n    ----------\n    n_components : int, default=2\n        Number of dimensions in which to immerse the dissimilarities.\n\n    metric : bool, default=True\n        If ``True``, perform metric MDS; otherwise, perform nonmetric MDS.\n        When ``False`` (i.e. non-metric MDS), dissimilarities with 0 are considered as\n        missing values.\n\n    n_init : int, default=4\n        Number of times the SMACOF algorithm will be run with different\n        initializations. The final results will be the best output of the runs,\n        determined by the run with the smallest final stress.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the SMACOF algorithm for a single run.\n\n    verbose : int, default=0\n        Level of verbosity.\n\n    eps : float, default=1e-3\n        Relative tolerance with respect to stress at which to declare\n        convergence. The value of `eps` should be tuned separately depending\n        on whether or not `normalized_stress` is being used.\n\n    n_jobs : int, default=None\n        The number of jobs to use for the computation. If multiple\n        initializations are used (``n_init``), each run of the algorithm is\n        computed in parallel.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines the random number generator used to initialize the centers.\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    dissimilarity : {'euclidean', 'precomputed'}, default='euclidean'\n        Dissimilarity measure to use:\n\n        - 'euclidean':\n            Pairwise Euclidean distances between points in the dataset.\n\n        - 'precomputed':\n            Pre-computed dissimilarities are passed directly to ``fit`` and\n            ``fit_transform``.\n\n    normalized_stress : bool or \"auto\" default=\"auto\"\n        Whether use and return normed stress value (Stress-1) instead of raw\n        stress calculated by default. Only supported in non-metric MDS.\n\n        .. versionadded:: 1.2\n\n        .. versionchanged:: 1.4\n           The default value changed from `False` to `\"auto\"` in version 1.4.\n\n    Attributes\n    ----------\n    embedding_ : ndarray of shape (n_samples, n_components)\n        Stores the position of the dataset in the embedding space.\n\n    stress_ : float\n        The final value of the stress (sum of squared distance of the\n        disparities and the distances for all constrained points).\n        If `normalized_stress=True`, and `metric=False` returns Stress-1.\n        A value of 0 indicates \"perfect\" fit, 0.025 excellent, 0.05 good,\n        0.1 fair, and 0.2 poor [1]_.\n\n    dissimilarity_matrix_ : ndarray of shape (n_samples, n_samples)\n        Pairwise dissimilarities between the points. Symmetric matrix that:\n\n        - either uses a custom dissimilarity matrix by setting `dissimilarity`\n          to 'precomputed';\n        - or constructs a dissimilarity matrix from data using\n          Euclidean distances.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : int\n        The number of iterations corresponding to the best stress.\n\n    See Also\n    --------\n    sklearn.decomposition.PCA : Principal component analysis that is a linear\n        dimensionality reduction method.\n    sklearn.decomposition.KernelPCA : Non-linear dimensionality reduction using\n        kernels and PCA.\n    TSNE : T-distributed Stochastic Neighbor Embedding.\n    Isomap : Manifold learning based on Isometric Mapping.\n    LocallyLinearEmbedding : Manifold learning using Locally Linear Embedding.\n    SpectralEmbedding : Spectral embedding for non-linear dimensionality.\n\n    References\n    ----------\n    .. [1] \"Nonmetric multidimensional scaling: a numerical method\" Kruskal, J.\n       Psychometrika, 29 (1964)\n\n    .. [2] \"Multidimensional scaling by optimizing goodness of fit to a nonmetric\n       hypothesis\" Kruskal, J. Psychometrika, 29, (1964)\n\n    .. [3] \"Modern Multidimensional Scaling - Theory and Applications\" Borg, I.;\n       Groenen P. Springer Series in Statistics (1997)\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.manifold import MDS\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> X.shape\n    (1797, 64)\n    >>> embedding = MDS(n_components=2, normalized_stress='auto')\n    >>> X_transformed = embedding.fit_transform(X[:100])\n    >>> X_transformed.shape\n    (100, 2)\n\n    For a more detailed example of usage, see\n    :ref:`sphx_glr_auto_examples_manifold_plot_mds.py`.\n\n    For a comparison of manifold learning techniques, see\n    :ref:`sphx_glr_auto_examples_manifold_plot_compare_methods.py`.\n    ",
    "sklearn.manifold.SpectralEmbedding": "Spectral embedding for non-linear dimensionality reduction.\n\n    Forms an affinity matrix given by the specified function and\n    applies spectral decomposition to the corresponding graph laplacian.\n    The resulting transformation is given by the value of the\n    eigenvectors for each data point.\n\n    Note : Laplacian Eigenmaps is the actual algorithm implemented here.\n\n    Read more in the :ref:`User Guide <spectral_embedding>`.\n\n    Parameters\n    ----------\n    n_components : int, default=2\n        The dimension of the projected subspace.\n\n    affinity : {'nearest_neighbors', 'rbf', 'precomputed',                 'precomputed_nearest_neighbors'} or callable,                 default='nearest_neighbors'\n        How to construct the affinity matrix.\n         - 'nearest_neighbors' : construct the affinity matrix by computing a\n           graph of nearest neighbors.\n         - 'rbf' : construct the affinity matrix by computing a radial basis\n           function (RBF) kernel.\n         - 'precomputed' : interpret ``X`` as a precomputed affinity matrix.\n         - 'precomputed_nearest_neighbors' : interpret ``X`` as a sparse graph\n           of precomputed nearest neighbors, and constructs the affinity matrix\n           by selecting the ``n_neighbors`` nearest neighbors.\n         - callable : use passed in function as affinity\n           the function takes in data matrix (n_samples, n_features)\n           and return affinity matrix (n_samples, n_samples).\n\n    gamma : float, default=None\n        Kernel coefficient for rbf kernel. If None, gamma will be set to\n        1/n_features.\n\n    random_state : int, RandomState instance or None, default=None\n        A pseudo random number generator used for the initialization\n        of the lobpcg eigen vectors decomposition when `eigen_solver ==\n        'amg'`, and for the K-Means initialization. Use an int to make\n        the results deterministic across calls (See\n        :term:`Glossary <random_state>`).\n\n        .. note::\n            When using `eigen_solver == 'amg'`,\n            it is necessary to also fix the global numpy seed with\n            `np.random.seed(int)` to get deterministic results. See\n            https://github.com/pyamg/pyamg/issues/139 for further\n            information.\n\n    eigen_solver : {'arpack', 'lobpcg', 'amg'}, default=None\n        The eigenvalue decomposition strategy to use. AMG requires pyamg\n        to be installed. It can be faster on very large, sparse problems.\n        If None, then ``'arpack'`` is used.\n\n    eigen_tol : float, default=\"auto\"\n        Stopping criterion for eigendecomposition of the Laplacian matrix.\n        If `eigen_tol=\"auto\"` then the passed tolerance will depend on the\n        `eigen_solver`:\n\n        - If `eigen_solver=\"arpack\"`, then `eigen_tol=0.0`;\n        - If `eigen_solver=\"lobpcg\"` or `eigen_solver=\"amg\"`, then\n          `eigen_tol=None` which configures the underlying `lobpcg` solver to\n          automatically resolve the value according to their heuristics. See,\n          :func:`scipy.sparse.linalg.lobpcg` for details.\n\n        Note that when using `eigen_solver=\"lobpcg\"` or `eigen_solver=\"amg\"`\n        values of `tol<1e-5` may lead to convergence issues and should be\n        avoided.\n\n        .. versionadded:: 1.2\n\n    n_neighbors : int, default=None\n        Number of nearest neighbors for nearest_neighbors graph building.\n        If None, n_neighbors will be set to max(n_samples/10, 1).\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    embedding_ : ndarray of shape (n_samples, n_components)\n        Spectral embedding of the training matrix.\n\n    affinity_matrix_ : ndarray of shape (n_samples, n_samples)\n        Affinity_matrix constructed from samples or precomputed.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_neighbors_ : int\n        Number of nearest neighbors effectively used.\n\n    See Also\n    --------\n    Isomap : Non-linear dimensionality reduction through Isometric Mapping.\n\n    References\n    ----------\n\n    - :doi:`A Tutorial on Spectral Clustering, 2007\n      Ulrike von Luxburg\n      <10.1007/s11222-007-9033-z>`\n\n    - `On Spectral Clustering: Analysis and an algorithm, 2001\n      Andrew Y. Ng, Michael I. Jordan, Yair Weiss\n      <https://citeseerx.ist.psu.edu/doc_view/pid/796c5d6336fc52aa84db575fb821c78918b65f58>`_\n\n    - :doi:`Normalized cuts and image segmentation, 2000\n      Jianbo Shi, Jitendra Malik\n      <10.1109/34.868688>`\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.manifold import SpectralEmbedding\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> X.shape\n    (1797, 64)\n    >>> embedding = SpectralEmbedding(n_components=2)\n    >>> X_transformed = embedding.fit_transform(X[:100])\n    >>> X_transformed.shape\n    (100, 2)\n    ",
    "sklearn.manifold.TSNE": "T-distributed Stochastic Neighbor Embedding.\n\n    t-SNE [1] is a tool to visualize high-dimensional data. It converts\n    similarities between data points to joint probabilities and tries\n    to minimize the Kullback-Leibler divergence between the joint\n    probabilities of the low-dimensional embedding and the\n    high-dimensional data. t-SNE has a cost function that is not convex,\n    i.e. with different initializations we can get different results.\n\n    It is highly recommended to use another dimensionality reduction\n    method (e.g. PCA for dense data or TruncatedSVD for sparse data)\n    to reduce the number of dimensions to a reasonable amount (e.g. 50)\n    if the number of features is very high. This will suppress some\n    noise and speed up the computation of pairwise distances between\n    samples. For more tips see Laurens van der Maaten's FAQ [2].\n\n    Read more in the :ref:`User Guide <t_sne>`.\n\n    Parameters\n    ----------\n    n_components : int, default=2\n        Dimension of the embedded space.\n\n    perplexity : float, default=30.0\n        The perplexity is related to the number of nearest neighbors that\n        is used in other manifold learning algorithms. Larger datasets\n        usually require a larger perplexity. Consider selecting a value\n        between 5 and 50. Different values can result in significantly\n        different results. The perplexity must be less than the number\n        of samples.\n\n    early_exaggeration : float, default=12.0\n        Controls how tight natural clusters in the original space are in\n        the embedded space and how much space will be between them. For\n        larger values, the space between natural clusters will be larger\n        in the embedded space. Again, the choice of this parameter is not\n        very critical. If the cost function increases during initial\n        optimization, the early exaggeration factor or the learning rate\n        might be too high.\n\n    learning_rate : float or \"auto\", default=\"auto\"\n        The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If\n        the learning rate is too high, the data may look like a 'ball' with any\n        point approximately equidistant from its nearest neighbours. If the\n        learning rate is too low, most points may look compressed in a dense\n        cloud with few outliers. If the cost function gets stuck in a bad local\n        minimum increasing the learning rate may help.\n        Note that many other t-SNE implementations (bhtsne, FIt-SNE, openTSNE,\n        etc.) use a definition of learning_rate that is 4 times smaller than\n        ours. So our learning_rate=200 corresponds to learning_rate=800 in\n        those other implementations. The 'auto' option sets the learning_rate\n        to `max(N / early_exaggeration / 4, 50)` where N is the sample size,\n        following [4] and [5].\n\n        .. versionchanged:: 1.2\n           The default value changed to `\"auto\"`.\n\n    max_iter : int, default=1000\n        Maximum number of iterations for the optimization. Should be at\n        least 250.\n\n        .. versionchanged:: 1.5\n            Parameter name changed from `n_iter` to `max_iter`.\n\n    n_iter_without_progress : int, default=300\n        Maximum number of iterations without progress before we abort the\n        optimization, used after 250 initial iterations with early\n        exaggeration. Note that progress is only checked every 50 iterations so\n        this value is rounded to the next multiple of 50.\n\n        .. versionadded:: 0.17\n           parameter *n_iter_without_progress* to control stopping criteria.\n\n    min_grad_norm : float, default=1e-7\n        If the gradient norm is below this threshold, the optimization will\n        be stopped.\n\n    metric : str or callable, default='euclidean'\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string, it must be one of the options\n        allowed by scipy.spatial.distance.pdist for its metric parameter, or\n        a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\n        If metric is \"precomputed\", X is assumed to be a distance matrix.\n        Alternatively, if metric is a callable function, it is called on each\n        pair of instances (rows) and the resulting value recorded. The callable\n        should take two arrays from X as input and return a value indicating\n        the distance between them. The default is \"euclidean\" which is\n        interpreted as squared euclidean distance.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n        .. versionadded:: 1.1\n\n    init : {\"random\", \"pca\"} or ndarray of shape (n_samples, n_components),             default=\"pca\"\n        Initialization of embedding.\n        PCA initialization cannot be used with precomputed distances and is\n        usually more globally stable than random initialization.\n\n        .. versionchanged:: 1.2\n           The default value changed to `\"pca\"`.\n\n    verbose : int, default=0\n        Verbosity level.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines the random number generator. Pass an int for reproducible\n        results across multiple function calls. Note that different\n        initializations might result in different local minima of the cost\n        function. See :term:`Glossary <random_state>`.\n\n    method : {'barnes_hut', 'exact'}, default='barnes_hut'\n        By default the gradient calculation algorithm uses Barnes-Hut\n        approximation running in O(NlogN) time. method='exact'\n        will run on the slower, but exact, algorithm in O(N^2) time. The\n        exact algorithm should be used when nearest-neighbor errors need\n        to be better than 3%. However, the exact method cannot scale to\n        millions of examples.\n\n        .. versionadded:: 0.17\n           Approximate optimization *method* via the Barnes-Hut.\n\n    angle : float, default=0.5\n        Only used if method='barnes_hut'\n        This is the trade-off between speed and accuracy for Barnes-Hut T-SNE.\n        'angle' is the angular size (referred to as theta in [3]) of a distant\n        node as measured from a point. If this size is below 'angle' then it is\n        used as a summary node of all points contained within it.\n        This method is not very sensitive to changes in this parameter\n        in the range of 0.2 - 0.8. Angle less than 0.2 has quickly increasing\n        computation time and angle greater 0.8 has quickly increasing error.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search. This parameter\n        has no impact when ``metric=\"precomputed\"`` or\n        (``metric=\"euclidean\"`` and ``method=\"exact\"``).\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.22\n\n    n_iter : int\n        Maximum number of iterations for the optimization. Should be at\n        least 250.\n\n        .. deprecated:: 1.5\n            `n_iter` was deprecated in version 1.5 and will be removed in 1.7.\n            Please use `max_iter` instead.\n\n    Attributes\n    ----------\n    embedding_ : array-like of shape (n_samples, n_components)\n        Stores the embedding vectors.\n\n    kl_divergence_ : float\n        Kullback-Leibler divergence after optimization.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    learning_rate_ : float\n        Effective learning rate.\n\n        .. versionadded:: 1.2\n\n    n_iter_ : int\n        Number of iterations run.\n\n    See Also\n    --------\n    sklearn.decomposition.PCA : Principal component analysis that is a linear\n        dimensionality reduction method.\n    sklearn.decomposition.KernelPCA : Non-linear dimensionality reduction using\n        kernels and PCA.\n    MDS : Manifold learning using multidimensional scaling.\n    Isomap : Manifold learning based on Isometric Mapping.\n    LocallyLinearEmbedding : Manifold learning using Locally Linear Embedding.\n    SpectralEmbedding : Spectral embedding for non-linear dimensionality.\n\n    Notes\n    -----\n    For an example of using :class:`~sklearn.manifold.TSNE` in combination with\n    :class:`~sklearn.neighbors.KNeighborsTransformer` see\n    :ref:`sphx_glr_auto_examples_neighbors_approximate_nearest_neighbors.py`.\n\n    References\n    ----------\n\n    [1] van der Maaten, L.J.P.; Hinton, G.E. Visualizing High-Dimensional Data\n        Using t-SNE. Journal of Machine Learning Research 9:2579-2605, 2008.\n\n    [2] van der Maaten, L.J.P. t-Distributed Stochastic Neighbor Embedding\n        https://lvdmaaten.github.io/tsne/\n\n    [3] L.J.P. van der Maaten. Accelerating t-SNE using Tree-Based Algorithms.\n        Journal of Machine Learning Research 15(Oct):3221-3245, 2014.\n        https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf\n\n    [4] Belkina, A. C., Ciccolella, C. O., Anno, R., Halpert, R., Spidlen, J.,\n        & Snyder-Cappione, J. E. (2019). Automated optimized parameters for\n        T-distributed stochastic neighbor embedding improve visualization\n        and analysis of large datasets. Nature Communications, 10(1), 1-12.\n\n    [5] Kobak, D., & Berens, P. (2019). The art of using t-SNE for single-cell\n        transcriptomics. Nature Communications, 10(1), 1-14.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.manifold import TSNE\n    >>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n    >>> X_embedded = TSNE(n_components=2, learning_rate='auto',\n    ...                   init='random', perplexity=3).fit_transform(X)\n    >>> X_embedded.shape\n    (4, 2)\n    ",
    "sklearn.manifold.locally_linear_embedding": "Perform a Locally Linear Embedding analysis on the data.\n\n    Read more in the :ref:`User Guide <locally_linear_embedding>`.\n\n    Parameters\n    ----------\n    X : {array-like, NearestNeighbors}\n        Sample data, shape = (n_samples, n_features), in the form of a\n        numpy array or a NearestNeighbors object.\n\n    n_neighbors : int\n        Number of neighbors to consider for each point.\n\n    n_components : int\n        Number of coordinates for the manifold.\n\n    reg : float, default=1e-3\n        Regularization constant, multiplies the trace of the local covariance\n        matrix of the distances.\n\n    eigen_solver : {'auto', 'arpack', 'dense'}, default='auto'\n        auto : algorithm will attempt to choose the best method for input data\n\n        arpack : use arnoldi iteration in shift-invert mode.\n                    For this method, M may be a dense matrix, sparse matrix,\n                    or general linear operator.\n                    Warning: ARPACK can be unstable for some problems.  It is\n                    best to try several random seeds in order to check results.\n\n        dense  : use standard dense matrix operations for the eigenvalue\n                    decomposition.  For this method, M must be an array\n                    or matrix type.  This method should be avoided for\n                    large problems.\n\n    tol : float, default=1e-6\n        Tolerance for 'arpack' method\n        Not used if eigen_solver=='dense'.\n\n    max_iter : int, default=100\n        Maximum number of iterations for the arpack solver.\n\n    method : {'standard', 'hessian', 'modified', 'ltsa'}, default='standard'\n        standard : use the standard locally linear embedding algorithm.\n                   see reference [1]_\n        hessian  : use the Hessian eigenmap method.  This method requires\n                   n_neighbors > n_components * (1 + (n_components + 1) / 2.\n                   see reference [2]_\n        modified : use the modified locally linear embedding algorithm.\n                   see reference [3]_\n        ltsa     : use local tangent space alignment algorithm\n                   see reference [4]_\n\n    hessian_tol : float, default=1e-4\n        Tolerance for Hessian eigenmapping method.\n        Only used if method == 'hessian'.\n\n    modified_tol : float, default=1e-12\n        Tolerance for modified LLE method.\n        Only used if method == 'modified'.\n\n    random_state : int, RandomState instance, default=None\n        Determines the random number generator when ``solver`` == 'arpack'.\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    n_jobs : int or None, default=None\n        The number of parallel jobs to run for neighbors search.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Returns\n    -------\n    Y : ndarray of shape (n_samples, n_components)\n        Embedding vectors.\n\n    squared_error : float\n        Reconstruction error for the embedding vectors. Equivalent to\n        ``norm(Y - W Y, 'fro')**2``, where W are the reconstruction weights.\n\n    References\n    ----------\n\n    .. [1] Roweis, S. & Saul, L. Nonlinear dimensionality reduction\n        by locally linear embedding.  Science 290:2323 (2000).\n    .. [2] Donoho, D. & Grimes, C. Hessian eigenmaps: Locally\n        linear embedding techniques for high-dimensional data.\n        Proc Natl Acad Sci U S A.  100:5591 (2003).\n    .. [3] `Zhang, Z. & Wang, J. MLLE: Modified Locally Linear\n        Embedding Using Multiple Weights.\n        <https://citeseerx.ist.psu.edu/doc_view/pid/0b060fdbd92cbcc66b383bcaa9ba5e5e624d7ee3>`_\n    .. [4] Zhang, Z. & Zha, H. Principal manifolds and nonlinear\n        dimensionality reduction via tangent space alignment.\n        Journal of Shanghai Univ.  8:406 (2004)\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.manifold import locally_linear_embedding\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> X.shape\n    (1797, 64)\n    >>> embedding, _ = locally_linear_embedding(X[:100],n_neighbors=5, n_components=2)\n    >>> embedding.shape\n    (100, 2)\n    ",
    "sklearn.manifold.smacof": "Compute multidimensional scaling using the SMACOF algorithm.\n\n    The SMACOF (Scaling by MAjorizing a COmplicated Function) algorithm is a\n    multidimensional scaling algorithm which minimizes an objective function\n    (the *stress*) using a majorization technique. Stress majorization, also\n    known as the Guttman Transform, guarantees a monotone convergence of\n    stress, and is more powerful than traditional techniques such as gradient\n    descent.\n\n    The SMACOF algorithm for metric MDS can be summarized by the following\n    steps:\n\n    1. Set an initial start configuration, randomly or not.\n    2. Compute the stress\n    3. Compute the Guttman Transform\n    4. Iterate 2 and 3 until convergence.\n\n    The nonmetric algorithm adds a monotonic regression step before computing\n    the stress.\n\n    Parameters\n    ----------\n    dissimilarities : array-like of shape (n_samples, n_samples)\n        Pairwise dissimilarities between the points. Must be symmetric.\n\n    metric : bool, default=True\n        Compute metric or nonmetric SMACOF algorithm.\n        When ``False`` (i.e. non-metric MDS), dissimilarities with 0 are considered as\n        missing values.\n\n    n_components : int, default=2\n        Number of dimensions in which to immerse the dissimilarities. If an\n        ``init`` array is provided, this option is overridden and the shape of\n        ``init`` is used to determine the dimensionality of the embedding\n        space.\n\n    init : array-like of shape (n_samples, n_components), default=None\n        Starting configuration of the embedding to initialize the algorithm. By\n        default, the algorithm is initialized with a randomly chosen array.\n\n    n_init : int, default=8\n        Number of times the SMACOF algorithm will be run with different\n        initializations. The final results will be the best output of the runs,\n        determined by the run with the smallest final stress. If ``init`` is\n        provided, this option is overridden and a single run is performed.\n\n    n_jobs : int, default=None\n        The number of jobs to use for the computation. If multiple\n        initializations are used (``n_init``), each run of the algorithm is\n        computed in parallel.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    max_iter : int, default=300\n        Maximum number of iterations of the SMACOF algorithm for a single run.\n\n    verbose : int, default=0\n        Level of verbosity.\n\n    eps : float, default=1e-3\n        Relative tolerance with respect to stress at which to declare\n        convergence. The value of `eps` should be tuned separately depending\n        on whether or not `normalized_stress` is being used.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines the random number generator used to initialize the centers.\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    return_n_iter : bool, default=False\n        Whether or not to return the number of iterations.\n\n    normalized_stress : bool or \"auto\" default=\"auto\"\n        Whether use and return normed stress value (Stress-1) instead of raw\n        stress calculated by default. Only supported in non-metric MDS.\n\n        .. versionadded:: 1.2\n\n        .. versionchanged:: 1.4\n           The default value changed from `False` to `\"auto\"` in version 1.4.\n\n    Returns\n    -------\n    X : ndarray of shape (n_samples, n_components)\n        Coordinates of the points in a ``n_components``-space.\n\n    stress : float\n        The final value of the stress (sum of squared distance of the\n        disparities and the distances for all constrained points).\n        If `normalized_stress=True`, and `metric=False` returns Stress-1.\n        A value of 0 indicates \"perfect\" fit, 0.025 excellent, 0.05 good,\n        0.1 fair, and 0.2 poor [1]_.\n\n    n_iter : int\n        The number of iterations corresponding to the best stress. Returned\n        only if ``return_n_iter`` is set to ``True``.\n\n    References\n    ----------\n    .. [1] \"Nonmetric multidimensional scaling: a numerical method\" Kruskal, J.\n           Psychometrika, 29 (1964)\n\n    .. [2] \"Multidimensional scaling by optimizing goodness of fit to a nonmetric\n           hypothesis\" Kruskal, J. Psychometrika, 29, (1964)\n\n    .. [3] \"Modern Multidimensional Scaling - Theory and Applications\" Borg, I.;\n           Groenen P. Springer Series in Statistics (1997)\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.manifold import smacof\n    >>> from sklearn.metrics import euclidean_distances\n    >>> X = np.array([[0, 1, 2], [1, 0, 3],[2, 3, 0]])\n    >>> dissimilarities = euclidean_distances(X)\n    >>> mds_result, stress = smacof(dissimilarities, n_components=2, random_state=42)\n    >>> mds_result\n    array([[ 0.05... -1.07... ],\n           [ 1.74..., -0.75...],\n           [-1.79...,  1.83...]])\n    >>> stress\n    np.float64(0.0012...)\n    ",
    "sklearn.manifold.spectral_embedding": "Project the sample on the first eigenvectors of the graph Laplacian.\n\n    The adjacency matrix is used to compute a normalized graph Laplacian\n    whose spectrum (especially the eigenvectors associated to the\n    smallest eigenvalues) has an interpretation in terms of minimal\n    number of cuts necessary to split the graph into comparably sized\n    components.\n\n    This embedding can also 'work' even if the ``adjacency`` variable is\n    not strictly the adjacency matrix of a graph but more generally\n    an affinity or similarity matrix between samples (for instance the\n    heat kernel of a euclidean distance matrix or a k-NN matrix).\n\n    However care must taken to always make the affinity matrix symmetric\n    so that the eigenvector decomposition works as expected.\n\n    Note : Laplacian Eigenmaps is the actual algorithm implemented here.\n\n    Read more in the :ref:`User Guide <spectral_embedding>`.\n\n    Parameters\n    ----------\n    adjacency : {array-like, sparse graph} of shape (n_samples, n_samples)\n        The adjacency matrix of the graph to embed.\n\n    n_components : int, default=8\n        The dimension of the projection subspace.\n\n    eigen_solver : {'arpack', 'lobpcg', 'amg'}, default=None\n        The eigenvalue decomposition strategy to use. AMG requires pyamg\n        to be installed. It can be faster on very large, sparse problems,\n        but may also lead to instabilities. If None, then ``'arpack'`` is\n        used.\n\n    random_state : int, RandomState instance or None, default=None\n        A pseudo random number generator used for the initialization\n        of the lobpcg eigen vectors decomposition when `eigen_solver ==\n        'amg'`, and for the K-Means initialization. Use an int to make\n        the results deterministic across calls (See\n        :term:`Glossary <random_state>`).\n\n        .. note::\n            When using `eigen_solver == 'amg'`,\n            it is necessary to also fix the global numpy seed with\n            `np.random.seed(int)` to get deterministic results. See\n            https://github.com/pyamg/pyamg/issues/139 for further\n            information.\n\n    eigen_tol : float, default=\"auto\"\n        Stopping criterion for eigendecomposition of the Laplacian matrix.\n        If `eigen_tol=\"auto\"` then the passed tolerance will depend on the\n        `eigen_solver`:\n\n        - If `eigen_solver=\"arpack\"`, then `eigen_tol=0.0`;\n        - If `eigen_solver=\"lobpcg\"` or `eigen_solver=\"amg\"`, then\n          `eigen_tol=None` which configures the underlying `lobpcg` solver to\n          automatically resolve the value according to their heuristics. See,\n          :func:`scipy.sparse.linalg.lobpcg` for details.\n\n        Note that when using `eigen_solver=\"amg\"` values of `tol<1e-5` may lead\n        to convergence issues and should be avoided.\n\n        .. versionadded:: 1.2\n           Added 'auto' option.\n\n    norm_laplacian : bool, default=True\n        If True, then compute symmetric normalized Laplacian.\n\n    drop_first : bool, default=True\n        Whether to drop the first eigenvector. For spectral embedding, this\n        should be True as the first eigenvector should be constant vector for\n        connected graph, but for spectral clustering, this should be kept as\n        False to retain the first eigenvector.\n\n    Returns\n    -------\n    embedding : ndarray of shape (n_samples, n_components)\n        The reduced samples.\n\n    Notes\n    -----\n    Spectral Embedding (Laplacian Eigenmaps) is most useful when the graph\n    has one connected component. If there graph has many components, the first\n    few eigenvectors will simply uncover the connected components of the graph.\n\n    References\n    ----------\n    * https://en.wikipedia.org/wiki/LOBPCG\n\n    * :doi:`\"Toward the Optimal Preconditioned Eigensolver: Locally Optimal\n      Block Preconditioned Conjugate Gradient Method\",\n      Andrew V. Knyazev\n      <10.1137/S1064827500366124>`\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.neighbors import kneighbors_graph\n    >>> from sklearn.manifold import spectral_embedding\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> X = X[:100]\n    >>> affinity_matrix = kneighbors_graph(\n    ...     X, n_neighbors=int(X.shape[0] / 10), include_self=True\n    ... )\n    >>> # make the matrix symmetric\n    >>> affinity_matrix = 0.5 * (affinity_matrix + affinity_matrix.T)\n    >>> embedding = spectral_embedding(affinity_matrix, n_components=2, random_state=42)\n    >>> embedding.shape\n    (100, 2)\n    ",
    "sklearn.manifold.trustworthiness": "Indicate to what extent the local structure is retained.\n\n    The trustworthiness is within [0, 1]. It is defined as\n\n    .. math::\n\n        T(k) = 1 - \\frac{2}{nk (2n - 3k - 1)} \\sum^n_{i=1}\n            \\sum_{j \\in \\mathcal{N}_{i}^{k}} \\max(0, (r(i, j) - k))\n\n    where for each sample i, :math:`\\mathcal{N}_{i}^{k}` are its k nearest\n    neighbors in the output space, and every sample j is its :math:`r(i, j)`-th\n    nearest neighbor in the input space. In other words, any unexpected nearest\n    neighbors in the output space are penalised in proportion to their rank in\n    the input space.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features) or \\\n        (n_samples, n_samples)\n        If the metric is 'precomputed' X must be a square distance\n        matrix. Otherwise it contains a sample per row.\n\n    X_embedded : {array-like, sparse matrix} of shape (n_samples, n_components)\n        Embedding of the training data in low-dimensional space.\n\n    n_neighbors : int, default=5\n        The number of neighbors that will be considered. Should be fewer than\n        `n_samples / 2` to ensure the trustworthiness to lies within [0, 1], as\n        mentioned in [1]_. An error will be raised otherwise.\n\n    metric : str or callable, default='euclidean'\n        Which metric to use for computing pairwise distances between samples\n        from the original input space. If metric is 'precomputed', X must be a\n        matrix of pairwise distances or squared distances. Otherwise, for a list\n        of available metrics, see the documentation of argument metric in\n        `sklearn.pairwise.pairwise_distances` and metrics listed in\n        `sklearn.metrics.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`. Note that the\n        \"cosine\" metric uses :func:`~sklearn.metrics.pairwise.cosine_distances`.\n\n        .. versionadded:: 0.20\n\n    Returns\n    -------\n    trustworthiness : float\n        Trustworthiness of the low-dimensional embedding.\n\n    References\n    ----------\n    .. [1] Jarkko Venna and Samuel Kaski. 2001. Neighborhood\n           Preservation in Nonlinear Projection Methods: An Experimental Study.\n           In Proceedings of the International Conference on Artificial Neural Networks\n           (ICANN '01). Springer-Verlag, Berlin, Heidelberg, 485-491.\n\n    .. [2] Laurens van der Maaten. Learning a Parametric Embedding by Preserving\n           Local Structure. Proceedings of the Twelfth International Conference on\n           Artificial Intelligence and Statistics, PMLR 5:384-391, 2009.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_blobs\n    >>> from sklearn.decomposition import PCA\n    >>> from sklearn.manifold import trustworthiness\n    >>> X, _ = make_blobs(n_samples=100, n_features=10, centers=3, random_state=42)\n    >>> X_embedded = PCA(n_components=2).fit_transform(X)\n    >>> print(f\"{trustworthiness(X, X_embedded, n_neighbors=5):.2f}\")\n    0.92\n    ",
    "umap.umap_.BaseEstimator": "Base class for all estimators in scikit-learn.\n\n    Inheriting from this class provides default implementations of:\n\n    - setting and getting parameters used by `GridSearchCV` and friends;\n    - textual and HTML representation displayed in terminals and IDEs;\n    - estimator serialization;\n    - parameters validation;\n    - data validation;\n    - feature names validation.\n\n    Read more in the :ref:`User Guide <rolling_your_own_estimator>`.\n\n\n    Notes\n    -----\n    All estimators should specify all the parameters that can be set\n    at the class level in their ``__init__`` as explicit keyword\n    arguments (no ``*args`` or ``**kwargs``).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.base import BaseEstimator\n    >>> class MyEstimator(BaseEstimator):\n    ...     def __init__(self, *, param=1):\n    ...         self.param = param\n    ...     def fit(self, X, y=None):\n    ...         self.is_fitted_ = True\n    ...         return self\n    ...     def predict(self, X):\n    ...         return np.full(shape=X.shape[0], fill_value=self.param)\n    >>> estimator = MyEstimator(param=2)\n    >>> estimator.get_params()\n    {'param': 2}\n    >>> X = np.array([[1, 2], [2, 3], [3, 4]])\n    >>> y = np.array([1, 0, 1])\n    >>> estimator.fit(X, y).predict(X)\n    array([2, 2, 2])\n    >>> estimator.set_params(param=3).fit(X, y).predict(X)\n    array([3, 3, 3])\n    ",
    "umap.umap_.KDTree": "KDTree for fast generalized N-point problems\n\nRead more in the :ref:`User Guide <unsupervised_neighbors>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    n_samples is the number of points in the data set, and\n    n_features is the dimension of the parameter space.\n    Note: if X is a C-contiguous array of doubles then data will\n    not be copied. Otherwise, an internal copy will be made.\n\nleaf_size : positive int, default=40\n    Number of points at which to switch to brute-force. Changing\n    leaf_size will not affect the results of a query, but can\n    significantly impact the speed of a query and the memory required\n    to store the constructed tree.  The amount of memory needed to\n    store the tree scales as approximately n_samples / leaf_size.\n    For a specified ``leaf_size``, a leaf node is guaranteed to\n    satisfy ``leaf_size <= n_points <= 2 * leaf_size``, except in\n    the case that ``n_samples < leaf_size``.\n\nmetric : str or DistanceMetric64 object, default='minkowski'\n    Metric to use for distance computation. Default is \"minkowski\", which\n    results in the standard Euclidean distance when p = 2.\n    A list of valid metrics for KDTree is given by the attribute\n    `valid_metrics`.\n    See the documentation of `scipy.spatial.distance\n    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n    the metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for\n    more information on any distance metric.\n\nAdditional keywords are passed to the distance metric class.\nNote: Callable functions in the metric parameter are NOT supported for KDTree\nand Ball Tree. Function call overhead will result in very poor performance.\n\nAttributes\n----------\ndata : memory view\n    The training data\nvalid_metrics: list of str\n    List of valid distance metrics.\n\nExamples\n--------\nQuery for k-nearest neighbors\n\n    >>> import numpy as np\n    >>> from sklearn.neighbors import KDTree\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = KDTree(X, leaf_size=2)              # doctest: +SKIP\n    >>> dist, ind = tree.query(X[:1], k=3)                # doctest: +SKIP\n    >>> print(ind)  # indices of 3 closest neighbors\n    [0 3 1]\n    >>> print(dist)  # distances to 3 closest neighbors\n    [ 0.          0.19662693  0.29473397]\n\nPickle and Unpickle a tree.  Note that the state of the tree is saved in the\npickle operation: the tree needs not be rebuilt upon unpickling.\n\n    >>> import numpy as np\n    >>> import pickle\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = KDTree(X, leaf_size=2)        # doctest: +SKIP\n    >>> s = pickle.dumps(tree)                     # doctest: +SKIP\n    >>> tree_copy = pickle.loads(s)                # doctest: +SKIP\n    >>> dist, ind = tree_copy.query(X[:1], k=3)     # doctest: +SKIP\n    >>> print(ind)  # indices of 3 closest neighbors\n    [0 3 1]\n    >>> print(dist)  # distances to 3 closest neighbors\n    [ 0.          0.19662693  0.29473397]\n\nQuery for neighbors within a given radius\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions\n    >>> tree = KDTree(X, leaf_size=2)     # doctest: +SKIP\n    >>> print(tree.query_radius(X[:1], r=0.3, count_only=True))\n    3\n    >>> ind = tree.query_radius(X[:1], r=0.3)  # doctest: +SKIP\n    >>> print(ind)  # indices of neighbors within distance 0.3\n    [3 0 1]\n\n\nCompute a gaussian kernel density estimate:\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(42)\n    >>> X = rng.random_sample((100, 3))\n    >>> tree = KDTree(X)                # doctest: +SKIP\n    >>> tree.kernel_density(X[:3], h=0.1, kernel='gaussian')\n    array([ 6.94114649,  7.83281226,  7.2071716 ])\n\nCompute a two-point auto-correlation function\n\n    >>> import numpy as np\n    >>> rng = np.random.RandomState(0)\n    >>> X = rng.random_sample((30, 3))\n    >>> r = np.linspace(0, 1, 5)\n    >>> tree = KDTree(X)                # doctest: +SKIP\n    >>> tree.two_point_correlation(X, r)\n    array([ 30,  62, 278, 580, 820])\n\n",
    "umap.umap_.NNDescent": "NNDescent for fast approximate nearest neighbor queries. NNDescent is\n    very flexible and supports a wide variety of distances, including\n    non-metric distances. NNDescent also scales well against high dimensional\n    graph_data in many cases. This implementation provides a straightfoward\n    interface, with access to some tuning parameters.\n\n    Parameters\n    ----------\n    data: array of shape (n_samples, n_features)\n        The training graph_data set to find nearest neighbors in.\n\n    metric: string or callable (optional, default='euclidean')\n        The metric to use for computing nearest neighbors. If a callable is\n        used it must be a numba njit compiled function. Supported metrics\n        include:\n            * euclidean\n            * manhattan\n            * chebyshev\n            * minkowski\n            * canberra\n            * braycurtis\n            * mahalanobis\n            * wminkowski\n            * seuclidean\n            * cosine\n            * correlation\n            * haversine\n            * hamming\n            * jaccard\n            * dice\n            * russelrao\n            * kulsinski\n            * rogerstanimoto\n            * sokalmichener\n            * sokalsneath\n            * yule\n            * hellinger\n            * wasserstein-1d\n        Metrics that take arguments (such as minkowski, mahalanobis etc.)\n        can have arguments passed via the metric_kwds dictionary. At this\n        time care must be taken and dictionary elements must be ordered\n        appropriately; this will hopefully be fixed in the future.\n\n    metric_kwds: dict (optional, default {})\n        Arguments to pass on to the metric, such as the ``p`` value for\n        Minkowski distance.\n\n    n_neighbors: int (optional, default=30)\n        The number of neighbors to use in k-neighbor graph graph_data structure\n        used for fast approximate nearest neighbor search. Larger values\n        will result in more accurate search results at the cost of\n        computation time.\n\n    n_trees: int (optional, default=None)\n        This implementation uses random projection forests for initializing the index\n        build process. This parameter controls the number of trees in that forest. A\n        larger number will result in more accurate neighbor computation at the cost\n        of performance. The default of None means a value will be chosen based on the\n        size of the graph_data.\n\n    leaf_size: int (optional, default=None)\n        The maximum number of points in a leaf for the random projection trees.\n        The default of None means a value will be chosen based on n_neighbors.\n\n    pruning_degree_multiplier: float (optional, default=1.5)\n        How aggressively to prune the graph. Since the search graph is undirected\n        (and thus includes nearest neighbors and reverse nearest neighbors) vertices\n        can have very high degree -- the graph will be pruned such that no\n        vertex has degree greater than\n        ``pruning_degree_multiplier * n_neighbors``.\n\n    diversify_prob: float (optional, default=1.0)\n        The search graph get \"diversified\" by removing potentially unnecessary\n        edges. This controls the volume of edges removed. A value of 0.0 ensures\n        that no edges get removed, and larger values result in significantly more\n        aggressive edge removal. A value of 1.0 will prune all edges that it can.\n\n    n_search_trees: int (optional, default=1)\n        The number of random projection trees to use in initializing searching or\n        querying.\n\n        .. deprecated:: 0.5.5\n\n    tree_init: bool (optional, default=True)\n        Whether to use random projection trees for initialization.\n\n    init_graph: np.ndarray (optional, default=None)\n        2D array of indices of candidate neighbours of the shape\n        (data.shape[0], n_neighbours). If the j-th neighbour of the i-th\n        instances is unknown, use init_graph[i, j] = -1\n\n    init_dist: np.ndarray (optional, default=None)\n        2D array with the same shape as init_graph,\n        such that metric(data[i], data[init_graph[i, j]]) equals\n        init_dist[i, j]\n\n    random_state: int, RandomState instance or None, optional (default: None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    algorithm: string (optional, default='standard')\n        This implementation provides an alternative algorithm for\n        construction of the k-neighbors graph used as a search index. The\n        alternative algorithm can be fast for large ``n_neighbors`` values.\n        The``'alternative'`` algorithm has been deprecated and is no longer\n        available.\n\n    low_memory: boolean (optional, default=True)\n        Whether to use a lower memory, but more computationally expensive\n        approach to index construction.\n\n    max_candidates: int (optional, default=None)\n        Internally each \"self-join\" keeps a maximum number of candidates (\n        nearest neighbors and reverse nearest neighbors) to be considered.\n        This value controls this aspect of the algorithm. Larger values will\n        provide more accurate search results later, but potentially at\n        non-negligible computation cost in building the index. Don't tweak\n        this value unless you know what you're doing.\n\n    max_rptree_depth: int (optional, default=100)\n        Maximum depth of random projection trees. Increasing this may result in a\n        richer, deeper random projection forest, but it may be composed of many\n        degenerate branches. Increase leaf_size in order to keep shallower, wider\n        nondegenerate trees. Such wide trees, however, may yield poor performance\n        of the preparation of the NN descent.\n\n    n_iters: int (optional, default=None)\n        The maximum number of NN-descent iterations to perform. The\n        NN-descent algorithm can abort early if limited progress is being\n        made, so this only controls the worst case. Don't tweak\n        this value unless you know what you're doing. The default of None means\n        a value will be chosen based on the size of the graph_data.\n\n    delta: float (optional, default=0.001)\n        Controls the early abort due to limited progress. Larger values\n        will result in earlier aborts, providing less accurate indexes,\n        and less accurate searching. Don't tweak this value unless you know\n        what you're doing.\n\n    n_jobs: int or None, optional (default=None)\n        The number of parallel jobs to run for neighbors index construction.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors.\n\n    compressed: bool (optional, default=False)\n        Whether to prune out data not needed for searching the index. This will\n        result in a significantly smaller index, particularly useful for saving,\n        but will remove information that might otherwise be useful.\n\n    parallel_batch_queries: bool (optional, default=False)\n        Whether to use parallelism of batched queries. This can be useful for large\n        batches of queries on multicore machines, but results in performance degradation\n        for single queries, so is poor for streaming use.\n\n    verbose: bool (optional, default=False)\n        Whether to print status graph_data during the computation.\n    ",
    "umap.umap_.PCA": "Principal component analysis (PCA).\n\n    Linear dimensionality reduction using Singular Value Decomposition of the\n    data to project it to a lower dimensional space. The input data is centered\n    but not scaled for each feature before applying the SVD.\n\n    It uses the LAPACK implementation of the full SVD or a randomized truncated\n    SVD by the method of Halko et al. 2009, depending on the shape of the input\n    data and the number of components to extract.\n\n    With sparse inputs, the ARPACK implementation of the truncated SVD can be\n    used (i.e. through :func:`scipy.sparse.linalg.svds`). Alternatively, one\n    may consider :class:`TruncatedSVD` where the data are not centered.\n\n    Notice that this class only supports sparse inputs for some solvers such as\n    \"arpack\" and \"covariance_eigh\". See :class:`TruncatedSVD` for an\n    alternative with sparse data.\n\n    For a usage example, see\n    :ref:`sphx_glr_auto_examples_decomposition_plot_pca_iris.py`\n\n    Read more in the :ref:`User Guide <PCA>`.\n\n    Parameters\n    ----------\n    n_components : int, float or 'mle', default=None\n        Number of components to keep.\n        if n_components is not set all components are kept::\n\n            n_components == min(n_samples, n_features)\n\n        If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\n        MLE is used to guess the dimension. Use of ``n_components == 'mle'``\n        will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n\n        If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\n        number of components such that the amount of variance that needs to be\n        explained is greater than the percentage specified by n_components.\n\n        If ``svd_solver == 'arpack'``, the number of components must be\n        strictly less than the minimum of n_features and n_samples.\n\n        Hence, the None case results in::\n\n            n_components == min(n_samples, n_features) - 1\n\n    copy : bool, default=True\n        If False, data passed to fit are overwritten and running\n        fit(X).transform(X) will not yield the expected results,\n        use fit_transform(X) instead.\n\n    whiten : bool, default=False\n        When True (False by default) the `components_` vectors are multiplied\n        by the square root of n_samples and then divided by the singular values\n        to ensure uncorrelated outputs with unit component-wise variances.\n\n        Whitening will remove some information from the transformed signal\n        (the relative variance scales of the components) but can sometime\n        improve the predictive accuracy of the downstream estimators by\n        making their data respect some hard-wired assumptions.\n\n    svd_solver : {'auto', 'full', 'covariance_eigh', 'arpack', 'randomized'},            default='auto'\n        \"auto\" :\n            The solver is selected by a default 'auto' policy is based on `X.shape` and\n            `n_components`: if the input data has fewer than 1000 features and\n            more than 10 times as many samples, then the \"covariance_eigh\"\n            solver is used. Otherwise, if the input data is larger than 500x500\n            and the number of components to extract is lower than 80% of the\n            smallest dimension of the data, then the more efficient\n            \"randomized\" method is selected. Otherwise the exact \"full\" SVD is\n            computed and optionally truncated afterwards.\n        \"full\" :\n            Run exact full SVD calling the standard LAPACK solver via\n            `scipy.linalg.svd` and select the components by postprocessing\n        \"covariance_eigh\" :\n            Precompute the covariance matrix (on centered data), run a\n            classical eigenvalue decomposition on the covariance matrix\n            typically using LAPACK and select the components by postprocessing.\n            This solver is very efficient for n_samples >> n_features and small\n            n_features. It is, however, not tractable otherwise for large\n            n_features (large memory footprint required to materialize the\n            covariance matrix). Also note that compared to the \"full\" solver,\n            this solver effectively doubles the condition number and is\n            therefore less numerical stable (e.g. on input data with a large\n            range of singular values).\n        \"arpack\" :\n            Run SVD truncated to `n_components` calling ARPACK solver via\n            `scipy.sparse.linalg.svds`. It requires strictly\n            `0 < n_components < min(X.shape)`\n        \"randomized\" :\n            Run randomized SVD by the method of Halko et al.\n\n        .. versionadded:: 0.18.0\n\n        .. versionchanged:: 1.5\n            Added the 'covariance_eigh' solver.\n\n    tol : float, default=0.0\n        Tolerance for singular values computed by svd_solver == 'arpack'.\n        Must be of range [0.0, infinity).\n\n        .. versionadded:: 0.18.0\n\n    iterated_power : int or 'auto', default='auto'\n        Number of iterations for the power method computed by\n        svd_solver == 'randomized'.\n        Must be of range [0, infinity).\n\n        .. versionadded:: 0.18.0\n\n    n_oversamples : int, default=10\n        This parameter is only relevant when `svd_solver=\"randomized\"`.\n        It corresponds to the additional number of random vectors to sample the\n        range of `X` so as to ensure proper conditioning. See\n        :func:`~sklearn.utils.extmath.randomized_svd` for more details.\n\n        .. versionadded:: 1.1\n\n    power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n        Power iteration normalizer for randomized SVD solver.\n        Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd`\n        for more details.\n\n        .. versionadded:: 1.1\n\n    random_state : int, RandomState instance or None, default=None\n        Used when the 'arpack' or 'randomized' solvers are used. Pass an int\n        for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n        .. versionadded:: 0.18.0\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Principal axes in feature space, representing the directions of\n        maximum variance in the data. Equivalently, the right singular\n        vectors of the centered input data, parallel to its eigenvectors.\n        The components are sorted by decreasing ``explained_variance_``.\n\n    explained_variance_ : ndarray of shape (n_components,)\n        The amount of variance explained by each of the selected components.\n        The variance estimation uses `n_samples - 1` degrees of freedom.\n\n        Equal to n_components largest eigenvalues\n        of the covariance matrix of X.\n\n        .. versionadded:: 0.18\n\n    explained_variance_ratio_ : ndarray of shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n\n        If ``n_components`` is not set then all components are stored and the\n        sum of the ratios is equal to 1.0.\n\n    singular_values_ : ndarray of shape (n_components,)\n        The singular values corresponding to each of the selected components.\n        The singular values are equal to the 2-norms of the ``n_components``\n        variables in the lower-dimensional space.\n\n        .. versionadded:: 0.19\n\n    mean_ : ndarray of shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n\n        Equal to `X.mean(axis=0)`.\n\n    n_components_ : int\n        The estimated number of components. When n_components is set\n        to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this\n        number is estimated from input data. Otherwise it equals the parameter\n        n_components, or the lesser value of n_features and n_samples\n        if n_components is None.\n\n    n_samples_ : int\n        Number of samples in the training data.\n\n    noise_variance_ : float\n        The estimated noise covariance following the Probabilistic PCA model\n        from Tipping and Bishop 1999. See \"Pattern Recognition and\n        Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n        http://www.miketipping.com/papers/met-mppca.pdf. It is required to\n        compute the estimated data covariance and score samples.\n\n        Equal to the average of (min(n_features, n_samples) - n_components)\n        smallest eigenvalues of the covariance matrix of X.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    KernelPCA : Kernel Principal Component Analysis.\n    SparsePCA : Sparse Principal Component Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n    IncrementalPCA : Incremental Principal Component Analysis.\n\n    References\n    ----------\n    For n_components == 'mle', this class uses the method from:\n    `Minka, T. P.. \"Automatic choice of dimensionality for PCA\".\n    In NIPS, pp. 598-604 <https://tminka.github.io/papers/pca/minka-pca.pdf>`_\n\n    Implements the probabilistic PCA model from:\n    `Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal\n    component analysis\". Journal of the Royal Statistical Society:\n    Series B (Statistical Methodology), 61(3), 611-622.\n    <http://www.miketipping.com/papers/met-mppca.pdf>`_\n    via the score and score_samples methods.\n\n    For svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.\n\n    For svd_solver == 'randomized', see:\n    :doi:`Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n    \"Finding structure with randomness: Probabilistic algorithms for\n    constructing approximate matrix decompositions\".\n    SIAM review, 53(2), 217-288.\n    <10.1137/090771806>`\n    and also\n    :doi:`Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n    \"A randomized algorithm for the decomposition of matrices\".\n    Applied and Computational Harmonic Analysis, 30(1), 47-68.\n    <10.1016/j.acha.2010.02.003>`\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.decomposition import PCA\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> pca = PCA(n_components=2)\n    >>> pca.fit(X)\n    PCA(n_components=2)\n    >>> print(pca.explained_variance_ratio_)\n    [0.9924... 0.0075...]\n    >>> print(pca.singular_values_)\n    [6.30061... 0.54980...]\n\n    >>> pca = PCA(n_components=2, svd_solver='full')\n    >>> pca.fit(X)\n    PCA(n_components=2, svd_solver='full')\n    >>> print(pca.explained_variance_ratio_)\n    [0.9924... 0.00755...]\n    >>> print(pca.singular_values_)\n    [6.30061... 0.54980...]\n\n    >>> pca = PCA(n_components=1, svd_solver='arpack')\n    >>> pca.fit(X)\n    PCA(n_components=1, svd_solver='arpack')\n    >>> print(pca.explained_variance_ratio_)\n    [0.99244...]\n    >>> print(pca.singular_values_)\n    [6.30061...]\n    ",
    "umap.umap_.TruncatedSVD": "Dimensionality reduction using truncated SVD (aka LSA).\n\n    This transformer performs linear dimensionality reduction by means of\n    truncated singular value decomposition (SVD). Contrary to PCA, this\n    estimator does not center the data before computing the singular value\n    decomposition. This means it can work with sparse matrices\n    efficiently.\n\n    In particular, truncated SVD works on term count/tf-idf matrices as\n    returned by the vectorizers in :mod:`sklearn.feature_extraction.text`. In\n    that context, it is known as latent semantic analysis (LSA).\n\n    This estimator supports two algorithms: a fast randomized SVD solver, and\n    a \"naive\" algorithm that uses ARPACK as an eigensolver on `X * X.T` or\n    `X.T * X`, whichever is more efficient.\n\n    Read more in the :ref:`User Guide <LSA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=2\n        Desired dimensionality of output data.\n        If algorithm='arpack', must be strictly less than the number of features.\n        If algorithm='randomized', must be less than or equal to the number of features.\n        The default value is useful for visualisation. For LSA, a value of\n        100 is recommended.\n\n    algorithm : {'arpack', 'randomized'}, default='randomized'\n        SVD solver to use. Either \"arpack\" for the ARPACK wrapper in SciPy\n        (scipy.sparse.linalg.svds), or \"randomized\" for the randomized\n        algorithm due to Halko (2009).\n\n    n_iter : int, default=5\n        Number of iterations for randomized SVD solver. Not used by ARPACK. The\n        default is larger than the default in\n        :func:`~sklearn.utils.extmath.randomized_svd` to handle sparse\n        matrices that may have large slowly decaying spectrum.\n\n    n_oversamples : int, default=10\n        Number of oversamples for randomized SVD solver. Not used by ARPACK.\n        See :func:`~sklearn.utils.extmath.randomized_svd` for a complete\n        description.\n\n        .. versionadded:: 1.1\n\n    power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n        Power iteration normalizer for randomized SVD solver.\n        Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd`\n        for more details.\n\n        .. versionadded:: 1.1\n\n    random_state : int, RandomState instance or None, default=None\n        Used during randomized svd. Pass an int for reproducible results across\n        multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    tol : float, default=0.0\n        Tolerance for ARPACK. 0 means machine precision. Ignored by randomized\n        SVD solver.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        The right singular vectors of the input data.\n\n    explained_variance_ : ndarray of shape (n_components,)\n        The variance of the training samples transformed by a projection to\n        each component.\n\n    explained_variance_ratio_ : ndarray of shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n\n    singular_values_ : ndarray of shape (n_components,)\n        The singular values corresponding to each of the selected components.\n        The singular values are equal to the 2-norms of the ``n_components``\n        variables in the lower-dimensional space.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    FactorAnalysis : A simple linear generative model with\n        Gaussian latent variables.\n    IncrementalPCA : Incremental principal components analysis.\n    KernelPCA : Kernel Principal component analysis.\n    NMF : Non-Negative Matrix Factorization.\n    PCA : Principal component analysis.\n\n    Notes\n    -----\n    SVD suffers from a problem called \"sign indeterminacy\", which means the\n    sign of the ``components_`` and the output from transform depend on the\n    algorithm and random state. To work around this, fit instances of this\n    class to data once, then keep the instance around to do transformations.\n\n    References\n    ----------\n    :arxiv:`Halko, et al. (2009). \"Finding structure with randomness:\n    Stochastic algorithms for constructing approximate matrix decompositions\"\n    <0909.4061>`\n\n    Examples\n    --------\n    >>> from sklearn.decomposition import TruncatedSVD\n    >>> from scipy.sparse import csr_matrix\n    >>> import numpy as np\n    >>> np.random.seed(0)\n    >>> X_dense = np.random.rand(100, 100)\n    >>> X_dense[:, 2 * np.arange(50)] = 0\n    >>> X = csr_matrix(X_dense)\n    >>> svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n    >>> svd.fit(X)\n    TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n    >>> print(svd.explained_variance_ratio_)\n    [0.0157... 0.0512... 0.0499... 0.0479... 0.0453...]\n    >>> print(svd.explained_variance_ratio_.sum())\n    0.2102...\n    >>> print(svd.singular_values_)\n    [35.2410...  4.5981...   4.5420...  4.4486...  4.3288...]\n    ",
    "umap.umap_.UMAP": "Uniform Manifold Approximation and Projection\n\n    Finds a low dimensional embedding of the data that approximates\n    an underlying manifold.\n\n    Parameters\n    ----------\n    n_neighbors: float (optional, default 15)\n        The size of local neighborhood (in terms of number of neighboring\n        sample points) used for manifold approximation. Larger values\n        result in more global views of the manifold, while smaller\n        values result in more local data being preserved. In general\n        values should be in the range 2 to 100.\n\n    n_components: int (optional, default 2)\n        The dimension of the space to embed into. This defaults to 2 to\n        provide easy visualization, but can reasonably be set to any\n        integer value in the range 2 to 100.\n\n    metric: string or function (optional, default 'euclidean')\n        The metric to use to compute distances in high dimensional space.\n        If a string is passed it must match a valid predefined metric. If\n        a general metric is required a function that takes two 1d arrays and\n        returns a float can be provided. For performance purposes it is\n        required that this be a numba jit'd function. Valid string metrics\n        include:\n\n        * euclidean\n        * manhattan\n        * chebyshev\n        * minkowski\n        * canberra\n        * braycurtis\n        * mahalanobis\n        * wminkowski\n        * seuclidean\n        * cosine\n        * correlation\n        * haversine\n        * hamming\n        * jaccard\n        * dice\n        * russelrao\n        * kulsinski\n        * ll_dirichlet\n        * hellinger\n        * rogerstanimoto\n        * sokalmichener\n        * sokalsneath\n        * yule\n\n        Metrics that take arguments (such as minkowski, mahalanobis etc.)\n        can have arguments passed via the metric_kwds dictionary. At this\n        time care must be taken and dictionary elements must be ordered\n        appropriately; this will hopefully be fixed in the future.\n\n    n_epochs: int (optional, default None)\n        The number of training epochs to be used in optimizing the\n        low dimensional embedding. Larger values result in more accurate\n        embeddings. If None is specified a value will be selected based on\n        the size of the input dataset (200 for large datasets, 500 for small).\n\n    learning_rate: float (optional, default 1.0)\n        The initial learning rate for the embedding optimization.\n\n    init: string (optional, default 'spectral')\n        How to initialize the low dimensional embedding. Options are:\n\n            * 'spectral': use a spectral embedding of the fuzzy 1-skeleton\n            * 'random': assign initial embedding positions at random.\n            * 'pca': use the first n_components from PCA applied to the\n                input data.\n            * 'tswspectral': use a spectral embedding of the fuzzy\n                1-skeleton, using a truncated singular value decomposition to\n                \"warm\" up the eigensolver. This is intended as an alternative\n                to the 'spectral' method, if that takes an  excessively long\n                time to complete initialization (or fails to complete).\n            * A numpy array of initial embedding positions.\n\n    min_dist: float (optional, default 0.1)\n        The effective minimum distance between embedded points. Smaller values\n        will result in a more clustered/clumped embedding where nearby points\n        on the manifold are drawn closer together, while larger values will\n        result on a more even dispersal of points. The value should be set\n        relative to the ``spread`` value, which determines the scale at which\n        embedded points will be spread out.\n\n    spread: float (optional, default 1.0)\n        The effective scale of embedded points. In combination with ``min_dist``\n        this determines how clustered/clumped the embedded points are.\n\n    low_memory: bool (optional, default True)\n        For some datasets the nearest neighbor computation can consume a lot of\n        memory. If you find that UMAP is failing due to memory constraints\n        consider setting this option to True. This approach is more\n        computationally expensive, but avoids excessive memory use.\n\n    set_op_mix_ratio: float (optional, default 1.0)\n        Interpolate between (fuzzy) union and intersection as the set operation\n        used to combine local fuzzy simplicial sets to obtain a global fuzzy\n        simplicial sets. Both fuzzy set operations use the product t-norm.\n        The value of this parameter should be between 0.0 and 1.0; a value of\n        1.0 will use a pure fuzzy union, while 0.0 will use a pure fuzzy\n        intersection.\n\n    local_connectivity: int (optional, default 1)\n        The local connectivity required -- i.e. the number of nearest\n        neighbors that should be assumed to be connected at a local level.\n        The higher this value the more connected the manifold becomes\n        locally. In practice this should be not more than the local intrinsic\n        dimension of the manifold.\n\n    repulsion_strength: float (optional, default 1.0)\n        Weighting applied to negative samples in low dimensional embedding\n        optimization. Values higher than one will result in greater weight\n        being given to negative samples.\n\n    negative_sample_rate: int (optional, default 5)\n        The number of negative samples to select per positive sample\n        in the optimization process. Increasing this value will result\n        in greater repulsive force being applied, greater optimization\n        cost, but slightly more accuracy.\n\n    transform_queue_size: float (optional, default 4.0)\n        For transform operations (embedding new points using a trained model\n        this will control how aggressively to search for nearest neighbors.\n        Larger values will result in slower performance but more accurate\n        nearest neighbor evaluation.\n\n    a: float (optional, default None)\n        More specific parameters controlling the embedding. If None these\n        values are set automatically as determined by ``min_dist`` and\n        ``spread``.\n    b: float (optional, default None)\n        More specific parameters controlling the embedding. If None these\n        values are set automatically as determined by ``min_dist`` and\n        ``spread``.\n\n    random_state: int, RandomState instance or None, optional (default: None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    metric_kwds: dict (optional, default None)\n        Arguments to pass on to the metric, such as the ``p`` value for\n        Minkowski distance. If None then no arguments are passed on.\n\n    angular_rp_forest: bool (optional, default False)\n        Whether to use an angular random projection forest to initialise\n        the approximate nearest neighbor search. This can be faster, but is\n        mostly only useful for a metric that uses an angular style distance such\n        as cosine, correlation etc. In the case of those metrics angular forests\n        will be chosen automatically.\n\n    target_n_neighbors: int (optional, default -1)\n        The number of nearest neighbors to use to construct the target simplicial\n        set. If set to -1 use the ``n_neighbors`` value.\n\n    target_metric: string or callable (optional, default 'categorical')\n        The metric used to measure distance for a target array is using supervised\n        dimension reduction. By default this is 'categorical' which will measure\n        distance in terms of whether categories match or are different. Furthermore,\n        if semi-supervised is required target values of -1 will be trated as\n        unlabelled under the 'categorical' metric. If the target array takes\n        continuous values (e.g. for a regression problem) then metric of 'l1'\n        or 'l2' is probably more appropriate.\n\n    target_metric_kwds: dict (optional, default None)\n        Keyword argument to pass to the target metric when performing\n        supervised dimension reduction. If None then no arguments are passed on.\n\n    target_weight: float (optional, default 0.5)\n        weighting factor between data topology and target topology. A value of\n        0.0 weights predominantly on data, a value of 1.0 places a strong emphasis on\n        target. The default of 0.5 balances the weighting equally between data and\n        target.\n\n    transform_seed: int (optional, default 42)\n        Random seed used for the stochastic aspects of the transform operation.\n        This ensures consistency in transform operations.\n\n    verbose: bool (optional, default False)\n        Controls verbosity of logging.\n\n    tqdm_kwds: dict (optional, defaul None)\n        Key word arguments to be used by the tqdm progress bar.\n\n    unique: bool (optional, default False)\n        Controls if the rows of your data should be uniqued before being\n        embedded.  If you have more duplicates than you have ``n_neighbors``\n        you can have the identical data points lying in different regions of\n        your space.  It also violates the definition of a metric.\n        For to map from internal structures back to your data use the variable\n        _unique_inverse_.\n\n    densmap: bool (optional, default False)\n        Specifies whether the density-augmented objective of densMAP\n        should be used for optimization. Turning on this option generates\n        an embedding where the local densities are encouraged to be correlated\n        with those in the original space. Parameters below with the prefix 'dens'\n        further control the behavior of this extension.\n\n    dens_lambda: float (optional, default 2.0)\n        Controls the regularization weight of the density correlation term\n        in densMAP. Higher values prioritize density preservation over the\n        UMAP objective, and vice versa for values closer to zero. Setting this\n        parameter to zero is equivalent to running the original UMAP algorithm.\n\n    dens_frac: float (optional, default 0.3)\n        Controls the fraction of epochs (between 0 and 1) where the\n        density-augmented objective is used in densMAP. The first\n        (1 - dens_frac) fraction of epochs optimize the original UMAP objective\n        before introducing the density correlation term.\n\n    dens_var_shift: float (optional, default 0.1)\n        A small constant added to the variance of local radii in the\n        embedding when calculating the density correlation objective to\n        prevent numerical instability from dividing by a small number\n\n    output_dens: float (optional, default False)\n        Determines whether the local radii of the final embedding (an inverse\n        measure of local density) are computed and returned in addition to\n        the embedding. If set to True, local radii of the original data\n        are also included in the output for comparison; the output is a tuple\n        (embedding, original local radii, embedding local radii). This option\n        can also be used when densmap=False to calculate the densities for\n        UMAP embeddings.\n\n    disconnection_distance: float (optional, default np.inf or maximal value for bounded distances)\n        Disconnect any vertices of distance greater than or equal to disconnection_distance when approximating the\n        manifold via our k-nn graph. This is particularly useful in the case that you have a bounded metric.  The\n        UMAP assumption that we have a connected manifold can be problematic when you have points that are maximally\n        different from all the rest of your data.  The connected manifold assumption will make such points have perfect\n        similarity to a random set of other points.  Too many such points will artificially connect your space.\n\n    precomputed_knn: tuple (optional, default (None,None,None))\n        If the k-nearest neighbors of each point has already been calculated you\n        can pass them in here to save computation time. The number of nearest\n        neighbors in the precomputed_knn must be greater or equal to the\n        n_neighbors parameter. This should be a tuple containing the output\n        of the nearest_neighbors() function or attributes from a previously fit\n        UMAP object; (knn_indices, knn_dists, knn_search_index). If you wish to use\n        k-nearest neighbors data calculated by another package then provide a tuple of\n        the form (knn_indices, knn_dists). The contents of the tuple should be two numpy\n        arrays of shape (N, n_neighbors) where N is the number of items in the\n        input data. The first array should be the integer indices of the nearest\n        neighbors, and the second array should be the corresponding distances. The\n        nearest neighbor of each item should be itself, e.g. the nearest neighbor of\n        item 0 should be 0, the nearest neighbor of item 1 is 1 and so on. Please note\n        that you will *not* be able to transform new data in this case.\n    ",
    "umap.umap_.check_array": "Input validation on an array, list, sparse matrix or similar.\n\n    By default, the input is checked to be a non-empty 2D array containing\n    only finite values. If the dtype of the array is object, attempt\n    converting to float, raising on failure.\n\n    Parameters\n    ----------\n    array : object\n        Input object to check / convert.\n\n    accept_sparse : str, bool or list/tuple of str, default=False\n        String[s] representing allowed sparse matrix formats, such as 'csc',\n        'csr', etc. If the input is sparse but not in the allowed format,\n        it will be converted to the first listed format. True allows the input\n        to be any format. False means that a sparse matrix input will\n        raise an error.\n\n    accept_large_sparse : bool, default=True\n        If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by\n        accept_sparse, accept_large_sparse=False will cause it to be accepted\n        only if its indices are stored with a 32-bit dtype.\n\n        .. versionadded:: 0.20\n\n    dtype : 'numeric', type, list of type or None, default='numeric'\n        Data type of result. If None, the dtype of the input is preserved.\n        If \"numeric\", dtype is preserved unless array.dtype is object.\n        If dtype is a list of types, conversion on the first type is only\n        performed if the dtype of the input is not in the list.\n\n    order : {'F', 'C'} or None, default=None\n        Whether an array will be forced to be fortran or c-style.\n        When order is None (default), then if copy=False, nothing is ensured\n        about the memory layout of the output array; otherwise (copy=True)\n        the memory layout of the returned array is kept as close as possible\n        to the original array.\n\n    copy : bool, default=False\n        Whether a forced copy will be triggered. If copy=False, a copy might\n        be triggered by a conversion.\n\n    force_writeable : bool, default=False\n        Whether to force the output array to be writeable. If True, the returned array\n        is guaranteed to be writeable, which may require a copy. Otherwise the\n        writeability of the input array is preserved.\n\n        .. versionadded:: 1.6\n\n    force_all_finite : bool or 'allow-nan', default=True\n        Whether to raise an error on np.inf, np.nan, pd.NA in array. The\n        possibilities are:\n\n        - True: Force all values of array to be finite.\n        - False: accepts np.inf, np.nan, pd.NA in array.\n        - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n          cannot be infinite.\n\n        .. versionadded:: 0.20\n           ``force_all_finite`` accepts the string ``'allow-nan'``.\n\n        .. versionchanged:: 0.23\n           Accepts `pd.NA` and converts it into `np.nan`\n\n        .. deprecated:: 1.6\n           `force_all_finite` was renamed to `ensure_all_finite` and will be removed\n           in 1.8.\n\n    ensure_all_finite : bool or 'allow-nan', default=True\n        Whether to raise an error on np.inf, np.nan, pd.NA in array. The\n        possibilities are:\n\n        - True: Force all values of array to be finite.\n        - False: accepts np.inf, np.nan, pd.NA in array.\n        - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n          cannot be infinite.\n\n        .. versionadded:: 1.6\n           `force_all_finite` was renamed to `ensure_all_finite`.\n\n    ensure_non_negative : bool, default=False\n        Make sure the array has only non-negative values. If True, an array that\n        contains negative values will raise a ValueError.\n\n        .. versionadded:: 1.6\n\n    ensure_2d : bool, default=True\n        Whether to raise a value error if array is not 2D.\n\n    allow_nd : bool, default=False\n        Whether to allow array.ndim > 2.\n\n    ensure_min_samples : int, default=1\n        Make sure that the array has a minimum number of samples in its first\n        axis (rows for a 2D array). Setting to 0 disables this check.\n\n    ensure_min_features : int, default=1\n        Make sure that the 2D array has some minimum number of features\n        (columns). The default value of 1 rejects empty datasets.\n        This check is only enforced when the input data has effectively 2\n        dimensions or is originally 1D and ``ensure_2d`` is True. Setting to 0\n        disables this check.\n\n    estimator : str or estimator instance, default=None\n        If passed, include the name of the estimator in warning messages.\n\n    input_name : str, default=\"\"\n        The data name used to construct the error message. In particular\n        if `input_name` is \"X\" and the data has NaN values and\n        allow_nan is False, the error message will link to the imputer\n        documentation.\n\n        .. versionadded:: 1.1.0\n\n    Returns\n    -------\n    array_converted : object\n        The converted and validated array.\n\n    Examples\n    --------\n    >>> from sklearn.utils.validation import check_array\n    >>> X = [[1, 2, 3], [4, 5, 6]]\n    >>> X_checked = check_array(X)\n    >>> X_checked\n    array([[1, 2, 3], [4, 5, 6]])\n    ",
    "umap.umap_.check_is_fitted": "Perform is_fitted validation for estimator.\n\n    Checks if the estimator is fitted by verifying the presence of\n    fitted attributes (ending with a trailing underscore) and otherwise\n    raises a :class:`~sklearn.exceptions.NotFittedError` with the given message.\n\n    If an estimator does not set any attributes with a trailing underscore, it\n    can define a ``__sklearn_is_fitted__`` method returning a boolean to\n    specify if the estimator is fitted or not. See\n    :ref:`sphx_glr_auto_examples_developing_estimators_sklearn_is_fitted.py`\n    for an example on how to use the API.\n\n    If no `attributes` are passed, this fuction will pass if an estimator is stateless.\n    An estimator can indicate it's stateless by setting the `requires_fit` tag. See\n    :ref:`estimator_tags` for more information. Note that the `requires_fit` tag\n    is ignored if `attributes` are passed.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Raises\n    ------\n    TypeError\n        If the estimator is a class or not an estimator instance\n\n    NotFittedError\n        If the attributes are not found.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.utils.validation import check_is_fitted\n    >>> from sklearn.exceptions import NotFittedError\n    >>> lr = LogisticRegression()\n    >>> try:\n    ...     check_is_fitted(lr)\n    ... except NotFittedError as exc:\n    ...     print(f\"Model is not fitted yet.\")\n    Model is not fitted yet.\n    >>> lr.fit([[1, 2], [1, 3]], [1, 0])\n    LogisticRegression()\n    >>> check_is_fitted(lr)\n    ",
    "umap.umap_.check_random_state": "Turn seed into a np.random.RandomState instance.\n\n    Parameters\n    ----------\n    seed : None, int or instance of RandomState\n        If seed is None, return the RandomState singleton used by np.random.\n        If seed is an int, return a new RandomState instance seeded with seed.\n        If seed is already a RandomState instance, return it.\n        Otherwise raise ValueError.\n\n    Returns\n    -------\n    :class:`numpy:numpy.random.RandomState`\n        The random state object based on `seed` parameter.\n\n    Examples\n    --------\n    >>> from sklearn.utils.validation import check_random_state\n    >>> check_random_state(42)\n    RandomState(MT19937) at 0x...\n    ",
    "umap.umap_.compute_membership_strengths": "Construct the membership strength data for the 1-skeleton of each local\n    fuzzy simplicial set -- this is formed as a sparse matrix where each row is\n    a local fuzzy simplicial set, with a membership strength for the\n    1-simplex to each other data point.\n\n    Parameters\n    ----------\n    knn_indices: array of shape (n_samples, n_neighbors)\n        The indices on the ``n_neighbors`` closest points in the dataset.\n\n    knn_dists: array of shape (n_samples, n_neighbors)\n        The distances to the ``n_neighbors`` closest points in the dataset.\n\n    sigmas: array of shape(n_samples)\n        The normalization factor derived from the metric tensor approximation.\n\n    rhos: array of shape(n_samples)\n        The local connectivity adjustment.\n\n    return_dists: bool (optional, default False)\n        Whether to return the pairwise distance associated with each edge.\n\n    bipartite: bool (optional, default False)\n        Does the nearest neighbour set represent a bipartite graph? That is, are the\n        nearest neighbour indices from the same point set as the row indices?\n\n    Returns\n    -------\n    rows: array of shape (n_samples * n_neighbors)\n        Row data for the resulting sparse matrix (coo format)\n\n    cols: array of shape (n_samples * n_neighbors)\n        Column data for the resulting sparse matrix (coo format)\n\n    vals: array of shape (n_samples * n_neighbors)\n        Entries for the resulting sparse matrix (coo format)\n\n    dists: array of shape (n_samples * n_neighbors)\n        Distance associated with each entry in the resulting sparse matrix\n    ",
    "umap.umap_.csr_unique": "Find the unique elements of a sparse csr matrix.\n    We don't explicitly construct the unique matrix leaving that to the user\n    who may not want to duplicate a massive array in memory.\n    Returns the indices of the input array that give the unique values.\n    Returns the indices of the unique array that reconstructs the input array.\n    Returns the number of times each unique row appears in the input matrix.\n\n    matrix: a csr matrix\n    return_index = bool, optional\n        If true, return the row indices of 'matrix'\n    return_inverse: bool, optional\n        If true, return the indices of the unique array that can be\n           used to reconstruct 'matrix'.\n    return_counts = bool, optional\n        If true, returns the number of times each unique item appears in 'matrix'\n\n    The unique matrix can computed via\n    unique_matrix = matrix[index]\n    and the original matrix reconstructed via\n    unique_matrix[inverse]\n    ",
    "umap.umap_.curve_fit": "\n    Use non-linear least squares to fit a function, f, to data.\n\n    Assumes ``ydata = f(xdata, *params) + eps``.\n\n    Parameters\n    ----------\n    f : callable\n        The model function, f(x, ...). It must take the independent\n        variable as the first argument and the parameters to fit as\n        separate remaining arguments.\n    xdata : array_like\n        The independent variable where the data is measured.\n        Should usually be an M-length sequence or an (k,M)-shaped array for\n        functions with k predictors, and each element should be float\n        convertible if it is an array like object.\n    ydata : array_like\n        The dependent data, a length M array - nominally ``f(xdata, ...)``.\n    p0 : array_like, optional\n        Initial guess for the parameters (length N). If None, then the\n        initial values will all be 1 (if the number of parameters for the\n        function can be determined using introspection, otherwise a\n        ValueError is raised).\n    sigma : None or scalar or M-length sequence or MxM array, optional\n        Determines the uncertainty in `ydata`. If we define residuals as\n        ``r = ydata - f(xdata, *popt)``, then the interpretation of `sigma`\n        depends on its number of dimensions:\n\n            - A scalar or 1-D `sigma` should contain values of standard deviations of\n              errors in `ydata`. In this case, the optimized function is\n              ``chisq = sum((r / sigma) ** 2)``.\n\n            - A 2-D `sigma` should contain the covariance matrix of\n              errors in `ydata`. In this case, the optimized function is\n              ``chisq = r.T @ inv(sigma) @ r``.\n\n              .. versionadded:: 0.19\n\n        None (default) is equivalent of 1-D `sigma` filled with ones.\n    absolute_sigma : bool, optional\n        If True, `sigma` is used in an absolute sense and the estimated parameter\n        covariance `pcov` reflects these absolute values.\n\n        If False (default), only the relative magnitudes of the `sigma` values matter.\n        The returned parameter covariance matrix `pcov` is based on scaling\n        `sigma` by a constant factor. This constant is set by demanding that the\n        reduced `chisq` for the optimal parameters `popt` when using the\n        *scaled* `sigma` equals unity. In other words, `sigma` is scaled to\n        match the sample variance of the residuals after the fit. Default is False.\n        Mathematically,\n        ``pcov(absolute_sigma=False) = pcov(absolute_sigma=True) * chisq(popt)/(M-N)``\n    check_finite : bool, optional\n        If True, check that the input arrays do not contain nans of infs,\n        and raise a ValueError if they do. Setting this parameter to\n        False may silently produce nonsensical results if the input arrays\n        do contain nans. Default is True if `nan_policy` is not specified\n        explicitly and False otherwise.\n    bounds : 2-tuple of array_like or `Bounds`, optional\n        Lower and upper bounds on parameters. Defaults to no bounds.\n        There are two ways to specify the bounds:\n\n            - Instance of `Bounds` class.\n\n            - 2-tuple of array_like: Each element of the tuple must be either\n              an array with the length equal to the number of parameters, or a\n              scalar (in which case the bound is taken to be the same for all\n              parameters). Use ``np.inf`` with an appropriate sign to disable\n              bounds on all or some parameters.\n\n    method : {'lm', 'trf', 'dogbox'}, optional\n        Method to use for optimization. See `least_squares` for more details.\n        Default is 'lm' for unconstrained problems and 'trf' if `bounds` are\n        provided. The method 'lm' won't work when the number of observations\n        is less than the number of variables, use 'trf' or 'dogbox' in this\n        case.\n\n        .. versionadded:: 0.17\n    jac : callable, string or None, optional\n        Function with signature ``jac(x, ...)`` which computes the Jacobian\n        matrix of the model function with respect to parameters as a dense\n        array_like structure. It will be scaled according to provided `sigma`.\n        If None (default), the Jacobian will be estimated numerically.\n        String keywords for 'trf' and 'dogbox' methods can be used to select\n        a finite difference scheme, see `least_squares`.\n\n        .. versionadded:: 0.18\n    full_output : boolean, optional\n        If True, this function returns additioal information: `infodict`,\n        `mesg`, and `ier`.\n\n        .. versionadded:: 1.9\n    nan_policy : {'raise', 'omit', None}, optional\n        Defines how to handle when input contains nan.\n        The following options are available (default is None):\n\n          * 'raise': throws an error\n          * 'omit': performs the calculations ignoring nan values\n          * None: no special handling of NaNs is performed\n            (except what is done by check_finite); the behavior when NaNs\n            are present is implementation-dependent and may change.\n\n        Note that if this value is specified explicitly (not None),\n        `check_finite` will be set as False.\n\n        .. versionadded:: 1.11\n    **kwargs\n        Keyword arguments passed to `leastsq` for ``method='lm'`` or\n        `least_squares` otherwise.\n\n    Returns\n    -------\n    popt : array\n        Optimal values for the parameters so that the sum of the squared\n        residuals of ``f(xdata, *popt) - ydata`` is minimized.\n    pcov : 2-D array\n        The estimated approximate covariance of popt. The diagonals provide\n        the variance of the parameter estimate. To compute one standard\n        deviation errors on the parameters, use\n        ``perr = np.sqrt(np.diag(pcov))``. Note that the relationship between\n        `cov` and parameter error estimates is derived based on a linear\n        approximation to the model function around the optimum [1].\n        When this approximation becomes inaccurate, `cov` may not provide an\n        accurate measure of uncertainty.\n\n        How the `sigma` parameter affects the estimated covariance\n        depends on `absolute_sigma` argument, as described above.\n\n        If the Jacobian matrix at the solution doesn't have a full rank, then\n        'lm' method returns a matrix filled with ``np.inf``, on the other hand\n        'trf'  and 'dogbox' methods use Moore-Penrose pseudoinverse to compute\n        the covariance matrix. Covariance matrices with large condition numbers\n        (e.g. computed with `numpy.linalg.cond`) may indicate that results are\n        unreliable.\n    infodict : dict (returned only if `full_output` is True)\n        a dictionary of optional outputs with the keys:\n\n        ``nfev``\n            The number of function calls. Methods 'trf' and 'dogbox' do not\n            count function calls for numerical Jacobian approximation,\n            as opposed to 'lm' method.\n        ``fvec``\n            The residual values evaluated at the solution, for a 1-D `sigma`\n            this is ``(f(x, *popt) - ydata)/sigma``.\n        ``fjac``\n            A permutation of the R matrix of a QR\n            factorization of the final approximate\n            Jacobian matrix, stored column wise.\n            Together with ipvt, the covariance of the\n            estimate can be approximated.\n            Method 'lm' only provides this information.\n        ``ipvt``\n            An integer array of length N which defines\n            a permutation matrix, p, such that\n            fjac*p = q*r, where r is upper triangular\n            with diagonal elements of nonincreasing\n            magnitude. Column j of p is column ipvt(j)\n            of the identity matrix.\n            Method 'lm' only provides this information.\n        ``qtf``\n            The vector (transpose(q) * fvec).\n            Method 'lm' only provides this information.\n\n        .. versionadded:: 1.9\n    mesg : str (returned only if `full_output` is True)\n        A string message giving information about the solution.\n\n        .. versionadded:: 1.9\n    ier : int (returned only if `full_output` is True)\n        An integer flag. If it is equal to 1, 2, 3 or 4, the solution was\n        found. Otherwise, the solution was not found. In either case, the\n        optional output variable `mesg` gives more information.\n\n        .. versionadded:: 1.9\n\n    Raises\n    ------\n    ValueError\n        if either `ydata` or `xdata` contain NaNs, or if incompatible options\n        are used.\n\n    RuntimeError\n        if the least-squares minimization fails.\n\n    OptimizeWarning\n        if covariance of the parameters can not be estimated.\n\n    See Also\n    --------\n    least_squares : Minimize the sum of squares of nonlinear functions.\n    scipy.stats.linregress : Calculate a linear least squares regression for\n                             two sets of measurements.\n\n    Notes\n    -----\n    Users should ensure that inputs `xdata`, `ydata`, and the output of `f`\n    are ``float64``, or else the optimization may return incorrect results.\n\n    With ``method='lm'``, the algorithm uses the Levenberg-Marquardt algorithm\n    through `leastsq`. Note that this algorithm can only deal with\n    unconstrained problems.\n\n    Box constraints can be handled by methods 'trf' and 'dogbox'. Refer to\n    the docstring of `least_squares` for more information.\n\n    Parameters to be fitted must have similar scale. Differences of multiple\n    orders of magnitude can lead to incorrect results. For the 'trf' and\n    'dogbox' methods, the `x_scale` keyword argument can be used to scale\n    the parameters.\n\n    References\n    ----------\n    [1] K. Vugrin et al. Confidence region estimation techniques for nonlinear\n        regression in groundwater flow: Three case studies. Water Resources\n        Research, Vol. 43, W03423, :doi:`10.1029/2005WR004804`\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import matplotlib.pyplot as plt\n    >>> from scipy.optimize import curve_fit\n\n    >>> def func(x, a, b, c):\n    ...     return a * np.exp(-b * x) + c\n\n    Define the data to be fit with some noise:\n\n    >>> xdata = np.linspace(0, 4, 50)\n    >>> y = func(xdata, 2.5, 1.3, 0.5)\n    >>> rng = np.random.default_rng()\n    >>> y_noise = 0.2 * rng.normal(size=xdata.size)\n    >>> ydata = y + y_noise\n    >>> plt.plot(xdata, ydata, 'b-', label='data')\n\n    Fit for the parameters a, b, c of the function `func`:\n\n    >>> popt, pcov = curve_fit(func, xdata, ydata)\n    >>> popt\n    array([2.56274217, 1.37268521, 0.47427475])\n    >>> plt.plot(xdata, func(xdata, *popt), 'r-',\n    ...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n\n    Constrain the optimization to the region of ``0 <= a <= 3``,\n    ``0 <= b <= 1`` and ``0 <= c <= 0.5``:\n\n    >>> popt, pcov = curve_fit(func, xdata, ydata, bounds=(0, [3., 1., 0.5]))\n    >>> popt\n    array([2.43736712, 1.        , 0.34463856])\n    >>> plt.plot(xdata, func(xdata, *popt), 'g--',\n    ...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n\n    >>> plt.xlabel('x')\n    >>> plt.ylabel('y')\n    >>> plt.legend()\n    >>> plt.show()\n\n    For reliable results, the model `func` should not be overparametrized;\n    redundant parameters can cause unreliable covariance matrices and, in some\n    cases, poorer quality fits. As a quick check of whether the model may be\n    overparameterized, calculate the condition number of the covariance matrix:\n\n    >>> np.linalg.cond(pcov)\n    34.571092161547405  # may vary\n\n    The value is small, so it does not raise much concern. If, however, we were\n    to add a fourth parameter ``d`` to `func` with the same effect as ``a``:\n\n    >>> def func2(x, a, b, c, d):\n    ...     return a * d * np.exp(-b * x) + c  # a and d are redundant\n    >>> popt, pcov = curve_fit(func2, xdata, ydata)\n    >>> np.linalg.cond(pcov)\n    1.13250718925596e+32  # may vary\n\n    Such a large value is cause for concern. The diagonal elements of the\n    covariance matrix, which is related to uncertainty of the fit, gives more\n    information:\n\n    >>> np.diag(pcov)\n    array([1.48814742e+29, 3.78596560e-02, 5.39253738e-03, 2.76417220e+28])  # may vary\n\n    Note that the first and last terms are much larger than the other elements,\n    suggesting that the optimal values of these parameters are ambiguous and\n    that only one of these parameters is needed in the model.\n\n    If the optimal parameters of `f` differ by multiple orders of magnitude, the\n    resulting fit can be inaccurate. Sometimes, `curve_fit` can fail to find any\n    results:\n\n    >>> ydata = func(xdata, 500000, 0.01, 15)\n    >>> try:\n    ...     popt, pcov = curve_fit(func, xdata, ydata, method = 'trf')\n    ... except RuntimeError as e:\n    ...     print(e)\n    Optimal parameters not found: The maximum number of function evaluations is\n    exceeded.\n\n    If parameter scale is roughly known beforehand, it can be defined in\n    `x_scale` argument:\n\n    >>> popt, pcov = curve_fit(func, xdata, ydata, method = 'trf',\n    ...                        x_scale = [1000, 1, 1])\n    >>> popt\n    array([5.00000000e+05, 1.00000000e-02, 1.49999999e+01])\n    ",
    "umap.umap_.discrete_metric_simplicial_set_intersection": "Combine a fuzzy simplicial set with another fuzzy simplicial set\n    generated from discrete metric data using discrete distances. The target\n    data is assumed to be categorical label data (a vector of labels),\n    and this will update the fuzzy simplicial set to respect that label data.\n\n    TODO: optional category cardinality based weighting of distance\n\n    Parameters\n    ----------\n    simplicial_set: sparse matrix\n        The input fuzzy simplicial set.\n\n    discrete_space: array of shape (n_samples)\n        The categorical labels to use in the intersection.\n\n    unknown_dist: float (optional, default 1.0)\n        The distance an unknown label (-1) is assumed to be from any point.\n\n    far_dist: float (optional, default 5.0)\n        The distance between unmatched labels.\n\n    metric: str (optional, default None)\n        If not None, then use this metric to determine the\n        distance between values.\n\n    metric_scale: float (optional, default 1.0)\n        If using a custom metric scale the distance values by\n        this value -- this controls the weighting of the\n        intersection. Larger values weight more toward target.\n\n    Returns\n    -------\n    simplicial_set: sparse matrix\n        The resulting intersected fuzzy simplicial set.\n    ",
    "umap.umap_.fast_intersection": "Under the assumption of categorical distance for the intersecting\n    simplicial set perform a fast intersection.\n\n    Parameters\n    ----------\n    rows: array\n        An array of the row of each non-zero in the sparse matrix\n        representation.\n\n    cols: array\n        An array of the column of each non-zero in the sparse matrix\n        representation.\n\n    values: array\n        An array of the value of each non-zero in the sparse matrix\n        representation.\n\n    target: array of shape (n_samples)\n        The categorical labels to use in the intersection.\n\n    unknown_dist: float (optional, default 1.0)\n        The distance an unknown label (-1) is assumed to be from any point.\n\n    far_dist float (optional, default 5.0)\n        The distance between unmatched labels.\n\n    Returns\n    -------\n    None\n    ",
    "umap.umap_.fast_knn_indices": "A fast computation of knn indices.\n\n    Parameters\n    ----------\n    X: array of shape (n_samples, n_features)\n        The input data to compute the k-neighbor indices of.\n\n    n_neighbors: int\n        The number of nearest neighbors to compute for each sample in ``X``.\n\n    Returns\n    -------\n    knn_indices: array of shape (n_samples, n_neighbors)\n        The indices on the ``n_neighbors`` closest points in the dataset.\n    ",
    "umap.umap_.fast_metric_intersection": "Under the assumption of categorical distance for the intersecting\n    simplicial set perform a fast intersection.\n\n    Parameters\n    ----------\n    rows: array\n        An array of the row of each non-zero in the sparse matrix\n        representation.\n\n    cols: array\n        An array of the column of each non-zero in the sparse matrix\n        representation.\n\n    values: array of shape\n        An array of the values of each non-zero in the sparse matrix\n        representation.\n\n    discrete_space: array of shape (n_samples, n_features)\n        The vectors of categorical labels to use in the intersection.\n\n    metric: numba function\n        The function used to calculate distance over the target array.\n\n    scale: float\n        A scaling to apply to the metric.\n\n    Returns\n    -------\n    None\n    ",
    "umap.umap_.find_ab_params": "Fit a, b params for the differentiable curve used in lower\n    dimensional fuzzy simplicial complex construction. We want the\n    smooth curve (from a pre-defined family with simple gradient) that\n    best matches an offset exponential decay.\n    ",
    "umap.umap_.fuzzy_simplicial_set": "Given a set of data X, a neighborhood size, and a measure of distance\n    compute the fuzzy simplicial set (here represented as a fuzzy graph in\n    the form of a sparse matrix) associated to the data. This is done by\n    locally approximating geodesic distance at each point, creating a fuzzy\n    simplicial set for each such point, and then combining all the local\n    fuzzy simplicial sets into a global one via a fuzzy union.\n\n    Parameters\n    ----------\n    X: array of shape (n_samples, n_features)\n        The data to be modelled as a fuzzy simplicial set.\n\n    n_neighbors: int\n        The number of neighbors to use to approximate geodesic distance.\n        Larger numbers induce more global estimates of the manifold that can\n        miss finer detail, while smaller values will focus on fine manifold\n        structure to the detriment of the larger picture.\n\n    random_state: numpy RandomState or equivalent\n        A state capable being used as a numpy random state.\n\n    metric: string or function (optional, default 'euclidean')\n        The metric to use to compute distances in high dimensional space.\n        If a string is passed it must match a valid predefined metric. If\n        a general metric is required a function that takes two 1d arrays and\n        returns a float can be provided. For performance purposes it is\n        required that this be a numba jit'd function. Valid string metrics\n        include:\n\n        * euclidean (or l2)\n        * manhattan (or l1)\n        * cityblock\n        * braycurtis\n        * canberra\n        * chebyshev\n        * correlation\n        * cosine\n        * dice\n        * hamming\n        * jaccard\n        * kulsinski\n        * ll_dirichlet\n        * mahalanobis\n        * matching\n        * minkowski\n        * rogerstanimoto\n        * russellrao\n        * seuclidean\n        * sokalmichener\n        * sokalsneath\n        * sqeuclidean\n        * yule\n        * wminkowski\n\n        Metrics that take arguments (such as minkowski, mahalanobis etc.)\n        can have arguments passed via the metric_kwds dictionary. At this\n        time care must be taken and dictionary elements must be ordered\n        appropriately; this will hopefully be fixed in the future.\n\n    metric_kwds: dict (optional, default {})\n        Arguments to pass on to the metric, such as the ``p`` value for\n        Minkowski distance.\n\n    knn_indices: array of shape (n_samples, n_neighbors) (optional)\n        If the k-nearest neighbors of each point has already been calculated\n        you can pass them in here to save computation time. This should be\n        an array with the indices of the k-nearest neighbors as a row for\n        each data point.\n\n    knn_dists: array of shape (n_samples, n_neighbors) (optional)\n        If the k-nearest neighbors of each point has already been calculated\n        you can pass them in here to save computation time. This should be\n        an array with the distances of the k-nearest neighbors as a row for\n        each data point.\n\n    angular: bool (optional, default False)\n        Whether to use angular/cosine distance for the random projection\n        forest for seeding NN-descent to determine approximate nearest\n        neighbors.\n\n    set_op_mix_ratio: float (optional, default 1.0)\n        Interpolate between (fuzzy) union and intersection as the set operation\n        used to combine local fuzzy simplicial sets to obtain a global fuzzy\n        simplicial sets. Both fuzzy set operations use the product t-norm.\n        The value of this parameter should be between 0.0 and 1.0; a value of\n        1.0 will use a pure fuzzy union, while 0.0 will use a pure fuzzy\n        intersection.\n\n    local_connectivity: int (optional, default 1)\n        The local connectivity required -- i.e. the number of nearest\n        neighbors that should be assumed to be connected at a local level.\n        The higher this value the more connected the manifold becomes\n        locally. In practice this should be not more than the local intrinsic\n        dimension of the manifold.\n\n    verbose: bool (optional, default False)\n        Whether to report information on the current progress of the algorithm.\n\n    return_dists: bool or None (optional, default None)\n        Whether to return the pairwise distance associated with each edge.\n\n    Returns\n    -------\n    fuzzy_simplicial_set: coo_matrix\n        A fuzzy simplicial set represented as a sparse matrix. The (i,\n        j) entry of the matrix represents the membership strength of the\n        1-simplex between the ith and jth sample points.\n    ",
    "umap.umap_.init_graph_transform": "Given a bipartite graph representing the 1-simplices and strengths between the\n    new points and the original data set along with an embedding of the original points\n    initialize the positions of new points relative to the strengths (of their neighbors in the source data).\n\n    If a point is in our original data set it embeds at the original points coordinates.\n    If a point has no neighbours in our original dataset it embeds as the np.nan vector.\n    Otherwise a point is the weighted average of it's neighbours embedding locations.\n\n    Parameters\n    ----------\n    graph: csr_matrix (n_new_samples, n_samples)\n        A matrix indicating the 1-simplices and their associated strengths.  These strengths should\n        be values between zero and one and not normalized.  One indicating that the new point was identical\n        to one of our original points.\n\n    embedding: array of shape (n_samples, dim)\n        The original embedding of the source data.\n\n    Returns\n    -------\n    new_embedding: array of shape (n_new_samples, dim)\n        An initial embedding of the new sample points.\n    ",
    "umap.umap_.init_transform": "Given indices and weights and an original embeddings\n    initialize the positions of new points relative to the\n    indices and weights (of their neighbors in the source data).\n\n    Parameters\n    ----------\n    indices: array of shape (n_new_samples, n_neighbors)\n        The indices of the neighbors of each new sample\n\n    weights: array of shape (n_new_samples, n_neighbors)\n        The membership strengths of associated 1-simplices\n        for each of the new samples.\n\n    embedding: array of shape (n_samples, dim)\n        The original embedding of the source data.\n\n    Returns\n    -------\n    new_embedding: array of shape (n_new_samples, dim)\n        An initial embedding of the new sample points.\n    ",
    "umap.umap_.make_epochs_per_sample": "Given a set of weights and number of epochs generate the number of\n    epochs per sample for each weight.\n\n    Parameters\n    ----------\n    weights: array of shape (n_1_simplices)\n        The weights of how much we wish to sample each 1-simplex.\n\n    n_epochs: int\n        The total number of epochs we want to train for.\n\n    Returns\n    -------\n    An array of number of epochs per sample, one for each 1-simplex.\n    ",
    "umap.umap_.nearest_neighbors": "Compute the ``n_neighbors`` nearest points for each data point in ``X``\n    under ``metric``. This may be exact, but more likely is approximated via\n    nearest neighbor descent.\n\n    Parameters\n    ----------\n    X: array of shape (n_samples, n_features)\n        The input data to compute the k-neighbor graph of.\n\n    n_neighbors: int\n        The number of nearest neighbors to compute for each sample in ``X``.\n\n    metric: string or callable\n        The metric to use for the computation.\n\n    metric_kwds: dict\n        Any arguments to pass to the metric computation function.\n\n    angular: bool\n        Whether to use angular rp trees in NN approximation.\n\n    random_state: np.random state\n        The random state to use for approximate NN computations.\n\n    low_memory: bool (optional, default True)\n        Whether to pursue lower memory NNdescent.\n\n    verbose: bool (optional, default False)\n        Whether to print status data during the computation.\n\n    Returns\n    -------\n    knn_indices: array of shape (n_samples, n_neighbors)\n        The indices on the ``n_neighbors`` closest points in the dataset.\n\n    knn_dists: array of shape (n_samples, n_neighbors)\n        The distances to the ``n_neighbors`` closest points in the dataset.\n\n    rp_forest: list of trees\n        The random projection forest used for searching (if used, None otherwise).\n    ",
    "umap.umap_.normalize": "Scale input vectors individually to unit norm (vector length).\n\n    Read more in the :ref:`User Guide <preprocessing_normalization>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The data to normalize, element by element.\n        scipy.sparse matrices should be in CSR format to avoid an\n        un-necessary copy.\n\n    norm : {'l1', 'l2', 'max'}, default='l2'\n        The norm to use to normalize each non zero sample (or each non-zero\n        feature if axis is 0).\n\n    axis : {0, 1}, default=1\n        Define axis used to normalize the data along. If 1, independently\n        normalize each sample, otherwise (if 0) normalize each feature.\n\n    copy : bool, default=True\n        If False, try to avoid a copy and normalize in place.\n        This is not guaranteed to always work in place; e.g. if the data is\n        a numpy array with an int dtype, a copy will be returned even with\n        copy=False.\n\n    return_norm : bool, default=False\n        Whether to return the computed norms.\n\n    Returns\n    -------\n    X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        Normalized input X.\n\n    norms : ndarray of shape (n_samples, ) if axis=1 else (n_features, )\n        An array of norms along given axis for X.\n        When X is sparse, a NotImplementedError will be raised\n        for norm 'l1' or 'l2'.\n\n    See Also\n    --------\n    Normalizer : Performs normalization using the Transformer API\n        (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`).\n\n    Notes\n    -----\n    For a comparison of the different scalers, transformers, and normalizers,\n    see: :ref:`sphx_glr_auto_examples_preprocessing_plot_all_scaling.py`.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import normalize\n    >>> X = [[-2, 1, 2], [-1, 0, 1]]\n    >>> normalize(X, norm=\"l1\")  # L1 normalization each row independently\n    array([[-0.4,  0.2,  0.4],\n           [-0.5,  0. ,  0.5]])\n    >>> normalize(X, norm=\"l2\")  # L2 normalization each row independently\n    array([[-0.66...,  0.33...,  0.66...],\n           [-0.70...,  0.     ,  0.70...]])\n    ",
    "umap.umap_.optimize_layout_euclidean": "Improve an embedding using stochastic gradient descent to minimize the\n    fuzzy set cross entropy between the 1-skeletons of the high dimensional\n    and low dimensional fuzzy simplicial sets. In practice this is done by\n    sampling edges based on their membership strength (with the (1-p) terms\n    coming from negative sampling similar to word2vec).\n    Parameters\n    ----------\n    head_embedding: array of shape (n_samples, n_components)\n        The initial embedding to be improved by SGD.\n    tail_embedding: array of shape (source_samples, n_components)\n        The reference embedding of embedded points. If not embedding new\n        previously unseen points with respect to an existing embedding this\n        is simply the head_embedding (again); otherwise it provides the\n        existing embedding to embed with respect to.\n    head: array of shape (n_1_simplices)\n        The indices of the heads of 1-simplices with non-zero membership.\n    tail: array of shape (n_1_simplices)\n        The indices of the tails of 1-simplices with non-zero membership.\n    n_epochs: int, or list of int\n        The number of training epochs to use in optimization, or a list of\n        epochs at which to save the embedding. In case of a list, the optimization\n        will use the maximum number of epochs in the list, and will return a list\n        of embedding in the order of increasing epoch, regardless of the order in\n        the epoch list.\n    n_vertices: int\n        The number of vertices (0-simplices) in the dataset.\n    epochs_per_sample: array of shape (n_1_simplices)\n        A float value of the number of epochs per 1-simplex. 1-simplices with\n        weaker membership strength will have more epochs between being sampled.\n    a: float\n        Parameter of differentiable approximation of right adjoint functor\n    b: float\n        Parameter of differentiable approximation of right adjoint functor\n    rng_state: array of int64, shape (3,)\n        The internal state of the rng\n    gamma: float (optional, default 1.0)\n        Weight to apply to negative samples.\n    initial_alpha: float (optional, default 1.0)\n        Initial learning rate for the SGD.\n    negative_sample_rate: int (optional, default 5)\n        Number of negative samples to use per positive sample.\n    parallel: bool (optional, default False)\n        Whether to run the computation using numba parallel.\n        Running in parallel is non-deterministic, and is not used\n        if a random seed has been set, to ensure reproducibility.\n    verbose: bool (optional, default False)\n        Whether to report information on the current progress of the algorithm.\n    densmap: bool (optional, default False)\n        Whether to use the density-augmented densMAP objective\n    densmap_kwds: dict (optional, default None)\n        Auxiliary data for densMAP\n    tqdm_kwds: dict (optional, default None)\n        Keyword arguments for tqdm progress bar.\n    move_other: bool (optional, default False)\n        Whether to adjust tail_embedding alongside head_embedding\n    Returns\n    -------\n    embedding: array of shape (n_samples, n_components)\n        The optimized embedding.\n    ",
    "umap.umap_.optimize_layout_generic": "Improve an embedding using stochastic gradient descent to minimize the\n    fuzzy set cross entropy between the 1-skeletons of the high dimensional\n    and low dimensional fuzzy simplicial sets. In practice this is done by\n    sampling edges based on their membership strength (with the (1-p) terms\n    coming from negative sampling similar to word2vec).\n\n    Parameters\n    ----------\n    head_embedding: array of shape (n_samples, n_components)\n        The initial embedding to be improved by SGD.\n\n    tail_embedding: array of shape (source_samples, n_components)\n        The reference embedding of embedded points. If not embedding new\n        previously unseen points with respect to an existing embedding this\n        is simply the head_embedding (again); otherwise it provides the\n        existing embedding to embed with respect to.\n\n    head: array of shape (n_1_simplices)\n        The indices of the heads of 1-simplices with non-zero membership.\n\n    tail: array of shape (n_1_simplices)\n        The indices of the tails of 1-simplices with non-zero membership.\n\n    n_epochs: int\n        The number of training epochs to use in optimization.\n\n    n_vertices: int\n        The number of vertices (0-simplices) in the dataset.\n\n    epochs_per_sample: array of shape (n_1_simplices)\n        A float value of the number of epochs per 1-simplex. 1-simplices with\n        weaker membership strength will have more epochs between being sampled.\n\n    a: float\n        Parameter of differentiable approximation of right adjoint functor\n\n    b: float\n        Parameter of differentiable approximation of right adjoint functor\n\n    rng_state: array of int64, shape (3,)\n        The internal state of the rng\n\n    gamma: float (optional, default 1.0)\n        Weight to apply to negative samples.\n\n    initial_alpha: float (optional, default 1.0)\n        Initial learning rate for the SGD.\n\n    negative_sample_rate: int (optional, default 5)\n        Number of negative samples to use per positive sample.\n\n    verbose: bool (optional, default False)\n        Whether to report information on the current progress of the algorithm.\n\n    tqdm_kwds: dict (optional, default None)\n        Keyword arguments for tqdm progress bar.\n\n    move_other: bool (optional, default False)\n        Whether to adjust tail_embedding alongside head_embedding\n\n    Returns\n    -------\n    embedding: array of shape (n_samples, n_components)\n        The optimized embedding.\n    ",
    "umap.umap_.optimize_layout_inverse": "Improve an embedding using stochastic gradient descent to minimize the\n    fuzzy set cross entropy between the 1-skeletons of the high dimensional\n    and low dimensional fuzzy simplicial sets. In practice this is done by\n    sampling edges based on their membership strength (with the (1-p) terms\n    coming from negative sampling similar to word2vec).\n\n    Parameters\n    ----------\n    head_embedding: array of shape (n_samples, n_components)\n        The initial embedding to be improved by SGD.\n\n    tail_embedding: array of shape (source_samples, n_components)\n        The reference embedding of embedded points. If not embedding new\n        previously unseen points with respect to an existing embedding this\n        is simply the head_embedding (again); otherwise it provides the\n        existing embedding to embed with respect to.\n\n    head: array of shape (n_1_simplices)\n        The indices of the heads of 1-simplices with non-zero membership.\n\n    tail: array of shape (n_1_simplices)\n        The indices of the tails of 1-simplices with non-zero membership.\n\n    weight: array of shape (n_1_simplices)\n        The membership weights of the 1-simplices.\n\n    sigmas:\n\n    rhos:\n\n    n_epochs: int\n        The number of training epochs to use in optimization.\n\n    n_vertices: int\n        The number of vertices (0-simplices) in the dataset.\n\n    epochs_per_sample: array of shape (n_1_simplices)\n        A float value of the number of epochs per 1-simplex. 1-simplices with\n        weaker membership strength will have more epochs between being sampled.\n\n    a: float\n        Parameter of differentiable approximation of right adjoint functor\n\n    b: float\n        Parameter of differentiable approximation of right adjoint functor\n\n    rng_state: array of int64, shape (3,)\n        The internal state of the rng\n\n    gamma: float (optional, default 1.0)\n        Weight to apply to negative samples.\n\n    initial_alpha: float (optional, default 1.0)\n        Initial learning rate for the SGD.\n\n    negative_sample_rate: int (optional, default 5)\n        Number of negative samples to use per positive sample.\n\n    verbose: bool (optional, default False)\n        Whether to report information on the current progress of the algorithm.\n\n    tqdm_kwds: dict (optional, default None)\n        Keyword arguments for tqdm progress bar.\n\n    move_other: bool (optional, default False)\n        Whether to adjust tail_embedding alongside head_embedding\n\n    Returns\n    -------\n    embedding: array of shape (n_samples, n_components)\n        The optimized embedding.\n    ",
    "umap.umap_.pairwise_distances": "Compute the distance matrix from a vector array X and optional Y.\n\n    This method takes either a vector array or a distance matrix, and returns\n    a distance matrix.\n    If the input is a vector array, the distances are computed.\n    If the input is a distances matrix, it is returned instead.\n    If the input is a collection of non-numeric data (e.g. a list of strings or a\n    boolean array), a custom metric must be passed.\n\n    This method provides a safe way to take a distance matrix as input, while\n    preserving compatibility with many other algorithms that take a vector\n    array.\n\n    If Y is given (default is None), then the returned matrix is the pairwise\n    distance between the arrays from both X and Y.\n\n    Valid values for metric are:\n\n    - From scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n      'manhattan']. These metrics support sparse matrix\n      inputs.\n      ['nan_euclidean'] but it does not yet support sparse matrices.\n\n    - From scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n      'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis',\n      'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n      'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']\n      See the documentation for scipy.spatial.distance for details on these\n      metrics. These metrics do not support sparse matrix inputs.\n\n    .. note::\n        `'kulsinski'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.\n\n    .. note::\n        `'matching'` has been removed in SciPy 1.9 (use `'hamming'` instead).\n\n    Note that in the case of 'cityblock', 'cosine' and 'euclidean' (which are\n    valid scipy.spatial.distance metrics), the scikit-learn implementation\n    will be used, which is faster and has support for sparse matrices (except\n    for 'cityblock'). For a verbose description of the metrics from\n    scikit-learn, see :func:`sklearn.metrics.pairwise.distance_metrics`\n    function.\n\n    Read more in the :ref:`User Guide <metrics>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n        Array of pairwise distances between samples, or a feature array.\n        The shape of the array should be (n_samples_X, n_samples_X) if\n        metric == \"precomputed\" and (n_samples_X, n_features) otherwise.\n\n    Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None\n        An optional second feature array. Only allowed if\n        metric != \"precomputed\".\n\n    metric : str or callable, default='euclidean'\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string, it must be one of the options\n        allowed by scipy.spatial.distance.pdist for its metric parameter, or\n        a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``.\n        If metric is \"precomputed\", X is assumed to be a distance matrix.\n        Alternatively, if metric is a callable function, it is called on each\n        pair of instances (rows) and the resulting value recorded. The callable\n        should take two arrays from X as input and return a value indicating\n        the distance between them.\n\n    n_jobs : int, default=None\n        The number of jobs to use for the computation. This works by breaking\n        down the pairwise matrix into n_jobs even slices and computing them\n        using multithreading.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        The \"euclidean\" and \"cosine\" metrics rely heavily on BLAS which is already\n        multithreaded. So, increasing `n_jobs` would likely cause oversubscription\n        and quickly degrade performance.\n\n    force_all_finite : bool or 'allow-nan', default=True\n        Whether to raise an error on np.inf, np.nan, pd.NA in array. Ignored\n        for a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``. The\n        possibilities are:\n\n        - True: Force all values of array to be finite.\n        - False: accepts np.inf, np.nan, pd.NA in array.\n        - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n          cannot be infinite.\n\n        .. versionadded:: 0.22\n           ``force_all_finite`` accepts the string ``'allow-nan'``.\n\n        .. versionchanged:: 0.23\n           Accepts `pd.NA` and converts it into `np.nan`.\n\n        .. deprecated:: 1.6\n           `force_all_finite` was renamed to `ensure_all_finite` and will be removed\n           in 1.8.\n\n    ensure_all_finite : bool or 'allow-nan', default=True\n        Whether to raise an error on np.inf, np.nan, pd.NA in array. Ignored\n        for a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``. The\n        possibilities are:\n\n        - True: Force all values of array to be finite.\n        - False: accepts np.inf, np.nan, pd.NA in array.\n        - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n          cannot be infinite.\n\n        .. versionadded:: 1.6\n           `force_all_finite` was renamed to `ensure_all_finite`.\n\n    **kwds : optional keyword parameters\n        Any further parameters are passed directly to the distance function.\n        If using a scipy.spatial.distance metric, the parameters are still\n        metric dependent. See the scipy docs for usage examples.\n\n    Returns\n    -------\n    D : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_samples_Y)\n        A distance matrix D such that D_{i, j} is the distance between the\n        ith and jth vectors of the given matrix X, if Y is None.\n        If Y is not None, then D_{i, j} is the distance between the ith array\n        from X and the jth array from Y.\n\n    See Also\n    --------\n    pairwise_distances_chunked : Performs the same calculation as this\n        function, but returns a generator of chunks of the distance matrix, in\n        order to limit memory usage.\n    sklearn.metrics.pairwise.paired_distances : Computes the distances between\n        corresponding elements of two arrays.\n\n    Examples\n    --------\n    >>> from sklearn.metrics.pairwise import pairwise_distances\n    >>> X = [[0, 0, 0], [1, 1, 1]]\n    >>> Y = [[1, 0, 0], [1, 1, 0]]\n    >>> pairwise_distances(X, Y, metric='sqeuclidean')\n    array([[1., 2.],\n           [2., 1.]])\n    ",
    "umap.umap_.raise_disconnected_warning": "A simple wrapper function to avoid large amounts of code repetition.",
    "umap.umap_.reset_local_connectivity": "Reset the local connectivity requirement -- each data sample should\n    have complete confidence in at least one 1-simplex in the simplicial set.\n    We can enforce this by locally rescaling confidences, and then remerging the\n    different local simplicial sets together.\n\n    Parameters\n    ----------\n    simplicial_set: sparse matrix\n        The simplicial set for which to recalculate with respect to local\n        connectivity.\n\n    Returns\n    -------\n    simplicial_set: sparse_matrix\n        The recalculated simplicial set, now with the local connectivity\n        assumption restored.\n    ",
    "umap.umap_.simplicial_set_embedding": "Perform a fuzzy simplicial set embedding, using a specified\n    initialisation method and then minimizing the fuzzy set cross entropy\n    between the 1-skeletons of the high and low dimensional fuzzy simplicial\n    sets.\n\n    Parameters\n    ----------\n    data: array of shape (n_samples, n_features)\n        The source data to be embedded by UMAP.\n\n    graph: sparse matrix\n        The 1-skeleton of the high dimensional fuzzy simplicial set as\n        represented by a graph for which we require a sparse matrix for the\n        (weighted) adjacency matrix.\n\n    n_components: int\n        The dimensionality of the euclidean space into which to embed the data.\n\n    initial_alpha: float\n        Initial learning rate for the SGD.\n\n    a: float\n        Parameter of differentiable approximation of right adjoint functor\n\n    b: float\n        Parameter of differentiable approximation of right adjoint functor\n\n    gamma: float\n        Weight to apply to negative samples.\n\n    negative_sample_rate: int (optional, default 5)\n        The number of negative samples to select per positive sample\n        in the optimization process. Increasing this value will result\n        in greater repulsive force being applied, greater optimization\n        cost, but slightly more accuracy.\n\n    n_epochs: int (optional, default 0), or list of int\n        The number of training epochs to be used in optimizing the\n        low dimensional embedding. Larger values result in more accurate\n        embeddings. If 0 is specified a value will be selected based on\n        the size of the input dataset (200 for large datasets, 500 for small).\n        If a list of int is specified, then the intermediate embeddings at the\n        different epochs specified in that list are returned in\n        ``aux_data[\"embedding_list\"]``.\n\n    init: string\n        How to initialize the low dimensional embedding. Options are:\n\n            * 'spectral': use a spectral embedding of the fuzzy 1-skeleton\n            * 'random': assign initial embedding positions at random.\n            * 'pca': use the first n_components from PCA applied to the input data.\n            * A numpy array of initial embedding positions.\n\n    random_state: numpy RandomState or equivalent\n        A state capable being used as a numpy random state.\n\n    metric: string or callable\n        The metric used to measure distance in high dimensional space; used if\n        multiple connected components need to be layed out.\n\n    metric_kwds: dict\n        Key word arguments to be passed to the metric function; used if\n        multiple connected components need to be layed out.\n\n    densmap: bool\n        Whether to use the density-augmented objective function to optimize\n        the embedding according to the densMAP algorithm.\n\n    densmap_kwds: dict\n        Key word arguments to be used by the densMAP optimization.\n\n    output_dens: bool\n        Whether to output local radii in the original data and the embedding.\n\n    output_metric: function\n        Function returning the distance between two points in embedding space and\n        the gradient of the distance wrt the first argument.\n\n    output_metric_kwds: dict\n        Key word arguments to be passed to the output_metric function.\n\n    euclidean_output: bool\n        Whether to use the faster code specialised for euclidean output metrics\n\n    parallel: bool (optional, default False)\n        Whether to run the computation using numba parallel.\n        Running in parallel is non-deterministic, and is not used\n        if a random seed has been set, to ensure reproducibility.\n\n    verbose: bool (optional, default False)\n        Whether to report information on the current progress of the algorithm.\n\n    tqdm_kwds: dict\n        Key word arguments to be used by the tqdm progress bar.\n\n    Returns\n    -------\n    embedding: array of shape (n_samples, n_components)\n        The optimized of ``graph`` into an ``n_components`` dimensional\n        euclidean space.\n\n    aux_data: dict\n        Auxiliary output returned with the embedding. When densMAP extension\n        is turned on, this dictionary includes local radii in the original\n        data (``rad_orig``) and in the embedding (``rad_emb``).\n    ",
    "umap.umap_.smooth_knn_dist": "Compute a continuous version of the distance to the kth nearest\n    neighbor. That is, this is similar to knn-distance but allows continuous\n    k values rather than requiring an integral k. In essence we are simply\n    computing the distance such that the cardinality of fuzzy set we generate\n    is k.\n\n    Parameters\n    ----------\n    distances: array of shape (n_samples, n_neighbors)\n        Distances to nearest neighbors for each sample. Each row should be a\n        sorted list of distances to a given samples nearest neighbors.\n\n    k: float\n        The number of nearest neighbors to approximate for.\n\n    n_iter: int (optional, default 64)\n        We need to binary search for the correct distance value. This is the\n        max number of iterations to use in such a search.\n\n    local_connectivity: int (optional, default 1)\n        The local connectivity required -- i.e. the number of nearest\n        neighbors that should be assumed to be connected at a local level.\n        The higher this value the more connected the manifold becomes\n        locally. In practice this should be not more than the local intrinsic\n        dimension of the manifold.\n\n    bandwidth: float (optional, default 1)\n        The target bandwidth of the kernel, larger values will produce\n        larger return values.\n\n    Returns\n    -------\n    knn_dist: array of shape (n_samples,)\n        The distance to kth nearest neighbor, as suitably approximated.\n\n    nn_dist: array of shape (n_samples,)\n        The distance to the 1st nearest neighbor for each point.\n    ",
    "umap.umap_.sparse_tril": "Return the lower triangular portion of a sparse array or matrix\n\n    Returns the elements on or below the k-th diagonal of A.\n        - k = 0 corresponds to the main diagonal\n        - k > 0 is above the main diagonal\n        - k < 0 is below the main diagonal\n\n    Parameters\n    ----------\n    A : dense or sparse array or matrix\n        Matrix whose lower trianglar portion is desired.\n    k : integer : optional\n        The top-most diagonal of the lower triangle.\n    format : string\n        Sparse format of the result, e.g. format=\"csr\", etc.\n\n    Returns\n    -------\n    L : sparse matrix\n        Lower triangular portion of A in sparse format.\n\n    See Also\n    --------\n    triu : upper triangle in sparse format\n\n    Examples\n    --------\n    >>> from scipy.sparse import csr_array, tril\n    >>> A = csr_array([[1, 2, 0, 0, 3], [4, 5, 0, 6, 7], [0, 0, 8, 9, 0]],\n    ...               dtype='int32')\n    >>> A.toarray()\n    array([[1, 2, 0, 0, 3],\n           [4, 5, 0, 6, 7],\n           [0, 0, 8, 9, 0]])\n    >>> tril(A).toarray()\n    array([[1, 0, 0, 0, 0],\n           [4, 5, 0, 0, 0],\n           [0, 0, 8, 0, 0]])\n    >>> tril(A).nnz\n    4\n    >>> tril(A, k=1).toarray()\n    array([[1, 2, 0, 0, 0],\n           [4, 5, 0, 0, 0],\n           [0, 0, 8, 9, 0]])\n    >>> tril(A, k=-1).toarray()\n    array([[0, 0, 0, 0, 0],\n           [4, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0]])\n    >>> tril(A, format='csc')\n    <Compressed Sparse Column sparse array of dtype 'int32'\n        with 4 stored elements and shape (3, 5)>\n\n    ",
    "umap.umap_.sparse_triu": "Return the upper triangular portion of a sparse array or matrix\n\n    Returns the elements on or above the k-th diagonal of A.\n        - k = 0 corresponds to the main diagonal\n        - k > 0 is above the main diagonal\n        - k < 0 is below the main diagonal\n\n    Parameters\n    ----------\n    A : dense or sparse array or matrix\n        Matrix whose upper trianglar portion is desired.\n    k : integer : optional\n        The bottom-most diagonal of the upper triangle.\n    format : string\n        Sparse format of the result, e.g. format=\"csr\", etc.\n\n    Returns\n    -------\n    L : sparse array or matrix \n        Upper triangular portion of A in sparse format.\n        Sparse array if A is a sparse array, otherwise matrix.\n\n    See Also\n    --------\n    tril : lower triangle in sparse format\n\n    Examples\n    --------\n    >>> from scipy.sparse import csr_array, triu\n    >>> A = csr_array([[1, 2, 0, 0, 3], [4, 5, 0, 6, 7], [0, 0, 8, 9, 0]],\n    ...                dtype='int32')\n    >>> A.toarray()\n    array([[1, 2, 0, 0, 3],\n           [4, 5, 0, 6, 7],\n           [0, 0, 8, 9, 0]])\n    >>> triu(A).toarray()\n    array([[1, 2, 0, 0, 3],\n           [0, 5, 0, 6, 7],\n           [0, 0, 8, 9, 0]])\n    >>> triu(A).nnz\n    8\n    >>> triu(A, k=1).toarray()\n    array([[0, 2, 0, 0, 3],\n           [0, 0, 0, 6, 7],\n           [0, 0, 0, 9, 0]])\n    >>> triu(A, k=-1).toarray()\n    array([[1, 2, 0, 0, 3],\n           [4, 5, 0, 6, 7],\n           [0, 0, 8, 9, 0]])\n    >>> triu(A, format='csc')\n    <Compressed Sparse Column sparse array of dtype 'int32'\n        with 8 stored elements and shape (3, 5)>\n\n    ",
    "umap.umap_.spectral_layout": "\n    Given a graph compute the spectral embedding of the graph. This is\n    simply the eigenvectors of the laplacian of the graph. Here we use the\n    normalized laplacian.\n\n    Parameters\n    ----------\n    data: array of shape (n_samples, n_features)\n        The source data\n\n    graph: sparse matrix\n        The (weighted) adjacency matrix of the graph as a sparse matrix.\n\n    dim: int\n        The dimension of the space into which to embed.\n\n    random_state: numpy RandomState or equivalent\n        A state capable being used as a numpy random state.\n\n    tol: float, default chosen by implementation\n        Stopping tolerance for the numerical algorithm computing the embedding.\n\n    maxiter: int, default chosen by implementation\n        Number of iterations the numerical algorithm will go through at most as it\n        attempts to compute the embedding.\n\n    Returns\n    -------\n    embedding: array of shape (n_vertices, dim)\n        The spectral embedding of the graph.\n    ",
    "umap.umap_.submatrix": "Return a submatrix given an orginal matrix and the indices to keep.\n\n    Parameters\n    ----------\n    dmat: array, shape (n_samples, n_samples)\n        Original matrix.\n\n    indices_col: array, shape (n_samples, n_neighbors)\n        Indices to keep. Each row consists of the indices of the columns.\n\n    n_neighbors: int\n        Number of neighbors.\n\n    Returns\n    -------\n    submat: array, shape (n_samples, n_neighbors)\n        The corresponding submatrix.\n    ",
    "umap.umap_.tswspectral_layout": "Given a graph, compute the spectral embedding of the graph. This is\n    simply the eigenvectors of the Laplacian of the graph. Here we use the\n    normalized laplacian and a truncated SVD-based guess of the\n    eigenvectors to \"warm\" up the eigensolver. This function should\n    give results of similar accuracy to the spectral_layout function, but\n    may converge more quickly for graph Laplacians that cause\n    spectral_layout to take an excessive amount of time to complete.\n\n    Parameters\n    ----------\n    data: array of shape (n_samples, n_features)\n        The source data\n\n    graph: sparse matrix\n        The (weighted) adjacency matrix of the graph as a sparse matrix.\n\n    dim: int\n        The dimension of the space into which to embed.\n\n    random_state: numpy RandomState or equivalent\n        A state capable being used as a numpy random state.\n\n    metric: string or callable (optional, default 'euclidean')\n        The metric used to measure distances among the source data points.\n        Used only if the multiple connected components are found in the\n        graph.\n\n    metric_kwds: dict (optional, default {})\n        Keyword arguments to be passed to the metric function.\n        If metric is 'precomputed', 'linkage' keyword can be used to specify\n        'average', 'complete', or 'single' linkage. Default is 'average'.\n        Used only if the multiple connected components are found in the\n        graph.\n\n    method: str (optional, default None, values either 'eigsh' or 'lobpcg')\n        Name of the eigenvalue computation method to use to compute the spectral\n        embedding. If left to None (or empty string), as by default, the method is\n        chosen from the number of vectors in play: larger vector collections are\n        handled with lobpcg, smaller collections with eigsh. Method names correspond\n        to SciPy routines in scipy.sparse.linalg.\n\n    tol: float, default chosen by implementation\n        Stopping tolerance for the numerical algorithm computing the embedding.\n\n    maxiter: int, default chosen by implementation\n        Number of iterations the numerical algorithm will go through at most as it\n        attempts to compute the embedding.\n\n    Returns\n    -------\n    embedding: array of shape (n_vertices, dim)\n        The spectral embedding of the graph.\n    ",
    "umap.umap_.warn": "Issue a warning, or maybe ignore it or raise an exception.",
    "matplotlib.pyplot": "\n`matplotlib.pyplot` is a state-based interface to matplotlib. It provides\nan implicit,  MATLAB-like, way of plotting.  It also opens figures on your\nscreen, and acts as the figure GUI manager.\n\npyplot is mainly intended for interactive plots and simple cases of\nprogrammatic plot generation::\n\n    import numpy as np\n    import matplotlib.pyplot as plt\n\n    x = np.arange(0, 5, 0.1)\n    y = np.sin(x)\n    plt.plot(x, y)\n    plt.show()\n\nThe explicit object-oriented API is recommended for complex plots, though\npyplot is still usually used to create the figure and often the Axes in the\nfigure. See `.pyplot.figure`, `.pyplot.subplots`, and\n`.pyplot.subplot_mosaic` to create figures, and\n:doc:`Axes API </api/axes_api>` for the plotting methods on an Axes::\n\n    import numpy as np\n    import matplotlib.pyplot as plt\n\n    x = np.arange(0, 5, 0.1)\n    y = np.sin(x)\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    plt.show()\n\n\nSee :ref:`api_interfaces` for an explanation of the tradeoffs between the\nimplicit and explicit interfaces.\n",
    "matplotlib.pyplot.AbstractContextManager": "An abstract base class for context managers.",
    "matplotlib.pyplot.Annotation": "\n    An `.Annotation` is a `.Text` that can refer to a specific position *xy*.\n    Optionally an arrow pointing from the text to *xy* can be drawn.\n\n    Attributes\n    ----------\n    xy\n        The annotated position.\n    xycoords\n        The coordinate system for *xy*.\n    arrow_patch\n        A `.FancyArrowPatch` to point from *xytext* to *xy*.\n    ",
    "matplotlib.pyplot.Arrow": "An arrow patch.",
    "matplotlib.pyplot.Artist": "\n    Abstract base class for objects that render into a FigureCanvas.\n\n    Typically, all visible elements in a figure are subclasses of Artist.\n    ",
    "matplotlib.pyplot.AutoLocator": "\n    Place evenly spaced ticks, with the step size and maximum number of ticks chosen\n    automatically.\n\n    This is a subclass of `~matplotlib.ticker.MaxNLocator`, with parameters\n    *nbins = 'auto'* and *steps = [1, 2, 2.5, 5, 10]*.\n    ",
    "matplotlib.pyplot.AxLine": "\n    A helper class that implements `~.Axes.axline`, by recomputing the artist\n    transform at draw time.\n    ",
    "matplotlib.pyplot.Axes": "An Axes object encapsulates all the elements of an individual (sub-)plot in\na figure.\n\nIt contains most of the (sub-)plot elements: `~.axis.Axis`,\n`~.axis.Tick`, `~.lines.Line2D`, `~.text.Text`, `~.patches.Polygon`, etc.,\nand sets the coordinate system.\n\nLike all visible elements in a figure, Axes is an `.Artist` subclass.\n\nThe `Axes` instance supports callbacks through a callbacks attribute which\nis a `~.cbook.CallbackRegistry` instance.  The events you can connect to\nare 'xlim_changed' and 'ylim_changed' and the callback will be called with\nfunc(*ax*) where *ax* is the `Axes` instance.\n\n.. note::\n\n    As a user, you do not instantiate Axes directly, but use Axes creation\n    methods instead; e.g. from `.pyplot` or `.Figure`:\n    `~.pyplot.subplots`, `~.pyplot.subplot_mosaic` or `.Figure.add_axes`.",
    "matplotlib.pyplot.BackendFilter": "\n    Filter used with :meth:`~matplotlib.backends.registry.BackendRegistry.list_builtin`\n\n    .. versionadded:: 3.9\n    ",
    "matplotlib.pyplot.Button": "\n    A GUI neutral button.\n\n    For the button to remain responsive you must keep a reference to it.\n    Call `.on_clicked` to connect to the button.\n\n    Attributes\n    ----------\n    ax\n        The `~.axes.Axes` the button renders into.\n    label\n        A `.Text` instance.\n    color\n        The color of the button when not hovering.\n    hovercolor\n        The color of the button when hovering.\n    ",
    "matplotlib.pyplot.Circle": "\n    A circle patch.\n    ",
    "matplotlib.pyplot.Colorizer": "\n    Data to color pipeline.\n\n    This pipeline is accessible via `.Colorizer.to_rgba` and executed via\n    the `.Colorizer.norm` and `.Colorizer.cmap` attributes.\n\n    Parameters\n    ----------\n    cmap: colorbar.Colorbar or str or None, default: None\n        The colormap used to color data.\n\n    norm: colors.Normalize or str or None, default: None\n        The normalization used to normalize the data\n    ",
    "matplotlib.pyplot.ColorizingArtist": "\n    Base class for artists that make map data to color using a `.colorizer.Colorizer`.\n\n    The `.colorizer.Colorizer` applies data normalization before\n    returning RGBA colors from a `~matplotlib.colors.Colormap`.\n\n    ",
    "matplotlib.pyplot.Colormap": "\n    Baseclass for all scalar to RGBA mappings.\n\n    Typically, Colormap instances are used to convert data values (floats)\n    from the interval ``[0, 1]`` to the RGBA color that the respective\n    Colormap represents. For scaling of data into the ``[0, 1]`` interval see\n    `matplotlib.colors.Normalize`. Subclasses of `matplotlib.cm.ScalarMappable`\n    make heavy use of this ``data -> normalize -> map-to-color`` processing\n    chain.\n    ",
    "matplotlib.pyplot.Enum": "\n    Create a collection of name/value pairs.\n\n    Example enumeration:\n\n    >>> class Color(Enum):\n    ...     RED = 1\n    ...     BLUE = 2\n    ...     GREEN = 3\n\n    Access them by:\n\n    - attribute access::\n\n    >>> Color.RED\n    <Color.RED: 1>\n\n    - value lookup:\n\n    >>> Color(1)\n    <Color.RED: 1>\n\n    - name lookup:\n\n    >>> Color['RED']\n    <Color.RED: 1>\n\n    Enumerations can be iterated over, and know how many members they have:\n\n    >>> len(Color)\n    3\n\n    >>> list(Color)\n    [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]\n\n    Methods can be added to enumerations, and members can have their own\n    attributes -- see the documentation for details.\n    ",
    "matplotlib.pyplot.ExitStack": "Context manager for dynamic management of a stack of exit callbacks.\n\n    For example:\n        with ExitStack() as stack:\n            files = [stack.enter_context(open(fname)) for fname in filenames]\n            # All opened files will automatically be closed at the end of\n            # the with statement, even if attempts to open files later\n            # in the list raise an exception.\n    ",
    "matplotlib.pyplot.Figure": "The top level container for all the plot elements.\n\nSee `matplotlib.figure` for an index of class methods.\n\nAttributes\n----------\npatch\n    The `.Rectangle` instance representing the figure background patch.\n\nsuppressComposite\n    For multiple images, the figure will make composite images\n    depending on the renderer option_image_nocomposite function.  If\n    *suppressComposite* is a boolean, this will override the renderer.",
    "matplotlib.pyplot.FigureBase": "\n    Base class for `.Figure` and `.SubFigure` containing the methods that add\n    artists to the figure or subfigure, create Axes, etc.\n    ",
    "matplotlib.pyplot.FigureCanvasBase": "\n    The canvas the figure renders into.\n\n    Attributes\n    ----------\n    figure : `~matplotlib.figure.Figure`\n        A high-level figure instance.\n    ",
    "matplotlib.pyplot.FigureManagerBase": "\n    A backend-independent abstraction of a figure container and controller.\n\n    The figure manager is used by pyplot to interact with the window in a\n    backend-independent way. It's an adapter for the real (GUI) framework that\n    represents the visual figure on screen.\n\n    The figure manager is connected to a specific canvas instance, which in turn\n    is connected to a specific figure instance. To access a figure manager for\n    a given figure in user code, you typically use ``fig.canvas.manager``.\n\n    GUI backends derive from this class to translate common operations such\n    as *show* or *resize* to the GUI-specific code. Non-GUI backends do not\n    support these operations and can just use the base class.\n\n    This following basic operations are accessible:\n\n    **Window operations**\n\n    - `~.FigureManagerBase.show`\n    - `~.FigureManagerBase.destroy`\n    - `~.FigureManagerBase.full_screen_toggle`\n    - `~.FigureManagerBase.resize`\n    - `~.FigureManagerBase.get_window_title`\n    - `~.FigureManagerBase.set_window_title`\n\n    **Key and mouse button press handling**\n\n    The figure manager sets up default key and mouse button press handling by\n    hooking up the `.key_press_handler` to the matplotlib event system. This\n    ensures the same shortcuts and mouse actions across backends.\n\n    **Other operations**\n\n    Subclasses will have additional attributes and functions to access\n    additional functionality. This is of course backend-specific. For example,\n    most GUI backends have ``window`` and ``toolbar`` attributes that give\n    access to the native GUI widgets of the respective framework.\n\n    Attributes\n    ----------\n    canvas : `FigureCanvasBase`\n        The backend-specific canvas instance.\n\n    num : int or str\n        The figure number.\n\n    key_press_handler_id : int\n        The default key handler cid, when using the toolmanager.\n        To disable the default key press handling use::\n\n            figure.canvas.mpl_disconnect(\n                figure.canvas.manager.key_press_handler_id)\n\n    button_press_handler_id : int\n        The default mouse button handler cid, when using the toolmanager.\n        To disable the default button press handling use::\n\n            figure.canvas.mpl_disconnect(\n                figure.canvas.manager.button_press_handler_id)\n    ",
    "matplotlib.pyplot.FixedFormatter": "\n    Return fixed strings for tick labels based only on position, not value.\n\n    .. note::\n        `.FixedFormatter` should only be used together with `.FixedLocator`.\n        Otherwise, the labels may end up in unexpected positions.\n    ",
    "matplotlib.pyplot.FixedLocator": "\n    Place ticks at a set of fixed values.\n\n    If *nbins* is None ticks are placed at all values. Otherwise, the *locs* array of\n    possible positions will be subsampled to keep the number of ticks\n    :math:`\\leq nbins + 1`. The subsampling will be done to include the smallest\n    absolute value; for example, if zero is included in the array of possibilities, then\n    it will be included in the chosen ticks.\n    ",
    "matplotlib.pyplot.FormatStrFormatter": "\n    Use an old-style ('%' operator) format string to format the tick.\n\n    The format string should have a single variable format (%) in it.\n    It will be applied to the value (not the position) of the tick.\n\n    Negative numeric values (e.g., -1) will use a dash, not a Unicode minus;\n    use mathtext to get a Unicode minus by wrapping the format specifier with $\n    (e.g. \"$%g$\").\n    ",
    "matplotlib.pyplot.Formatter": "\n    Create a string based on a tick value and location.\n    ",
    "matplotlib.pyplot.FuncFormatter": "\n    Use a user-defined function for formatting.\n\n    The function should take in two inputs (a tick value ``x`` and a\n    position ``pos``), and return a string containing the corresponding\n    tick label.\n    ",
    "matplotlib.pyplot.GridSpec": "\n    A grid layout to place subplots within a figure.\n\n    The location of the grid cells is determined in a similar way to\n    `.SubplotParams` using *left*, *right*, *top*, *bottom*, *wspace*\n    and *hspace*.\n\n    Indexing a GridSpec instance returns a `.SubplotSpec`.\n    ",
    "matplotlib.pyplot.IndexLocator": "\n    Place ticks at every nth point plotted.\n\n    IndexLocator assumes index plotting; i.e., that the ticks are placed at integer\n    values in the range between 0 and len(data) inclusive.\n    ",
    "matplotlib.pyplot.Line2D": "A line - the line can have both a solid linestyle connecting all\nthe vertices, and a marker at each vertex.  Additionally, the\ndrawing of the solid line is influenced by the drawstyle, e.g., one\ncan create \"stepped\" lines in various styles.",
    "matplotlib.pyplot.LinearLocator": "\n    Place ticks at evenly spaced values.\n\n    The first time this function is called it will try to set the\n    number of ticks to make a nice tick partitioning.  Thereafter, the\n    number of ticks will be fixed so that interactive navigation will\n    be nice\n\n    ",
    "matplotlib.pyplot.Locator": "\n    Determine tick locations.\n\n    Note that the same locator should not be used across multiple\n    `~matplotlib.axis.Axis` because the locator stores references to the Axis\n    data and view limits.\n    ",
    "matplotlib.pyplot.LogFormatter": "\n    Base class for formatting ticks on a log or symlog scale.\n\n    It may be instantiated directly, or subclassed.\n\n    Parameters\n    ----------\n    base : float, default: 10.\n        Base of the logarithm used in all calculations.\n\n    labelOnlyBase : bool, default: False\n        If True, label ticks only at integer powers of base.\n        This is normally True for major ticks and False for\n        minor ticks.\n\n    minor_thresholds : (subset, all), default: (1, 0.4)\n        If labelOnlyBase is False, these two numbers control\n        the labeling of ticks that are not at integer powers of\n        base; normally these are the minor ticks. The controlling\n        parameter is the log of the axis data range.  In the typical\n        case where base is 10 it is the number of decades spanned\n        by the axis, so we can call it 'numdec'. If ``numdec <= all``,\n        all minor ticks will be labeled.  If ``all < numdec <= subset``,\n        then only a subset of minor ticks will be labeled, so as to\n        avoid crowding. If ``numdec > subset`` then no minor ticks will\n        be labeled.\n\n    linthresh : None or float, default: None\n        If a symmetric log scale is in use, its ``linthresh``\n        parameter must be supplied here.\n\n    Notes\n    -----\n    The `set_locs` method must be called to enable the subsetting\n    logic controlled by the ``minor_thresholds`` parameter.\n\n    In some cases such as the colorbar, there is no distinction between\n    major and minor ticks; the tick locations might be set manually,\n    or by a locator that puts ticks at integer powers of base and\n    at intermediate locations.  For this situation, disable the\n    minor_thresholds logic by using ``minor_thresholds=(np.inf, np.inf)``,\n    so that all ticks will be labeled.\n\n    To disable labeling of minor ticks when 'labelOnlyBase' is False,\n    use ``minor_thresholds=(0, 0)``.  This is the default for the\n    \"classic\" style.\n\n    Examples\n    --------\n    To label a subset of minor ticks when the view limits span up\n    to 2 decades, and all of the ticks when zoomed in to 0.5 decades\n    or less, use ``minor_thresholds=(2, 0.5)``.\n\n    To label all minor ticks when the view limits span up to 1.5\n    decades, use ``minor_thresholds=(1.5, 1.5)``.\n    ",
    "matplotlib.pyplot.LogFormatterExponent": "\n    Format values for log axis using ``exponent = log_base(value)``.\n    ",
    "matplotlib.pyplot.LogFormatterMathtext": "\n    Format values for log axis using ``exponent = log_base(value)``.\n    ",
    "matplotlib.pyplot.LogLocator": "\n    Place logarithmically spaced ticks.\n\n    Places ticks at the values ``subs[j] * base**i``.\n    ",
    "matplotlib.pyplot.MaxNLocator": "\n    Place evenly spaced ticks, with a cap on the total number of ticks.\n\n    Finds nice tick locations with no more than :math:`nbins + 1` ticks being within the\n    view limits. Locations beyond the limits are added to support autoscaling.\n    ",
    "matplotlib.pyplot.MultipleLocator": "\n    Place ticks at every integer multiple of a base plus an offset.\n    ",
    "matplotlib.pyplot.Normalize": "\n    A class which, when called, maps values within the interval\n    ``[vmin, vmax]`` linearly to the interval ``[0.0, 1.0]``. The mapping of\n    values outside ``[vmin, vmax]`` depends on *clip*.\n\n    Examples\n    --------\n    ::\n\n        x = [-2, -1, 0, 1, 2]\n\n        norm = mpl.colors.Normalize(vmin=-1, vmax=1, clip=False)\n        norm(x)  # [-0.5, 0., 0.5, 1., 1.5]\n        norm = mpl.colors.Normalize(vmin=-1, vmax=1, clip=True)\n        norm(x)  # [0., 0., 0.5, 1., 1.]\n\n    See Also\n    --------\n    :ref:`colormapnorms`\n    ",
    "matplotlib.pyplot.NullFormatter": "Always return the empty string.",
    "matplotlib.pyplot.NullLocator": "\n    No ticks\n    ",
    "matplotlib.pyplot.PolarAxes": "\n    A polar graph projection, where the input dimensions are *theta*, *r*.\n\n    Theta starts pointing east and goes anti-clockwise.\n    ",
    "matplotlib.pyplot.Polygon": "A general polygon patch.",
    "matplotlib.pyplot.Rectangle": "\n    A rectangle defined via an anchor point *xy* and its *width* and *height*.\n\n    The rectangle extends from ``xy[0]`` to ``xy[0] + width`` in x-direction\n    and from ``xy[1]`` to ``xy[1] + height`` in y-direction. ::\n\n      :                +------------------+\n      :                |                  |\n      :              height               |\n      :                |                  |\n      :               (xy)---- width -----+\n\n    One may picture *xy* as the bottom left corner, but which corner *xy* is\n    actually depends on the direction of the axis and the sign of *width*\n    and *height*; e.g. *xy* would be the bottom right corner if the x-axis\n    was inverted or if *width* was negative.\n    ",
    "matplotlib.pyplot.ScalarFormatter": "\n    Format tick values as a number.\n\n    Parameters\n    ----------\n    useOffset : bool or float, default: :rc:`axes.formatter.useoffset`\n        Whether to use offset notation. See `.set_useOffset`.\n    useMathText : bool, default: :rc:`axes.formatter.use_mathtext`\n        Whether to use fancy math formatting. See `.set_useMathText`.\n    useLocale : bool, default: :rc:`axes.formatter.use_locale`.\n        Whether to use locale settings for decimal sign and positive sign.\n        See `.set_useLocale`.\n    usetex : bool, default: :rc:`text.usetex`\n        To enable/disable the use of TeX's math mode for rendering the\n        numbers in the formatter.\n\n        .. versionadded:: 3.10\n\n    Notes\n    -----\n    In addition to the parameters above, the formatting of scientific vs.\n    floating point representation can be configured via `.set_scientific`\n    and `.set_powerlimits`).\n\n    **Offset notation and scientific notation**\n\n    Offset notation and scientific notation look quite similar at first sight.\n    Both split some information from the formatted tick values and display it\n    at the end of the axis.\n\n    - The scientific notation splits up the order of magnitude, i.e. a\n      multiplicative scaling factor, e.g. ``1e6``.\n\n    - The offset notation separates an additive constant, e.g. ``+1e6``. The\n      offset notation label is always prefixed with a ``+`` or ``-`` sign\n      and is thus distinguishable from the order of magnitude label.\n\n    The following plot with x limits ``1_000_000`` to ``1_000_010`` illustrates\n    the different formatting. Note the labels at the right edge of the x axis.\n\n    .. plot::\n\n        lim = (1_000_000, 1_000_010)\n\n        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, gridspec_kw={'hspace': 2})\n        ax1.set(title='offset notation', xlim=lim)\n        ax2.set(title='scientific notation', xlim=lim)\n        ax2.xaxis.get_major_formatter().set_useOffset(False)\n        ax3.set(title='floating-point notation', xlim=lim)\n        ax3.xaxis.get_major_formatter().set_useOffset(False)\n        ax3.xaxis.get_major_formatter().set_scientific(False)\n\n    ",
    "matplotlib.pyplot.Slider": "\n    A slider representing a floating point range.\n\n    Create a slider from *valmin* to *valmax* in Axes *ax*. For the slider to\n    remain responsive you must maintain a reference to it. Call\n    :meth:`on_changed` to connect to the slider event.\n\n    Attributes\n    ----------\n    val : float\n        Slider value.\n    ",
    "matplotlib.pyplot.Subplot": "An Axes object encapsulates all the elements of an individual (sub-)plot in\na figure.\n\nIt contains most of the (sub-)plot elements: `~.axis.Axis`,\n`~.axis.Tick`, `~.lines.Line2D`, `~.text.Text`, `~.patches.Polygon`, etc.,\nand sets the coordinate system.\n\nLike all visible elements in a figure, Axes is an `.Artist` subclass.\n\nThe `Axes` instance supports callbacks through a callbacks attribute which\nis a `~.cbook.CallbackRegistry` instance.  The events you can connect to\nare 'xlim_changed' and 'ylim_changed' and the callback will be called with\nfunc(*ax*) where *ax* is the `Axes` instance.\n\n.. note::\n\n    As a user, you do not instantiate Axes directly, but use Axes creation\n    methods instead; e.g. from `.pyplot` or `.Figure`:\n    `~.pyplot.subplots`, `~.pyplot.subplot_mosaic` or `.Figure.add_axes`.",
    "matplotlib.pyplot.SubplotSpec": "\n    The location of a subplot in a `GridSpec`.\n\n    .. note::\n\n        Likely, you will never instantiate a `SubplotSpec` yourself. Instead,\n        you will typically obtain one from a `GridSpec` using item-access.\n\n    Parameters\n    ----------\n    gridspec : `~matplotlib.gridspec.GridSpec`\n        The GridSpec, which the subplot is referencing.\n    num1, num2 : int\n        The subplot will occupy the *num1*-th cell of the given\n        *gridspec*.  If *num2* is provided, the subplot will span between\n        *num1*-th cell and *num2*-th cell **inclusive**.\n\n        The index starts from 0.\n    ",
    "matplotlib.pyplot.Text": "Handle storing and drawing of text in window or data coordinates.",
    "matplotlib.pyplot.Widget": "\n    Abstract base class for GUI neutral widgets.\n    ",
    "matplotlib.pyplot._ColorizerInterface": "\n    Base class that contains the interface to `Colorizer` objects from\n    a `ColorizingArtist` or `.cm.ScalarMappable`.\n\n    Note: This class only contain functions that interface the .colorizer\n    attribute. Other functions that as shared between `.ColorizingArtist`\n    and `.cm.ScalarMappable` are not included.\n    ",
    "matplotlib.pyplot._add_pyplot_note": "\n    Add a note to the docstring of *func* that it is a pyplot wrapper.\n\n    The note is added to the \"Notes\" section of the docstring. If that does\n    not exist, a \"Notes\" section is created. In numpydoc, the \"Notes\"\n    section is the third last possible section, only potentially followed by\n    \"References\" and \"Examples\".\n    ",
    "matplotlib.pyplot._auto_draw_if_interactive": "\n    An internal helper function for making sure that auto-redrawing\n    works as intended in the plain python repl.\n\n    Parameters\n    ----------\n    fig : Figure\n        A figure object which is assumed to be associated with a canvas\n    ",
    "matplotlib.pyplot._colormaps": "\n    Container for colormaps that are known to Matplotlib by name.\n\n    The universal registry instance is `matplotlib.colormaps`. There should be\n    no need for users to instantiate `.ColormapRegistry` themselves.\n\n    Read access uses a dict-like interface mapping names to `.Colormap`\\s::\n\n        import matplotlib as mpl\n        cmap = mpl.colormaps['viridis']\n\n    Returned `.Colormap`\\s are copies, so that their modification does not\n    change the global definition of the colormap.\n\n    Additional colormaps can be added via `.ColormapRegistry.register`::\n\n        mpl.colormaps.register(my_colormap)\n\n    To get a list of all registered colormaps, you can do::\n\n        from matplotlib import colormaps\n        list(colormaps)\n    ",
    "matplotlib.pyplot._get_backend_mod": "\n    Ensure that a backend is selected and return it.\n\n    This is currently private, but may be made public in the future.\n    ",
    "matplotlib.pyplot.acorr": "Plot the autocorrelation of *x*.\n\nParameters\n----------\nx : array-like\n    Not run through Matplotlib's unit conversion, so this should\n    be a unit-less array.\n\ndetrend : callable, default: `.mlab.detrend_none` (no detrending)\n    A detrending function applied to *x*.  It must have the\n    signature ::\n\n        detrend(x: np.ndarray) -> np.ndarray\n\nnormed : bool, default: True\n    If ``True``, input vectors are normalised to unit length.\n\nusevlines : bool, default: True\n    Determines the plot style.\n\n    If ``True``, vertical lines are plotted from 0 to the acorr value\n    using `.Axes.vlines`. Additionally, a horizontal line is plotted\n    at y=0 using `.Axes.axhline`.\n\n    If ``False``, markers are plotted at the acorr values using\n    `.Axes.plot`.\n\nmaxlags : int, default: 10\n    Number of lags to show. If ``None``, will return all\n    ``2 * len(x) - 1`` lags.\n\nReturns\n-------\nlags : array (length ``2*maxlags+1``)\n    The lag vector.\nc : array  (length ``2*maxlags+1``)\n    The auto correlation vector.\nline : `.LineCollection` or `.Line2D`\n    `.Artist` added to the Axes of the correlation:\n\n    - `.LineCollection` if *usevlines* is True.\n    - `.Line2D` if *usevlines* is False.\nb : `~matplotlib.lines.Line2D` or None\n    Horizontal line at 0 if *usevlines* is True\n    None *usevlines* is False.\n\nOther Parameters\n----------------\nlinestyle : `~matplotlib.lines.Line2D` property, optional\n    The linestyle for plotting the data points.\n    Only used if *usevlines* is ``False``.\n\nmarker : str, default: 'o'\n    The marker for plotting the data points.\n    Only used if *usevlines* is ``False``.\n\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *x*\n\n**kwargs\n    Additional parameters are passed to `.Axes.vlines` and\n    `.Axes.axhline` if *usevlines* is ``True``; otherwise they are\n    passed to `.Axes.plot`.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.acorr`.\n\nThe cross correlation is performed with `numpy.correlate` with\n``mode = \"full\"``.",
    "matplotlib.pyplot.angle_spectrum": "Plot the angle spectrum.\n\nCompute the angle spectrum (wrapped phase spectrum) of *x*.\nData is padded to a length of *pad_to* and the windowing function\n*window* is applied to the signal.\n\nParameters\n----------\nx : 1-D array or sequence\n    Array or sequence containing the data.\n\nFs : float, default: 2\n    The sampling frequency (samples per time unit).  It is used to calculate\n    the Fourier frequencies, *freqs*, in cycles per time unit.\n\nwindow : callable or ndarray, default: `.window_hanning`\n    A function or a vector of length *NFFT*.  To create window vectors see\n    `.window_hanning`, `.window_none`, `numpy.blackman`, `numpy.hamming`,\n    `numpy.bartlett`, `scipy.signal`, `scipy.signal.get_window`, etc.  If a\n    function is passed as the argument, it must take a data segment as an\n    argument and return the windowed version of the segment.\n\nsides : {'default', 'onesided', 'twosided'}, optional\n    Which sides of the spectrum to return. 'default' is one-sided for real\n    data and two-sided for complex data. 'onesided' forces the return of a\n    one-sided spectrum, while 'twosided' forces two-sided.\n\npad_to : int, optional\n    The number of points to which the data segment is padded when performing\n    the FFT.  While not increasing the actual resolution of the spectrum (the\n    minimum distance between resolvable peaks), this can give more points in\n    the plot, allowing for more detail. This corresponds to the *n* parameter\n    in the call to `~numpy.fft.fft`.  The default is None, which sets *pad_to*\n    equal to the length of the input signal (i.e. no padding).\n\nFc : int, default: 0\n    The center frequency of *x*, which offsets the x extents of the\n    plot to reflect the frequency range used when a signal is acquired\n    and then filtered and downsampled to baseband.\n\nReturns\n-------\nspectrum : 1-D array\n    The values for the angle spectrum in radians (real valued).\n\nfreqs : 1-D array\n    The frequencies corresponding to the elements in *spectrum*.\n\nline : `~matplotlib.lines.Line2D`\n    The line created by this function.\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *x*\n\n**kwargs\n    Keyword arguments control the `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: :mpltype:`color`\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: :mpltype:`color` or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: :mpltype:`color`\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: :mpltype:`color`\n    markerfacecoloralt or mfcalt: :mpltype:`color`\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nmagnitude_spectrum\n    Plots the magnitudes of the corresponding frequencies.\nphase_spectrum\n    Plots the unwrapped version of this function.\nspecgram\n    Can plot the angle spectrum of segments within the signal in a\n    colormap.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.angle_spectrum`.\n",
    "matplotlib.pyplot.annotate": "Annotate the point *xy* with text *text*.\n\nIn the simplest form, the text is placed at *xy*.\n\nOptionally, the text can be displayed in another position *xytext*.\nAn arrow pointing from the text to the annotated point *xy* can then\nbe added by defining *arrowprops*.\n\nParameters\n----------\ntext : str\n    The text of the annotation.\n\nxy : (float, float)\n    The point *(x, y)* to annotate. The coordinate system is determined\n    by *xycoords*.\n\nxytext : (float, float), default: *xy*\n    The position *(x, y)* to place the text at. The coordinate system\n    is determined by *textcoords*.\n\nxycoords : single or two-tuple of str or `.Artist` or `.Transform` or callable, default: 'data'\n\n    The coordinate system that *xy* is given in. The following types\n    of values are supported:\n\n    - One of the following strings:\n\n      ==================== ============================================\n      Value                Description\n      ==================== ============================================\n      'figure points'      Points from the lower left of the figure\n      'figure pixels'      Pixels from the lower left of the figure\n      'figure fraction'    Fraction of figure from lower left\n      'subfigure points'   Points from the lower left of the subfigure\n      'subfigure pixels'   Pixels from the lower left of the subfigure\n      'subfigure fraction' Fraction of subfigure from lower left\n      'axes points'        Points from lower left corner of the Axes\n      'axes pixels'        Pixels from lower left corner of the Axes\n      'axes fraction'      Fraction of Axes from lower left\n      'data'               Use the coordinate system of the object\n                           being annotated (default)\n      'polar'              *(theta, r)* if not native 'data'\n                           coordinates\n      ==================== ============================================\n\n      Note that 'subfigure pixels' and 'figure pixels' are the same\n      for the parent figure, so users who want code that is usable in\n      a subfigure can use 'subfigure pixels'.\n\n    - An `.Artist`: *xy* is interpreted as a fraction of the artist's\n      `~matplotlib.transforms.Bbox`. E.g. *(0, 0)* would be the lower\n      left corner of the bounding box and *(0.5, 1)* would be the\n      center top of the bounding box.\n\n    - A `.Transform` to transform *xy* to screen coordinates.\n\n    - A function with one of the following signatures::\n\n        def transform(renderer) -> Bbox\n        def transform(renderer) -> Transform\n\n      where *renderer* is a `.RendererBase` subclass.\n\n      The result of the function is interpreted like the `.Artist` and\n      `.Transform` cases above.\n\n    - A tuple *(xcoords, ycoords)* specifying separate coordinate\n      systems for *x* and *y*. *xcoords* and *ycoords* must each be\n      of one of the above described types.\n\n    See :ref:`plotting-guide-annotation` for more details.\n\ntextcoords : single or two-tuple of str or `.Artist` or `.Transform` or callable, default: value of *xycoords*\n    The coordinate system that *xytext* is given in.\n\n    All *xycoords* values are valid as well as the following strings:\n\n    =================   =================================================\n    Value               Description\n    =================   =================================================\n    'offset points'     Offset, in points, from the *xy* value\n    'offset pixels'     Offset, in pixels, from the *xy* value\n    'offset fontsize'   Offset, relative to fontsize, from the *xy* value\n    =================   =================================================\n\narrowprops : dict, optional\n    The properties used to draw a `.FancyArrowPatch` arrow between the\n    positions *xy* and *xytext*.  Defaults to None, i.e. no arrow is\n    drawn.\n\n    For historical reasons there are two different ways to specify\n    arrows, \"simple\" and \"fancy\":\n\n    **Simple arrow:**\n\n    If *arrowprops* does not contain the key 'arrowstyle' the\n    allowed keys are:\n\n    ==========  =================================================\n    Key         Description\n    ==========  =================================================\n    width       The width of the arrow in points\n    headwidth   The width of the base of the arrow head in points\n    headlength  The length of the arrow head in points\n    shrink      Fraction of total length to shrink from both ends\n    ?           Any `.FancyArrowPatch` property\n    ==========  =================================================\n\n    The arrow is attached to the edge of the text box, the exact\n    position (corners or centers) depending on where it's pointing to.\n\n    **Fancy arrow:**\n\n    This is used if 'arrowstyle' is provided in the *arrowprops*.\n\n    Valid keys are the following `.FancyArrowPatch` parameters:\n\n    ===============  ===================================\n    Key              Description\n    ===============  ===================================\n    arrowstyle       The arrow style\n    connectionstyle  The connection style\n    relpos           See below; default is (0.5, 0.5)\n    patchA           Default is bounding box of the text\n    patchB           Default is None\n    shrinkA          In points. Default is 2 points\n    shrinkB          In points. Default is 2 points\n    mutation_scale   Default is text size (in points)\n    mutation_aspect  Default is 1\n    ?                Any `.FancyArrowPatch` property\n    ===============  ===================================\n\n    The exact starting point position of the arrow is defined by\n    *relpos*. It's a tuple of relative coordinates of the text box,\n    where (0, 0) is the lower left corner and (1, 1) is the upper\n    right corner. Values <0 and >1 are supported and specify points\n    outside the text box. By default (0.5, 0.5), so the starting point\n    is centered in the text box.\n\nannotation_clip : bool or None, default: None\n    Whether to clip (i.e. not draw) the annotation when the annotation\n    point *xy* is outside the Axes area.\n\n    - If *True*, the annotation will be clipped when *xy* is outside\n      the Axes.\n    - If *False*, the annotation will always be drawn.\n    - If *None*, the annotation will be clipped when *xy* is outside\n      the Axes and *xycoords* is 'data'.\n\n**kwargs\n    Additional kwargs are passed to `.Text`.\n\nReturns\n-------\n`.Annotation`\n\nSee Also\n--------\n:ref:`annotations`\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.annotate`.\n",
    "matplotlib.pyplot.arrow": "Add an arrow to the Axes.\n\nThis draws an arrow from ``(x, y)`` to ``(x+dx, y+dy)``.\n\nParameters\n----------\nx, y : float\n    The x and y coordinates of the arrow base.\n\ndx, dy : float\n    The length of the arrow along x and y direction.\n\nwidth : float, default: 0.001\n    Width of full arrow tail.\n\nlength_includes_head : bool, default: False\n    True if head is to be counted in calculating the length.\n\nhead_width : float or None, default: 3*width\n    Total width of the full arrow head.\n\nhead_length : float or None, default: 1.5*head_width\n    Length of arrow head.\n\nshape : {'full', 'left', 'right'}, default: 'full'\n    Draw the left-half, right-half, or full arrow.\n\noverhang : float, default: 0\n    Fraction that the arrow is swept back (0 overhang means\n    triangular shape). Can be negative or greater than one.\n\nhead_starts_at_zero : bool, default: False\n    If True, the head starts being drawn at coordinate 0\n    instead of ending at coordinate 0.\n\n**kwargs\n    `.Patch` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: unknown\n    animated: bool\n    antialiased or aa: bool or None\n    capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color: :mpltype:`color`\n    edgecolor or ec: :mpltype:`color` or None\n    facecolor or fc: :mpltype:`color` or None\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fill: bool\n    gid: str\n    hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n    hatch_linewidth: unknown\n    in_layout: bool\n    joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float or None\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: None or bool or float or callable\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    visible: bool\n    zorder: float\n\nReturns\n-------\n`.FancyArrow`\n    The created `.FancyArrow` object.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.arrow`.\n\nThe resulting arrow is affected by the Axes aspect ratio and limits.\nThis may produce an arrow whose head is not square with its stem. To\ncreate an arrow whose head is square with its stem,\nuse :meth:`annotate` for example:\n\n>>> ax.annotate(\"\", xy=(0.5, 0.5), xytext=(0, 0),\n...             arrowprops=dict(arrowstyle=\"->\"))",
    "matplotlib.pyplot.autoscale": "Autoscale the axis view to the data (toggle).\n\nConvenience method for simple axis view autoscaling.\nIt turns autoscaling on or off, and then,\nif autoscaling for either axis is on, it performs\nthe autoscaling on the specified axis or Axes.\n\nParameters\n----------\nenable : bool or None, default: True\n    True turns autoscaling on, False turns it off.\n    None leaves the autoscaling state unchanged.\naxis : {'both', 'x', 'y'}, default: 'both'\n    The axis on which to operate.  (For 3D Axes, *axis* can also be set\n    to 'z', and 'both' refers to all three Axes.)\ntight : bool or None, default: None\n    If True, first set the margins to zero.  Then, this argument is\n    forwarded to `~.axes.Axes.autoscale_view` (regardless of\n    its value); see the description of its behavior there.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.autoscale`.\n",
    "matplotlib.pyplot.autumn": "\n    Set the colormap to 'autumn'.\n\n    This changes the default colormap as well as the colormap of the current\n    image if there is one. See ``help(colormaps)`` for more information.\n    ",
    "matplotlib.pyplot.axes": "Add an Axes to the current figure and make it the current Axes.\n\nCall signatures::\n\n    plt.axes()\n    plt.axes(rect, projection=None, polar=False, **kwargs)\n    plt.axes(ax)\n\nParameters\n----------\narg : None or 4-tuple\n    The exact behavior of this function depends on the type:\n\n    - *None*: A new full window Axes is added using\n      ``subplot(**kwargs)``.\n    - 4-tuple of floats *rect* = ``(left, bottom, width, height)``.\n      A new Axes is added with dimensions *rect* in normalized\n      (0, 1) units using `~.Figure.add_axes` on the current figure.\n\nprojection : {None, 'aitoff', 'hammer', 'lambert', 'mollweide', 'polar', 'rectilinear', str}, optional\n    The projection type of the `~.axes.Axes`. *str* is the name of\n    a custom projection, see `~matplotlib.projections`. The default\n    None results in a 'rectilinear' projection.\n\npolar : bool, default: False\n    If True, equivalent to projection='polar'.\n\nsharex, sharey : `~matplotlib.axes.Axes`, optional\n    Share the x or y `~matplotlib.axis` with sharex and/or sharey.\n    The axis will have the same limits, ticks, and scale as the axis\n    of the shared Axes.\n\nlabel : str\n    A label for the returned Axes.\n\nReturns\n-------\n`~.axes.Axes`, or a subclass of `~.axes.Axes`\n    The returned Axes class depends on the projection used. It is\n    `~.axes.Axes` if rectilinear projection is used and\n    `.projections.polar.PolarAxes` if polar projection is used.\n\nOther Parameters\n----------------\n**kwargs\n    This method also takes the keyword arguments for\n    the returned Axes class. The keyword arguments for the\n    rectilinear Axes class `~.axes.Axes` can be found in\n    the following table but there might also be other keyword\n    arguments if another projection is used, see the actual Axes\n    class.\n\n    Properties:\n    adjustable: {'box', 'datalim'}\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    anchor: (float, float) or {'C', 'SW', 'S', 'SE', 'E', 'NE', ...}\n    animated: bool\n    aspect: {'auto', 'equal'} or float\n    autoscale_on: bool\n    autoscalex_on: unknown\n    autoscaley_on: unknown\n    axes_locator: Callable[[Axes, Renderer], Bbox]\n    axisbelow: bool or 'line'\n    box_aspect: float or None\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    facecolor or fc: :mpltype:`color`\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    forward_navigation_events: bool or \"auto\"\n    frame_on: bool\n    gid: str\n    in_layout: bool\n    label: object\n    mouseover: bool\n    navigate: bool\n    navigate_mode: unknown\n    path_effects: list of `.AbstractPathEffect`\n    picker: None or bool or float or callable\n    position: [left, bottom, width, height] or `~matplotlib.transforms.Bbox`\n    prop_cycle: `~cycler.Cycler`\n    rasterization_zorder: float or None\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    subplotspec: unknown\n    title: str\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    visible: bool\n    xbound: (lower: float, upper: float)\n    xlabel: str\n    xlim: (left: float, right: float)\n    xmargin: float greater than -0.5\n    xscale: unknown\n    xticklabels: unknown\n    xticks: unknown\n    ybound: (lower: float, upper: float)\n    ylabel: str\n    ylim: (bottom: float, top: float)\n    ymargin: float greater than -0.5\n    yscale: unknown\n    yticklabels: unknown\n    yticks: unknown\n    zorder: float\n\nSee Also\n--------\n.Figure.add_axes\n.pyplot.subplot\n.Figure.add_subplot\n.Figure.subplots\n.pyplot.subplots\n\nExamples\n--------\n::\n\n    # Creating a new full window Axes\n    plt.axes()\n\n    # Creating a new Axes with specified dimensions and a grey background\n    plt.axes((left, bottom, width, height), facecolor='grey')",
    "matplotlib.pyplot.axhline": "Add a horizontal line spanning the whole or fraction of the Axes.\n\nNote: If you want to set x-limits in data coordinates, use\n`~.Axes.hlines` instead.\n\nParameters\n----------\ny : float, default: 0\n    y position in :ref:`data coordinates <coordinate-systems>`.\n\nxmin : float, default: 0\n    The start x-position in :ref:`axes coordinates <coordinate-systems>`.\n    Should be between 0 and 1, 0 being the far left of the plot,\n    1 the far right of the plot.\n\nxmax : float, default: 1\n    The end x-position in :ref:`axes coordinates <coordinate-systems>`.\n    Should be between 0 and 1, 0 being the far left of the plot,\n    1 the far right of the plot.\n\nReturns\n-------\n`~matplotlib.lines.Line2D`\n    A `.Line2D` specified via two points ``(xmin, y)``, ``(xmax, y)``.\n    Its transform is set such that *x* is in\n    :ref:`axes coordinates <coordinate-systems>` and *y* is in\n    :ref:`data coordinates <coordinate-systems>`.\n\n    This is still a generic line and the horizontal character is only\n    realized through using identical *y* values for both points. Thus,\n    if you want to change the *y* value later, you have to provide two\n    values ``line.set_ydata([3, 3])``.\n\nOther Parameters\n----------------\n**kwargs\n    Valid keyword arguments are `.Line2D` properties, except for\n    'transform':\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: :mpltype:`color`\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: :mpltype:`color` or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: :mpltype:`color`\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: :mpltype:`color`\n    markerfacecoloralt or mfcalt: :mpltype:`color`\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nhlines : Add horizontal lines in data coordinates.\naxhspan : Add a horizontal span (rectangle) across the axis.\naxline : Add a line with an arbitrary slope.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.axhline`.\n\nExamples\n--------\n* draw a thick red hline at 'y' = 0 that spans the xrange::\n\n    >>> axhline(linewidth=4, color='r')\n\n* draw a default hline at 'y' = 1 that spans the xrange::\n\n    >>> axhline(y=1)\n\n* draw a default hline at 'y' = .5 that spans the middle half of\n  the xrange::\n\n    >>> axhline(y=.5, xmin=0.25, xmax=0.75)",
    "matplotlib.pyplot.axhspan": "Add a horizontal span (rectangle) across the Axes.\n\nThe rectangle spans from *ymin* to *ymax* vertically, and, by default,\nthe whole x-axis horizontally.  The x-span can be set using *xmin*\n(default: 0) and *xmax* (default: 1) which are in axis units; e.g.\n``xmin = 0.5`` always refers to the middle of the x-axis regardless of\nthe limits set by `~.Axes.set_xlim`.\n\nParameters\n----------\nymin : float\n    Lower y-coordinate of the span, in data units.\nymax : float\n    Upper y-coordinate of the span, in data units.\nxmin : float, default: 0\n    Lower x-coordinate of the span, in x-axis (0-1) units.\nxmax : float, default: 1\n    Upper x-coordinate of the span, in x-axis (0-1) units.\n\nReturns\n-------\n`~matplotlib.patches.Rectangle`\n    Horizontal span (rectangle) from (xmin, ymin) to (xmax, ymax).\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.patches.Rectangle` properties\n\nProperties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    angle: unknown\n    animated: bool\n    antialiased or aa: bool or None\n    bounds: (left, bottom, width, height)\n    capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color: :mpltype:`color`\n    edgecolor or ec: :mpltype:`color` or None\n    facecolor or fc: :mpltype:`color` or None\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fill: bool\n    gid: str\n    hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n    hatch_linewidth: unknown\n    height: unknown\n    in_layout: bool\n    joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float or None\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: None or bool or float or callable\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    visible: bool\n    width: unknown\n    x: unknown\n    xy: (float, float)\n    y: unknown\n    zorder: float\n\nSee Also\n--------\naxvspan : Add a vertical span across the Axes.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.axhspan`.\n",
    "matplotlib.pyplot.axis": "Convenience method to get or set some axis properties.\n\nCall signatures::\n\n  xmin, xmax, ymin, ymax = axis()\n  xmin, xmax, ymin, ymax = axis([xmin, xmax, ymin, ymax])\n  xmin, xmax, ymin, ymax = axis(option)\n  xmin, xmax, ymin, ymax = axis(**kwargs)\n\nParameters\n----------\nxmin, xmax, ymin, ymax : float, optional\n    The axis limits to be set.  This can also be achieved using ::\n\n        ax.set(xlim=(xmin, xmax), ylim=(ymin, ymax))\n\noption : bool or str\n    If a bool, turns axis lines and labels on or off. If a string,\n    possible values are:\n\n    ================ ===========================================================\n    Value            Description\n    ================ ===========================================================\n    'off' or `False` Hide all axis decorations, i.e. axis labels, spines,\n                     tick marks, tick labels, and grid lines.\n                     This is the same as `~.Axes.set_axis_off()`.\n    'on' or `True`   Do not hide all axis decorations, i.e. axis labels, spines,\n                     tick marks, tick labels, and grid lines.\n                     This is the same as `~.Axes.set_axis_on()`.\n    'equal'          Set equal scaling (i.e., make circles circular) by\n                     changing the axis limits. This is the same as\n                     ``ax.set_aspect('equal', adjustable='datalim')``.\n                     Explicit data limits may not be respected in this case.\n    'scaled'         Set equal scaling (i.e., make circles circular) by\n                     changing dimensions of the plot box. This is the same as\n                     ``ax.set_aspect('equal', adjustable='box', anchor='C')``.\n                     Additionally, further autoscaling will be disabled.\n    'tight'          Set limits just large enough to show all data, then\n                     disable further autoscaling.\n    'auto'           Automatic scaling (fill plot box with data).\n    'image'          'scaled' with axis limits equal to data limits.\n    'square'         Square plot; similar to 'scaled', but initially forcing\n                     ``xmax-xmin == ymax-ymin``.\n    ================ ===========================================================\n\nemit : bool, default: True\n    Whether observers are notified of the axis limit change.\n    This option is passed on to `~.Axes.set_xlim` and\n    `~.Axes.set_ylim`.\n\nReturns\n-------\nxmin, xmax, ymin, ymax : float\n    The axis limits.\n\nSee Also\n--------\nmatplotlib.axes.Axes.set_xlim\nmatplotlib.axes.Axes.set_ylim\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.axis`.\n\nFor 3D Axes, this method additionally takes *zmin*, *zmax* as\nparameters and likewise returns them.",
    "matplotlib.pyplot.axline": "Add an infinitely long straight line.\n\nThe line can be defined either by two points *xy1* and *xy2*, or\nby one point *xy1* and a *slope*.\n\nThis draws a straight line \"on the screen\", regardless of the x and y\nscales, and is thus also suitable for drawing exponential decays in\nsemilog plots, power laws in loglog plots, etc. However, *slope*\nshould only be used with linear scales; It has no clear meaning for\nall other scales, and thus the behavior is undefined. Please specify\nthe line using the points *xy1*, *xy2* for non-linear scales.\n\nThe *transform* keyword argument only applies to the points *xy1*,\n*xy2*. The *slope* (if given) is always in data coordinates. This can\nbe used e.g. with ``ax.transAxes`` for drawing grid lines with a fixed\nslope.\n\nParameters\n----------\nxy1, xy2 : (float, float)\n    Points for the line to pass through.\n    Either *xy2* or *slope* has to be given.\nslope : float, optional\n    The slope of the line. Either *xy2* or *slope* has to be given.\n\nReturns\n-------\n`.AxLine`\n\nOther Parameters\n----------------\n**kwargs\n    Valid kwargs are `.Line2D` properties\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: :mpltype:`color`\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: :mpltype:`color` or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: :mpltype:`color`\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: :mpltype:`color`\n    markerfacecoloralt or mfcalt: :mpltype:`color`\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\naxhline : for horizontal lines\naxvline : for vertical lines\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.axline`.\n\nExamples\n--------\nDraw a thick red line passing through (0, 0) and (1, 1)::\n\n    >>> axline((0, 0), (1, 1), linewidth=4, color='r')",
    "matplotlib.pyplot.axvline": "Add a vertical line spanning the whole or fraction of the Axes.\n\nNote: If you want to set y-limits in data coordinates, use\n`~.Axes.vlines` instead.\n\nParameters\n----------\nx : float, default: 0\n    y position in :ref:`data coordinates <coordinate-systems>`.\n\nymin : float, default: 0\n    The start y-position in :ref:`axes coordinates <coordinate-systems>`.\n    Should be between 0 and 1, 0 being the bottom of the plot, 1 the\n    top of the plot.\n\nymax : float, default: 1\n    The end y-position in :ref:`axes coordinates <coordinate-systems>`.\n    Should be between 0 and 1, 0 being the bottom of the plot, 1 the\n    top of the plot.\n\nReturns\n-------\n`~matplotlib.lines.Line2D`\n    A `.Line2D` specified via two points ``(x, ymin)``, ``(x, ymax)``.\n    Its transform is set such that *x* is in\n    :ref:`data coordinates <coordinate-systems>` and *y* is in\n    :ref:`axes coordinates <coordinate-systems>`.\n\n    This is still a generic line and the vertical character is only\n    realized through using identical *x* values for both points. Thus,\n    if you want to change the *x* value later, you have to provide two\n    values ``line.set_xdata([3, 3])``.\n\nOther Parameters\n----------------\n**kwargs\n    Valid keyword arguments are `.Line2D` properties, except for\n    'transform':\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: :mpltype:`color`\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: :mpltype:`color` or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: :mpltype:`color`\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: :mpltype:`color`\n    markerfacecoloralt or mfcalt: :mpltype:`color`\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nvlines : Add vertical lines in data coordinates.\naxvspan : Add a vertical span (rectangle) across the axis.\naxline : Add a line with an arbitrary slope.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.axvline`.\n\nExamples\n--------\n* draw a thick red vline at *x* = 0 that spans the yrange::\n\n    >>> axvline(linewidth=4, color='r')\n\n* draw a default vline at *x* = 1 that spans the yrange::\n\n    >>> axvline(x=1)\n\n* draw a default vline at *x* = .5 that spans the middle half of\n  the yrange::\n\n    >>> axvline(x=.5, ymin=0.25, ymax=0.75)",
    "matplotlib.pyplot.axvspan": "Add a vertical span (rectangle) across the Axes.\n\nThe rectangle spans from *xmin* to *xmax* horizontally, and, by\ndefault, the whole y-axis vertically.  The y-span can be set using\n*ymin* (default: 0) and *ymax* (default: 1) which are in axis units;\ne.g. ``ymin = 0.5`` always refers to the middle of the y-axis\nregardless of the limits set by `~.Axes.set_ylim`.\n\nParameters\n----------\nxmin : float\n    Lower x-coordinate of the span, in data units.\nxmax : float\n    Upper x-coordinate of the span, in data units.\nymin : float, default: 0\n    Lower y-coordinate of the span, in y-axis units (0-1).\nymax : float, default: 1\n    Upper y-coordinate of the span, in y-axis units (0-1).\n\nReturns\n-------\n`~matplotlib.patches.Rectangle`\n    Vertical span (rectangle) from (xmin, ymin) to (xmax, ymax).\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.patches.Rectangle` properties\n\nProperties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    angle: unknown\n    animated: bool\n    antialiased or aa: bool or None\n    bounds: (left, bottom, width, height)\n    capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color: :mpltype:`color`\n    edgecolor or ec: :mpltype:`color` or None\n    facecolor or fc: :mpltype:`color` or None\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fill: bool\n    gid: str\n    hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n    hatch_linewidth: unknown\n    height: unknown\n    in_layout: bool\n    joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float or None\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: None or bool or float or callable\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    visible: bool\n    width: unknown\n    x: unknown\n    xy: (float, float)\n    y: unknown\n    zorder: float\n\nSee Also\n--------\naxhspan : Add a horizontal span across the Axes.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.axvspan`.\n\nExamples\n--------\nDraw a vertical, green, translucent rectangle from x = 1.25 to\nx = 1.55 that spans the yrange of the Axes.\n\n>>> axvspan(1.25, 1.55, facecolor='g', alpha=0.5)",
    "matplotlib.pyplot.bar": "Make a bar plot.\n\nThe bars are positioned at *x* with the given *align*\\ment. Their\ndimensions are given by *height* and *width*. The vertical baseline\nis *bottom* (default 0).\n\nMany parameters can take either a single value applying to all bars\nor a sequence of values, one for each bar.\n\nParameters\n----------\nx : float or array-like\n    The x coordinates of the bars. See also *align* for the\n    alignment of the bars to the coordinates.\n\nheight : float or array-like\n    The height(s) of the bars.\n\n    Note that if *bottom* has units (e.g. datetime), *height* should be in\n    units that are a difference from the value of *bottom* (e.g. timedelta).\n\nwidth : float or array-like, default: 0.8\n    The width(s) of the bars.\n\n    Note that if *x* has units (e.g. datetime), then *width* should be in\n    units that are a difference (e.g. timedelta) around the *x* values.\n\nbottom : float or array-like, default: 0\n    The y coordinate(s) of the bottom side(s) of the bars.\n\n    Note that if *bottom* has units, then the y-axis will get a Locator and\n    Formatter appropriate for the units (e.g. dates, or categorical).\n\nalign : {'center', 'edge'}, default: 'center'\n    Alignment of the bars to the *x* coordinates:\n\n    - 'center': Center the base on the *x* positions.\n    - 'edge': Align the left edges of the bars with the *x* positions.\n\n    To align the bars on the right edge pass a negative *width* and\n    ``align='edge'``.\n\nReturns\n-------\n`.BarContainer`\n    Container with all the bars and optionally errorbars.\n\nOther Parameters\n----------------\ncolor : :mpltype:`color` or list of :mpltype:`color`, optional\n    The colors of the bar faces. This is an alias for *facecolor*.\n    If both are given, *facecolor* takes precedence.\n\nfacecolor : :mpltype:`color` or list of :mpltype:`color`, optional\n    The colors of the bar faces.\n    If both *color* and *facecolor are given, *facecolor* takes precedence.\n\nedgecolor : :mpltype:`color` or list of :mpltype:`color`, optional\n    The colors of the bar edges.\n\nlinewidth : float or array-like, optional\n    Width of the bar edge(s). If 0, don't draw edges.\n\ntick_label : str or list of str, optional\n    The tick labels of the bars.\n    Default: None (Use default numeric labels.)\n\nlabel : str or list of str, optional\n    A single label is attached to the resulting `.BarContainer` as a\n    label for the whole dataset.\n    If a list is provided, it must be the same length as *x* and\n    labels the individual bars. Repeated labels are not de-duplicated\n    and will cause repeated label entries, so this is best used when\n    bars also differ in style (e.g., by passing a list to *color*.)\n\nxerr, yerr : float or array-like of shape(N,) or shape(2, N), optional\n    If not *None*, add horizontal / vertical errorbars to the bar tips.\n    The values are +/- sizes relative to the data:\n\n    - scalar: symmetric +/- values for all bars\n    - shape(N,): symmetric +/- values for each bar\n    - shape(2, N): Separate - and + values for each bar. First row\n      contains the lower errors, the second row contains the upper\n      errors.\n    - *None*: No errorbar. (Default)\n\n    See :doc:`/gallery/statistics/errorbar_features` for an example on\n    the usage of *xerr* and *yerr*.\n\necolor : :mpltype:`color` or list of :mpltype:`color`, default: 'black'\n    The line color of the errorbars.\n\ncapsize : float, default: :rc:`errorbar.capsize`\n   The length of the error bar caps in points.\n\nerror_kw : dict, optional\n    Dictionary of keyword arguments to be passed to the\n    `~.Axes.errorbar` method. Values of *ecolor* or *capsize* defined\n    here take precedence over the independent keyword arguments.\n\nlog : bool, default: False\n    If *True*, set the y-axis to be log scale.\n\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``.\n\n**kwargs : `.Rectangle` properties\n\nProperties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    angle: unknown\n    animated: bool\n    antialiased or aa: bool or None\n    bounds: (left, bottom, width, height)\n    capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color: :mpltype:`color`\n    edgecolor or ec: :mpltype:`color` or None\n    facecolor or fc: :mpltype:`color` or None\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fill: bool\n    gid: str\n    hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n    hatch_linewidth: unknown\n    height: unknown\n    in_layout: bool\n    joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float or None\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: None or bool or float or callable\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    visible: bool\n    width: unknown\n    x: unknown\n    xy: (float, float)\n    y: unknown\n    zorder: float\n\nSee Also\n--------\nbarh : Plot a horizontal bar plot.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.bar`.\n\nStacked bars can be achieved by passing individual *bottom* values per\nbar. See :doc:`/gallery/lines_bars_and_markers/bar_stacked`.",
    "matplotlib.pyplot.bar_label": "Label a bar plot.\n\nAdds labels to bars in the given `.BarContainer`.\nYou may need to adjust the axis limits to fit the labels.\n\nParameters\n----------\ncontainer : `.BarContainer`\n    Container with all the bars and optionally errorbars, likely\n    returned from `.bar` or `.barh`.\n\nlabels : array-like, optional\n    A list of label texts, that should be displayed. If not given, the\n    label texts will be the data values formatted with *fmt*.\n\nfmt : str or callable, default: '%g'\n    An unnamed %-style or {}-style format string for the label or a\n    function to call with the value as the first argument.\n    When *fmt* is a string and can be interpreted in both formats,\n    %-style takes precedence over {}-style.\n\n    .. versionadded:: 3.7\n       Support for {}-style format string and callables.\n\nlabel_type : {'edge', 'center'}, default: 'edge'\n    The label type. Possible values:\n\n    - 'edge': label placed at the end-point of the bar segment, and the\n      value displayed will be the position of that end-point.\n    - 'center': label placed in the center of the bar segment, and the\n      value displayed will be the length of that segment.\n      (useful for stacked bars, i.e.,\n      :doc:`/gallery/lines_bars_and_markers/bar_label_demo`)\n\npadding : float, default: 0\n    Distance of label from the end of the bar, in points.\n\n**kwargs\n    Any remaining keyword arguments are passed through to\n    `.Axes.annotate`. The alignment parameters (\n    *horizontalalignment* / *ha*, *verticalalignment* / *va*) are\n    not supported because the labels are automatically aligned to\n    the bars.\n\nReturns\n-------\nlist of `.Annotation`\n    A list of `.Annotation` instances for the labels.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.bar_label`.\n",
    "matplotlib.pyplot.barbs": "Plot a 2D field of wind barbs.\n\nCall signature::\n\n  barbs([X, Y], U, V, [C], /, **kwargs)\n\nWhere *X*, *Y* define the barb locations, *U*, *V* define the barb\ndirections, and *C* optionally sets the color.\n\nThe arguments *X*, *Y*, *U*, *V*, *C* are positional-only and may be\n1D or 2D. *U*, *V*, *C* may be masked arrays, but masked *X*, *Y*\nare not supported at present.\n\nBarbs are traditionally used in meteorology as a way to plot the speed\nand direction of wind observations, but can technically be used to\nplot any two dimensional vector quantity.  As opposed to arrows, which\ngive vector magnitude by the length of the arrow, the barbs give more\nquantitative information about the vector magnitude by putting slanted\nlines or a triangle for various increments in magnitude, as show\nschematically below::\n\n  :                   /\\    \\\n  :                  /  \\    \\\n  :                 /    \\    \\    \\\n  :                /      \\    \\    \\\n  :               ------------------------------\n\nThe largest increment is given by a triangle (or \"flag\"). After those\ncome full lines (barbs). The smallest increment is a half line.  There\nis only, of course, ever at most 1 half line.  If the magnitude is\nsmall and only needs a single half-line and no full lines or\ntriangles, the half-line is offset from the end of the barb so that it\ncan be easily distinguished from barbs with a single full line.  The\nmagnitude for the barb shown above would nominally be 65, using the\nstandard increments of 50, 10, and 5.\n\nSee also https://en.wikipedia.org/wiki/Wind_barb.\n\nParameters\n----------\nX, Y : 1D or 2D array-like, optional\n    The x and y coordinates of the barb locations. See *pivot* for how the\n    barbs are drawn to the x, y positions.\n\n    If not given, they will be generated as a uniform integer meshgrid based\n    on the dimensions of *U* and *V*.\n\n    If *X* and *Y* are 1D but *U*, *V* are 2D, *X*, *Y* are expanded to 2D\n    using ``X, Y = np.meshgrid(X, Y)``. In this case ``len(X)`` and ``len(Y)``\n    must match the column and row dimensions of *U* and *V*.\n\nU, V : 1D or 2D array-like\n    The x and y components of the barb shaft.\n\nC : 1D or 2D array-like, optional\n    Numeric data that defines the barb colors by colormapping via *norm* and\n    *cmap*.\n\n    This does not support explicit colors. If you want to set colors directly,\n    use *barbcolor* instead.\n\nlength : float, default: 7\n    Length of the barb in points; the other parts of the barb\n    are scaled against this.\n\npivot : {'tip', 'middle'} or float, default: 'tip'\n    The part of the arrow that is anchored to the *X*, *Y* grid. The barb\n    rotates about this point. This can also be a number, which shifts the\n    start of the barb that many points away from grid point.\n\nbarbcolor : :mpltype:`color` or color sequence\n    The color of all parts of the barb except for the flags.  This parameter\n    is analogous to the *edgecolor* parameter for polygons, which can be used\n    instead. However this parameter will override facecolor.\n\nflagcolor : :mpltype:`color` or color sequence\n    The color of any flags on the barb.  This parameter is analogous to the\n    *facecolor* parameter for polygons, which can be used instead. However,\n    this parameter will override facecolor.  If this is not set (and *C* has\n    not either) then *flagcolor* will be set to match *barbcolor* so that the\n    barb has a uniform color. If *C* has been set, *flagcolor* has no effect.\n\nsizes : dict, optional\n    A dictionary of coefficients specifying the ratio of a given\n    feature to the length of the barb. Only those values one wishes to\n    override need to be included.  These features include:\n\n    - 'spacing' - space between features (flags, full/half barbs)\n    - 'height' - height (distance from shaft to top) of a flag or full barb\n    - 'width' - width of a flag, twice the width of a full barb\n    - 'emptybarb' - radius of the circle used for low magnitudes\n\nfill_empty : bool, default: False\n    Whether the empty barbs (circles) that are drawn should be filled with\n    the flag color.  If they are not filled, the center is transparent.\n\nrounding : bool, default: True\n    Whether the vector magnitude should be rounded when allocating barb\n    components.  If True, the magnitude is rounded to the nearest multiple\n    of the half-barb increment.  If False, the magnitude is simply truncated\n    to the next lowest multiple.\n\nbarb_increments : dict, optional\n    A dictionary of increments specifying values to associate with\n    different parts of the barb. Only those values one wishes to\n    override need to be included.\n\n    - 'half' - half barbs (Default is 5)\n    - 'full' - full barbs (Default is 10)\n    - 'flag' - flags (default is 50)\n\nflip_barb : bool or array-like of bool, default: False\n    Whether the lines and flags should point opposite to normal.\n    Normal behavior is for the barbs and lines to point right (comes from wind\n    barbs having these features point towards low pressure in the Northern\n    Hemisphere).\n\n    A single value is applied to all barbs. Individual barbs can be flipped by\n    passing a bool array of the same size as *U* and *V*.\n\nReturns\n-------\nbarbs : `~matplotlib.quiver.Barbs`\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``.\n\n**kwargs\n    The barbs can further be customized using `.PolyCollection` keyword\n    arguments:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: array-like or scalar or None\n    animated: bool\n    antialiased or aa or antialiaseds: bool or list of bools\n    array: array-like or None\n    capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    clim: (vmin: float, vmax: float)\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    cmap: `.Colormap` or str or None\n    color: :mpltype:`color` or list of RGBA tuples\n    edgecolor or ec or edgecolors: :mpltype:`color` or list of :mpltype:`color` or 'face'\n    facecolor or facecolors or fc: :mpltype:`color` or list of :mpltype:`color`\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    gid: str\n    hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n    hatch_linewidth: unknown\n    in_layout: bool\n    joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    label: object\n    linestyle or dashes or linestyles or ls: str or tuple or list thereof\n    linewidth or linewidths or lw: float or list of floats\n    mouseover: bool\n    norm: `.Normalize` or str or None\n    offset_transform or transOffset: `.Transform`\n    offsets: (N, 2) or (2,) array-like\n    path_effects: list of `.AbstractPathEffect`\n    paths: list of array-like\n    picker: None or bool or float or callable\n    pickradius: float\n    rasterized: bool\n    sizes: `numpy.ndarray` or None\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    urls: list of str or None\n    verts: list of array-like\n    verts_and_codes: unknown\n    visible: bool\n    zorder: float\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.barbs`.\n",
    "matplotlib.pyplot.barh": "Make a horizontal bar plot.\n\nThe bars are positioned at *y* with the given *align*\\ment. Their\ndimensions are given by *width* and *height*. The horizontal baseline\nis *left* (default 0).\n\nMany parameters can take either a single value applying to all bars\nor a sequence of values, one for each bar.\n\nParameters\n----------\ny : float or array-like\n    The y coordinates of the bars. See also *align* for the\n    alignment of the bars to the coordinates.\n\nwidth : float or array-like\n    The width(s) of the bars.\n\n    Note that if *left* has units (e.g. datetime), *width* should be in\n    units that are a difference from the value of *left* (e.g. timedelta).\n\nheight : float or array-like, default: 0.8\n    The heights of the bars.\n\n    Note that if *y* has units (e.g. datetime), then *height* should be in\n    units that are a difference (e.g. timedelta) around the *y* values.\n\nleft : float or array-like, default: 0\n    The x coordinates of the left side(s) of the bars.\n\n    Note that if *left* has units, then the x-axis will get a Locator and\n    Formatter appropriate for the units (e.g. dates, or categorical).\n\nalign : {'center', 'edge'}, default: 'center'\n    Alignment of the base to the *y* coordinates*:\n\n    - 'center': Center the bars on the *y* positions.\n    - 'edge': Align the bottom edges of the bars with the *y*\n      positions.\n\n    To align the bars on the top edge pass a negative *height* and\n    ``align='edge'``.\n\nReturns\n-------\n`.BarContainer`\n    Container with all the bars and optionally errorbars.\n\nOther Parameters\n----------------\ncolor : :mpltype:`color` or list of :mpltype:`color`, optional\n    The colors of the bar faces.\n\nedgecolor : :mpltype:`color` or list of :mpltype:`color`, optional\n    The colors of the bar edges.\n\nlinewidth : float or array-like, optional\n    Width of the bar edge(s). If 0, don't draw edges.\n\ntick_label : str or list of str, optional\n    The tick labels of the bars.\n    Default: None (Use default numeric labels.)\n\nlabel : str or list of str, optional\n    A single label is attached to the resulting `.BarContainer` as a\n    label for the whole dataset.\n    If a list is provided, it must be the same length as *y* and\n    labels the individual bars. Repeated labels are not de-duplicated\n    and will cause repeated label entries, so this is best used when\n    bars also differ in style (e.g., by passing a list to *color*.)\n\nxerr, yerr : float or array-like of shape(N,) or shape(2, N), optional\n    If not *None*, add horizontal / vertical errorbars to the bar tips.\n    The values are +/- sizes relative to the data:\n\n    - scalar: symmetric +/- values for all bars\n    - shape(N,): symmetric +/- values for each bar\n    - shape(2, N): Separate - and + values for each bar. First row\n      contains the lower errors, the second row contains the upper\n      errors.\n    - *None*: No errorbar. (default)\n\n    See :doc:`/gallery/statistics/errorbar_features` for an example on\n    the usage of *xerr* and *yerr*.\n\necolor : :mpltype:`color` or list of :mpltype:`color`, default: 'black'\n    The line color of the errorbars.\n\ncapsize : float, default: :rc:`errorbar.capsize`\n   The length of the error bar caps in points.\n\nerror_kw : dict, optional\n    Dictionary of keyword arguments to be passed to the\n    `~.Axes.errorbar` method. Values of *ecolor* or *capsize* defined\n    here take precedence over the independent keyword arguments.\n\nlog : bool, default: False\n    If ``True``, set the x-axis to be log scale.\n\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if  ``s`` is a key in ``data``.\n\n**kwargs : `.Rectangle` properties\n\nProperties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    angle: unknown\n    animated: bool\n    antialiased or aa: bool or None\n    bounds: (left, bottom, width, height)\n    capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color: :mpltype:`color`\n    edgecolor or ec: :mpltype:`color` or None\n    facecolor or fc: :mpltype:`color` or None\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fill: bool\n    gid: str\n    hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n    hatch_linewidth: unknown\n    height: unknown\n    in_layout: bool\n    joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float or None\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: None or bool or float or callable\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    visible: bool\n    width: unknown\n    x: unknown\n    xy: (float, float)\n    y: unknown\n    zorder: float\n\nSee Also\n--------\nbar : Plot a vertical bar plot.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.barh`.\n\nStacked bars can be achieved by passing individual *left* values per\nbar. See\n:doc:`/gallery/lines_bars_and_markers/horizontal_barchart_distribution`.",
    "matplotlib.pyplot.bone": "\n    Set the colormap to 'bone'.\n\n    This changes the default colormap as well as the colormap of the current\n    image if there is one. See ``help(colormaps)`` for more information.\n    ",
    "matplotlib.pyplot.box": "\n    Turn the Axes box on or off on the current Axes.\n\n    Parameters\n    ----------\n    on : bool or None\n        The new `~matplotlib.axes.Axes` box state. If ``None``, toggle\n        the state.\n\n    See Also\n    --------\n    :meth:`matplotlib.axes.Axes.set_frame_on`\n    :meth:`matplotlib.axes.Axes.get_frame_on`\n    ",
    "matplotlib.pyplot.boxplot": "Draw a box and whisker plot.\n\nThe box extends from the first quartile (Q1) to the third\nquartile (Q3) of the data, with a line at the median.\nThe whiskers extend from the box to the farthest data point\nlying within 1.5x the inter-quartile range (IQR) from the box.\nFlier points are those past the end of the whiskers.\nSee https://en.wikipedia.org/wiki/Box_plot for reference.\n\n.. code-block:: none\n\n          Q1-1.5IQR   Q1   median  Q3   Q3+1.5IQR\n                       |-----:-----|\n       o      |--------|     :     |--------|    o  o\n                       |-----:-----|\n     flier             <----------->            fliers\n                            IQR\n\n\nParameters\n----------\nx : Array or a sequence of vectors.\n    The input data.  If a 2D array, a boxplot is drawn for each column\n    in *x*.  If a sequence of 1D arrays, a boxplot is drawn for each\n    array in *x*.\n\nnotch : bool, default: :rc:`boxplot.notch`\n    Whether to draw a notched boxplot (`True`), or a rectangular\n    boxplot (`False`).  The notches represent the confidence interval\n    (CI) around the median.  The documentation for *bootstrap*\n    describes how the locations of the notches are computed by\n    default, but their locations may also be overridden by setting the\n    *conf_intervals* parameter.\n\n    .. note::\n\n        In cases where the values of the CI are less than the\n        lower quartile or greater than the upper quartile, the\n        notches will extend beyond the box, giving it a\n        distinctive \"flipped\" appearance. This is expected\n        behavior and consistent with other statistical\n        visualization packages.\n\nsym : str, optional\n    The default symbol for flier points.  An empty string ('') hides\n    the fliers.  If `None`, then the fliers default to 'b+'.  More\n    control is provided by the *flierprops* parameter.\n\nvert : bool, optional\n    .. deprecated:: 3.11\n        Use *orientation* instead.\n\n        This is a pending deprecation for 3.10, with full deprecation\n        in 3.11 and removal in 3.13.\n        If this is given during the deprecation period, it overrides\n        the *orientation* parameter.\n\n    If True, plots the boxes vertically.\n    If False, plots the boxes horizontally.\n\norientation : {'vertical', 'horizontal'}, default: 'vertical'\n    If 'horizontal', plots the boxes horizontally.\n    Otherwise, plots the boxes vertically.\n\n    .. versionadded:: 3.10\n\nwhis : float or (float, float), default: 1.5\n    The position of the whiskers.\n\n    If a float, the lower whisker is at the lowest datum above\n    ``Q1 - whis*(Q3-Q1)``, and the upper whisker at the highest datum\n    below ``Q3 + whis*(Q3-Q1)``, where Q1 and Q3 are the first and\n    third quartiles.  The default value of ``whis = 1.5`` corresponds\n    to Tukey's original definition of boxplots.\n\n    If a pair of floats, they indicate the percentiles at which to\n    draw the whiskers (e.g., (5, 95)).  In particular, setting this to\n    (0, 100) results in whiskers covering the whole range of the data.\n\n    In the edge case where ``Q1 == Q3``, *whis* is automatically set\n    to (0, 100) (cover the whole range of the data) if *autorange* is\n    True.\n\n    Beyond the whiskers, data are considered outliers and are plotted\n    as individual points.\n\nbootstrap : int, optional\n    Specifies whether to bootstrap the confidence intervals\n    around the median for notched boxplots. If *bootstrap* is\n    None, no bootstrapping is performed, and notches are\n    calculated using a Gaussian-based asymptotic approximation\n    (see McGill, R., Tukey, J.W., and Larsen, W.A., 1978, and\n    Kendall and Stuart, 1967). Otherwise, bootstrap specifies\n    the number of times to bootstrap the median to determine its\n    95% confidence intervals. Values between 1000 and 10000 are\n    recommended.\n\nusermedians : 1D array-like, optional\n    A 1D array-like of length ``len(x)``.  Each entry that is not\n    `None` forces the value of the median for the corresponding\n    dataset.  For entries that are `None`, the medians are computed\n    by Matplotlib as normal.\n\nconf_intervals : array-like, optional\n    A 2D array-like of shape ``(len(x), 2)``.  Each entry that is not\n    None forces the location of the corresponding notch (which is\n    only drawn if *notch* is `True`).  For entries that are `None`,\n    the notches are computed by the method specified by the other\n    parameters (e.g., *bootstrap*).\n\npositions : array-like, optional\n    The positions of the boxes. The ticks and limits are\n    automatically set to match the positions. Defaults to\n    ``range(1, N+1)`` where N is the number of boxes to be drawn.\n\nwidths : float or array-like\n    The widths of the boxes.  The default is 0.5, or ``0.15*(distance\n    between extreme positions)``, if that is smaller.\n\npatch_artist : bool, default: :rc:`boxplot.patchartist`\n    If `False` produces boxes with the Line2D artist. Otherwise,\n    boxes are drawn with Patch artists.\n\ntick_labels : list of str, optional\n    The tick labels of each boxplot.\n    Ticks are always placed at the box *positions*. If *tick_labels* is given,\n    the ticks are labelled accordingly. Otherwise, they keep their numeric\n    values.\n\n    .. versionchanged:: 3.9\n        Renamed from *labels*, which is deprecated since 3.9\n        and will be removed in 3.11.\n\nmanage_ticks : bool, default: True\n    If True, the tick locations and labels will be adjusted to match\n    the boxplot positions.\n\nautorange : bool, default: False\n    When `True` and the data are distributed such that the 25th and\n    75th percentiles are equal, *whis* is set to (0, 100) such\n    that the whisker ends are at the minimum and maximum of the data.\n\nmeanline : bool, default: :rc:`boxplot.meanline`\n    If `True` (and *showmeans* is `True`), will try to render the\n    mean as a line spanning the full width of the box according to\n    *meanprops* (see below).  Not recommended if *shownotches* is also\n    True.  Otherwise, means will be shown as points.\n\nzorder : float, default: ``Line2D.zorder = 2``\n    The zorder of the boxplot.\n\nReturns\n-------\ndict\n  A dictionary mapping each component of the boxplot to a list\n  of the `.Line2D` instances created. That dictionary has the\n  following keys (assuming vertical boxplots):\n\n  - ``boxes``: the main body of the boxplot showing the\n    quartiles and the median's confidence intervals if\n    enabled.\n\n  - ``medians``: horizontal lines at the median of each box.\n\n  - ``whiskers``: the vertical lines extending to the most\n    extreme, non-outlier data points.\n\n  - ``caps``: the horizontal lines at the ends of the\n    whiskers.\n\n  - ``fliers``: points representing data that extend beyond\n    the whiskers (fliers).\n\n  - ``means``: points or lines representing the means.\n\nOther Parameters\n----------------\nshowcaps : bool, default: :rc:`boxplot.showcaps`\n    Show the caps on the ends of whiskers.\nshowbox : bool, default: :rc:`boxplot.showbox`\n    Show the central box.\nshowfliers : bool, default: :rc:`boxplot.showfliers`\n    Show the outliers beyond the caps.\nshowmeans : bool, default: :rc:`boxplot.showmeans`\n    Show the arithmetic means.\ncapprops : dict, default: None\n    The style of the caps.\ncapwidths : float or array, default: None\n    The widths of the caps.\nboxprops : dict, default: None\n    The style of the box.\nwhiskerprops : dict, default: None\n    The style of the whiskers.\nflierprops : dict, default: None\n    The style of the fliers.\nmedianprops : dict, default: None\n    The style of the median.\nmeanprops : dict, default: None\n    The style of the mean.\nlabel : str or list of str, optional\n    Legend labels. Use a single string when all boxes have the same style and\n    you only want a single legend entry for them. Use a list of strings to\n    label all boxes individually. To be distinguishable, the boxes should be\n    styled individually, which is currently only possible by modifying the\n    returned artists, see e.g. :doc:`/gallery/statistics/boxplot_demo`.\n\n    In the case of a single string, the legend entry will technically be\n    associated with the first box only. By default, the legend will show the\n    median line (``result[\"medians\"]``); if *patch_artist* is True, the legend\n    will show the box `.Patch` artists (``result[\"boxes\"]``) instead.\n\n    .. versionadded:: 3.9\n\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``.\n\nSee Also\n--------\n.Axes.bxp : Draw a boxplot from pre-computed statistics.\nviolinplot : Draw an estimate of the probability density function.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.boxplot`.\n",
    "matplotlib.pyplot.broken_barh": "Plot a horizontal sequence of rectangles.\n\nA rectangle is drawn for each element of *xranges*. All rectangles\nhave the same vertical position and size defined by *yrange*.\n\nParameters\n----------\nxranges : sequence of tuples (*xmin*, *xwidth*)\n    The x-positions and extents of the rectangles. For each tuple\n    (*xmin*, *xwidth*) a rectangle is drawn from *xmin* to *xmin* +\n    *xwidth*.\nyrange : (*ymin*, *yheight*)\n    The y-position and extent for all the rectangles.\n\nReturns\n-------\n`~.collections.PolyCollection`\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``.\n**kwargs : `.PolyCollection` properties\n\n    Each *kwarg* can be either a single argument applying to all\n    rectangles, e.g.::\n\n        facecolors='black'\n\n    or a sequence of arguments over which is cycled, e.g.::\n\n        facecolors=('black', 'blue')\n\n    would create interleaving black and blue rectangles.\n\n    Supported keywords:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: array-like or scalar or None\n    animated: bool\n    antialiased or aa or antialiaseds: bool or list of bools\n    array: array-like or None\n    capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    clim: (vmin: float, vmax: float)\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    cmap: `.Colormap` or str or None\n    color: :mpltype:`color` or list of RGBA tuples\n    edgecolor or ec or edgecolors: :mpltype:`color` or list of :mpltype:`color` or 'face'\n    facecolor or facecolors or fc: :mpltype:`color` or list of :mpltype:`color`\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    gid: str\n    hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n    hatch_linewidth: unknown\n    in_layout: bool\n    joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    label: object\n    linestyle or dashes or linestyles or ls: str or tuple or list thereof\n    linewidth or linewidths or lw: float or list of floats\n    mouseover: bool\n    norm: `.Normalize` or str or None\n    offset_transform or transOffset: `.Transform`\n    offsets: (N, 2) or (2,) array-like\n    path_effects: list of `.AbstractPathEffect`\n    paths: list of array-like\n    picker: None or bool or float or callable\n    pickradius: float\n    rasterized: bool\n    sizes: `numpy.ndarray` or None\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    urls: list of str or None\n    verts: list of array-like\n    verts_and_codes: unknown\n    visible: bool\n    zorder: float\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.broken_barh`.\n",
    "matplotlib.pyplot.cast": "Cast a value to a type.\n\n    This returns the value unchanged.  To the type checker this\n    signals that the return value has the designated type, but at\n    runtime we intentionally don't check anything (we want this\n    to be as fast as possible).\n    ",
    "matplotlib.pyplot.cla": "Clear the current Axes.",
    "matplotlib.pyplot.clabel": "Label a contour plot.\n\nAdds labels to line contours in given `.ContourSet`.\n\nParameters\n----------\nCS : `.ContourSet` instance\n    Line contours to label.\n\nlevels : array-like, optional\n    A list of level values, that should be labeled. The list must be\n    a subset of ``CS.levels``. If not given, all levels are labeled.\n\n**kwargs\n    All other parameters are documented in `~.ContourLabeler.clabel`.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.clabel`.\n",
    "matplotlib.pyplot.clf": "Clear the current figure.",
    "matplotlib.pyplot.clim": "\n    Set the color limits of the current image.\n\n    If either *vmin* or *vmax* is None, the image min/max respectively\n    will be used for color scaling.\n\n    If you want to set the clim of multiple images, use\n    `~.ScalarMappable.set_clim` on every image, for example::\n\n      for im in gca().get_images():\n          im.set_clim(0, 0.5)\n\n    ",
    "matplotlib.pyplot.close": "\n    Close a figure window.\n\n    Parameters\n    ----------\n    fig : None or int or str or `.Figure`\n        The figure to close. There are a number of ways to specify this:\n\n        - *None*: the current figure\n        - `.Figure`: the given `.Figure` instance\n        - ``int``: a figure number\n        - ``str``: a figure name\n        - 'all': all figures\n\n    ",
    "matplotlib.pyplot.cohere": "Plot the coherence between *x* and *y*.\n\nCoherence is the normalized cross spectral density:\n\n.. math::\n\n  C_{xy} = \\frac{|P_{xy}|^2}{P_{xx}P_{yy}}\n\nParameters\n----------\nFs : float, default: 2\n    The sampling frequency (samples per time unit).  It is used to calculate\n    the Fourier frequencies, *freqs*, in cycles per time unit.\n\nwindow : callable or ndarray, default: `.window_hanning`\n    A function or a vector of length *NFFT*.  To create window vectors see\n    `.window_hanning`, `.window_none`, `numpy.blackman`, `numpy.hamming`,\n    `numpy.bartlett`, `scipy.signal`, `scipy.signal.get_window`, etc.  If a\n    function is passed as the argument, it must take a data segment as an\n    argument and return the windowed version of the segment.\n\nsides : {'default', 'onesided', 'twosided'}, optional\n    Which sides of the spectrum to return. 'default' is one-sided for real\n    data and two-sided for complex data. 'onesided' forces the return of a\n    one-sided spectrum, while 'twosided' forces two-sided.\n\npad_to : int, optional\n    The number of points to which the data segment is padded when performing\n    the FFT.  This can be different from *NFFT*, which specifies the number\n    of data points used.  While not increasing the actual resolution of the\n    spectrum (the minimum distance between resolvable peaks), this can give\n    more points in the plot, allowing for more detail. This corresponds to\n    the *n* parameter in the call to `~numpy.fft.fft`. The default is None,\n    which sets *pad_to* equal to *NFFT*\n\nNFFT : int, default: 256\n    The number of data points used in each block for the FFT.  A power 2 is\n    most efficient.  This should *NOT* be used to get zero padding, or the\n    scaling of the result will be incorrect; use *pad_to* for this instead.\n\ndetrend : {'none', 'mean', 'linear'} or callable, default: 'none'\n    The function applied to each segment before fft-ing, designed to remove\n    the mean or linear trend.  Unlike in MATLAB, where the *detrend* parameter\n    is a vector, in Matplotlib it is a function.  The :mod:`~matplotlib.mlab`\n    module defines `.detrend_none`, `.detrend_mean`, and `.detrend_linear`,\n    but you can use a custom function as well.  You can also use a string to\n    choose one of the functions: 'none' calls `.detrend_none`. 'mean' calls\n    `.detrend_mean`. 'linear' calls `.detrend_linear`.\n\nscale_by_freq : bool, default: True\n    Whether the resulting density values should be scaled by the scaling\n    frequency, which gives density in units of 1/Hz.  This allows for\n    integration over the returned frequency values.  The default is True for\n    MATLAB compatibility.\n\nnoverlap : int, default: 0 (no overlap)\n    The number of points of overlap between blocks.\n\nFc : int, default: 0\n    The center frequency of *x*, which offsets the x extents of the\n    plot to reflect the frequency range used when a signal is acquired\n    and then filtered and downsampled to baseband.\n\nReturns\n-------\nCxy : 1-D array\n    The coherence vector.\n\nfreqs : 1-D array\n    The frequencies for the elements in *Cxy*.\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *x*, *y*\n\n**kwargs\n    Keyword arguments control the `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: :mpltype:`color`\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: :mpltype:`color` or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: :mpltype:`color`\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: :mpltype:`color`\n    markerfacecoloralt or mfcalt: :mpltype:`color`\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.cohere`.\n\nReferences\n----------\nBendat & Piersol -- Random Data: Analysis and Measurement Procedures,\nJohn Wiley & Sons (1986)",
    "matplotlib.pyplot.colorbar": "Add a colorbar to a plot.\n\nParameters\n----------\nmappable\n    The `matplotlib.cm.ScalarMappable` (i.e., `.AxesImage`,\n    `.ContourSet`, etc.) described by this colorbar.  This argument is\n    mandatory for the `.Figure.colorbar` method but optional for the\n    `.pyplot.colorbar` function, which sets the default to the current\n    image.\n\n    Note that one can create a `.ScalarMappable` \"on-the-fly\" to\n    generate colorbars not attached to a previously drawn artist, e.g.\n    ::\n\n        fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)\n\ncax : `~matplotlib.axes.Axes`, optional\n    Axes into which the colorbar will be drawn.  If `None`, then a new\n    Axes is created and the space for it will be stolen from the Axes(s)\n    specified in *ax*.\n\nax : `~matplotlib.axes.Axes` or iterable or `numpy.ndarray` of Axes, optional\n    The one or more parent Axes from which space for a new colorbar Axes\n    will be stolen. This parameter is only used if *cax* is not set.\n\n    Defaults to the Axes that contains the mappable used to create the\n    colorbar.\n\nuse_gridspec : bool, optional\n    If *cax* is ``None``, a new *cax* is created as an instance of\n    Axes.  If *ax* is positioned with a subplotspec and *use_gridspec*\n    is ``True``, then *cax* is also positioned with a subplotspec.\n\nReturns\n-------\ncolorbar : `~matplotlib.colorbar.Colorbar`\n\nOther Parameters\n----------------\n\nlocation : None or {'left', 'right', 'top', 'bottom'}\n    The location, relative to the parent Axes, where the colorbar Axes\n    is created.  It also determines the *orientation* of the colorbar\n    (colorbars on the left and right are vertical, colorbars at the top\n    and bottom are horizontal).  If None, the location will come from the\n    *orientation* if it is set (vertical colorbars on the right, horizontal\n    ones at the bottom), or default to 'right' if *orientation* is unset.\n\norientation : None or {'vertical', 'horizontal'}\n    The orientation of the colorbar.  It is preferable to set the *location*\n    of the colorbar, as that also determines the *orientation*; passing\n    incompatible values for *location* and *orientation* raises an exception.\n\nfraction : float, default: 0.15\n    Fraction of original Axes to use for colorbar.\n\nshrink : float, default: 1.0\n    Fraction by which to multiply the size of the colorbar.\n\naspect : float, default: 20\n    Ratio of long to short dimensions.\n\npad : float, default: 0.05 if vertical, 0.15 if horizontal\n    Fraction of original Axes between colorbar and new image Axes.\n\nanchor : (float, float), optional\n    The anchor point of the colorbar Axes.\n    Defaults to (0.0, 0.5) if vertical; (0.5, 1.0) if horizontal.\n\npanchor : (float, float), or *False*, optional\n    The anchor point of the colorbar parent Axes. If *False*, the parent\n    axes' anchor will be unchanged.\n    Defaults to (1.0, 0.5) if vertical; (0.5, 0.0) if horizontal.\n\nextend : {'neither', 'both', 'min', 'max'}\n    Make pointed end(s) for out-of-range values (unless 'neither').  These are\n    set for a given colormap using the colormap set_under and set_over methods.\n\nextendfrac : {*None*, 'auto', length, lengths}\n    If set to *None*, both the minimum and maximum triangular colorbar\n    extensions will have a length of 5% of the interior colorbar length (this\n    is the default setting).\n\n    If set to 'auto', makes the triangular colorbar extensions the same lengths\n    as the interior boxes (when *spacing* is set to 'uniform') or the same\n    lengths as the respective adjacent interior boxes (when *spacing* is set to\n    'proportional').\n\n    If a scalar, indicates the length of both the minimum and maximum\n    triangular colorbar extensions as a fraction of the interior colorbar\n    length.  A two-element sequence of fractions may also be given, indicating\n    the lengths of the minimum and maximum colorbar extensions respectively as\n    a fraction of the interior colorbar length.\n\nextendrect : bool\n    If *False* the minimum and maximum colorbar extensions will be triangular\n    (the default).  If *True* the extensions will be rectangular.\n\nticks : None or list of ticks or Locator\n    If None, ticks are determined automatically from the input.\n\nformat : None or str or Formatter\n    If None, `~.ticker.ScalarFormatter` is used.\n    Format strings, e.g., ``\"%4.2e\"`` or ``\"{x:.2e}\"``, are supported.\n    An alternative `~.ticker.Formatter` may be given instead.\n\ndrawedges : bool\n    Whether to draw lines at color boundaries.\n\nlabel : str\n    The label on the colorbar's long axis.\n\nboundaries, values : None or a sequence\n    If unset, the colormap will be displayed on a 0-1 scale.\n    If sequences, *values* must have a length 1 less than *boundaries*.  For\n    each region delimited by adjacent entries in *boundaries*, the color mapped\n    to the corresponding value in *values* will be used.  The size of each\n    region is determined by the *spacing* parameter.\n    Normally only useful for indexed colors (i.e. ``norm=NoNorm()``) or other\n    unusual circumstances.\n\nspacing : {'uniform', 'proportional'}\n    For discrete colorbars (`.BoundaryNorm` or contours), 'uniform' gives each\n    color the same space; 'proportional' makes the space proportional to the\n    data interval.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.Figure.colorbar`.\n\nIf *mappable* is a `~.contour.ContourSet`, its *extend* kwarg is\nincluded automatically.\n\nThe *shrink* kwarg provides a simple way to scale the colorbar with\nrespect to the Axes. Note that if *cax* is specified, it determines the\nsize of the colorbar, and *shrink* and *aspect* are ignored.\n\nFor more precise control, you can manually specify the positions of the\naxes objects in which the mappable and the colorbar are drawn.  In this\ncase, do not use any of the Axes properties kwargs.\n\nIt is known that some vector graphics viewers (svg and pdf) render\nwhite gaps between segments of the colorbar.  This is due to bugs in\nthe viewers, not Matplotlib.  As a workaround, the colorbar can be\nrendered with overlapping segments::\n\n    cbar = colorbar()\n    cbar.solids.set_edgecolor(\"face\")\n    draw()\n\nHowever, this has negative consequences in other circumstances, e.g.\nwith semi-transparent images (alpha < 1) and colorbar extensions;\ntherefore, this workaround is not used by default (see issue #1188).",
    "matplotlib.pyplot.colormaps": "\n    Container for colormaps that are known to Matplotlib by name.\n\n    The universal registry instance is `matplotlib.colormaps`. There should be\n    no need for users to instantiate `.ColormapRegistry` themselves.\n\n    Read access uses a dict-like interface mapping names to `.Colormap`\\s::\n\n        import matplotlib as mpl\n        cmap = mpl.colormaps['viridis']\n\n    Returned `.Colormap`\\s are copies, so that their modification does not\n    change the global definition of the colormap.\n\n    Additional colormaps can be added via `.ColormapRegistry.register`::\n\n        mpl.colormaps.register(my_colormap)\n\n    To get a list of all registered colormaps, you can do::\n\n        from matplotlib import colormaps\n        list(colormaps)\n    ",
    "matplotlib.pyplot.connect": "Bind function *func* to event *s*.\n\nParameters\n----------\ns : str\n    One of the following events ids:\n\n    - 'button_press_event'\n    - 'button_release_event'\n    - 'draw_event'\n    - 'key_press_event'\n    - 'key_release_event'\n    - 'motion_notify_event'\n    - 'pick_event'\n    - 'resize_event'\n    - 'scroll_event'\n    - 'figure_enter_event',\n    - 'figure_leave_event',\n    - 'axes_enter_event',\n    - 'axes_leave_event'\n    - 'close_event'.\n\nfunc : callable\n    The callback function to be executed, which must have the\n    signature::\n\n        def func(event: Event) -> Any\n\n    For the location events (button and key press/release), if the\n    mouse is over the Axes, the ``inaxes`` attribute of the event will\n    be set to the `~matplotlib.axes.Axes` the event occurs is over, and\n    additionally, the variables ``xdata`` and ``ydata`` attributes will\n    be set to the mouse location in data coordinates.  See `.KeyEvent`\n    and `.MouseEvent` for more info.\n\n    .. note::\n\n        If func is a method, this only stores a weak reference to the\n        method. Thus, the figure does not influence the lifetime of\n        the associated object. Usually, you want to make sure that the\n        object is kept alive throughout the lifetime of the figure by\n        holding a reference to it.\n\nReturns\n-------\ncid\n    A connection id that can be used with\n    `.FigureCanvasBase.mpl_disconnect`.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.FigureCanvasBase.mpl_connect`.\n\nExamples\n--------\n::\n\n    def on_press(event):\n        print('you pressed', event.button, event.xdata, event.ydata)\n\n    cid = canvas.mpl_connect('button_press_event', on_press)",
    "matplotlib.pyplot.contour": "Plot contour lines.\n\nCall signature::\n\n    contour([X, Y,] Z, /, [levels], **kwargs)\n\nThe arguments *X*, *Y*, *Z* are positional-only.\n\n`.contour` and `.contourf` draw contour lines and filled contours,\nrespectively.  Except as noted, function signatures and return values\nare the same for both versions.\n\nParameters\n----------\nX, Y : array-like, optional\n    The coordinates of the values in *Z*.\n\n    *X* and *Y* must both be 2D with the same shape as *Z* (e.g.\n    created via `numpy.meshgrid`), or they must both be 1-D such\n    that ``len(X) == N`` is the number of columns in *Z* and\n    ``len(Y) == M`` is the number of rows in *Z*.\n\n    *X* and *Y* must both be ordered monotonically.\n\n    If not given, they are assumed to be integer indices, i.e.\n    ``X = range(N)``, ``Y = range(M)``.\n\nZ : (M, N) array-like\n    The height values over which the contour is drawn.  Color-mapping is\n    controlled by *cmap*, *norm*, *vmin*, and *vmax*.\n\nlevels : int or array-like, optional\n    Determines the number and positions of the contour lines / regions.\n\n    If an int *n*, use `~matplotlib.ticker.MaxNLocator`, which tries\n    to automatically choose no more than *n+1* \"nice\" contour levels\n    between minimum and maximum numeric values of *Z*.\n\n    If array-like, draw contour lines at the specified levels.\n    The values must be in increasing order.\n\nReturns\n-------\n`~.contour.QuadContourSet`\n\nOther Parameters\n----------------\ncorner_mask : bool, default: :rc:`contour.corner_mask`\n    Enable/disable corner masking, which only has an effect if *Z* is\n    a masked array.  If ``False``, any quad touching a masked point is\n    masked out.  If ``True``, only the triangular corners of quads\n    nearest those points are always masked out, other triangular\n    corners comprising three unmasked points are contoured as usual.\n\ncolors : :mpltype:`color` or list of :mpltype:`color`, optional\n    The colors of the levels, i.e. the lines for `.contour` and the\n    areas for `.contourf`.\n\n    The sequence is cycled for the levels in ascending order. If the\n    sequence is shorter than the number of levels, it's repeated.\n\n    As a shortcut, a single color may be used in place of one-element lists, i.e.\n    ``'red'`` instead of ``['red']`` to color all levels with the same color.\n\n    .. versionchanged:: 3.10\n        Previously a single color had to be expressed as a string, but now any\n        valid color format may be passed.\n\n    By default (value *None*), the colormap specified by *cmap*\n    will be used.\n\nalpha : float, default: 1\n    The alpha blending value, between 0 (transparent) and 1 (opaque).\n\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    The Colormap instance or registered colormap name used to map scalar data\n    to colors.\n\n    This parameter is ignored if *colors* is set.\n\nnorm : str or `~matplotlib.colors.Normalize`, optional\n    The normalization method used to scale scalar data to the [0, 1] range\n    before mapping to colors using *cmap*. By default, a linear scaling is\n    used, mapping the lowest value to 0 and the highest to 1.\n\n    If given, this can be one of the following:\n\n    - An instance of `.Normalize` or one of its subclasses\n      (see :ref:`colormapnorms`).\n    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n      list of available scales, call `matplotlib.scale.get_scale_names()`.\n      In that case, a suitable `.Normalize` subclass is dynamically generated\n      and instantiated.\n\n    This parameter is ignored if *colors* is set.\n\nvmin, vmax : float, optional\n    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n    the data range that the colormap covers. By default, the colormap covers\n    the complete value range of the supplied data. It is an error to use\n    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n    name together with *vmin*/*vmax* is acceptable).\n\n    If *vmin* or *vmax* are not given, the default color scaling is based on\n    *levels*.\n\n    This parameter is ignored if *colors* is set.\n\ncolorizer : `~matplotlib.colorizer.Colorizer` or None, default: None\n    The Colorizer object used to map color to data. If None, a Colorizer\n    object is created from a *norm* and *cmap*.\n\n    This parameter is ignored if *colors* is set.\n\norigin : {*None*, 'upper', 'lower', 'image'}, default: None\n    Determines the orientation and exact position of *Z* by specifying\n    the position of ``Z[0, 0]``.  This is only relevant, if *X*, *Y*\n    are not given.\n\n    - *None*: ``Z[0, 0]`` is at X=0, Y=0 in the lower left corner.\n    - 'lower': ``Z[0, 0]`` is at X=0.5, Y=0.5 in the lower left corner.\n    - 'upper': ``Z[0, 0]`` is at X=N+0.5, Y=0.5 in the upper left\n      corner.\n    - 'image': Use the value from :rc:`image.origin`.\n\nextent : (x0, x1, y0, y1), optional\n    If *origin* is not *None*, then *extent* is interpreted as in\n    `.imshow`: it gives the outer pixel boundaries. In this case, the\n    position of Z[0, 0] is the center of the pixel, not a corner. If\n    *origin* is *None*, then (*x0*, *y0*) is the position of Z[0, 0],\n    and (*x1*, *y1*) is the position of Z[-1, -1].\n\n    This argument is ignored if *X* and *Y* are specified in the call\n    to contour.\n\nlocator : ticker.Locator subclass, optional\n    The locator is used to determine the contour levels if they\n    are not given explicitly via *levels*.\n    Defaults to `~.ticker.MaxNLocator`.\n\nextend : {'neither', 'both', 'min', 'max'}, default: 'neither'\n    Determines the ``contourf``-coloring of values that are outside the\n    *levels* range.\n\n    If 'neither', values outside the *levels* range are not colored.\n    If 'min', 'max' or 'both', color the values below, above or below\n    and above the *levels* range.\n\n    Values below ``min(levels)`` and above ``max(levels)`` are mapped\n    to the under/over values of the `.Colormap`. Note that most\n    colormaps do not have dedicated colors for these by default, so\n    that the over and under values are the edge values of the colormap.\n    You may want to set these values explicitly using\n    `.Colormap.set_under` and `.Colormap.set_over`.\n\n    .. note::\n\n        An existing `.QuadContourSet` does not get notified if\n        properties of its colormap are changed. Therefore, an explicit\n        call `~.ContourSet.changed()` is needed after modifying the\n        colormap. The explicit call can be left out, if a colorbar is\n        assigned to the `.QuadContourSet` because it internally calls\n        `~.ContourSet.changed()`.\n\n    Example::\n\n        x = np.arange(1, 10)\n        y = x.reshape(-1, 1)\n        h = x * y\n\n        cs = plt.contourf(h, levels=[10, 30, 50],\n            colors=['#808080', '#A0A0A0', '#C0C0C0'], extend='both')\n        cs.cmap.set_over('red')\n        cs.cmap.set_under('blue')\n        cs.changed()\n\nxunits, yunits : registered units, optional\n    Override axis units by specifying an instance of a\n    :class:`matplotlib.units.ConversionInterface`.\n\nantialiased : bool, optional\n    Enable antialiasing, overriding the defaults.  For\n    filled contours, the default is *False*.  For line contours,\n    it is taken from :rc:`lines.antialiased`.\n\nnchunk : int >= 0, optional\n    If 0, no subdivision of the domain.  Specify a positive integer to\n    divide the domain into subdomains of *nchunk* by *nchunk* quads.\n    Chunking reduces the maximum length of polygons generated by the\n    contouring algorithm which reduces the rendering workload passed\n    on to the backend and also requires slightly less RAM.  It can\n    however introduce rendering artifacts at chunk boundaries depending\n    on the backend, the *antialiased* flag and value of *alpha*.\n\nlinewidths : float or array-like, default: :rc:`contour.linewidth`\n    *Only applies to* `.contour`.\n\n    The line width of the contour lines.\n\n    If a number, all levels will be plotted with this linewidth.\n\n    If a sequence, the levels in ascending order will be plotted with\n    the linewidths in the order specified.\n\n    If None, this falls back to :rc:`lines.linewidth`.\n\nlinestyles : {*None*, 'solid', 'dashed', 'dashdot', 'dotted'}, optional\n    *Only applies to* `.contour`.\n\n    If *linestyles* is *None*, the default is 'solid' unless the lines are\n    monochrome. In that case, negative contours will instead take their\n    linestyle from the *negative_linestyles* argument.\n\n    *linestyles* can also be an iterable of the above strings specifying a set\n    of linestyles to be used. If this iterable is shorter than the number of\n    contour levels it will be repeated as necessary.\n\nnegative_linestyles : {*None*, 'solid', 'dashed', 'dashdot', 'dotted'},                        optional\n    *Only applies to* `.contour`.\n\n    If *linestyles* is *None* and the lines are monochrome, this argument\n    specifies the line style for negative contours.\n\n    If *negative_linestyles* is *None*, the default is taken from\n    :rc:`contour.negative_linestyle`.\n\n    *negative_linestyles* can also be an iterable of the above strings\n    specifying a set of linestyles to be used. If this iterable is shorter than\n    the number of contour levels it will be repeated as necessary.\n\nhatches : list[str], optional\n    *Only applies to* `.contourf`.\n\n    A list of cross hatch patterns to use on the filled areas.\n    If None, no hatching will be added to the contour.\n\nalgorithm : {'mpl2005', 'mpl2014', 'serial', 'threaded'}, optional\n    Which contouring algorithm to use to calculate the contour lines and\n    polygons. The algorithms are implemented in\n    `ContourPy <https://github.com/contourpy/contourpy>`_, consult the\n    `ContourPy documentation <https://contourpy.readthedocs.io>`_ for\n    further information.\n\n    The default is taken from :rc:`contour.algorithm`.\n\nclip_path : `~matplotlib.patches.Patch` or `.Path` or `.TransformedPath`\n    Set the clip path.  See `~matplotlib.artist.Artist.set_clip_path`.\n\n    .. versionadded:: 3.8\n\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.contour`.\n\n1. `.contourf` differs from the MATLAB version in that it does not draw\n   the polygon edges. To draw edges, add line contours with calls to\n   `.contour`.\n\n2. `.contourf` fills intervals that are closed at the top; that is, for\n   boundaries *z1* and *z2*, the filled region is::\n\n      z1 < Z <= z2\n\n   except for the lowest interval, which is closed on both sides (i.e.\n   it includes the lowest value).\n\n3. `.contour` and `.contourf` use a `marching squares\n   <https://en.wikipedia.org/wiki/Marching_squares>`_ algorithm to\n   compute contour locations.  More information can be found in\n   `ContourPy documentation <https://contourpy.readthedocs.io>`_.",
    "matplotlib.pyplot.contourf": "Plot filled contours.\n\nCall signature::\n\n    contourf([X, Y,] Z, /, [levels], **kwargs)\n\nThe arguments *X*, *Y*, *Z* are positional-only.\n\n`.contour` and `.contourf` draw contour lines and filled contours,\nrespectively.  Except as noted, function signatures and return values\nare the same for both versions.\n\nParameters\n----------\nX, Y : array-like, optional\n    The coordinates of the values in *Z*.\n\n    *X* and *Y* must both be 2D with the same shape as *Z* (e.g.\n    created via `numpy.meshgrid`), or they must both be 1-D such\n    that ``len(X) == N`` is the number of columns in *Z* and\n    ``len(Y) == M`` is the number of rows in *Z*.\n\n    *X* and *Y* must both be ordered monotonically.\n\n    If not given, they are assumed to be integer indices, i.e.\n    ``X = range(N)``, ``Y = range(M)``.\n\nZ : (M, N) array-like\n    The height values over which the contour is drawn.  Color-mapping is\n    controlled by *cmap*, *norm*, *vmin*, and *vmax*.\n\nlevels : int or array-like, optional\n    Determines the number and positions of the contour lines / regions.\n\n    If an int *n*, use `~matplotlib.ticker.MaxNLocator`, which tries\n    to automatically choose no more than *n+1* \"nice\" contour levels\n    between minimum and maximum numeric values of *Z*.\n\n    If array-like, draw contour lines at the specified levels.\n    The values must be in increasing order.\n\nReturns\n-------\n`~.contour.QuadContourSet`\n\nOther Parameters\n----------------\ncorner_mask : bool, default: :rc:`contour.corner_mask`\n    Enable/disable corner masking, which only has an effect if *Z* is\n    a masked array.  If ``False``, any quad touching a masked point is\n    masked out.  If ``True``, only the triangular corners of quads\n    nearest those points are always masked out, other triangular\n    corners comprising three unmasked points are contoured as usual.\n\ncolors : :mpltype:`color` or list of :mpltype:`color`, optional\n    The colors of the levels, i.e. the lines for `.contour` and the\n    areas for `.contourf`.\n\n    The sequence is cycled for the levels in ascending order. If the\n    sequence is shorter than the number of levels, it's repeated.\n\n    As a shortcut, a single color may be used in place of one-element lists, i.e.\n    ``'red'`` instead of ``['red']`` to color all levels with the same color.\n\n    .. versionchanged:: 3.10\n        Previously a single color had to be expressed as a string, but now any\n        valid color format may be passed.\n\n    By default (value *None*), the colormap specified by *cmap*\n    will be used.\n\nalpha : float, default: 1\n    The alpha blending value, between 0 (transparent) and 1 (opaque).\n\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    The Colormap instance or registered colormap name used to map scalar data\n    to colors.\n\n    This parameter is ignored if *colors* is set.\n\nnorm : str or `~matplotlib.colors.Normalize`, optional\n    The normalization method used to scale scalar data to the [0, 1] range\n    before mapping to colors using *cmap*. By default, a linear scaling is\n    used, mapping the lowest value to 0 and the highest to 1.\n\n    If given, this can be one of the following:\n\n    - An instance of `.Normalize` or one of its subclasses\n      (see :ref:`colormapnorms`).\n    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n      list of available scales, call `matplotlib.scale.get_scale_names()`.\n      In that case, a suitable `.Normalize` subclass is dynamically generated\n      and instantiated.\n\n    This parameter is ignored if *colors* is set.\n\nvmin, vmax : float, optional\n    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n    the data range that the colormap covers. By default, the colormap covers\n    the complete value range of the supplied data. It is an error to use\n    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n    name together with *vmin*/*vmax* is acceptable).\n\n    If *vmin* or *vmax* are not given, the default color scaling is based on\n    *levels*.\n\n    This parameter is ignored if *colors* is set.\n\ncolorizer : `~matplotlib.colorizer.Colorizer` or None, default: None\n    The Colorizer object used to map color to data. If None, a Colorizer\n    object is created from a *norm* and *cmap*.\n\n    This parameter is ignored if *colors* is set.\n\norigin : {*None*, 'upper', 'lower', 'image'}, default: None\n    Determines the orientation and exact position of *Z* by specifying\n    the position of ``Z[0, 0]``.  This is only relevant, if *X*, *Y*\n    are not given.\n\n    - *None*: ``Z[0, 0]`` is at X=0, Y=0 in the lower left corner.\n    - 'lower': ``Z[0, 0]`` is at X=0.5, Y=0.5 in the lower left corner.\n    - 'upper': ``Z[0, 0]`` is at X=N+0.5, Y=0.5 in the upper left\n      corner.\n    - 'image': Use the value from :rc:`image.origin`.\n\nextent : (x0, x1, y0, y1), optional\n    If *origin* is not *None*, then *extent* is interpreted as in\n    `.imshow`: it gives the outer pixel boundaries. In this case, the\n    position of Z[0, 0] is the center of the pixel, not a corner. If\n    *origin* is *None*, then (*x0*, *y0*) is the position of Z[0, 0],\n    and (*x1*, *y1*) is the position of Z[-1, -1].\n\n    This argument is ignored if *X* and *Y* are specified in the call\n    to contour.\n\nlocator : ticker.Locator subclass, optional\n    The locator is used to determine the contour levels if they\n    are not given explicitly via *levels*.\n    Defaults to `~.ticker.MaxNLocator`.\n\nextend : {'neither', 'both', 'min', 'max'}, default: 'neither'\n    Determines the ``contourf``-coloring of values that are outside the\n    *levels* range.\n\n    If 'neither', values outside the *levels* range are not colored.\n    If 'min', 'max' or 'both', color the values below, above or below\n    and above the *levels* range.\n\n    Values below ``min(levels)`` and above ``max(levels)`` are mapped\n    to the under/over values of the `.Colormap`. Note that most\n    colormaps do not have dedicated colors for these by default, so\n    that the over and under values are the edge values of the colormap.\n    You may want to set these values explicitly using\n    `.Colormap.set_under` and `.Colormap.set_over`.\n\n    .. note::\n\n        An existing `.QuadContourSet` does not get notified if\n        properties of its colormap are changed. Therefore, an explicit\n        call `~.ContourSet.changed()` is needed after modifying the\n        colormap. The explicit call can be left out, if a colorbar is\n        assigned to the `.QuadContourSet` because it internally calls\n        `~.ContourSet.changed()`.\n\n    Example::\n\n        x = np.arange(1, 10)\n        y = x.reshape(-1, 1)\n        h = x * y\n\n        cs = plt.contourf(h, levels=[10, 30, 50],\n            colors=['#808080', '#A0A0A0', '#C0C0C0'], extend='both')\n        cs.cmap.set_over('red')\n        cs.cmap.set_under('blue')\n        cs.changed()\n\nxunits, yunits : registered units, optional\n    Override axis units by specifying an instance of a\n    :class:`matplotlib.units.ConversionInterface`.\n\nantialiased : bool, optional\n    Enable antialiasing, overriding the defaults.  For\n    filled contours, the default is *False*.  For line contours,\n    it is taken from :rc:`lines.antialiased`.\n\nnchunk : int >= 0, optional\n    If 0, no subdivision of the domain.  Specify a positive integer to\n    divide the domain into subdomains of *nchunk* by *nchunk* quads.\n    Chunking reduces the maximum length of polygons generated by the\n    contouring algorithm which reduces the rendering workload passed\n    on to the backend and also requires slightly less RAM.  It can\n    however introduce rendering artifacts at chunk boundaries depending\n    on the backend, the *antialiased* flag and value of *alpha*.\n\nlinewidths : float or array-like, default: :rc:`contour.linewidth`\n    *Only applies to* `.contour`.\n\n    The line width of the contour lines.\n\n    If a number, all levels will be plotted with this linewidth.\n\n    If a sequence, the levels in ascending order will be plotted with\n    the linewidths in the order specified.\n\n    If None, this falls back to :rc:`lines.linewidth`.\n\nlinestyles : {*None*, 'solid', 'dashed', 'dashdot', 'dotted'}, optional\n    *Only applies to* `.contour`.\n\n    If *linestyles* is *None*, the default is 'solid' unless the lines are\n    monochrome. In that case, negative contours will instead take their\n    linestyle from the *negative_linestyles* argument.\n\n    *linestyles* can also be an iterable of the above strings specifying a set\n    of linestyles to be used. If this iterable is shorter than the number of\n    contour levels it will be repeated as necessary.\n\nnegative_linestyles : {*None*, 'solid', 'dashed', 'dashdot', 'dotted'},                        optional\n    *Only applies to* `.contour`.\n\n    If *linestyles* is *None* and the lines are monochrome, this argument\n    specifies the line style for negative contours.\n\n    If *negative_linestyles* is *None*, the default is taken from\n    :rc:`contour.negative_linestyle`.\n\n    *negative_linestyles* can also be an iterable of the above strings\n    specifying a set of linestyles to be used. If this iterable is shorter than\n    the number of contour levels it will be repeated as necessary.\n\nhatches : list[str], optional\n    *Only applies to* `.contourf`.\n\n    A list of cross hatch patterns to use on the filled areas.\n    If None, no hatching will be added to the contour.\n\nalgorithm : {'mpl2005', 'mpl2014', 'serial', 'threaded'}, optional\n    Which contouring algorithm to use to calculate the contour lines and\n    polygons. The algorithms are implemented in\n    `ContourPy <https://github.com/contourpy/contourpy>`_, consult the\n    `ContourPy documentation <https://contourpy.readthedocs.io>`_ for\n    further information.\n\n    The default is taken from :rc:`contour.algorithm`.\n\nclip_path : `~matplotlib.patches.Patch` or `.Path` or `.TransformedPath`\n    Set the clip path.  See `~matplotlib.artist.Artist.set_clip_path`.\n\n    .. versionadded:: 3.8\n\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.contourf`.\n\n1. `.contourf` differs from the MATLAB version in that it does not draw\n   the polygon edges. To draw edges, add line contours with calls to\n   `.contour`.\n\n2. `.contourf` fills intervals that are closed at the top; that is, for\n   boundaries *z1* and *z2*, the filled region is::\n\n      z1 < Z <= z2\n\n   except for the lowest interval, which is closed on both sides (i.e.\n   it includes the lowest value).\n\n3. `.contour` and `.contourf` use a `marching squares\n   <https://en.wikipedia.org/wiki/Marching_squares>`_ algorithm to\n   compute contour locations.  More information can be found in\n   `ContourPy documentation <https://contourpy.readthedocs.io>`_.",
    "matplotlib.pyplot.cool": "\n    Set the colormap to 'cool'.\n\n    This changes the default colormap as well as the colormap of the current\n    image if there is one. See ``help(colormaps)`` for more information.\n    ",
    "matplotlib.pyplot.copper": "\n    Set the colormap to 'copper'.\n\n    This changes the default colormap as well as the colormap of the current\n    image if there is one. See ``help(colormaps)`` for more information.\n    ",
    "matplotlib.pyplot.csd": "Plot the cross-spectral density.\n\nThe cross spectral density :math:`P_{xy}` by Welch's average\nperiodogram method.  The vectors *x* and *y* are divided into\n*NFFT* length segments.  Each segment is detrended by function\n*detrend* and windowed by function *window*.  *noverlap* gives\nthe length of the overlap between segments.  The product of\nthe direct FFTs of *x* and *y* are averaged over each segment\nto compute :math:`P_{xy}`, with a scaling to correct for power\nloss due to windowing.\n\nIf len(*x*) < *NFFT* or len(*y*) < *NFFT*, they will be zero\npadded to *NFFT*.\n\nParameters\n----------\nx, y : 1-D arrays or sequences\n    Arrays or sequences containing the data.\n\nFs : float, default: 2\n    The sampling frequency (samples per time unit).  It is used to calculate\n    the Fourier frequencies, *freqs*, in cycles per time unit.\n\nwindow : callable or ndarray, default: `.window_hanning`\n    A function or a vector of length *NFFT*.  To create window vectors see\n    `.window_hanning`, `.window_none`, `numpy.blackman`, `numpy.hamming`,\n    `numpy.bartlett`, `scipy.signal`, `scipy.signal.get_window`, etc.  If a\n    function is passed as the argument, it must take a data segment as an\n    argument and return the windowed version of the segment.\n\nsides : {'default', 'onesided', 'twosided'}, optional\n    Which sides of the spectrum to return. 'default' is one-sided for real\n    data and two-sided for complex data. 'onesided' forces the return of a\n    one-sided spectrum, while 'twosided' forces two-sided.\n\npad_to : int, optional\n    The number of points to which the data segment is padded when performing\n    the FFT.  This can be different from *NFFT*, which specifies the number\n    of data points used.  While not increasing the actual resolution of the\n    spectrum (the minimum distance between resolvable peaks), this can give\n    more points in the plot, allowing for more detail. This corresponds to\n    the *n* parameter in the call to `~numpy.fft.fft`. The default is None,\n    which sets *pad_to* equal to *NFFT*\n\nNFFT : int, default: 256\n    The number of data points used in each block for the FFT.  A power 2 is\n    most efficient.  This should *NOT* be used to get zero padding, or the\n    scaling of the result will be incorrect; use *pad_to* for this instead.\n\ndetrend : {'none', 'mean', 'linear'} or callable, default: 'none'\n    The function applied to each segment before fft-ing, designed to remove\n    the mean or linear trend.  Unlike in MATLAB, where the *detrend* parameter\n    is a vector, in Matplotlib it is a function.  The :mod:`~matplotlib.mlab`\n    module defines `.detrend_none`, `.detrend_mean`, and `.detrend_linear`,\n    but you can use a custom function as well.  You can also use a string to\n    choose one of the functions: 'none' calls `.detrend_none`. 'mean' calls\n    `.detrend_mean`. 'linear' calls `.detrend_linear`.\n\nscale_by_freq : bool, default: True\n    Whether the resulting density values should be scaled by the scaling\n    frequency, which gives density in units of 1/Hz.  This allows for\n    integration over the returned frequency values.  The default is True for\n    MATLAB compatibility.\n\nnoverlap : int, default: 0 (no overlap)\n    The number of points of overlap between segments.\n\nFc : int, default: 0\n    The center frequency of *x*, which offsets the x extents of the\n    plot to reflect the frequency range used when a signal is acquired\n    and then filtered and downsampled to baseband.\n\nreturn_line : bool, default: False\n    Whether to include the line object plotted in the returned values.\n\nReturns\n-------\nPxy : 1-D array\n    The values for the cross spectrum :math:`P_{xy}` before scaling\n    (complex valued).\n\nfreqs : 1-D array\n    The frequencies corresponding to the elements in *Pxy*.\n\nline : `~matplotlib.lines.Line2D`\n    The line created by this function.\n    Only returned if *return_line* is True.\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *x*, *y*\n\n**kwargs\n    Keyword arguments control the `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: :mpltype:`color`\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: :mpltype:`color` or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: :mpltype:`color`\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: :mpltype:`color`\n    markerfacecoloralt or mfcalt: :mpltype:`color`\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\npsd : is equivalent to setting ``y = x``.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.csd`.\n\nFor plotting, the power is plotted as\n:math:`10 \\log_{10}(P_{xy})` for decibels, though :math:`P_{xy}` itself\nis returned.\n\nReferences\n----------\nBendat & Piersol -- Random Data: Analysis and Measurement Procedures,\nJohn Wiley & Sons (1986)",
    "matplotlib.pyplot.cycler": "\n    Create a new `Cycler` object from a single positional argument,\n    a pair of positional arguments, or the combination of keyword arguments.\n\n    cycler(arg)\n    cycler(label1=itr1[, label2=iter2[, ...]])\n    cycler(label, itr)\n\n    Form 1 simply copies a given `Cycler` object.\n\n    Form 2 composes a `Cycler` as an inner product of the\n    pairs of keyword arguments. In other words, all of the\n    iterables are cycled simultaneously, as if through zip().\n\n    Form 3 creates a `Cycler` from a label and an iterable.\n    This is useful for when the label cannot be a keyword argument\n    (e.g., an integer or a name that has a space in it).\n\n    Parameters\n    ----------\n    arg : Cycler\n        Copy constructor for Cycler (does a shallow copy of iterables).\n    label : name\n        The property key. In the 2-arg form of the function,\n        the label can be any hashable object. In the keyword argument\n        form of the function, it must be a valid python identifier.\n    itr : iterable\n        Finite length iterable of the property values.\n        Can be a single-property `Cycler` that would\n        be like a key change, but as a shallow copy.\n\n    Returns\n    -------\n    cycler : Cycler\n        New `Cycler` for the given property\n\n    ",
    "matplotlib.pyplot.delaxes": "\n    Remove an `~.axes.Axes` (defaulting to the current Axes) from its figure.\n    ",
    "matplotlib.pyplot.disconnect": "Disconnect the callback with id *cid*.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.FigureCanvasBase.mpl_disconnect`.\n\nExamples\n--------\n::\n\n    cid = canvas.mpl_connect('button_press_event', on_press)\n    # ... later\n    canvas.mpl_disconnect(cid)",
    "matplotlib.pyplot.draw": "\n    Redraw the current figure.\n\n    This is used to update a figure that has been altered, but not\n    automatically re-drawn.  If interactive mode is on (via `.ion()`), this\n    should be only rarely needed, but there may be ways to modify the state of\n    a figure without marking it as \"stale\".  Please report these cases as bugs.\n\n    This is equivalent to calling ``fig.canvas.draw_idle()``, where ``fig`` is\n    the current figure.\n\n    See Also\n    --------\n    .FigureCanvasBase.draw_idle\n    .FigureCanvasBase.draw\n    ",
    "matplotlib.pyplot.draw_all": "\n        Redraw all stale managed figures, or, if *force* is True, all managed\n        figures.\n        ",
    "matplotlib.pyplot.draw_if_interactive": "\n    Redraw the current figure if in interactive mode.\n\n    .. warning::\n\n        End users will typically not have to call this function because the\n        the interactive mode takes care of this.\n    ",
    "matplotlib.pyplot.ecdf": "Compute and plot the empirical cumulative distribution function of *x*.\n\n.. versionadded:: 3.8\n\nParameters\n----------\nx : 1d array-like\n    The input data.  Infinite entries are kept (and move the relevant\n    end of the ecdf from 0/1), but NaNs and masked values are errors.\n\nweights : 1d array-like or None, default: None\n    The weights of the entries; must have the same shape as *x*.\n    Weights corresponding to NaN data points are dropped, and then the\n    remaining weights are normalized to sum to 1.  If unset, all\n    entries have the same weight.\n\ncomplementary : bool, default: False\n    Whether to plot a cumulative distribution function, which increases\n    from 0 to 1 (the default), or a complementary cumulative\n    distribution function, which decreases from 1 to 0.\n\norientation : {\"vertical\", \"horizontal\"}, default: \"vertical\"\n    Whether the entries are plotted along the x-axis (\"vertical\", the\n    default) or the y-axis (\"horizontal\").  This parameter takes the\n    same values as in `~.Axes.hist`.\n\ncompress : bool, default: False\n    Whether multiple entries with the same values are grouped together\n    (with a summed weight) before plotting.  This is mainly useful if\n    *x* contains many identical data points, to decrease the rendering\n    complexity of the plot. If *x* contains no duplicate points, this\n    has no effect and just uses some time and memory.\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *x*, *weights*\n\n**kwargs\n    Keyword arguments control the `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: :mpltype:`color`\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: :mpltype:`color` or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: :mpltype:`color`\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: :mpltype:`color`\n    markerfacecoloralt or mfcalt: :mpltype:`color`\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nReturns\n-------\n`.Line2D`\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.ecdf`.\n\nThe ecdf plot can be thought of as a cumulative histogram with one bin\nper data entry; i.e. it reports on the entire dataset without any\narbitrary binning.\n\nIf *x* contains NaNs or masked entries, either remove them first from\nthe array (if they should not taken into account), or replace them by\n-inf or +inf (if they should be sorted at the beginning or the end of\nthe array).",
    "matplotlib.pyplot.errorbar": "Plot y versus x as lines and/or markers with attached errorbars.\n\n*x*, *y* define the data locations, *xerr*, *yerr* define the errorbar\nsizes. By default, this draws the data markers/lines as well as the\nerrorbars. Use fmt='none' to draw errorbars without any data markers.\n\n.. versionadded:: 3.7\n   Caps and error lines are drawn in polar coordinates on polar plots.\n\n\nParameters\n----------\nx, y : float or array-like\n    The data positions.\n\nxerr, yerr : float or array-like, shape(N,) or shape(2, N), optional\n    The errorbar sizes:\n\n    - scalar: Symmetric +/- values for all data points.\n    - shape(N,): Symmetric +/-values for each data point.\n    - shape(2, N): Separate - and + values for each bar. First row\n      contains the lower errors, the second row contains the upper\n      errors.\n    - *None*: No errorbar.\n\n    All values must be >= 0.\n\n    See :doc:`/gallery/statistics/errorbar_features`\n    for an example on the usage of ``xerr`` and ``yerr``.\n\nfmt : str, default: ''\n    The format for the data points / data lines. See `.plot` for\n    details.\n\n    Use 'none' (case-insensitive) to plot errorbars without any data\n    markers.\n\necolor : :mpltype:`color`, default: None\n    The color of the errorbar lines.  If None, use the color of the\n    line connecting the markers.\n\nelinewidth : float, default: None\n    The linewidth of the errorbar lines. If None, the linewidth of\n    the current style is used.\n\ncapsize : float, default: :rc:`errorbar.capsize`\n    The length of the error bar caps in points.\n\ncapthick : float, default: None\n    An alias to the keyword argument *markeredgewidth* (a.k.a. *mew*).\n    This setting is a more sensible name for the property that\n    controls the thickness of the error bar cap in points. For\n    backwards compatibility, if *mew* or *markeredgewidth* are given,\n    then they will over-ride *capthick*. This may change in future\n    releases.\n\nbarsabove : bool, default: False\n    If True, will plot the errorbars above the plot\n    symbols. Default is below.\n\nlolims, uplims, xlolims, xuplims : bool or array-like, default: False\n    These arguments can be used to indicate that a value gives only\n    upper/lower limits.  In that case a caret symbol is used to\n    indicate this. *lims*-arguments may be scalars, or array-likes of\n    the same length as *xerr* and *yerr*.  To use limits with inverted\n    axes, `~.Axes.set_xlim` or `~.Axes.set_ylim` must be called before\n    :meth:`errorbar`.  Note the tricky parameter names: setting e.g.\n    *lolims* to True means that the y-value is a *lower* limit of the\n    True value, so, only an *upward*-pointing arrow will be drawn!\n\nerrorevery : int or (int, int), default: 1\n    draws error bars on a subset of the data. *errorevery* =N draws\n    error bars on the points (x[::N], y[::N]).\n    *errorevery* =(start, N) draws error bars on the points\n    (x[start::N], y[start::N]). e.g. errorevery=(6, 3)\n    adds error bars to the data at (x[6], x[9], x[12], x[15], ...).\n    Used to avoid overlapping error bars when two series share x-axis\n    values.\n\nReturns\n-------\n`.ErrorbarContainer`\n    The container contains:\n\n    - data_line : A `~matplotlib.lines.Line2D` instance of x, y plot markers\n      and/or line.\n    - caplines : A tuple of `~matplotlib.lines.Line2D` instances of the error\n      bar caps.\n    - barlinecols : A tuple of `.LineCollection` with the horizontal and\n      vertical error ranges.\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *x*, *y*, *xerr*, *yerr*\n\n**kwargs\n    All other keyword arguments are passed on to the `~.Axes.plot` call\n    drawing the markers. For example, this code makes big red squares\n    with thick green edges::\n\n        x, y, yerr = rand(3, 10)\n        errorbar(x, y, yerr, marker='s', mfc='red',\n                 mec='green', ms=20, mew=4)\n\n    where *mfc*, *mec*, *ms* and *mew* are aliases for the longer\n    property names, *markerfacecolor*, *markeredgecolor*, *markersize*\n    and *markeredgewidth*.\n\n    Valid kwargs for the marker properties are:\n\n    - *dashes*\n    - *dash_capstyle*\n    - *dash_joinstyle*\n    - *drawstyle*\n    - *fillstyle*\n    - *linestyle*\n    - *marker*\n    - *markeredgecolor*\n    - *markeredgewidth*\n    - *markerfacecolor*\n    - *markerfacecoloralt*\n    - *markersize*\n    - *markevery*\n    - *solid_capstyle*\n    - *solid_joinstyle*\n\n    Refer to the corresponding `.Line2D` property for more details:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: :mpltype:`color`\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: :mpltype:`color` or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: :mpltype:`color`\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: :mpltype:`color`\n    markerfacecoloralt or mfcalt: :mpltype:`color`\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.errorbar`.\n",
    "matplotlib.pyplot.eventplot": "Plot identical parallel lines at the given positions.\n\nThis type of plot is commonly used in neuroscience for representing\nneural events, where it is usually called a spike raster, dot raster,\nor raster plot.\n\nHowever, it is useful in any situation where you wish to show the\ntiming or position of multiple sets of discrete events, such as the\narrival times of people to a business on each day of the month or the\ndate of hurricanes each year of the last century.\n\nParameters\n----------\npositions : array-like or list of array-like\n    A 1D array-like defines the positions of one sequence of events.\n\n    Multiple groups of events may be passed as a list of array-likes.\n    Each group can be styled independently by passing lists of values\n    to *lineoffsets*, *linelengths*, *linewidths*, *colors* and\n    *linestyles*.\n\n    Note that *positions* can be a 2D array, but in practice different\n    event groups usually have different counts so that one will use a\n    list of different-length arrays rather than a 2D array.\n\norientation : {'horizontal', 'vertical'}, default: 'horizontal'\n    The direction of the event sequence:\n\n    - 'horizontal': the events are arranged horizontally.\n      The indicator lines are vertical.\n    - 'vertical': the events are arranged vertically.\n      The indicator lines are horizontal.\n\nlineoffsets : float or array-like, default: 1\n    The offset of the center of the lines from the origin, in the\n    direction orthogonal to *orientation*.\n\n    If *positions* is 2D, this can be a sequence with length matching\n    the length of *positions*.\n\nlinelengths : float or array-like, default: 1\n    The total height of the lines (i.e. the lines stretches from\n    ``lineoffset - linelength/2`` to ``lineoffset + linelength/2``).\n\n    If *positions* is 2D, this can be a sequence with length matching\n    the length of *positions*.\n\nlinewidths : float or array-like, default: :rc:`lines.linewidth`\n    The line width(s) of the event lines, in points.\n\n    If *positions* is 2D, this can be a sequence with length matching\n    the length of *positions*.\n\ncolors : :mpltype:`color` or list of color, default: :rc:`lines.color`\n    The color(s) of the event lines.\n\n    If *positions* is 2D, this can be a sequence with length matching\n    the length of *positions*.\n\nalpha : float or array-like, default: 1\n    The alpha blending value(s), between 0 (transparent) and 1\n    (opaque).\n\n    If *positions* is 2D, this can be a sequence with length matching\n    the length of *positions*.\n\nlinestyles : str or tuple or list of such values, default: 'solid'\n    Default is 'solid'. Valid strings are ['solid', 'dashed',\n    'dashdot', 'dotted', '-', '--', '-.', ':']. Dash tuples\n    should be of the form::\n\n        (offset, onoffseq),\n\n    where *onoffseq* is an even length tuple of on and off ink\n    in points.\n\n    If *positions* is 2D, this can be a sequence with length matching\n    the length of *positions*.\n\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *positions*, *lineoffsets*, *linelengths*, *linewidths*, *colors*, *linestyles*\n\n**kwargs\n    Other keyword arguments are line collection properties.  See\n    `.LineCollection` for a list of the valid properties.\n\nReturns\n-------\nlist of `.EventCollection`\n    The `.EventCollection` that were added.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.eventplot`.\n\nFor *linelengths*, *linewidths*, *colors*, *alpha* and *linestyles*, if\nonly a single value is given, that value is applied to all lines. If an\narray-like is given, it must have the same length as *positions*, and\neach value will be applied to the corresponding row of the array.\n\nExamples\n--------\n.. plot:: gallery/lines_bars_and_markers/eventplot_demo.py",
    "matplotlib.pyplot.figaspect": "\n    Calculate the width and height for a figure with a specified aspect ratio.\n\n    While the height is taken from :rc:`figure.figsize`, the width is\n    adjusted to match the desired aspect ratio. Additionally, it is ensured\n    that the width is in the range [4., 16.] and the height is in the range\n    [2., 16.]. If necessary, the default height is adjusted to ensure this.\n\n    Parameters\n    ----------\n    arg : float or 2D array\n        If a float, this defines the aspect ratio (i.e. the ratio height /\n        width).\n        In case of an array the aspect ratio is number of rows / number of\n        columns, so that the array could be fitted in the figure undistorted.\n\n    Returns\n    -------\n    width, height : float\n        The figure size in inches.\n\n    Notes\n    -----\n    If you want to create an Axes within the figure, that still preserves the\n    aspect ratio, be sure to create it with equal width and height. See\n    examples below.\n\n    Thanks to Fernando Perez for this function.\n\n    Examples\n    --------\n    Make a figure twice as tall as it is wide::\n\n        w, h = figaspect(2.)\n        fig = Figure(figsize=(w, h))\n        ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n        ax.imshow(A, **kwargs)\n\n    Make a figure with the proper aspect for an array::\n\n        A = rand(5, 3)\n        w, h = figaspect(A)\n        fig = Figure(figsize=(w, h))\n        ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n        ax.imshow(A, **kwargs)\n    ",
    "matplotlib.pyplot.figimage": "Add a non-resampled image to the figure.\n\nThe image is attached to the lower or upper left corner depending on\n*origin*.\n\nParameters\n----------\nX\n    The image data. This is an array of one of the following shapes:\n\n    - (M, N): an image with scalar data.  Color-mapping is controlled\n      by *cmap*, *norm*, *vmin*, and *vmax*.\n    - (M, N, 3): an image with RGB values (0-1 float or 0-255 int).\n    - (M, N, 4): an image with RGBA values (0-1 float or 0-255 int),\n      i.e. including transparency.\n\nxo, yo : int\n    The *x*/*y* image offset in pixels.\n\nalpha : None or float\n    The alpha blending value.\n\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    The Colormap instance or registered colormap name used to map scalar data\n    to colors.\n\n    This parameter is ignored if *X* is RGB(A).\n\nnorm : str or `~matplotlib.colors.Normalize`, optional\n    The normalization method used to scale scalar data to the [0, 1] range\n    before mapping to colors using *cmap*. By default, a linear scaling is\n    used, mapping the lowest value to 0 and the highest to 1.\n\n    If given, this can be one of the following:\n\n    - An instance of `.Normalize` or one of its subclasses\n      (see :ref:`colormapnorms`).\n    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n      list of available scales, call `matplotlib.scale.get_scale_names()`.\n      In that case, a suitable `.Normalize` subclass is dynamically generated\n      and instantiated.\n\n    This parameter is ignored if *X* is RGB(A).\n\nvmin, vmax : float, optional\n    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n    the data range that the colormap covers. By default, the colormap covers\n    the complete value range of the supplied data. It is an error to use\n    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n    name together with *vmin*/*vmax* is acceptable).\n\n    This parameter is ignored if *X* is RGB(A).\n\norigin : {'upper', 'lower'}, default: :rc:`image.origin`\n    Indicates where the [0, 0] index of the array is in the upper left\n    or lower left corner of the Axes.\n\nresize : bool\n    If *True*, resize the figure to match the given image size.\n\ncolorizer : `~matplotlib.colorizer.Colorizer` or None, default: None\n    The Colorizer object used to map color to data. If None, a Colorizer\n    object is created from a *norm* and *cmap*.\n\n    This parameter is ignored if *X* is RGB(A).\n\nReturns\n-------\n`matplotlib.image.FigureImage`\n\nOther Parameters\n----------------\n**kwargs\n    Additional kwargs are `.Artist` kwargs passed on to `.FigureImage`.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.Figure.figimage`.\n\nfigimage complements the Axes image (`~matplotlib.axes.Axes.imshow`)\nwhich will be resampled to fit the current Axes.  If you want\na resampled image to fill the entire figure, you can define an\n`~matplotlib.axes.Axes` with extent [0, 0, 1, 1].\n\nExamples\n--------\n::\n\n    f = plt.figure()\n    nx = int(f.get_figwidth() * f.dpi)\n    ny = int(f.get_figheight() * f.dpi)\n    data = np.random.random((ny, nx))\n    f.figimage(data)\n    plt.show()",
    "matplotlib.pyplot.figlegend": "Place a legend on the figure.\n\nCall signatures::\n\n    figlegend()\n    figlegend(handles, labels)\n    figlegend(handles=handles)\n    figlegend(labels)\n\nThe call signatures correspond to the following different ways to use\nthis method:\n\n**1. Automatic detection of elements to be shown in the legend**\n\nThe elements to be added to the legend are automatically determined,\nwhen you do not pass in any extra arguments.\n\nIn this case, the labels are taken from the artist. You can specify\nthem either at artist creation or by calling the\n:meth:`~.Artist.set_label` method on the artist::\n\n    plt.plot([1, 2, 3], label='Inline label')\n    plt.figlegend()\n\nor::\n\n    line, = plt.plot([1, 2, 3])\n    line.set_label('Label via method')\n    plt.figlegend()\n\nSpecific lines can be excluded from the automatic legend element\nselection by defining a label starting with an underscore.\nThis is default for all artists, so calling `.Figure.legend` without\nany arguments and without setting the labels manually will result in\nno legend being drawn.\n\n\n**2. Explicitly listing the artists and labels in the legend**\n\nFor full control of which artists have a legend entry, it is possible\nto pass an iterable of legend artists followed by an iterable of\nlegend labels respectively::\n\n    plt.figlegend([line1, line2, line3], ['label1', 'label2', 'label3'])\n\n\n**3. Explicitly listing the artists in the legend**\n\nThis is similar to 2, but the labels are taken from the artists'\nlabel properties. Example::\n\n    line1, = ax1.plot([1, 2, 3], label='label1')\n    line2, = ax2.plot([1, 2, 3], label='label2')\n    plt.figlegend(handles=[line1, line2])\n\n\n**4. Labeling existing plot elements**\n\n.. admonition:: Discouraged\n\n    This call signature is discouraged, because the relation between\n    plot elements and labels is only implicit by their order and can\n    easily be mixed up.\n\nTo make a legend for all artists on all Axes, call this function with\nan iterable of strings, one for each legend item. For example::\n\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    ax1.plot([1, 3, 5], color='blue')\n    ax2.plot([2, 4, 6], color='red')\n    plt.figlegend(['the blues', 'the reds'])\n\n\nParameters\n----------\nhandles : list of `.Artist`, optional\n    A list of Artists (lines, patches) to be added to the legend.\n    Use this together with *labels*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\n    The length of handles and labels should be the same in this\n    case. If they are not, they are truncated to the smaller length.\n\nlabels : list of str, optional\n    A list of labels to show next to the artists.\n    Use this together with *handles*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\nReturns\n-------\n`~matplotlib.legend.Legend`\n\nOther Parameters\n----------------\n\nloc : str or pair of floats, default: 'upper right'\n    The location of the legend.\n\n    The strings ``'upper left'``, ``'upper right'``, ``'lower left'``,\n    ``'lower right'`` place the legend at the corresponding corner of the\n    figure.\n\n    The strings ``'upper center'``, ``'lower center'``, ``'center left'``,\n    ``'center right'`` place the legend at the center of the corresponding edge\n    of the figure.\n\n    The string ``'center'`` places the legend at the center of the figure.\n\n    The location can also be a 2-tuple giving the coordinates of the lower-left\n    corner of the legend in figure coordinates (in which case *bbox_to_anchor*\n    will be ignored).\n\n    For back-compatibility, ``'center right'`` (but no other location) can also\n    be spelled ``'right'``, and each \"string\" location can also be given as a\n    numeric value:\n\n    ==================   =============\n    Location String      Location Code\n    ==================   =============\n    'best' (Axes only)   0\n    'upper right'        1\n    'upper left'         2\n    'lower left'         3\n    'lower right'        4\n    'right'              5\n    'center left'        6\n    'center right'       7\n    'lower center'       8\n    'upper center'       9\n    'center'             10\n    ==================   =============\n    \n    If a figure is using the constrained layout manager, the string codes\n    of the *loc* keyword argument can get better layout behaviour using the\n    prefix 'outside'. There is ambiguity at the corners, so 'outside\n    upper right' will make space for the legend above the rest of the\n    axes in the layout, and 'outside right upper' will make space on the\n    right side of the layout.  In addition to the values of *loc*\n    listed above, we have 'outside right upper', 'outside right lower',\n    'outside left upper', and 'outside left lower'.  See\n    :ref:`legend_guide` for more details.\n\nbbox_to_anchor : `.BboxBase`, 2-tuple, or 4-tuple of floats\n    Box that is used to position the legend in conjunction with *loc*.\n    Defaults to ``axes.bbox`` (if called as a method to `.Axes.legend`) or\n    ``figure.bbox`` (if ``figure.legend``).  This argument allows arbitrary\n    placement of the legend.\n\n    Bbox coordinates are interpreted in the coordinate system given by\n    *bbox_transform*, with the default transform\n    Axes or Figure coordinates, depending on which ``legend`` is called.\n\n    If a 4-tuple or `.BboxBase` is given, then it specifies the bbox\n    ``(x, y, width, height)`` that the legend is placed in.\n    To put the legend in the best location in the bottom right\n    quadrant of the Axes (or figure)::\n\n        loc='best', bbox_to_anchor=(0.5, 0., 0.5, 0.5)\n\n    A 2-tuple ``(x, y)`` places the corner of the legend specified by *loc* at\n    x, y.  For example, to put the legend's upper right-hand corner in the\n    center of the Axes (or figure) the following keywords can be used::\n\n        loc='upper right', bbox_to_anchor=(0.5, 0.5)\n\nncols : int, default: 1\n    The number of columns that the legend has.\n\n    For backward compatibility, the spelling *ncol* is also supported\n    but it is discouraged. If both are given, *ncols* takes precedence.\n\nprop : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend. If None (default), the current\n    :data:`matplotlib.rcParams` will be used.\n\nfontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n    The font size of the legend. If the value is numeric the size will be the\n    absolute font size in points. String values are relative to the current\n    default font size. This argument is only used if *prop* is not specified.\n\nlabelcolor : str or list, default: :rc:`legend.labelcolor`\n    The color of the text in the legend. Either a valid color string\n    (for example, 'red'), or a list of color strings. The labelcolor can\n    also be made to match the color of the line or marker using 'linecolor',\n    'markerfacecolor' (or 'mfc'), or 'markeredgecolor' (or 'mec').\n\n    Labelcolor can be set globally using :rc:`legend.labelcolor`. If None,\n    use :rc:`text.color`.\n\nnumpoints : int, default: :rc:`legend.numpoints`\n    The number of marker points in the legend when creating a legend\n    entry for a `.Line2D` (line).\n\nscatterpoints : int, default: :rc:`legend.scatterpoints`\n    The number of marker points in the legend when creating\n    a legend entry for a `.PathCollection` (scatter plot).\n\nscatteryoffsets : iterable of floats, default: ``[0.375, 0.5, 0.3125]``\n    The vertical offset (relative to the font size) for the markers\n    created for a scatter plot legend entry. 0.0 is at the base the\n    legend text, and 1.0 is at the top. To draw all markers at the\n    same height, set to ``[0.5]``.\n\nmarkerscale : float, default: :rc:`legend.markerscale`\n    The relative size of legend markers compared to the originally drawn ones.\n\nmarkerfirst : bool, default: True\n    If *True*, legend marker is placed to the left of the legend label.\n    If *False*, legend marker is placed to the right of the legend label.\n\nreverse : bool, default: False\n    If *True*, the legend labels are displayed in reverse order from the input.\n    If *False*, the legend labels are displayed in the same order as the input.\n\n    .. versionadded:: 3.7\n\nframeon : bool, default: :rc:`legend.frameon`\n    Whether the legend should be drawn on a patch (frame).\n\nfancybox : bool, default: :rc:`legend.fancybox`\n    Whether round edges should be enabled around the `.FancyBboxPatch` which\n    makes up the legend's background.\n\nshadow : None, bool or dict, default: :rc:`legend.shadow`\n    Whether to draw a shadow behind the legend.\n    The shadow can be configured using `.Patch` keywords.\n    Customization via :rc:`legend.shadow` is currently not supported.\n\nframealpha : float, default: :rc:`legend.framealpha`\n    The alpha transparency of the legend's background.\n    If *shadow* is activated and *framealpha* is ``None``, the default value is\n    ignored.\n\nfacecolor : \"inherit\" or color, default: :rc:`legend.facecolor`\n    The legend's background color.\n    If ``\"inherit\"``, use :rc:`axes.facecolor`.\n\nedgecolor : \"inherit\" or color, default: :rc:`legend.edgecolor`\n    The legend's background patch edge color.\n    If ``\"inherit\"``, use :rc:`axes.edgecolor`.\n\nmode : {\"expand\", None}\n    If *mode* is set to ``\"expand\"`` the legend will be horizontally\n    expanded to fill the Axes area (or *bbox_to_anchor* if defines\n    the legend's size).\n\nbbox_transform : None or `~matplotlib.transforms.Transform`\n    The transform for the bounding box (*bbox_to_anchor*). For a value\n    of ``None`` (default) the Axes'\n    :data:`~matplotlib.axes.Axes.transAxes` transform will be used.\n\ntitle : str or None\n    The legend's title. Default is no title (``None``).\n\ntitle_fontproperties : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend's title. If None (default), the\n    *title_fontsize* argument will be used if present; if *title_fontsize* is\n    also None, the current :rc:`legend.title_fontsize` will be used.\n\ntitle_fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}, default: :rc:`legend.title_fontsize`\n    The font size of the legend's title.\n    Note: This cannot be combined with *title_fontproperties*. If you want\n    to set the fontsize alongside other font properties, use the *size*\n    parameter in *title_fontproperties*.\n\nalignment : {'center', 'left', 'right'}, default: 'center'\n    The alignment of the legend title and the box of entries. The entries\n    are aligned as a single block, so that markers always lined up.\n\nborderpad : float, default: :rc:`legend.borderpad`\n    The fractional whitespace inside the legend border, in font-size units.\n\nlabelspacing : float, default: :rc:`legend.labelspacing`\n    The vertical space between the legend entries, in font-size units.\n\nhandlelength : float, default: :rc:`legend.handlelength`\n    The length of the legend handles, in font-size units.\n\nhandleheight : float, default: :rc:`legend.handleheight`\n    The height of the legend handles, in font-size units.\n\nhandletextpad : float, default: :rc:`legend.handletextpad`\n    The pad between the legend handle and text, in font-size units.\n\nborderaxespad : float, default: :rc:`legend.borderaxespad`\n    The pad between the Axes and legend border, in font-size units.\n\ncolumnspacing : float, default: :rc:`legend.columnspacing`\n    The spacing between columns, in font-size units.\n\nhandler_map : dict or None\n    The custom dictionary mapping instances or types to a legend\n    handler. This *handler_map* updates the default handler map\n    found at `matplotlib.legend.Legend.get_legend_handler_map`.\n\ndraggable : bool, default: False\n    Whether the legend can be dragged with the mouse.\n\n\nSee Also\n--------\n.Axes.legend\n\nNotes\n-----\nSome artists are not supported by this function.  See\n:ref:`legend_guide` for details.",
    "matplotlib.pyplot.fignum_exists": "\n    Return whether the figure with the given id exists.\n\n    Parameters\n    ----------\n    num : int or str\n        A figure identifier.\n\n    Returns\n    -------\n    bool\n        Whether or not a figure with id *num* exists.\n    ",
    "matplotlib.pyplot.figtext": "Add text to figure.\n\nParameters\n----------\nx, y : float\n    The position to place the text. By default, this is in figure\n    coordinates, floats in [0, 1]. The coordinate system can be changed\n    using the *transform* keyword.\n\ns : str\n    The text string.\n\nfontdict : dict, optional\n    A dictionary to override the default text properties. If not given,\n    the defaults are determined by :rc:`font.*`. Properties passed as\n    *kwargs* override the corresponding ones given in *fontdict*.\n\nReturns\n-------\n`~.text.Text`\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other miscellaneous text parameters.\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased: bool\n    backgroundcolor: :mpltype:`color`\n    bbox: dict with properties for `.patches.FancyBboxPatch`\n    clip_box: unknown\n    clip_on: unknown\n    clip_path: unknown\n    color or c: :mpltype:`color`\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fontfamily or family or fontname: {FONTNAME, 'serif', 'sans-serif', 'cursive', 'fantasy', 'monospace'}\n    fontproperties or font or font_properties: `.font_manager.FontProperties` or `str` or `pathlib.Path`\n    fontsize or size: float or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n    fontstretch or stretch: {a numeric value in range 0-1000, 'ultra-condensed', 'extra-condensed', 'condensed', 'semi-condensed', 'normal', 'semi-expanded', 'expanded', 'extra-expanded', 'ultra-expanded'}\n    fontstyle or style: {'normal', 'italic', 'oblique'}\n    fontvariant or variant: {'normal', 'small-caps'}\n    fontweight or weight: {a numeric value in range 0-1000, 'ultralight', 'light', 'normal', 'regular', 'book', 'medium', 'roman', 'semibold', 'demibold', 'demi', 'bold', 'heavy', 'extra bold', 'black'}\n    gid: str\n    horizontalalignment or ha: {'left', 'center', 'right'}\n    in_layout: bool\n    label: object\n    linespacing: float (multiple of font size)\n    math_fontfamily: str\n    mouseover: bool\n    multialignment or ma: {'left', 'right', 'center'}\n    parse_math: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: None or bool or float or callable\n    position: (float, float)\n    rasterized: bool\n    rotation: float or {'vertical', 'horizontal'}\n    rotation_mode: {None, 'default', 'anchor'}\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    text: object\n    transform: `~matplotlib.transforms.Transform`\n    transform_rotates_text: bool\n    url: str\n    usetex: bool, default: :rc:`text.usetex`\n    verticalalignment or va: {'baseline', 'bottom', 'center', 'center_baseline', 'top'}\n    visible: bool\n    wrap: bool\n    x: float\n    y: float\n    zorder: float\n\nSee Also\n--------\n.Axes.text\n.pyplot.text\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.Figure.text`.\n",
    "matplotlib.pyplot.figure": "\n    Create a new figure, or activate an existing figure.\n\n    Parameters\n    ----------\n    num : int or str or `.Figure` or `.SubFigure`, optional\n        A unique identifier for the figure.\n\n        If a figure with that identifier already exists, this figure is made\n        active and returned. An integer refers to the ``Figure.number``\n        attribute, a string refers to the figure label.\n\n        If there is no figure with the identifier or *num* is not given, a new\n        figure is created, made active and returned.  If *num* is an int, it\n        will be used for the ``Figure.number`` attribute, otherwise, an\n        auto-generated integer value is used (starting at 1 and incremented\n        for each new figure). If *num* is a string, the figure label and the\n        window title is set to this value.  If num is a ``SubFigure``, its\n        parent ``Figure`` is activated.\n\n    figsize : (float, float), default: :rc:`figure.figsize`\n        Width, height in inches.\n\n    dpi : float, default: :rc:`figure.dpi`\n        The resolution of the figure in dots-per-inch.\n\n    facecolor : :mpltype:`color`, default: :rc:`figure.facecolor`\n        The background color.\n\n    edgecolor : :mpltype:`color`, default: :rc:`figure.edgecolor`\n        The border color.\n\n    frameon : bool, default: True\n        If False, suppress drawing the figure frame.\n\n    FigureClass : subclass of `~matplotlib.figure.Figure`\n        If set, an instance of this subclass will be created, rather than a\n        plain `.Figure`.\n\n    clear : bool, default: False\n        If True and the figure already exists, then it is cleared.\n\n    layout : {'constrained', 'compressed', 'tight', 'none', `.LayoutEngine`, None}, default: None\n        The layout mechanism for positioning of plot elements to avoid\n        overlapping Axes decorations (labels, ticks, etc). Note that layout\n        managers can measurably slow down figure display.\n\n        - 'constrained': The constrained layout solver adjusts Axes sizes\n          to avoid overlapping Axes decorations.  Can handle complex plot\n          layouts and colorbars, and is thus recommended.\n\n          See :ref:`constrainedlayout_guide`\n          for examples.\n\n        - 'compressed': uses the same algorithm as 'constrained', but\n          removes extra space between fixed-aspect-ratio Axes.  Best for\n          simple grids of Axes.\n\n        - 'tight': Use the tight layout mechanism. This is a relatively\n          simple algorithm that adjusts the subplot parameters so that\n          decorations do not overlap. See `.Figure.set_tight_layout` for\n          further details.\n\n        - 'none': Do not use a layout engine.\n\n        - A `.LayoutEngine` instance. Builtin layout classes are\n          `.ConstrainedLayoutEngine` and `.TightLayoutEngine`, more easily\n          accessible by 'constrained' and 'tight'.  Passing an instance\n          allows third parties to provide their own layout engine.\n\n        If not given, fall back to using the parameters *tight_layout* and\n        *constrained_layout*, including their config defaults\n        :rc:`figure.autolayout` and :rc:`figure.constrained_layout.use`.\n\n    **kwargs\n        Additional keyword arguments are passed to the `.Figure` constructor.\n\n    Returns\n    -------\n    `~matplotlib.figure.Figure`\n\n    Notes\n    -----\n    A newly created figure is passed to the `~.FigureCanvasBase.new_manager`\n    method or the `new_figure_manager` function provided by the current\n    backend, which install a canvas and a manager on the figure.\n\n    Once this is done, :rc:`figure.hooks` are called, one at a time, on the\n    figure; these hooks allow arbitrary customization of the figure (e.g.,\n    attaching callbacks) or of associated elements (e.g., modifying the\n    toolbar).  See :doc:`/gallery/user_interfaces/mplcvd` for an example of\n    toolbar customization.\n\n    If you are creating many figures, make sure you explicitly call\n    `.pyplot.close` on the figures you are not using, because this will\n    enable pyplot to properly clean up the memory.\n\n    `~matplotlib.rcParams` defines the default values, which can be modified\n    in the matplotlibrc file.\n    ",
    "matplotlib.pyplot.fill": "Plot filled polygons.\n\nParameters\n----------\n*args : sequence of x, y, [color]\n    Each polygon is defined by the lists of *x* and *y* positions of\n    its nodes, optionally followed by a *color* specifier. See\n    :mod:`matplotlib.colors` for supported color specifiers. The\n    standard color cycle is used for polygons without a color\n    specifier.\n\n    You can plot multiple polygons by providing multiple *x*, *y*,\n    *[color]* groups.\n\n    For example, each of the following is legal::\n\n        ax.fill(x, y)                    # a polygon with default color\n        ax.fill(x, y, \"b\")               # a blue polygon\n        ax.fill(x, y, x2, y2)            # two polygons\n        ax.fill(x, y, \"b\", x2, y2, \"r\")  # a blue and a red polygon\n\ndata : indexable object, optional\n    An object with labelled data. If given, provide the label names to\n    plot in *x* and *y*, e.g.::\n\n        ax.fill(\"time\", \"signal\",\n                data={\"time\": [0, 1, 2], \"signal\": [0, 1, 0]})\n\nReturns\n-------\nlist of `~matplotlib.patches.Polygon`\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.patches.Polygon` properties\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.fill`.\n\nUse :meth:`fill_between` if you would like to fill the region between\ntwo curves.",
    "matplotlib.pyplot.fill_between": "Fill the area between two horizontal curves.\n\nThe curves are defined by the points (*x*, *y1*) and (*x*,\n*y2*).  This creates one or multiple polygons describing the filled\narea.\n\nYou may exclude some horizontal sections from filling using *where*.\n\nBy default, the edges connect the given points directly.  Use *step*\nif the filling should be a step function, i.e. constant in between\n*x*.\n\nParameters\n----------\nx : array (length N)\n    The x coordinates of the nodes defining the curves.\n\ny1 : array (length N) or scalar\n    The y coordinates of the nodes defining the first curve.\n\ny2 : array (length N) or scalar, default: 0\n    The y coordinates of the nodes defining the second curve.\n\nwhere : array of bool (length N), optional\n    Define *where* to exclude some horizontal regions from being filled.\n    The filled regions are defined by the coordinates ``x[where]``.\n    More precisely, fill between ``x[i]`` and ``x[i+1]`` if\n    ``where[i] and where[i+1]``.  Note that this definition implies\n    that an isolated *True* value between two *False* values in *where*\n    will not result in filling.  Both sides of the *True* position\n    remain unfilled due to the adjacent *False* values.\n\ninterpolate : bool, default: False\n    This option is only relevant if *where* is used and the two curves\n    are crossing each other.\n\n    Semantically, *where* is often used for *y1* > *y2* or\n    similar.  By default, the nodes of the polygon defining the filled\n    region will only be placed at the positions in the *x* array.\n    Such a polygon cannot describe the above semantics close to the\n    intersection.  The x-sections containing the intersection are\n    simply clipped.\n\n    Setting *interpolate* to *True* will calculate the actual\n    intersection point and extend the filled region up to this point.\n\nstep : {'pre', 'post', 'mid'}, optional\n    Define *step* if the filling should be a step function,\n    i.e. constant in between *x*.  The value determines where the\n    step will occur:\n\n    - 'pre': The y value is continued constantly to the left from\n      every *x* position, i.e. the interval ``(x[i-1], x[i]]``\n      has the value ``y[i]``.\n    - 'post': The y value is continued constantly to the right from\n      every *x* position, i.e. the interval ``[x[i], x[i+1])``\n      has the value ``y[i]``.\n    - 'mid': Steps occur half-way between the *x* positions.\n\nReturns\n-------\n`.FillBetweenPolyCollection`\n    A `.FillBetweenPolyCollection` containing the plotted polygons.\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *x*, *y1*, *y2*, *where*\n\n**kwargs\n    All other keyword arguments are passed on to\n    `.FillBetweenPolyCollection`. They control the `.Polygon` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: array-like or scalar or None\n    animated: bool\n    antialiased or aa or antialiaseds: bool or list of bools\n    array: array-like or None\n    capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    clim: (vmin: float, vmax: float)\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    cmap: `.Colormap` or str or None\n    color: :mpltype:`color` or list of RGBA tuples\n    data: array (length N)\n    edgecolor or ec or edgecolors: :mpltype:`color` or list of :mpltype:`color` or 'face'\n    facecolor or facecolors or fc: :mpltype:`color` or list of :mpltype:`color`\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    gid: str\n    hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n    hatch_linewidth: unknown\n    in_layout: bool\n    joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    label: object\n    linestyle or dashes or linestyles or ls: str or tuple or list thereof\n    linewidth or linewidths or lw: float or list of floats\n    mouseover: bool\n    norm: `.Normalize` or str or None\n    offset_transform or transOffset: `.Transform`\n    offsets: (N, 2) or (2,) array-like\n    path_effects: list of `.AbstractPathEffect`\n    paths: list of array-like\n    picker: None or bool or float or callable\n    pickradius: float\n    rasterized: bool\n    sizes: `numpy.ndarray` or None\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    urls: list of str or None\n    verts: list of array-like\n    verts_and_codes: unknown\n    visible: bool\n    zorder: float\n\nSee Also\n--------\nfill_between : Fill between two sets of y-values.\nfill_betweenx : Fill between two sets of x-values.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.fill_between`.\n",
    "matplotlib.pyplot.fill_betweenx": "Fill the area between two vertical curves.\n\nThe curves are defined by the points (*y*, *x1*) and (*y*,\n*x2*).  This creates one or multiple polygons describing the filled\narea.\n\nYou may exclude some vertical sections from filling using *where*.\n\nBy default, the edges connect the given points directly.  Use *step*\nif the filling should be a step function, i.e. constant in between\n*y*.\n\nParameters\n----------\ny : array (length N)\n    The y coordinates of the nodes defining the curves.\n\nx1 : array (length N) or scalar\n    The x coordinates of the nodes defining the first curve.\n\nx2 : array (length N) or scalar, default: 0\n    The x coordinates of the nodes defining the second curve.\n\nwhere : array of bool (length N), optional\n    Define *where* to exclude some vertical regions from being filled.\n    The filled regions are defined by the coordinates ``y[where]``.\n    More precisely, fill between ``y[i]`` and ``y[i+1]`` if\n    ``where[i] and where[i+1]``.  Note that this definition implies\n    that an isolated *True* value between two *False* values in *where*\n    will not result in filling.  Both sides of the *True* position\n    remain unfilled due to the adjacent *False* values.\n\ninterpolate : bool, default: False\n    This option is only relevant if *where* is used and the two curves\n    are crossing each other.\n\n    Semantically, *where* is often used for *x1* > *x2* or\n    similar.  By default, the nodes of the polygon defining the filled\n    region will only be placed at the positions in the *y* array.\n    Such a polygon cannot describe the above semantics close to the\n    intersection.  The y-sections containing the intersection are\n    simply clipped.\n\n    Setting *interpolate* to *True* will calculate the actual\n    intersection point and extend the filled region up to this point.\n\nstep : {'pre', 'post', 'mid'}, optional\n    Define *step* if the filling should be a step function,\n    i.e. constant in between *y*.  The value determines where the\n    step will occur:\n\n    - 'pre': The x value is continued constantly to the left from\n      every *y* position, i.e. the interval ``(y[i-1], y[i]]``\n      has the value ``x[i]``.\n    - 'post': The y value is continued constantly to the right from\n      every *y* position, i.e. the interval ``[y[i], y[i+1])``\n      has the value ``x[i]``.\n    - 'mid': Steps occur half-way between the *y* positions.\n\nReturns\n-------\n`.FillBetweenPolyCollection`\n    A `.FillBetweenPolyCollection` containing the plotted polygons.\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *y*, *x1*, *x2*, *where*\n\n**kwargs\n    All other keyword arguments are passed on to\n    `.FillBetweenPolyCollection`. They control the `.Polygon` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: array-like or scalar or None\n    animated: bool\n    antialiased or aa or antialiaseds: bool or list of bools\n    array: array-like or None\n    capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    clim: (vmin: float, vmax: float)\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    cmap: `.Colormap` or str or None\n    color: :mpltype:`color` or list of RGBA tuples\n    data: array (length N)\n    edgecolor or ec or edgecolors: :mpltype:`color` or list of :mpltype:`color` or 'face'\n    facecolor or facecolors or fc: :mpltype:`color` or list of :mpltype:`color`\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    gid: str\n    hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n    hatch_linewidth: unknown\n    in_layout: bool\n    joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    label: object\n    linestyle or dashes or linestyles or ls: str or tuple or list thereof\n    linewidth or linewidths or lw: float or list of floats\n    mouseover: bool\n    norm: `.Normalize` or str or None\n    offset_transform or transOffset: `.Transform`\n    offsets: (N, 2) or (2,) array-like\n    path_effects: list of `.AbstractPathEffect`\n    paths: list of array-like\n    picker: None or bool or float or callable\n    pickradius: float\n    rasterized: bool\n    sizes: `numpy.ndarray` or None\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    urls: list of str or None\n    verts: list of array-like\n    verts_and_codes: unknown\n    visible: bool\n    zorder: float\n\nSee Also\n--------\nfill_between : Fill between two sets of y-values.\nfill_betweenx : Fill between two sets of x-values.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.fill_betweenx`.\n",
    "matplotlib.pyplot.findobj": "\n        Find artist objects.\n\n        Recursively find all `.Artist` instances contained in the artist.\n\n        Parameters\n        ----------\n        match\n            A filter criterion for the matches. This can be\n\n            - *None*: Return all objects contained in artist.\n            - A function with signature ``def match(artist: Artist) -> bool``.\n              The result will only contain artists for which the function\n              returns *True*.\n            - A class instance: e.g., `.Line2D`. The result will only contain\n              artists of this class or its subclasses (``isinstance`` check).\n\n        include_self : bool\n            Include *self* in the list to be checked for a match.\n\n        Returns\n        -------\n        list of `.Artist`\n\n        ",
    "matplotlib.pyplot.flag": "\n    Set the colormap to 'flag'.\n\n    This changes the default colormap as well as the colormap of the current\n    image if there is one. See ``help(colormaps)`` for more information.\n    ",
    "matplotlib.pyplot.gca": "Get the current Axes.\n\nIf there is currently no Axes on this Figure, a new one is created\nusing `.Figure.add_subplot`.  (To test whether there is currently an\nAxes on a Figure, check whether ``figure.axes`` is empty.  To test\nwhether there is currently a Figure on the pyplot figure stack, check\nwhether `.pyplot.get_fignums()` is empty.)\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.Figure.gca`.\n",
    "matplotlib.pyplot.gcf": "\n    Get the current figure.\n\n    If there is currently no figure on the pyplot figure stack, a new one is\n    created using `~.pyplot.figure()`.  (To test whether there is currently a\n    figure on the pyplot figure stack, check whether `~.pyplot.get_fignums()`\n    is empty.)\n    ",
    "matplotlib.pyplot.gci": "\n        Get the current colorable artist.\n\n        Specifically, returns the current `.ScalarMappable` instance (`.Image`\n        created by `imshow` or `figimage`, `.Collection` created by `pcolor` or\n        `scatter`, etc.), or *None* if no such instance has been defined.\n\n        The current image is an attribute of the current Axes, or the nearest\n        earlier Axes in the current figure that contains an image.\n\n        Notes\n        -----\n        Historically, the only colorable artists were images; hence the name\n        ``gci`` (get current image).\n        ",
    "matplotlib.pyplot.get": "Return the value of an `.Artist`'s *property*, or print all of them.\n\nParameters\n----------\nobj : `~matplotlib.artist.Artist`\n    The queried artist; e.g., a `.Line2D`, a `.Text`, or an `~.axes.Axes`.\n\nproperty : str or None, default: None\n    If *property* is 'somename', this function returns\n    ``obj.get_somename()``.\n\n    If it's None (or unset), it *prints* all gettable properties from\n    *obj*.  Many properties have aliases for shorter typing, e.g. 'lw' is\n    an alias for 'linewidth'.  In the output, aliases and full property\n    names will be listed as:\n\n      property or alias = value\n\n    e.g.:\n\n      linewidth or lw = 2\n\nSee Also\n--------\nsetp\n\nNotes\n-----\n\n.. note::\n\n    This is equivalent to `matplotlib.artist.getp`.\n",
    "matplotlib.pyplot.get_backend": "\n    Return the name of the current backend.\n\n    Parameters\n    ----------\n    auto_select : bool, default: True\n        Whether to trigger backend resolution if no backend has been\n        selected so far. If True, this ensures that a valid backend\n        is returned. If False, this returns None if no backend has been\n        selected so far.\n\n        .. versionadded:: 3.10\n\n        .. admonition:: Provisional\n\n           The *auto_select* flag is provisional. It may be changed or removed\n           without prior warning.\n\n    See Also\n    --------\n    matplotlib.use\n    ",
    "matplotlib.pyplot.get_cmap": "\n    Get a colormap instance, defaulting to rc values if *name* is None.\n\n    Parameters\n    ----------\n    name : `~matplotlib.colors.Colormap` or str or None, default: None\n        If a `.Colormap` instance, it will be returned. Otherwise, the name of\n        a colormap known to Matplotlib, which will be resampled by *lut*. The\n        default, None, means :rc:`image.cmap`.\n    lut : int or None, default: None\n        If *name* is not already a Colormap instance and *lut* is not None, the\n        colormap will be resampled to have *lut* entries in the lookup table.\n\n    Returns\n    -------\n    Colormap\n    ",
    "matplotlib.pyplot.get_current_fig_manager": "\n    Return the figure manager of the current figure.\n\n    The figure manager is a container for the actual backend-depended window\n    that displays the figure on screen.\n\n    If no current figure exists, a new one is created, and its figure\n    manager is returned.\n\n    Returns\n    -------\n    `.FigureManagerBase` or backend-dependent subclass thereof\n    ",
    "matplotlib.pyplot.get_figlabels": "Return a list of existing figure labels.",
    "matplotlib.pyplot.get_fignums": "Return a list of existing figure numbers.",
    "matplotlib.pyplot.get_plot_commands": "[*Deprecated*] Get a sorted list of all of the plotting commands.\n\nNotes\n-----\n.. deprecated:: 3.7\n   ",
    "matplotlib.pyplot.get_scale_names": "Return the names of the available scales.",
    "matplotlib.pyplot.getp": "Return the value of an `.Artist`'s *property*, or print all of them.\n\nParameters\n----------\nobj : `~matplotlib.artist.Artist`\n    The queried artist; e.g., a `.Line2D`, a `.Text`, or an `~.axes.Axes`.\n\nproperty : str or None, default: None\n    If *property* is 'somename', this function returns\n    ``obj.get_somename()``.\n\n    If it's None (or unset), it *prints* all gettable properties from\n    *obj*.  Many properties have aliases for shorter typing, e.g. 'lw' is\n    an alias for 'linewidth'.  In the output, aliases and full property\n    names will be listed as:\n\n      property or alias = value\n\n    e.g.:\n\n      linewidth or lw = 2\n\nSee Also\n--------\nsetp\n\nNotes\n-----\n\n.. note::\n\n    This is equivalent to `matplotlib.artist.getp`.\n",
    "matplotlib.pyplot.ginput": "Blocking call to interact with a figure.\n\nWait until the user clicks *n* times on the figure, and return the\ncoordinates of each click in a list.\n\nThere are three possible interactions:\n\n- Add a point.\n- Remove the most recently added point.\n- Stop the interaction and return the points added so far.\n\nThe actions are assigned to mouse buttons via the arguments\n*mouse_add*, *mouse_pop* and *mouse_stop*.\n\nParameters\n----------\nn : int, default: 1\n    Number of mouse clicks to accumulate. If negative, accumulate\n    clicks until the input is terminated manually.\ntimeout : float, default: 30 seconds\n    Number of seconds to wait before timing out. If zero or negative\n    will never time out.\nshow_clicks : bool, default: True\n    If True, show a red cross at the location of each click.\nmouse_add : `.MouseButton` or None, default: `.MouseButton.LEFT`\n    Mouse button used to add points.\nmouse_pop : `.MouseButton` or None, default: `.MouseButton.RIGHT`\n    Mouse button used to remove the most recently added point.\nmouse_stop : `.MouseButton` or None, default: `.MouseButton.MIDDLE`\n    Mouse button used to stop input.\n\nReturns\n-------\nlist of tuples\n    A list of the clicked (x, y) coordinates.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.Figure.ginput`.\n\nThe keyboard can also be used to select points in case your mouse\ndoes not have one or more of the buttons.  The delete and backspace\nkeys act like right-clicking (i.e., remove last point), the enter key\nterminates input and any other key (not already used by the window\nmanager) selects a point.",
    "matplotlib.pyplot.gray": "\n    Set the colormap to 'gray'.\n\n    This changes the default colormap as well as the colormap of the current\n    image if there is one. See ``help(colormaps)`` for more information.\n    ",
    "matplotlib.pyplot.grid": "Configure the grid lines.\n\nParameters\n----------\nvisible : bool or None, optional\n    Whether to show the grid lines.  If any *kwargs* are supplied, it\n    is assumed you want the grid on and *visible* will be set to True.\n\n    If *visible* is *None* and there are no *kwargs*, this toggles the\n    visibility of the lines.\n\nwhich : {'major', 'minor', 'both'}, optional\n    The grid lines to apply the changes on.\n\naxis : {'both', 'x', 'y'}, optional\n    The axis to apply the changes on.\n\n**kwargs : `~matplotlib.lines.Line2D` properties\n    Define the line properties of the grid, e.g.::\n\n        grid(color='r', linestyle='-', linewidth=2)\n\n    Valid keyword arguments are:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: :mpltype:`color`\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: :mpltype:`color` or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: :mpltype:`color`\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: :mpltype:`color`\n    markerfacecoloralt or mfcalt: :mpltype:`color`\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.grid`.\n\nThe axis is drawn as a unit, so the effective zorder for drawing the\ngrid is determined by the zorder of each axis, not by the zorder of the\n`.Line2D` objects comprising the grid.  Therefore, to set grid zorder,\nuse `.set_axisbelow` or, for more control, call the\n`~.Artist.set_zorder` method of each axis.",
    "matplotlib.pyplot.hexbin": "Make a 2D hexagonal binning plot of points *x*, *y*.\n\nIf *C* is *None*, the value of the hexagon is determined by the number\nof points in the hexagon. Otherwise, *C* specifies values at the\ncoordinate (x[i], y[i]). For each hexagon, these values are reduced\nusing *reduce_C_function*.\n\nParameters\n----------\nx, y : array-like\n    The data positions. *x* and *y* must be of the same length.\n\nC : array-like, optional\n    If given, these values are accumulated in the bins. Otherwise,\n    every point has a value of 1. Must be of the same length as *x*\n    and *y*.\n\ngridsize : int or (int, int), default: 100\n    If a single int, the number of hexagons in the *x*-direction.\n    The number of hexagons in the *y*-direction is chosen such that\n    the hexagons are approximately regular.\n\n    Alternatively, if a tuple (*nx*, *ny*), the number of hexagons\n    in the *x*-direction and the *y*-direction. In the\n    *y*-direction, counting is done along vertically aligned\n    hexagons, not along the zig-zag chains of hexagons; see the\n    following illustration.\n\n    .. plot::\n\n       import numpy\n       import matplotlib.pyplot as plt\n\n       np.random.seed(19680801)\n       n= 300\n       x = np.random.standard_normal(n)\n       y = np.random.standard_normal(n)\n\n       fig, ax = plt.subplots(figsize=(4, 4))\n       h = ax.hexbin(x, y, gridsize=(5, 3))\n       hx, hy = h.get_offsets().T\n       ax.plot(hx[24::3], hy[24::3], 'ro-')\n       ax.plot(hx[-3:], hy[-3:], 'ro-')\n       ax.set_title('gridsize=(5, 3)')\n       ax.axis('off')\n\n    To get approximately regular hexagons, choose\n    :math:`n_x = \\sqrt{3}\\,n_y`.\n\nbins : 'log' or int or sequence, default: None\n    Discretization of the hexagon values.\n\n    - If *None*, no binning is applied; the color of each hexagon\n      directly corresponds to its count value.\n    - If 'log', use a logarithmic scale for the colormap.\n      Internally, :math:`log_{10}(i+1)` is used to determine the\n      hexagon color. This is equivalent to ``norm=LogNorm()``.\n    - If an integer, divide the counts in the specified number\n      of bins, and color the hexagons accordingly.\n    - If a sequence of values, the values of the lower bound of\n      the bins to be used.\n\nxscale : {'linear', 'log'}, default: 'linear'\n    Use a linear or log10 scale on the horizontal axis.\n\nyscale : {'linear', 'log'}, default: 'linear'\n    Use a linear or log10 scale on the vertical axis.\n\nmincnt : int >= 0, default: *None*\n    If not *None*, only display cells with at least *mincnt*\n    number of points in the cell.\n\nmarginals : bool, default: *False*\n    If marginals is *True*, plot the marginal density as\n    colormapped rectangles along the bottom of the x-axis and\n    left of the y-axis.\n\nextent : 4-tuple of float, default: *None*\n    The limits of the bins (xmin, xmax, ymin, ymax).\n    The default assigns the limits based on\n    *gridsize*, *x*, *y*, *xscale* and *yscale*.\n\n    If *xscale* or *yscale* is set to 'log', the limits are\n    expected to be the exponent for a power of 10. E.g. for\n    x-limits of 1 and 50 in 'linear' scale and y-limits\n    of 10 and 1000 in 'log' scale, enter (1, 50, 1, 3).\n\nReturns\n-------\n`~matplotlib.collections.PolyCollection`\n    A `.PolyCollection` defining the hexagonal bins.\n\n    - `.PolyCollection.get_offsets` contains a Mx2 array containing\n      the x, y positions of the M hexagon centers in data coordinates.\n    - `.PolyCollection.get_array` contains the values of the M\n      hexagons.\n\n    If *marginals* is *True*, horizontal\n    bar and vertical bar (both PolyCollections) will be attached\n    to the return collection as attributes *hbar* and *vbar*.\n\nOther Parameters\n----------------\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    The Colormap instance or registered colormap name used to map scalar data\n    to colors.\n\nnorm : str or `~matplotlib.colors.Normalize`, optional\n    The normalization method used to scale scalar data to the [0, 1] range\n    before mapping to colors using *cmap*. By default, a linear scaling is\n    used, mapping the lowest value to 0 and the highest to 1.\n\n    If given, this can be one of the following:\n\n    - An instance of `.Normalize` or one of its subclasses\n      (see :ref:`colormapnorms`).\n    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n      list of available scales, call `matplotlib.scale.get_scale_names()`.\n      In that case, a suitable `.Normalize` subclass is dynamically generated\n      and instantiated.\n\nvmin, vmax : float, optional\n    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n    the data range that the colormap covers. By default, the colormap covers\n    the complete value range of the supplied data. It is an error to use\n    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n    name together with *vmin*/*vmax* is acceptable).\n\nalpha : float between 0 and 1, optional\n    The alpha blending value, between 0 (transparent) and 1 (opaque).\n\nlinewidths : float, default: *None*\n    If *None*, defaults to :rc:`patch.linewidth`.\n\nedgecolors : {'face', 'none', *None*} or color, default: 'face'\n    The color of the hexagon edges. Possible values are:\n\n    - 'face': Draw the edges in the same color as the fill color.\n    - 'none': No edges are drawn. This can sometimes lead to unsightly\n      unpainted pixels between the hexagons.\n    - *None*: Draw outlines in the default color.\n    - An explicit color.\n\nreduce_C_function : callable, default: `numpy.mean`\n    The function to aggregate *C* within the bins. It is ignored if\n    *C* is not given. This must have the signature::\n\n        def reduce_C_function(C: array) -> float\n\n    Commonly used functions are:\n\n    - `numpy.mean`: average of the points\n    - `numpy.sum`: integral of the point values\n    - `numpy.amax`: value taken from the largest point\n\n    By default will only reduce cells with at least 1 point because some\n    reduction functions (such as `numpy.amax`) will error/warn with empty\n    input. Changing *mincnt* will adjust the cutoff, and if set to 0 will\n    pass empty input to the reduction function.\n\ncolorizer : `~matplotlib.colorizer.Colorizer` or None, default: None\n    The Colorizer object used to map color to data. If None, a Colorizer\n    object is created from a *norm* and *cmap*.\n\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *x*, *y*, *C*\n\n**kwargs : `~matplotlib.collections.PolyCollection` properties\n    All other keyword arguments are passed on to `.PolyCollection`:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: array-like or scalar or None\n    animated: bool\n    antialiased or aa or antialiaseds: bool or list of bools\n    array: array-like or None\n    capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    clim: (vmin: float, vmax: float)\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    cmap: `.Colormap` or str or None\n    color: :mpltype:`color` or list of RGBA tuples\n    edgecolor or ec or edgecolors: :mpltype:`color` or list of :mpltype:`color` or 'face'\n    facecolor or facecolors or fc: :mpltype:`color` or list of :mpltype:`color`\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    gid: str\n    hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n    hatch_linewidth: unknown\n    in_layout: bool\n    joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    label: object\n    linestyle or dashes or linestyles or ls: str or tuple or list thereof\n    linewidth or linewidths or lw: float or list of floats\n    mouseover: bool\n    norm: `.Normalize` or str or None\n    offset_transform or transOffset: `.Transform`\n    offsets: (N, 2) or (2,) array-like\n    path_effects: list of `.AbstractPathEffect`\n    paths: list of array-like\n    picker: None or bool or float or callable\n    pickradius: float\n    rasterized: bool\n    sizes: `numpy.ndarray` or None\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    urls: list of str or None\n    verts: list of array-like\n    verts_and_codes: unknown\n    visible: bool\n    zorder: float\n\nSee Also\n--------\nhist2d : 2D histogram rectangular bins\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.hexbin`.\n",
    "matplotlib.pyplot.hist": "Compute and plot a histogram.\n\nThis method uses `numpy.histogram` to bin the data in *x* and count the\nnumber of values in each bin, then draws the distribution either as a\n`.BarContainer` or `.Polygon`. The *bins*, *range*, *density*, and\n*weights* parameters are forwarded to `numpy.histogram`.\n\nIf the data has already been binned and counted, use `~.bar` or\n`~.stairs` to plot the distribution::\n\n    counts, bins = np.histogram(x)\n    plt.stairs(counts, bins)\n\nAlternatively, plot pre-computed bins and counts using ``hist()`` by\ntreating each bin as a single point with a weight equal to its count::\n\n    plt.hist(bins[:-1], bins, weights=counts)\n\nThe data input *x* can be a singular array, a list of datasets of\npotentially different lengths ([*x0*, *x1*, ...]), or a 2D ndarray in\nwhich each column is a dataset. Note that the ndarray form is\ntransposed relative to the list form. If the input is an array, then\nthe return value is a tuple (*n*, *bins*, *patches*); if the input is a\nsequence of arrays, then the return value is a tuple\n([*n0*, *n1*, ...], *bins*, [*patches0*, *patches1*, ...]).\n\nMasked arrays are not supported.\n\nParameters\n----------\nx : (n,) array or sequence of (n,) arrays\n    Input values, this takes either a single array or a sequence of\n    arrays which are not required to be of the same length.\n\nbins : int or sequence or str, default: :rc:`hist.bins`\n    If *bins* is an integer, it defines the number of equal-width bins\n    in the range.\n\n    If *bins* is a sequence, it defines the bin edges, including the\n    left edge of the first bin and the right edge of the last bin;\n    in this case, bins may be unequally spaced.  All but the last\n    (righthand-most) bin is half-open.  In other words, if *bins* is::\n\n        [1, 2, 3, 4]\n\n    then the first bin is ``[1, 2)`` (including 1, but excluding 2) and\n    the second ``[2, 3)``.  The last bin, however, is ``[3, 4]``, which\n    *includes* 4.\n\n    If *bins* is a string, it is one of the binning strategies\n    supported by `numpy.histogram_bin_edges`: 'auto', 'fd', 'doane',\n    'scott', 'stone', 'rice', 'sturges', or 'sqrt'.\n\nrange : tuple or None, default: None\n    The lower and upper range of the bins. Lower and upper outliers\n    are ignored. If not provided, *range* is ``(x.min(), x.max())``.\n    Range has no effect if *bins* is a sequence.\n\n    If *bins* is a sequence or *range* is specified, autoscaling\n    is based on the specified bin range instead of the\n    range of x.\n\ndensity : bool, default: False\n    If ``True``, draw and return a probability density: each bin\n    will display the bin's raw count divided by the total number of\n    counts *and the bin width*\n    (``density = counts / (sum(counts) * np.diff(bins))``),\n    so that the area under the histogram integrates to 1\n    (``np.sum(density * np.diff(bins)) == 1``).\n\n    If *stacked* is also ``True``, the sum of the histograms is\n    normalized to 1.\n\nweights : (n,) array-like or None, default: None\n    An array of weights, of the same shape as *x*.  Each value in\n    *x* only contributes its associated weight towards the bin count\n    (instead of 1).  If *density* is ``True``, the weights are\n    normalized, so that the integral of the density over the range\n    remains 1.\n\ncumulative : bool or -1, default: False\n    If ``True``, then a histogram is computed where each bin gives the\n    counts in that bin plus all bins for smaller values. The last bin\n    gives the total number of datapoints.\n\n    If *density* is also ``True`` then the histogram is normalized such\n    that the last bin equals 1.\n\n    If *cumulative* is a number less than 0 (e.g., -1), the direction\n    of accumulation is reversed.  In this case, if *density* is also\n    ``True``, then the histogram is normalized such that the first bin\n    equals 1.\n\nbottom : array-like, scalar, or None, default: None\n    Location of the bottom of each bin, i.e. bins are drawn from\n    ``bottom`` to ``bottom + hist(x, bins)`` If a scalar, the bottom\n    of each bin is shifted by the same amount. If an array, each bin\n    is shifted independently and the length of bottom must match the\n    number of bins. If None, defaults to 0.\n\nhisttype : {'bar', 'barstacked', 'step', 'stepfilled'}, default: 'bar'\n    The type of histogram to draw.\n\n    - 'bar' is a traditional bar-type histogram.  If multiple data\n      are given the bars are arranged side by side.\n    - 'barstacked' is a bar-type histogram where multiple\n      data are stacked on top of each other.\n    - 'step' generates a lineplot that is by default unfilled.\n    - 'stepfilled' generates a lineplot that is by default filled.\n\nalign : {'left', 'mid', 'right'}, default: 'mid'\n    The horizontal alignment of the histogram bars.\n\n    - 'left': bars are centered on the left bin edges.\n    - 'mid': bars are centered between the bin edges.\n    - 'right': bars are centered on the right bin edges.\n\norientation : {'vertical', 'horizontal'}, default: 'vertical'\n    If 'horizontal', `~.Axes.barh` will be used for bar-type histograms\n    and the *bottom* kwarg will be the left edges.\n\nrwidth : float or None, default: None\n    The relative width of the bars as a fraction of the bin width.  If\n    ``None``, automatically compute the width.\n\n    Ignored if *histtype* is 'step' or 'stepfilled'.\n\nlog : bool, default: False\n    If ``True``, the histogram axis will be set to a log scale.\n\ncolor : :mpltype:`color` or list of :mpltype:`color` or None, default: None\n    Color or sequence of colors, one per dataset.  Default (``None``)\n    uses the standard line color sequence.\n\nlabel : str or list of str, optional\n    String, or sequence of strings to match multiple datasets.  Bar\n    charts yield multiple patches per dataset, but only the first gets\n    the label, so that `~.Axes.legend` will work as expected.\n\nstacked : bool, default: False\n    If ``True``, multiple data are stacked on top of each other If\n    ``False`` multiple data are arranged side by side if histtype is\n    'bar' or on top of each other if histtype is 'step'\n\nReturns\n-------\nn : array or list of arrays\n    The values of the histogram bins. See *density* and *weights* for a\n    description of the possible semantics.  If input *x* is an array,\n    then this is an array of length *nbins*. If input is a sequence of\n    arrays ``[data1, data2, ...]``, then this is a list of arrays with\n    the values of the histograms for each of the arrays in the same\n    order.  The dtype of the array *n* (or of its element arrays) will\n    always be float even if no weighting or normalization is used.\n\nbins : array\n    The edges of the bins. Length nbins + 1 (nbins left edges and right\n    edge of last bin).  Always a single array even when multiple data\n    sets are passed in.\n\npatches : `.BarContainer` or list of a single `.Polygon` or list of such objects\n    Container of individual artists used to create the histogram\n    or list of such containers if there are multiple input datasets.\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *x*, *weights*\n\n**kwargs\n    `~matplotlib.patches.Patch` properties. The following properties\n    additionally accept a sequence of values corresponding to the\n    datasets in *x*:\n    *edgecolor*, *facecolor*, *linewidth*, *linestyle*, *hatch*.\n\n    .. versionadded:: 3.10\n       Allowing sequences of values in above listed Patch properties.\n\nSee Also\n--------\nhist2d : 2D histogram with rectangular bins\nhexbin : 2D histogram with hexagonal bins\nstairs : Plot a pre-computed histogram\nbar : Plot a pre-computed histogram\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.hist`.\n\nFor large numbers of bins (>1000), plotting can be significantly\naccelerated by using `~.Axes.stairs` to plot a pre-computed histogram\n(``plt.stairs(*np.histogram(data))``), or by setting *histtype* to\n'step' or 'stepfilled' rather than 'bar' or 'barstacked'.",
    "matplotlib.pyplot.hist2d": "Make a 2D histogram plot.\n\nParameters\n----------\nx, y : array-like, shape (n, )\n    Input values\n\nbins : None or int or [int, int] or array-like or [array, array]\n\n    The bin specification:\n\n    - If int, the number of bins for the two dimensions\n      (``nx = ny = bins``).\n    - If ``[int, int]``, the number of bins in each dimension\n      (``nx, ny = bins``).\n    - If array-like, the bin edges for the two dimensions\n      (``x_edges = y_edges = bins``).\n    - If ``[array, array]``, the bin edges in each dimension\n      (``x_edges, y_edges = bins``).\n\n    The default value is 10.\n\nrange : array-like shape(2, 2), optional\n    The leftmost and rightmost edges of the bins along each dimension\n    (if not specified explicitly in the bins parameters): ``[[xmin,\n    xmax], [ymin, ymax]]``. All values outside of this range will be\n    considered outliers and not tallied in the histogram.\n\ndensity : bool, default: False\n    Normalize histogram.  See the documentation for the *density*\n    parameter of `~.Axes.hist` for more details.\n\nweights : array-like, shape (n, ), optional\n    An array of values w_i weighing each sample (x_i, y_i).\n\ncmin, cmax : float, default: None\n    All bins that has count less than *cmin* or more than *cmax* will not be\n    displayed (set to NaN before passing to `~.Axes.pcolormesh`) and these count\n    values in the return value count histogram will also be set to nan upon\n    return.\n\nReturns\n-------\nh : 2D array\n    The bi-dimensional histogram of samples x and y. Values in x are\n    histogrammed along the first dimension and values in y are\n    histogrammed along the second dimension.\nxedges : 1D array\n    The bin edges along the x-axis.\nyedges : 1D array\n    The bin edges along the y-axis.\nimage : `~.matplotlib.collections.QuadMesh`\n\nOther Parameters\n----------------\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    The Colormap instance or registered colormap name used to map scalar data\n    to colors.\n\nnorm : str or `~matplotlib.colors.Normalize`, optional\n    The normalization method used to scale scalar data to the [0, 1] range\n    before mapping to colors using *cmap*. By default, a linear scaling is\n    used, mapping the lowest value to 0 and the highest to 1.\n\n    If given, this can be one of the following:\n\n    - An instance of `.Normalize` or one of its subclasses\n      (see :ref:`colormapnorms`).\n    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n      list of available scales, call `matplotlib.scale.get_scale_names()`.\n      In that case, a suitable `.Normalize` subclass is dynamically generated\n      and instantiated.\n\nvmin, vmax : float, optional\n    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n    the data range that the colormap covers. By default, the colormap covers\n    the complete value range of the supplied data. It is an error to use\n    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n    name together with *vmin*/*vmax* is acceptable).\n\ncolorizer : `~matplotlib.colorizer.Colorizer` or None, default: None\n    The Colorizer object used to map color to data. If None, a Colorizer\n    object is created from a *norm* and *cmap*.\n\nalpha : ``0 <= scalar <= 1`` or ``None``, optional\n    The alpha blending value.\n\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *x*, *y*, *weights*\n\n**kwargs\n    Additional parameters are passed along to the\n    `~.Axes.pcolormesh` method and `~matplotlib.collections.QuadMesh`\n    constructor.\n\nSee Also\n--------\nhist : 1D histogram plotting\nhexbin : 2D histogram with hexagonal bins\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.hist2d`.\n\n- Currently ``hist2d`` calculates its own axis limits, and any limits\n  previously set are ignored.\n- Rendering the histogram with a logarithmic color scale is\n  accomplished by passing a `.colors.LogNorm` instance to the *norm*\n  keyword argument. Likewise, power-law normalization (similar\n  in effect to gamma correction) can be accomplished with\n  `.colors.PowerNorm`.",
    "matplotlib.pyplot.hlines": "Plot horizontal lines at each *y* from *xmin* to *xmax*.\n\nParameters\n----------\ny : float or array-like\n    y-indexes where to plot the lines.\n\nxmin, xmax : float or array-like\n    Respective beginning and end of each line. If scalars are\n    provided, all lines will have the same length.\n\ncolors : :mpltype:`color` or list of color , default: :rc:`lines.color`\n\nlinestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, default: 'solid'\n\nlabel : str, default: ''\n\nReturns\n-------\n`~matplotlib.collections.LineCollection`\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *y*, *xmin*, *xmax*, *colors*\n**kwargs :  `~matplotlib.collections.LineCollection` properties.\n\nSee Also\n--------\nvlines : vertical lines\naxhline : horizontal line across the Axes\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.hlines`.\n",
    "matplotlib.pyplot.hot": "\n    Set the colormap to 'hot'.\n\n    This changes the default colormap as well as the colormap of the current\n    image if there is one. See ``help(colormaps)`` for more information.\n    ",
    "matplotlib.pyplot.hsv": "\n    Set the colormap to 'hsv'.\n\n    This changes the default colormap as well as the colormap of the current\n    image if there is one. See ``help(colormaps)`` for more information.\n    ",
    "matplotlib.pyplot.imread": "Read an image from a file into an array.\n\n.. note::\n\n    This function exists for historical reasons.  It is recommended to\n    use `PIL.Image.open` instead for loading images.\n\nParameters\n----------\nfname : str or file-like\n    The image file to read: a filename, a URL or a file-like object opened\n    in read-binary mode.\n\n    Passing a URL is deprecated.  Please open the URL\n    for reading and pass the result to Pillow, e.g. with\n    ``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\nformat : str, optional\n    The image file format assumed for reading the data.  The image is\n    loaded as a PNG file if *format* is set to \"png\", if *fname* is a path\n    or opened file with a \".png\" extension, or if it is a URL.  In all\n    other cases, *format* is ignored and the format is auto-detected by\n    `PIL.Image.open`.\n\nReturns\n-------\n`numpy.array`\n    The image data. The returned array has shape\n\n    - (M, N) for grayscale images.\n    - (M, N, 3) for RGB images.\n    - (M, N, 4) for RGBA images.\n\n    PNG images are returned as float arrays (0-1).  All other formats are\n    returned as int arrays, with a bit depth determined by the file's\n    contents.\n\nNotes\n-----\n\n.. note::\n\n    This is equivalent to `matplotlib.image.imread`.\n",
    "matplotlib.pyplot.imsave": "Colormap and save an array as an image file.\n\nRGB(A) images are passed through.  Single channel images will be\ncolormapped according to *cmap* and *norm*.\n\n.. note::\n\n   If you want to save a single channel image as gray scale please use an\n   image I/O library (such as pillow, tifffile, or imageio) directly.\n\nParameters\n----------\nfname : str or path-like or file-like\n    A path or a file-like object to store the image in.\n    If *format* is not set, then the output format is inferred from the\n    extension of *fname*, if any, and from :rc:`savefig.format` otherwise.\n    If *format* is set, it determines the output format.\narr : array-like\n    The image data. The shape can be one of\n    MxN (luminance), MxNx3 (RGB) or MxNx4 (RGBA).\nvmin, vmax : float, optional\n    *vmin* and *vmax* set the color scaling for the image by fixing the\n    values that map to the colormap color limits. If either *vmin*\n    or *vmax* is None, that limit is determined from the *arr*\n    min/max value.\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    A Colormap instance or registered colormap name. The colormap\n    maps scalar data to colors. It is ignored for RGB(A) data.\nformat : str, optional\n    The file format, e.g. 'png', 'pdf', 'svg', ...  The behavior when this\n    is unset is documented under *fname*.\norigin : {'upper', 'lower'}, default: :rc:`image.origin`\n    Indicates whether the ``(0, 0)`` index of the array is in the upper\n    left or lower left corner of the Axes.\ndpi : float\n    The DPI to store in the metadata of the file.  This does not affect the\n    resolution of the output image.  Depending on file format, this may be\n    rounded to the nearest integer.\nmetadata : dict, optional\n    Metadata in the image file.  The supported keys depend on the output\n    format, see the documentation of the respective backends for more\n    information.\n    Currently only supported for \"png\", \"pdf\", \"ps\", \"eps\", and \"svg\".\npil_kwargs : dict, optional\n    Keyword arguments passed to `PIL.Image.Image.save`.  If the 'pnginfo'\n    key is present, it completely overrides *metadata*, including the\n    default 'Software' key.\n\nNotes\n-----\n\n.. note::\n\n    This is equivalent to `matplotlib.image.imsave`.\n",
    "matplotlib.pyplot.imshow": "Display data as an image, i.e., on a 2D regular raster.\n\nThe input may either be actual RGB(A) data, or 2D scalar data, which\nwill be rendered as a pseudocolor image. For displaying a grayscale\nimage, set up the colormapping using the parameters\n``cmap='gray', vmin=0, vmax=255``.\n\nThe number of pixels used to render an image is set by the Axes size\nand the figure *dpi*. This can lead to aliasing artifacts when\nthe image is resampled, because the displayed image size will usually\nnot match the size of *X* (see\n:doc:`/gallery/images_contours_and_fields/image_antialiasing`).\nThe resampling can be controlled via the *interpolation* parameter\nand/or :rc:`image.interpolation`.\n\nParameters\n----------\nX : array-like or PIL image\n    The image data. Supported array shapes are:\n\n    - (M, N): an image with scalar data. The values are mapped to\n      colors using normalization and a colormap. See parameters *norm*,\n      *cmap*, *vmin*, *vmax*.\n    - (M, N, 3): an image with RGB values (0-1 float or 0-255 int).\n    - (M, N, 4): an image with RGBA values (0-1 float or 0-255 int),\n      i.e. including transparency.\n\n    The first two dimensions (M, N) define the rows and columns of\n    the image.\n\n    Out-of-range RGB(A) values are clipped.\n\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    The Colormap instance or registered colormap name used to map scalar data\n    to colors.\n\n    This parameter is ignored if *X* is RGB(A).\n\nnorm : str or `~matplotlib.colors.Normalize`, optional\n    The normalization method used to scale scalar data to the [0, 1] range\n    before mapping to colors using *cmap*. By default, a linear scaling is\n    used, mapping the lowest value to 0 and the highest to 1.\n\n    If given, this can be one of the following:\n\n    - An instance of `.Normalize` or one of its subclasses\n      (see :ref:`colormapnorms`).\n    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n      list of available scales, call `matplotlib.scale.get_scale_names()`.\n      In that case, a suitable `.Normalize` subclass is dynamically generated\n      and instantiated.\n\n    This parameter is ignored if *X* is RGB(A).\n\nvmin, vmax : float, optional\n    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n    the data range that the colormap covers. By default, the colormap covers\n    the complete value range of the supplied data. It is an error to use\n    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n    name together with *vmin*/*vmax* is acceptable).\n\n    This parameter is ignored if *X* is RGB(A).\n\ncolorizer : `~matplotlib.colorizer.Colorizer` or None, default: None\n    The Colorizer object used to map color to data. If None, a Colorizer\n    object is created from a *norm* and *cmap*.\n\n    This parameter is ignored if *X* is RGB(A).\n\naspect : {'equal', 'auto'} or float or None, default: None\n    The aspect ratio of the Axes.  This parameter is particularly\n    relevant for images since it determines whether data pixels are\n    square.\n\n    This parameter is a shortcut for explicitly calling\n    `.Axes.set_aspect`. See there for further details.\n\n    - 'equal': Ensures an aspect ratio of 1. Pixels will be square\n      (unless pixel sizes are explicitly made non-square in data\n      coordinates using *extent*).\n    - 'auto': The Axes is kept fixed and the aspect is adjusted so\n      that the data fit in the Axes. In general, this will result in\n      non-square pixels.\n\n    Normally, None (the default) means to use :rc:`image.aspect`.  However, if\n    the image uses a transform that does not contain the axes data transform,\n    then None means to not modify the axes aspect at all (in that case, directly\n    call `.Axes.set_aspect` if desired).\n\ninterpolation : str, default: :rc:`image.interpolation`\n    The interpolation method used.\n\n    Supported values are 'none', 'auto', 'nearest', 'bilinear',\n    'bicubic', 'spline16', 'spline36', 'hanning', 'hamming', 'hermite',\n    'kaiser', 'quadric', 'catrom', 'gaussian', 'bessel', 'mitchell',\n    'sinc', 'lanczos', 'blackman'.\n\n    The data *X* is resampled to the pixel size of the image on the\n    figure canvas, using the interpolation method to either up- or\n    downsample the data.\n\n    If *interpolation* is 'none', then for the ps, pdf, and svg\n    backends no down- or upsampling occurs, and the image data is\n    passed to the backend as a native image.  Note that different ps,\n    pdf, and svg viewers may display these raw pixels differently. On\n    other backends, 'none' is the same as 'nearest'.\n\n    If *interpolation* is the default 'auto', then 'nearest'\n    interpolation is used if the image is upsampled by more than a\n    factor of three (i.e. the number of display pixels is at least\n    three times the size of the data array).  If the upsampling rate is\n    smaller than 3, or the image is downsampled, then 'hanning'\n    interpolation is used to act as an anti-aliasing filter, unless the\n    image happens to be upsampled by exactly a factor of two or one.\n\n    See\n    :doc:`/gallery/images_contours_and_fields/interpolation_methods`\n    for an overview of the supported interpolation methods, and\n    :doc:`/gallery/images_contours_and_fields/image_antialiasing` for\n    a discussion of image antialiasing.\n\n    Some interpolation methods require an additional radius parameter,\n    which can be set by *filterrad*. Additionally, the antigrain image\n    resize filter is controlled by the parameter *filternorm*.\n\ninterpolation_stage : {'auto', 'data', 'rgba'}, default: 'auto'\n    Supported values:\n\n    - 'data': Interpolation is carried out on the data provided by the user\n      This is useful if interpolating between pixels during upsampling.\n    - 'rgba': The interpolation is carried out in RGBA-space after the\n      color-mapping has been applied. This is useful if downsampling and\n      combining pixels visually.\n    - 'auto': Select a suitable interpolation stage automatically. This uses\n      'rgba' when downsampling, or upsampling at a rate less than 3, and\n      'data' when upsampling at a higher rate.\n\n    See :doc:`/gallery/images_contours_and_fields/image_antialiasing` for\n    a discussion of image antialiasing.\n\nalpha : float or array-like, optional\n    The alpha blending value, between 0 (transparent) and 1 (opaque).\n    If *alpha* is an array, the alpha blending values are applied pixel\n    by pixel, and *alpha* must have the same shape as *X*.\n\norigin : {'upper', 'lower'}, default: :rc:`image.origin`\n    Place the [0, 0] index of the array in the upper left or lower\n    left corner of the Axes. The convention (the default) 'upper' is\n    typically used for matrices and images.\n\n    Note that the vertical axis points upward for 'lower'\n    but downward for 'upper'.\n\n    See the :ref:`imshow_extent` tutorial for\n    examples and a more detailed description.\n\nextent : floats (left, right, bottom, top), optional\n    The bounding box in data coordinates that the image will fill.\n    These values may be unitful and match the units of the Axes.\n    The image is stretched individually along x and y to fill the box.\n\n    The default extent is determined by the following conditions.\n    Pixels have unit size in data coordinates. Their centers are on\n    integer coordinates, and their center coordinates range from 0 to\n    columns-1 horizontally and from 0 to rows-1 vertically.\n\n    Note that the direction of the vertical axis and thus the default\n    values for top and bottom depend on *origin*:\n\n    - For ``origin == 'upper'`` the default is\n      ``(-0.5, numcols-0.5, numrows-0.5, -0.5)``.\n    - For ``origin == 'lower'`` the default is\n      ``(-0.5, numcols-0.5, -0.5, numrows-0.5)``.\n\n    See the :ref:`imshow_extent` tutorial for\n    examples and a more detailed description.\n\nfilternorm : bool, default: True\n    A parameter for the antigrain image resize filter (see the\n    antigrain documentation).  If *filternorm* is set, the filter\n    normalizes integer values and corrects the rounding errors. It\n    doesn't do anything with the source floating point values, it\n    corrects only integers according to the rule of 1.0 which means\n    that any sum of pixel weights must be equal to 1.0.  So, the\n    filter function must produce a graph of the proper shape.\n\nfilterrad : float > 0, default: 4.0\n    The filter radius for filters that have a radius parameter, i.e.\n    when interpolation is one of: 'sinc', 'lanczos' or 'blackman'.\n\nresample : bool, default: :rc:`image.resample`\n    When *True*, use a full resampling method.  When *False*, only\n    resample when the output image is larger than the input image.\n\nurl : str, optional\n    Set the url of the created `.AxesImage`. See `.Artist.set_url`.\n\nReturns\n-------\n`~matplotlib.image.AxesImage`\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``.\n\n**kwargs : `~matplotlib.artist.Artist` properties\n    These parameters are passed on to the constructor of the\n    `.AxesImage` artist.\n\nSee Also\n--------\nmatshow : Plot a matrix or an array as an image.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.imshow`.\n\nUnless *extent* is used, pixel centers will be located at integer\ncoordinates. In other words: the origin will coincide with the center\nof pixel (0, 0).\n\nThere are two common representations for RGB images with an alpha\nchannel:\n\n-   Straight (unassociated) alpha: R, G, and B channels represent the\n    color of the pixel, disregarding its opacity.\n-   Premultiplied (associated) alpha: R, G, and B channels represent\n    the color of the pixel, adjusted for its opacity by multiplication.\n\n`~matplotlib.pyplot.imshow` expects RGB images adopting the straight\n(unassociated) alpha representation.",
    "matplotlib.pyplot.inferno": "\n    Set the colormap to 'inferno'.\n\n    This changes the default colormap as well as the colormap of the current\n    image if there is one. See ``help(colormaps)`` for more information.\n    ",
    "matplotlib.pyplot.install_repl_displayhook": "\n    Connect to the display hook of the current shell.\n\n    The display hook gets called when the read-evaluate-print-loop (REPL) of\n    the shell has finished the execution of a command. We use this callback\n    to be able to automatically update a figure in interactive mode.\n\n    This works both with IPython and with vanilla python shells.\n    ",
    "matplotlib.pyplot.interactive": "\n    Set whether to redraw after every plotting command (e.g. `.pyplot.xlabel`).\n    ",
    "matplotlib.pyplot.ioff": "\n    Disable interactive mode.\n\n    See `.pyplot.isinteractive` for more details.\n\n    See Also\n    --------\n    ion : Enable interactive mode.\n    isinteractive : Whether interactive mode is enabled.\n    show : Show all figures (and maybe block).\n    pause : Show all figures, and block for a time.\n\n    Notes\n    -----\n    For a temporary change, this can be used as a context manager::\n\n        # if interactive mode is on\n        # then figures will be shown on creation\n        plt.ion()\n        # This figure will be shown immediately\n        fig = plt.figure()\n\n        with plt.ioff():\n            # interactive mode will be off\n            # figures will not automatically be shown\n            fig2 = plt.figure()\n            # ...\n\n    To enable optional usage as a context manager, this function returns a\n    context manager object, which is not intended to be stored or\n    accessed by the user.\n    ",
    "matplotlib.pyplot.ion": "\n    Enable interactive mode.\n\n    See `.pyplot.isinteractive` for more details.\n\n    See Also\n    --------\n    ioff : Disable interactive mode.\n    isinteractive : Whether interactive mode is enabled.\n    show : Show all figures (and maybe block).\n    pause : Show all figures, and block for a time.\n\n    Notes\n    -----\n    For a temporary change, this can be used as a context manager::\n\n        # if interactive mode is off\n        # then figures will not be shown on creation\n        plt.ioff()\n        # This figure will not be shown immediately\n        fig = plt.figure()\n\n        with plt.ion():\n            # interactive mode will be on\n            # figures will automatically be shown\n            fig2 = plt.figure()\n            # ...\n\n    To enable optional usage as a context manager, this function returns a\n    context manager object, which is not intended to be stored or\n    accessed by the user.\n    ",
    "matplotlib.pyplot.isinteractive": "\n    Return whether plots are updated after every plotting command.\n\n    The interactive mode is mainly useful if you build plots from the command\n    line and want to see the effect of each command while you are building the\n    figure.\n\n    In interactive mode:\n\n    - newly created figures will be shown immediately;\n    - figures will automatically redraw on change;\n    - `.pyplot.show` will not block by default.\n\n    In non-interactive mode:\n\n    - newly created figures and changes to figures will not be reflected until\n      explicitly asked to be;\n    - `.pyplot.show` will block by default.\n\n    See Also\n    --------\n    ion : Enable interactive mode.\n    ioff : Disable interactive mode.\n    show : Show all figures (and maybe block).\n    pause : Show all figures, and block for a time.\n    ",
    "matplotlib.pyplot.jet": "\n    Set the colormap to 'jet'.\n\n    This changes the default colormap as well as the colormap of the current\n    image if there is one. See ``help(colormaps)`` for more information.\n    ",
    "matplotlib.pyplot.legend": "Place a legend on the Axes.\n\nCall signatures::\n\n    legend()\n    legend(handles, labels)\n    legend(handles=handles)\n    legend(labels)\n\nThe call signatures correspond to the following different ways to use\nthis method:\n\n**1. Automatic detection of elements to be shown in the legend**\n\nThe elements to be added to the legend are automatically determined,\nwhen you do not pass in any extra arguments.\n\nIn this case, the labels are taken from the artist. You can specify\nthem either at artist creation or by calling the\n:meth:`~.Artist.set_label` method on the artist::\n\n    ax.plot([1, 2, 3], label='Inline label')\n    ax.legend()\n\nor::\n\n    line, = ax.plot([1, 2, 3])\n    line.set_label('Label via method')\n    ax.legend()\n\n.. note::\n    Specific artists can be excluded from the automatic legend element\n    selection by using a label starting with an underscore, \"_\".\n    A string starting with an underscore is the default label for all\n    artists, so calling `.Axes.legend` without any arguments and\n    without setting the labels manually will result in a ``UserWarning``\n    and an empty legend being drawn.\n\n\n**2. Explicitly listing the artists and labels in the legend**\n\nFor full control of which artists have a legend entry, it is possible\nto pass an iterable of legend artists followed by an iterable of\nlegend labels respectively::\n\n    ax.legend([line1, line2, line3], ['label1', 'label2', 'label3'])\n\n\n**3. Explicitly listing the artists in the legend**\n\nThis is similar to 2, but the labels are taken from the artists'\nlabel properties. Example::\n\n    line1, = ax.plot([1, 2, 3], label='label1')\n    line2, = ax.plot([1, 2, 3], label='label2')\n    ax.legend(handles=[line1, line2])\n\n\n**4. Labeling existing plot elements**\n\n.. admonition:: Discouraged\n\n    This call signature is discouraged, because the relation between\n    plot elements and labels is only implicit by their order and can\n    easily be mixed up.\n\nTo make a legend for all artists on an Axes, call this function with\nan iterable of strings, one for each legend item. For example::\n\n    ax.plot([1, 2, 3])\n    ax.plot([5, 6, 7])\n    ax.legend(['First line', 'Second line'])\n\n\nParameters\n----------\nhandles : list of (`.Artist` or tuple of `.Artist`), optional\n    A list of Artists (lines, patches) to be added to the legend.\n    Use this together with *labels*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\n    The length of handles and labels should be the same in this\n    case. If they are not, they are truncated to the smaller length.\n\n    If an entry contains a tuple, then the legend handler for all Artists in the\n    tuple will be placed alongside a single label.\n\nlabels : list of str, optional\n    A list of labels to show next to the artists.\n    Use this together with *handles*, if you need full control on what\n    is shown in the legend and the automatic mechanism described above\n    is not sufficient.\n\nReturns\n-------\n`~matplotlib.legend.Legend`\n\nOther Parameters\n----------------\n\nloc : str or pair of floats, default: :rc:`legend.loc`\n    The location of the legend.\n\n    The strings ``'upper left'``, ``'upper right'``, ``'lower left'``,\n    ``'lower right'`` place the legend at the corresponding corner of the\n    axes.\n\n    The strings ``'upper center'``, ``'lower center'``, ``'center left'``,\n    ``'center right'`` place the legend at the center of the corresponding edge\n    of the axes.\n\n    The string ``'center'`` places the legend at the center of the axes.\n\n    The string ``'best'`` places the legend at the location, among the nine\n    locations defined so far, with the minimum overlap with other drawn\n    artists.  This option can be quite slow for plots with large amounts of\n    data; your plotting speed may benefit from providing a specific location.\n\n    The location can also be a 2-tuple giving the coordinates of the lower-left\n    corner of the legend in axes coordinates (in which case *bbox_to_anchor*\n    will be ignored).\n\n    For back-compatibility, ``'center right'`` (but no other location) can also\n    be spelled ``'right'``, and each \"string\" location can also be given as a\n    numeric value:\n\n    ==================   =============\n    Location String      Location Code\n    ==================   =============\n    'best' (Axes only)   0\n    'upper right'        1\n    'upper left'         2\n    'lower left'         3\n    'lower right'        4\n    'right'              5\n    'center left'        6\n    'center right'       7\n    'lower center'       8\n    'upper center'       9\n    'center'             10\n    ==================   =============\n    \nbbox_to_anchor : `.BboxBase`, 2-tuple, or 4-tuple of floats\n    Box that is used to position the legend in conjunction with *loc*.\n    Defaults to ``axes.bbox`` (if called as a method to `.Axes.legend`) or\n    ``figure.bbox`` (if ``figure.legend``).  This argument allows arbitrary\n    placement of the legend.\n\n    Bbox coordinates are interpreted in the coordinate system given by\n    *bbox_transform*, with the default transform\n    Axes or Figure coordinates, depending on which ``legend`` is called.\n\n    If a 4-tuple or `.BboxBase` is given, then it specifies the bbox\n    ``(x, y, width, height)`` that the legend is placed in.\n    To put the legend in the best location in the bottom right\n    quadrant of the Axes (or figure)::\n\n        loc='best', bbox_to_anchor=(0.5, 0., 0.5, 0.5)\n\n    A 2-tuple ``(x, y)`` places the corner of the legend specified by *loc* at\n    x, y.  For example, to put the legend's upper right-hand corner in the\n    center of the Axes (or figure) the following keywords can be used::\n\n        loc='upper right', bbox_to_anchor=(0.5, 0.5)\n\nncols : int, default: 1\n    The number of columns that the legend has.\n\n    For backward compatibility, the spelling *ncol* is also supported\n    but it is discouraged. If both are given, *ncols* takes precedence.\n\nprop : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend. If None (default), the current\n    :data:`matplotlib.rcParams` will be used.\n\nfontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n    The font size of the legend. If the value is numeric the size will be the\n    absolute font size in points. String values are relative to the current\n    default font size. This argument is only used if *prop* is not specified.\n\nlabelcolor : str or list, default: :rc:`legend.labelcolor`\n    The color of the text in the legend. Either a valid color string\n    (for example, 'red'), or a list of color strings. The labelcolor can\n    also be made to match the color of the line or marker using 'linecolor',\n    'markerfacecolor' (or 'mfc'), or 'markeredgecolor' (or 'mec').\n\n    Labelcolor can be set globally using :rc:`legend.labelcolor`. If None,\n    use :rc:`text.color`.\n\nnumpoints : int, default: :rc:`legend.numpoints`\n    The number of marker points in the legend when creating a legend\n    entry for a `.Line2D` (line).\n\nscatterpoints : int, default: :rc:`legend.scatterpoints`\n    The number of marker points in the legend when creating\n    a legend entry for a `.PathCollection` (scatter plot).\n\nscatteryoffsets : iterable of floats, default: ``[0.375, 0.5, 0.3125]``\n    The vertical offset (relative to the font size) for the markers\n    created for a scatter plot legend entry. 0.0 is at the base the\n    legend text, and 1.0 is at the top. To draw all markers at the\n    same height, set to ``[0.5]``.\n\nmarkerscale : float, default: :rc:`legend.markerscale`\n    The relative size of legend markers compared to the originally drawn ones.\n\nmarkerfirst : bool, default: True\n    If *True*, legend marker is placed to the left of the legend label.\n    If *False*, legend marker is placed to the right of the legend label.\n\nreverse : bool, default: False\n    If *True*, the legend labels are displayed in reverse order from the input.\n    If *False*, the legend labels are displayed in the same order as the input.\n\n    .. versionadded:: 3.7\n\nframeon : bool, default: :rc:`legend.frameon`\n    Whether the legend should be drawn on a patch (frame).\n\nfancybox : bool, default: :rc:`legend.fancybox`\n    Whether round edges should be enabled around the `.FancyBboxPatch` which\n    makes up the legend's background.\n\nshadow : None, bool or dict, default: :rc:`legend.shadow`\n    Whether to draw a shadow behind the legend.\n    The shadow can be configured using `.Patch` keywords.\n    Customization via :rc:`legend.shadow` is currently not supported.\n\nframealpha : float, default: :rc:`legend.framealpha`\n    The alpha transparency of the legend's background.\n    If *shadow* is activated and *framealpha* is ``None``, the default value is\n    ignored.\n\nfacecolor : \"inherit\" or color, default: :rc:`legend.facecolor`\n    The legend's background color.\n    If ``\"inherit\"``, use :rc:`axes.facecolor`.\n\nedgecolor : \"inherit\" or color, default: :rc:`legend.edgecolor`\n    The legend's background patch edge color.\n    If ``\"inherit\"``, use :rc:`axes.edgecolor`.\n\nmode : {\"expand\", None}\n    If *mode* is set to ``\"expand\"`` the legend will be horizontally\n    expanded to fill the Axes area (or *bbox_to_anchor* if defines\n    the legend's size).\n\nbbox_transform : None or `~matplotlib.transforms.Transform`\n    The transform for the bounding box (*bbox_to_anchor*). For a value\n    of ``None`` (default) the Axes'\n    :data:`~matplotlib.axes.Axes.transAxes` transform will be used.\n\ntitle : str or None\n    The legend's title. Default is no title (``None``).\n\ntitle_fontproperties : None or `~matplotlib.font_manager.FontProperties` or dict\n    The font properties of the legend's title. If None (default), the\n    *title_fontsize* argument will be used if present; if *title_fontsize* is\n    also None, the current :rc:`legend.title_fontsize` will be used.\n\ntitle_fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}, default: :rc:`legend.title_fontsize`\n    The font size of the legend's title.\n    Note: This cannot be combined with *title_fontproperties*. If you want\n    to set the fontsize alongside other font properties, use the *size*\n    parameter in *title_fontproperties*.\n\nalignment : {'center', 'left', 'right'}, default: 'center'\n    The alignment of the legend title and the box of entries. The entries\n    are aligned as a single block, so that markers always lined up.\n\nborderpad : float, default: :rc:`legend.borderpad`\n    The fractional whitespace inside the legend border, in font-size units.\n\nlabelspacing : float, default: :rc:`legend.labelspacing`\n    The vertical space between the legend entries, in font-size units.\n\nhandlelength : float, default: :rc:`legend.handlelength`\n    The length of the legend handles, in font-size units.\n\nhandleheight : float, default: :rc:`legend.handleheight`\n    The height of the legend handles, in font-size units.\n\nhandletextpad : float, default: :rc:`legend.handletextpad`\n    The pad between the legend handle and text, in font-size units.\n\nborderaxespad : float, default: :rc:`legend.borderaxespad`\n    The pad between the Axes and legend border, in font-size units.\n\ncolumnspacing : float, default: :rc:`legend.columnspacing`\n    The spacing between columns, in font-size units.\n\nhandler_map : dict or None\n    The custom dictionary mapping instances or types to a legend\n    handler. This *handler_map* updates the default handler map\n    found at `matplotlib.legend.Legend.get_legend_handler_map`.\n\ndraggable : bool, default: False\n    Whether the legend can be dragged with the mouse.\n\n\nSee Also\n--------\n.Figure.legend\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.legend`.\n\nSome artists are not supported by this function.  See\n:ref:`legend_guide` for details.\n\nExamples\n--------\n.. plot:: gallery/text_labels_and_annotations/legend.py",
    "matplotlib.pyplot.locator_params": "Control behavior of major tick locators.\n\nBecause the locator is involved in autoscaling, `~.Axes.autoscale_view`\nis called automatically after the parameters are changed.\n\nParameters\n----------\naxis : {'both', 'x', 'y'}, default: 'both'\n    The axis on which to operate.  (For 3D Axes, *axis* can also be\n    set to 'z', and 'both' refers to all three axes.)\ntight : bool or None, optional\n    Parameter passed to `~.Axes.autoscale_view`.\n    Default is None, for no change.\n\nOther Parameters\n----------------\n**kwargs\n    Remaining keyword arguments are passed to directly to the\n    ``set_params()`` method of the locator. Supported keywords depend\n    on the type of the locator. See for example\n    `~.ticker.MaxNLocator.set_params` for the `.ticker.MaxNLocator`\n    used by default for linear.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.locator_params`.\n\nExamples\n--------\nWhen plotting small subplots, one might want to reduce the maximum\nnumber of ticks and use tight bounds, for example::\n\n    ax.locator_params(tight=True, nbins=4)",
    "matplotlib.pyplot.loglog": "Make a plot with log scaling on both the x- and y-axis.\n\nCall signatures::\n\n    loglog([x], y, [fmt], data=None, **kwargs)\n    loglog([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThis is just a thin wrapper around `.plot` which additionally changes\nboth the x-axis and the y-axis to log scaling. All the concepts and\nparameters of plot can be used here as well.\n\nThe additional parameters *base*, *subs* and *nonpositive* control the\nx/y-axis properties. They are just forwarded to `.Axes.set_xscale` and\n`.Axes.set_yscale`. To use different properties on the x-axis and the\ny-axis, use e.g.\n``ax.set_xscale(\"log\", base=10); ax.set_yscale(\"log\", base=2)``.\n\nParameters\n----------\nbase : float, default: 10\n    Base of the logarithm.\n\nsubs : sequence, optional\n    The location of the minor ticks. If *None*, reasonable locations\n    are automatically chosen depending on the number of decades in the\n    plot. See `.Axes.set_xscale`/`.Axes.set_yscale` for details.\n\nnonpositive : {'mask', 'clip'}, default: 'clip'\n    Non-positive values can be masked as invalid, or clipped to a very\n    small positive number.\n\n**kwargs\n    All parameters supported by `.plot`.\n\nReturns\n-------\nlist of `.Line2D`\n    Objects representing the plotted data.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.loglog`.\n",
    "matplotlib.pyplot.magma": "\n    Set the colormap to 'magma'.\n\n    This changes the default colormap as well as the colormap of the current\n    image if there is one. See ``help(colormaps)`` for more information.\n    ",
    "matplotlib.pyplot.magnitude_spectrum": "Plot the magnitude spectrum.\n\nCompute the magnitude spectrum of *x*.  Data is padded to a\nlength of *pad_to* and the windowing function *window* is applied to\nthe signal.\n\nParameters\n----------\nx : 1-D array or sequence\n    Array or sequence containing the data.\n\nFs : float, default: 2\n    The sampling frequency (samples per time unit).  It is used to calculate\n    the Fourier frequencies, *freqs*, in cycles per time unit.\n\nwindow : callable or ndarray, default: `.window_hanning`\n    A function or a vector of length *NFFT*.  To create window vectors see\n    `.window_hanning`, `.window_none`, `numpy.blackman`, `numpy.hamming`,\n    `numpy.bartlett`, `scipy.signal`, `scipy.signal.get_window`, etc.  If a\n    function is passed as the argument, it must take a data segment as an\n    argument and return the windowed version of the segment.\n\nsides : {'default', 'onesided', 'twosided'}, optional\n    Which sides of the spectrum to return. 'default' is one-sided for real\n    data and two-sided for complex data. 'onesided' forces the return of a\n    one-sided spectrum, while 'twosided' forces two-sided.\n\npad_to : int, optional\n    The number of points to which the data segment is padded when performing\n    the FFT.  While not increasing the actual resolution of the spectrum (the\n    minimum distance between resolvable peaks), this can give more points in\n    the plot, allowing for more detail. This corresponds to the *n* parameter\n    in the call to `~numpy.fft.fft`.  The default is None, which sets *pad_to*\n    equal to the length of the input signal (i.e. no padding).\n\nscale : {'default', 'linear', 'dB'}\n    The scaling of the values in the *spec*.  'linear' is no scaling.\n    'dB' returns the values in dB scale, i.e., the dB amplitude\n    (20 * log10). 'default' is 'linear'.\n\nFc : int, default: 0\n    The center frequency of *x*, which offsets the x extents of the\n    plot to reflect the frequency range used when a signal is acquired\n    and then filtered and downsampled to baseband.\n\nReturns\n-------\nspectrum : 1-D array\n    The values for the magnitude spectrum before scaling (real valued).\n\nfreqs : 1-D array\n    The frequencies corresponding to the elements in *spectrum*.\n\nline : `~matplotlib.lines.Line2D`\n    The line created by this function.\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *x*\n\n**kwargs\n    Keyword arguments control the `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: :mpltype:`color`\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: :mpltype:`color` or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: :mpltype:`color`\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: :mpltype:`color`\n    markerfacecoloralt or mfcalt: :mpltype:`color`\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\npsd\n    Plots the power spectral density.\nangle_spectrum\n    Plots the angles of the corresponding frequencies.\nphase_spectrum\n    Plots the phase (unwrapped angle) of the corresponding frequencies.\nspecgram\n    Can plot the magnitude spectrum of segments within the signal in a\n    colormap.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.magnitude_spectrum`.\n",
    "matplotlib.pyplot.margins": "Set or retrieve margins around the data for autoscaling axis limits.\n\nThis allows to configure the padding around the data without having to\nset explicit limits using `~.Axes.set_xlim` / `~.Axes.set_ylim`.\n\nAutoscaling determines the axis limits by adding *margin* times the\ndata interval as padding around the data. See the following illustration:\n\n.. plot:: _embedded_plots/axes_margins.py\n\nAll input parameters must be floats greater than -0.5. Passing both\npositional and keyword arguments is invalid and will raise a TypeError.\nIf no arguments (positional or otherwise) are provided, the current\nmargins will remain unchanged and simply be returned.\n\nThe default margins are :rc:`axes.xmargin` and :rc:`axes.ymargin`.\n\nParameters\n----------\n*margins : float, optional\n    If a single positional argument is provided, it specifies\n    both margins of the x-axis and y-axis limits. If two\n    positional arguments are provided, they will be interpreted\n    as *xmargin*, *ymargin*. If setting the margin on a single\n    axis is desired, use the keyword arguments described below.\n\nx, y : float, optional\n    Specific margin values for the x-axis and y-axis,\n    respectively. These cannot be used with positional\n    arguments, but can be used individually to alter on e.g.,\n    only the y-axis.\n\ntight : bool or None, default: True\n    The *tight* parameter is passed to `~.axes.Axes.autoscale_view`,\n    which is executed after a margin is changed; the default\n    here is *True*, on the assumption that when margins are\n    specified, no additional padding to match tick marks is\n    usually desired.  Setting *tight* to *None* preserves\n    the previous setting.\n\nReturns\n-------\nxmargin, ymargin : float\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.margins`.\n\nIf a previously used Axes method such as :meth:`pcolor` has set\n`~.Axes.use_sticky_edges` to `True`, only the limits not set by\nthe \"sticky artists\" will be modified. To force all\nmargins to be set, set `~.Axes.use_sticky_edges` to `False`\nbefore calling :meth:`margins`.\n\nSee Also\n--------\n.Axes.set_xmargin, .Axes.set_ymargin",
    "matplotlib.pyplot.matshow": "\n    Display a 2D array as a matrix in a new figure window.\n\n    The origin is set at the upper left hand corner.\n    The indexing is ``(row, column)`` so that the first index runs vertically\n    and the second index runs horizontally in the figure:\n\n    .. code-block:: none\n\n        A[0, 0]   \u22ef A[0, M-1]\n           \u22ee             \u22ee\n        A[N-1, 0] \u22ef A[N-1, M-1]\n\n    The aspect ratio of the figure window is that of the array,\n    unless this would make an excessively short or narrow figure.\n\n    Tick labels for the xaxis are placed on top.\n\n    Parameters\n    ----------\n    A : 2D array-like\n        The matrix to be displayed.\n\n    fignum : None or int\n        If *None*, create a new, appropriately sized figure window.\n\n        If 0, use the current Axes (creating one if there is none, without ever\n        adjusting the figure size).\n\n        Otherwise, create a new Axes on the figure with the given number\n        (creating it at the appropriate size if it does not exist, but not\n        adjusting the figure size otherwise).  Note that this will be drawn on\n        top of any preexisting Axes on the figure.\n\n    Returns\n    -------\n    `~matplotlib.image.AxesImage`\n\n    Other Parameters\n    ----------------\n    **kwargs : `~matplotlib.axes.Axes.imshow` arguments\n\n    ",
    "matplotlib.pyplot.minorticks_off": "Remove minor ticks from the Axes.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.minorticks_off`.\n",
    "matplotlib.pyplot.minorticks_on": "Display minor ticks on the Axes.\n\nDisplaying minor ticks may reduce performance; you may turn them off\nusing `minorticks_off()` if drawing speed is a problem.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.minorticks_on`.\n",
    "matplotlib.pyplot.new_figure_manager": "Create a new figure manager instance.",
    "matplotlib.pyplot.nipy_spectral": "\n    Set the colormap to 'nipy_spectral'.\n\n    This changes the default colormap as well as the colormap of the current\n    image if there is one. See ``help(colormaps)`` for more information.\n    ",
    "matplotlib.pyplot.overload": "Decorator for overloaded functions/methods.\n\n    In a stub file, place two or more stub definitions for the same\n    function in a row, each decorated with @overload.\n\n    For example::\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n\n    In a non-stub file (i.e. a regular .py file), do the same but\n    follow it with an implementation.  The implementation should *not*\n    be decorated with @overload::\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n        def utf8(value):\n            ...  # implementation goes here\n\n    The overloads for a function can be retrieved at runtime using the\n    get_overloads() function.\n    ",
    "matplotlib.pyplot.pause": "\n    Run the GUI event loop for *interval* seconds.\n\n    If there is an active figure, it will be updated and displayed before the\n    pause, and the GUI event loop (if any) will run during the pause.\n\n    This can be used for crude animation.  For more complex animation use\n    :mod:`matplotlib.animation`.\n\n    If there is no active figure, sleep for *interval* seconds instead.\n\n    See Also\n    --------\n    matplotlib.animation : Proper animations\n    show : Show all figures and optional block until all figures are closed.\n    ",
    "matplotlib.pyplot.pcolor": "Create a pseudocolor plot with a non-regular rectangular grid.\n\nCall signature::\n\n    pcolor([X, Y,] C, /, **kwargs)\n\n*X* and *Y* can be used to specify the corners of the quadrilaterals.\n\nThe arguments *X*, *Y*, *C* are positional-only.\n\n.. hint::\n\n    ``pcolor()`` can be very slow for large arrays. In most\n    cases you should use the similar but much faster\n    `~.Axes.pcolormesh` instead. See\n    :ref:`Differences between pcolor() and pcolormesh()\n    <differences-pcolor-pcolormesh>` for a discussion of the\n    differences.\n\nParameters\n----------\nC : 2D array-like\n    The color-mapped values.  Color-mapping is controlled by *cmap*,\n    *norm*, *vmin*, and *vmax*.\n\nX, Y : array-like, optional\n    The coordinates of the corners of quadrilaterals of a pcolormesh::\n\n        (X[i+1, j], Y[i+1, j])       (X[i+1, j+1], Y[i+1, j+1])\n                              \u25cf\u2576\u2500\u2500\u2500\u2574\u25cf\n                              \u2502     \u2502\n                              \u25cf\u2576\u2500\u2500\u2500\u2574\u25cf\n            (X[i, j], Y[i, j])       (X[i, j+1], Y[i, j+1])\n\n    Note that the column index corresponds to the x-coordinate, and\n    the row index corresponds to y. For details, see the\n    :ref:`Notes <axes-pcolormesh-grid-orientation>` section below.\n\n    If ``shading='flat'`` the dimensions of *X* and *Y* should be one\n    greater than those of *C*, and the quadrilateral is colored due\n    to the value at ``C[i, j]``.  If *X*, *Y* and *C* have equal\n    dimensions, a warning will be raised and the last row and column\n    of *C* will be ignored.\n\n    If ``shading='nearest'``, the dimensions of *X* and *Y* should be\n    the same as those of *C* (if not, a ValueError will be raised). The\n    color ``C[i, j]`` will be centered on ``(X[i, j], Y[i, j])``.\n\n    If *X* and/or *Y* are 1-D arrays or column vectors they will be\n    expanded as needed into the appropriate 2D arrays, making a\n    rectangular grid.\n\nshading : {'flat', 'nearest', 'auto'}, default: :rc:`pcolor.shading`\n    The fill style for the quadrilateral. Possible values:\n\n    - 'flat': A solid color is used for each quad. The color of the\n      quad (i, j), (i+1, j), (i, j+1), (i+1, j+1) is given by\n      ``C[i, j]``. The dimensions of *X* and *Y* should be\n      one greater than those of *C*; if they are the same as *C*,\n      then a deprecation warning is raised, and the last row\n      and column of *C* are dropped.\n    - 'nearest': Each grid point will have a color centered on it,\n      extending halfway between the adjacent grid centers.  The\n      dimensions of *X* and *Y* must be the same as *C*.\n    - 'auto': Choose 'flat' if dimensions of *X* and *Y* are one\n      larger than *C*.  Choose 'nearest' if dimensions are the same.\n\n    See :doc:`/gallery/images_contours_and_fields/pcolormesh_grids`\n    for more description.\n\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    The Colormap instance or registered colormap name used to map scalar data\n    to colors.\n\nnorm : str or `~matplotlib.colors.Normalize`, optional\n    The normalization method used to scale scalar data to the [0, 1] range\n    before mapping to colors using *cmap*. By default, a linear scaling is\n    used, mapping the lowest value to 0 and the highest to 1.\n\n    If given, this can be one of the following:\n\n    - An instance of `.Normalize` or one of its subclasses\n      (see :ref:`colormapnorms`).\n    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n      list of available scales, call `matplotlib.scale.get_scale_names()`.\n      In that case, a suitable `.Normalize` subclass is dynamically generated\n      and instantiated.\n\nvmin, vmax : float, optional\n    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n    the data range that the colormap covers. By default, the colormap covers\n    the complete value range of the supplied data. It is an error to use\n    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n    name together with *vmin*/*vmax* is acceptable).\n\ncolorizer : `~matplotlib.colorizer.Colorizer` or None, default: None\n    The Colorizer object used to map color to data. If None, a Colorizer\n    object is created from a *norm* and *cmap*.\n\nedgecolors : {'none', None, 'face', color, color sequence}, optional\n    The color of the edges. Defaults to 'none'. Possible values:\n\n    - 'none' or '': No edge.\n    - *None*: :rc:`patch.edgecolor` will be used. Note that currently\n      :rc:`patch.force_edgecolor` has to be True for this to work.\n    - 'face': Use the adjacent face color.\n    - A color or sequence of colors will set the edge color.\n\n    The singular form *edgecolor* works as an alias.\n\nalpha : float, default: None\n    The alpha blending value of the face color, between 0 (transparent)\n    and 1 (opaque). Note: The edgecolor is currently not affected by\n    this.\n\nsnap : bool, default: False\n    Whether to snap the mesh to pixel boundaries.\n\nReturns\n-------\n`matplotlib.collections.PolyQuadMesh`\n\nOther Parameters\n----------------\nantialiaseds : bool, default: False\n    The default *antialiaseds* is False if the default\n    *edgecolors*\\ =\"none\" is used.  This eliminates artificial lines\n    at patch boundaries, and works regardless of the value of alpha.\n    If *edgecolors* is not \"none\", then the default *antialiaseds*\n    is taken from :rc:`patch.antialiased`.\n    Stroking the edges may be preferred if *alpha* is 1, but will\n    cause artifacts otherwise.\n\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``.\n\n**kwargs\n    Additionally, the following arguments are allowed. They are passed\n    along to the `~matplotlib.collections.PolyQuadMesh` constructor:\n\nProperties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: array-like or scalar or None\n    animated: bool\n    antialiased or aa or antialiaseds: bool or list of bools\n    array: array-like or None\n    capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    clim: (vmin: float, vmax: float)\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    cmap: `.Colormap` or str or None\n    color: :mpltype:`color` or list of RGBA tuples\n    edgecolor or ec or edgecolors: :mpltype:`color` or list of :mpltype:`color` or 'face'\n    facecolor or facecolors or fc: :mpltype:`color` or list of :mpltype:`color`\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    gid: str\n    hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n    hatch_linewidth: unknown\n    in_layout: bool\n    joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    label: object\n    linestyle or dashes or linestyles or ls: str or tuple or list thereof\n    linewidth or linewidths or lw: float or list of floats\n    mouseover: bool\n    norm: `.Normalize` or str or None\n    offset_transform or transOffset: `.Transform`\n    offsets: (N, 2) or (2,) array-like\n    path_effects: list of `.AbstractPathEffect`\n    paths: list of array-like\n    picker: None or bool or float or callable\n    pickradius: float\n    rasterized: bool\n    sizes: `numpy.ndarray` or None\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    urls: list of str or None\n    verts: list of array-like\n    verts_and_codes: unknown\n    visible: bool\n    zorder: float\n\nSee Also\n--------\npcolormesh : for an explanation of the differences between\n    pcolor and pcolormesh.\nimshow : If *X* and *Y* are each equidistant, `~.Axes.imshow` can be a\n    faster alternative.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.pcolor`.\n\n**Masked arrays**\n\n*X*, *Y* and *C* may be masked arrays. If either ``C[i, j]``, or one\nof the vertices surrounding ``C[i, j]`` (*X* or *Y* at\n``[i, j], [i+1, j], [i, j+1], [i+1, j+1]``) is masked, nothing is\nplotted.\n\n.. _axes-pcolor-grid-orientation:\n\n**Grid orientation**\n\nThe grid orientation follows the standard matrix convention: An array\n*C* with shape (nrows, ncolumns) is plotted with the column number as\n*X* and the row number as *Y*.",
    "matplotlib.pyplot.pcolormesh": "Create a pseudocolor plot with a non-regular rectangular grid.\n\nCall signature::\n\n    pcolormesh([X, Y,] C, /, **kwargs)\n\n*X* and *Y* can be used to specify the corners of the quadrilaterals.\n\nThe arguments *X*, *Y*, *C* are positional-only.\n\n.. hint::\n\n   `~.Axes.pcolormesh` is similar to `~.Axes.pcolor`. It is much faster\n   and preferred in most cases. For a detailed discussion on the\n   differences see :ref:`Differences between pcolor() and pcolormesh()\n   <differences-pcolor-pcolormesh>`.\n\nParameters\n----------\nC : array-like\n    The mesh data. Supported array shapes are:\n\n    - (M, N) or M*N: a mesh with scalar data. The values are mapped to\n      colors using normalization and a colormap. See parameters *norm*,\n      *cmap*, *vmin*, *vmax*.\n    - (M, N, 3): an image with RGB values (0-1 float or 0-255 int).\n    - (M, N, 4): an image with RGBA values (0-1 float or 0-255 int),\n      i.e. including transparency.\n\n    The first two dimensions (M, N) define the rows and columns of\n    the mesh data.\n\nX, Y : array-like, optional\n    The coordinates of the corners of quadrilaterals of a pcolormesh::\n\n        (X[i+1, j], Y[i+1, j])       (X[i+1, j+1], Y[i+1, j+1])\n                              \u25cf\u2576\u2500\u2500\u2500\u2574\u25cf\n                              \u2502     \u2502\n                              \u25cf\u2576\u2500\u2500\u2500\u2574\u25cf\n            (X[i, j], Y[i, j])       (X[i, j+1], Y[i, j+1])\n\n    Note that the column index corresponds to the x-coordinate, and\n    the row index corresponds to y. For details, see the\n    :ref:`Notes <axes-pcolormesh-grid-orientation>` section below.\n\n    If ``shading='flat'`` the dimensions of *X* and *Y* should be one\n    greater than those of *C*, and the quadrilateral is colored due\n    to the value at ``C[i, j]``.  If *X*, *Y* and *C* have equal\n    dimensions, a warning will be raised and the last row and column\n    of *C* will be ignored.\n\n    If ``shading='nearest'`` or ``'gouraud'``, the dimensions of *X*\n    and *Y* should be the same as those of *C* (if not, a ValueError\n    will be raised).  For ``'nearest'`` the color ``C[i, j]`` is\n    centered on ``(X[i, j], Y[i, j])``.  For ``'gouraud'``, a smooth\n    interpolation is carried out between the quadrilateral corners.\n\n    If *X* and/or *Y* are 1-D arrays or column vectors they will be\n    expanded as needed into the appropriate 2D arrays, making a\n    rectangular grid.\n\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    The Colormap instance or registered colormap name used to map scalar data\n    to colors.\n\nnorm : str or `~matplotlib.colors.Normalize`, optional\n    The normalization method used to scale scalar data to the [0, 1] range\n    before mapping to colors using *cmap*. By default, a linear scaling is\n    used, mapping the lowest value to 0 and the highest to 1.\n\n    If given, this can be one of the following:\n\n    - An instance of `.Normalize` or one of its subclasses\n      (see :ref:`colormapnorms`).\n    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n      list of available scales, call `matplotlib.scale.get_scale_names()`.\n      In that case, a suitable `.Normalize` subclass is dynamically generated\n      and instantiated.\n\nvmin, vmax : float, optional\n    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n    the data range that the colormap covers. By default, the colormap covers\n    the complete value range of the supplied data. It is an error to use\n    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n    name together with *vmin*/*vmax* is acceptable).\n\ncolorizer : `~matplotlib.colorizer.Colorizer` or None, default: None\n    The Colorizer object used to map color to data. If None, a Colorizer\n    object is created from a *norm* and *cmap*.\n\nedgecolors : {'none', None, 'face', color, color sequence}, optional\n    The color of the edges. Defaults to 'none'. Possible values:\n\n    - 'none' or '': No edge.\n    - *None*: :rc:`patch.edgecolor` will be used. Note that currently\n      :rc:`patch.force_edgecolor` has to be True for this to work.\n    - 'face': Use the adjacent face color.\n    - A color or sequence of colors will set the edge color.\n\n    The singular form *edgecolor* works as an alias.\n\nalpha : float, default: None\n    The alpha blending value, between 0 (transparent) and 1 (opaque).\n\nshading : {'flat', 'nearest', 'gouraud', 'auto'}, optional\n    The fill style for the quadrilateral; defaults to\n    :rc:`pcolor.shading`. Possible values:\n\n    - 'flat': A solid color is used for each quad. The color of the\n      quad (i, j), (i+1, j), (i, j+1), (i+1, j+1) is given by\n      ``C[i, j]``. The dimensions of *X* and *Y* should be\n      one greater than those of *C*; if they are the same as *C*,\n      then a deprecation warning is raised, and the last row\n      and column of *C* are dropped.\n    - 'nearest': Each grid point will have a color centered on it,\n      extending halfway between the adjacent grid centers.  The\n      dimensions of *X* and *Y* must be the same as *C*.\n    - 'gouraud': Each quad will be Gouraud shaded: The color of the\n      corners (i', j') are given by ``C[i', j']``. The color values of\n      the area in between is interpolated from the corner values.\n      The dimensions of *X* and *Y* must be the same as *C*. When\n      Gouraud shading is used, *edgecolors* is ignored.\n    - 'auto': Choose 'flat' if dimensions of *X* and *Y* are one\n      larger than *C*.  Choose 'nearest' if dimensions are the same.\n\n    See :doc:`/gallery/images_contours_and_fields/pcolormesh_grids`\n    for more description.\n\nsnap : bool, default: False\n    Whether to snap the mesh to pixel boundaries.\n\nrasterized : bool, optional\n    Rasterize the pcolormesh when drawing vector graphics.  This can\n    speed up rendering and produce smaller files for large data sets.\n    See also :doc:`/gallery/misc/rasterization_demo`.\n\nReturns\n-------\n`matplotlib.collections.QuadMesh`\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``.\n\n**kwargs\n    Additionally, the following arguments are allowed. They are passed\n    along to the `~matplotlib.collections.QuadMesh` constructor:\n\nProperties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: array-like or scalar or None\n    animated: bool\n    antialiased or aa or antialiaseds: bool or list of bools\n    array: array-like\n    capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    clim: (vmin: float, vmax: float)\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    cmap: `.Colormap` or str or None\n    color: :mpltype:`color` or list of RGBA tuples\n    edgecolor or ec or edgecolors: :mpltype:`color` or list of :mpltype:`color` or 'face'\n    facecolor or facecolors or fc: :mpltype:`color` or list of :mpltype:`color`\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    gid: str\n    hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n    hatch_linewidth: unknown\n    in_layout: bool\n    joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    label: object\n    linestyle or dashes or linestyles or ls: str or tuple or list thereof\n    linewidth or linewidths or lw: float or list of floats\n    mouseover: bool\n    norm: `.Normalize` or str or None\n    offset_transform or transOffset: `.Transform`\n    offsets: (N, 2) or (2,) array-like\n    path_effects: list of `.AbstractPathEffect`\n    picker: None or bool or float or callable\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    urls: list of str or None\n    visible: bool\n    zorder: float\n\nSee Also\n--------\npcolor : An alternative implementation with slightly different\n    features. For a detailed discussion on the differences see\n    :ref:`Differences between pcolor() and pcolormesh()\n    <differences-pcolor-pcolormesh>`.\nimshow : If *X* and *Y* are each equidistant, `~.Axes.imshow` can be a\n    faster alternative.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.pcolormesh`.\n\n**Masked arrays**\n\n*C* may be a masked array. If ``C[i, j]`` is masked, the corresponding\nquadrilateral will be transparent. Masking of *X* and *Y* is not\nsupported. Use `~.Axes.pcolor` if you need this functionality.\n\n.. _axes-pcolormesh-grid-orientation:\n\n**Grid orientation**\n\nThe grid orientation follows the standard matrix convention: An array\n*C* with shape (nrows, ncolumns) is plotted with the column number as\n*X* and the row number as *Y*.\n\n.. _differences-pcolor-pcolormesh:\n\n**Differences between pcolor() and pcolormesh()**\n\nBoth methods are used to create a pseudocolor plot of a 2D array\nusing quadrilaterals.\n\nThe main difference lies in the created object and internal data\nhandling:\nWhile `~.Axes.pcolor` returns a `.PolyQuadMesh`, `~.Axes.pcolormesh`\nreturns a `.QuadMesh`. The latter is more specialized for the given\npurpose and thus is faster. It should almost always be preferred.\n\nThere is also a slight difference in the handling of masked arrays.\nBoth `~.Axes.pcolor` and `~.Axes.pcolormesh` support masked arrays\nfor *C*. However, only `~.Axes.pcolor` supports masked arrays for *X*\nand *Y*. The reason lies in the internal handling of the masked values.\n`~.Axes.pcolor` leaves out the respective polygons from the\nPolyQuadMesh. `~.Axes.pcolormesh` sets the facecolor of the masked\nelements to transparent. You can see the difference when using\nedgecolors. While all edges are drawn irrespective of masking in a\nQuadMesh, the edge between two adjacent masked quadrilaterals in\n`~.Axes.pcolor` is not drawn as the corresponding polygons do not\nexist in the PolyQuadMesh. Because PolyQuadMesh draws each individual\npolygon, it also supports applying hatches and linestyles to the collection.\n\nAnother difference is the support of Gouraud shading in\n`~.Axes.pcolormesh`, which is not available with `~.Axes.pcolor`.",
    "matplotlib.pyplot.phase_spectrum": "Plot the phase spectrum.\n\nCompute the phase spectrum (unwrapped angle spectrum) of *x*.\nData is padded to a length of *pad_to* and the windowing function\n*window* is applied to the signal.\n\nParameters\n----------\nx : 1-D array or sequence\n    Array or sequence containing the data\n\nFs : float, default: 2\n    The sampling frequency (samples per time unit).  It is used to calculate\n    the Fourier frequencies, *freqs*, in cycles per time unit.\n\nwindow : callable or ndarray, default: `.window_hanning`\n    A function or a vector of length *NFFT*.  To create window vectors see\n    `.window_hanning`, `.window_none`, `numpy.blackman`, `numpy.hamming`,\n    `numpy.bartlett`, `scipy.signal`, `scipy.signal.get_window`, etc.  If a\n    function is passed as the argument, it must take a data segment as an\n    argument and return the windowed version of the segment.\n\nsides : {'default', 'onesided', 'twosided'}, optional\n    Which sides of the spectrum to return. 'default' is one-sided for real\n    data and two-sided for complex data. 'onesided' forces the return of a\n    one-sided spectrum, while 'twosided' forces two-sided.\n\npad_to : int, optional\n    The number of points to which the data segment is padded when performing\n    the FFT.  While not increasing the actual resolution of the spectrum (the\n    minimum distance between resolvable peaks), this can give more points in\n    the plot, allowing for more detail. This corresponds to the *n* parameter\n    in the call to `~numpy.fft.fft`.  The default is None, which sets *pad_to*\n    equal to the length of the input signal (i.e. no padding).\n\nFc : int, default: 0\n    The center frequency of *x*, which offsets the x extents of the\n    plot to reflect the frequency range used when a signal is acquired\n    and then filtered and downsampled to baseband.\n\nReturns\n-------\nspectrum : 1-D array\n    The values for the phase spectrum in radians (real valued).\n\nfreqs : 1-D array\n    The frequencies corresponding to the elements in *spectrum*.\n\nline : `~matplotlib.lines.Line2D`\n    The line created by this function.\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *x*\n\n**kwargs\n    Keyword arguments control the `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: :mpltype:`color`\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: :mpltype:`color` or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: :mpltype:`color`\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: :mpltype:`color`\n    markerfacecoloralt or mfcalt: :mpltype:`color`\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nmagnitude_spectrum\n    Plots the magnitudes of the corresponding frequencies.\nangle_spectrum\n    Plots the wrapped version of this function.\nspecgram\n    Can plot the phase spectrum of segments within the signal in a\n    colormap.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.phase_spectrum`.\n",
    "matplotlib.pyplot.pie": "Plot a pie chart.\n\nMake a pie chart of array *x*.  The fractional area of each wedge is\ngiven by ``x/sum(x)``.\n\nThe wedges are plotted counterclockwise, by default starting from the\nx-axis.\n\nParameters\n----------\nx : 1D array-like\n    The wedge sizes.\n\nexplode : array-like, default: None\n    If not *None*, is a ``len(x)`` array which specifies the fraction\n    of the radius with which to offset each wedge.\n\nlabels : list, default: None\n    A sequence of strings providing the labels for each wedge\n\ncolors : :mpltype:`color` or list of :mpltype:`color`, default: None\n    A sequence of colors through which the pie chart will cycle.  If\n    *None*, will use the colors in the currently active cycle.\n\nhatch : str or list, default: None\n    Hatching pattern applied to all pie wedges or sequence of patterns\n    through which the chart will cycle. For a list of valid patterns,\n    see :doc:`/gallery/shapes_and_collections/hatch_style_reference`.\n\n    .. versionadded:: 3.7\n\nautopct : None or str or callable, default: None\n    If not *None*, *autopct* is a string or function used to label the\n    wedges with their numeric value. The label will be placed inside\n    the wedge. If *autopct* is a format string, the label will be\n    ``fmt % pct``. If *autopct* is a function, then it will be called.\n\npctdistance : float, default: 0.6\n    The relative distance along the radius at which the text\n    generated by *autopct* is drawn. To draw the text outside the pie,\n    set *pctdistance* > 1. This parameter is ignored if *autopct* is\n    ``None``.\n\nlabeldistance : float or None, default: 1.1\n    The relative distance along the radius at which the labels are\n    drawn. To draw the labels inside the pie, set  *labeldistance* < 1.\n    If set to ``None``, labels are not drawn but are still stored for\n    use in `.legend`.\n\nshadow : bool or dict, default: False\n    If bool, whether to draw a shadow beneath the pie. If dict, draw a shadow\n    passing the properties in the dict to `.Shadow`.\n\n    .. versionadded:: 3.8\n        *shadow* can be a dict.\n\nstartangle : float, default: 0 degrees\n    The angle by which the start of the pie is rotated,\n    counterclockwise from the x-axis.\n\nradius : float, default: 1\n    The radius of the pie.\n\ncounterclock : bool, default: True\n    Specify fractions direction, clockwise or counterclockwise.\n\nwedgeprops : dict, default: None\n    Dict of arguments passed to each `.patches.Wedge` of the pie.\n    For example, ``wedgeprops = {'linewidth': 3}`` sets the width of\n    the wedge border lines equal to 3. By default, ``clip_on=False``.\n    When there is a conflict between these properties and other\n    keywords, properties passed to *wedgeprops* take precedence.\n\ntextprops : dict, default: None\n    Dict of arguments to pass to the text objects.\n\ncenter : (float, float), default: (0, 0)\n    The coordinates of the center of the chart.\n\nframe : bool, default: False\n    Plot Axes frame with the chart if true.\n\nrotatelabels : bool, default: False\n    Rotate each label to the angle of the corresponding slice if true.\n\nnormalize : bool, default: True\n    When *True*, always make a full pie by normalizing x so that\n    ``sum(x) == 1``. *False* makes a partial pie if ``sum(x) <= 1``\n    and raises a `ValueError` for ``sum(x) > 1``.\n\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *x*, *explode*, *labels*, *colors*\n\nReturns\n-------\npatches : list\n    A sequence of `matplotlib.patches.Wedge` instances\n\ntexts : list\n    A list of the label `.Text` instances.\n\nautotexts : list\n    A list of `.Text` instances for the numeric labels. This will only\n    be returned if the parameter *autopct* is not *None*.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.pie`.\n\nThe pie chart will probably look best if the figure and Axes are\nsquare, or the Axes aspect is equal.\nThis method sets the aspect ratio of the axis to \"equal\".\nThe Axes aspect ratio can be controlled with `.Axes.set_aspect`.",
    "matplotlib.pyplot.pink": "\n    Set the colormap to 'pink'.\n\n    This changes the default colormap as well as the colormap of the current\n    image if there is one. See ``help(colormaps)`` for more information.\n    ",
    "matplotlib.pyplot.plasma": "\n    Set the colormap to 'plasma'.\n\n    This changes the default colormap as well as the colormap of the current\n    image if there is one. See ``help(colormaps)`` for more information.\n    ",
    "matplotlib.pyplot.plot": "Plot y versus x as lines and/or markers.\n\nCall signatures::\n\n    plot([x], y, [fmt], *, data=None, **kwargs)\n    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThe coordinates of the points or line nodes are given by *x*, *y*.\n\nThe optional parameter *fmt* is a convenient way for defining basic\nformatting like color, marker and linestyle. It's a shortcut string\nnotation described in the *Notes* section below.\n\n>>> plot(x, y)        # plot x and y using default line style and color\n>>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n>>> plot(y)           # plot y using x as index array 0..N-1\n>>> plot(y, 'r+')     # ditto, but with red plusses\n\nYou can use `.Line2D` properties as keyword arguments for more\ncontrol on the appearance. Line properties and *fmt* can be mixed.\nThe following two calls yield identical results:\n\n>>> plot(x, y, 'go--', linewidth=2, markersize=12)\n>>> plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n\nWhen conflicting with *fmt*, keyword arguments take precedence.\n\n\n**Plotting labelled data**\n\nThere's a convenient way for plotting objects with labelled data (i.e.\ndata that can be accessed by index ``obj['y']``). Instead of giving\nthe data in *x* and *y*, you can provide the object in the *data*\nparameter and just give the labels for *x* and *y*::\n\n>>> plot('xlabel', 'ylabel', data=obj)\n\nAll indexable objects are supported. This could e.g. be a `dict`, a\n`pandas.DataFrame` or a structured numpy array.\n\n\n**Plotting multiple sets of data**\n\nThere are various ways to plot multiple sets of data.\n\n- The most straight forward way is just to call `plot` multiple times.\n  Example:\n\n  >>> plot(x1, y1, 'bo')\n  >>> plot(x2, y2, 'go')\n\n- If *x* and/or *y* are 2D arrays, a separate data set will be drawn\n  for every column. If both *x* and *y* are 2D, they must have the\n  same shape. If only one of them is 2D with shape (N, m) the other\n  must have length N and will be used for every data set m.\n\n  Example:\n\n  >>> x = [1, 2, 3]\n  >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n  >>> plot(x, y)\n\n  is equivalent to:\n\n  >>> for col in range(y.shape[1]):\n  ...     plot(x, y[:, col])\n\n- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n  groups::\n\n  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n\n  In this case, any additional keyword argument applies to all\n  datasets. Also, this syntax cannot be combined with the *data*\n  parameter.\n\nBy default, each line is assigned a different style specified by a\n'style cycle'. The *fmt* and line property parameters are only\nnecessary if you want explicit deviations from these defaults.\nAlternatively, you can also change the style cycle using\n:rc:`axes.prop_cycle`.\n\n\nParameters\n----------\nx, y : array-like or scalar\n    The horizontal / vertical coordinates of the data points.\n    *x* values are optional and default to ``range(len(y))``.\n\n    Commonly, these parameters are 1D arrays.\n\n    They can also be scalars, or two-dimensional (in that case, the\n    columns represent separate data sets).\n\n    These arguments cannot be passed as keywords.\n\nfmt : str, optional\n    A format string, e.g. 'ro' for red circles. See the *Notes*\n    section for a full description of the format strings.\n\n    Format strings are just an abbreviation for quickly setting\n    basic line properties. All of these and more can also be\n    controlled by keyword arguments.\n\n    This argument cannot be passed as keyword.\n\ndata : indexable object, optional\n    An object with labelled data. If given, provide the label names to\n    plot in *x* and *y*.\n\n    .. note::\n        Technically there's a slight ambiguity in calls where the\n        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n        the former interpretation is chosen, but a warning is issued.\n        You may suppress the warning by adding an empty format string\n        ``plot('n', 'o', '', data=obj)``.\n\nReturns\n-------\nlist of `.Line2D`\n    A list of lines representing the plotted data.\n\nOther Parameters\n----------------\nscalex, scaley : bool, default: True\n    These parameters determine if the view limits are adapted to the\n    data limits. The values are passed on to\n    `~.axes.Axes.autoscale_view`.\n\n**kwargs : `~matplotlib.lines.Line2D` properties, optional\n    *kwargs* are used to specify properties like a line label (for\n    auto legends), linewidth, antialiasing, marker face color.\n    Example::\n\n    >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n    >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n    If you specify multiple lines with one plot call, the kwargs apply\n    to all those lines. In case the label object is iterable, each\n    element is used as labels for each set of data.\n\n    Here is a list of available `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: :mpltype:`color`\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: :mpltype:`color` or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: :mpltype:`color`\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: :mpltype:`color`\n    markerfacecoloralt or mfcalt: :mpltype:`color`\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nscatter : XY scatter plot with markers of varying size and/or color (\n    sometimes also called bubble chart).\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.plot`.\n\n**Format Strings**\n\nA format string consists of a part for color, marker and line::\n\n    fmt = '[marker][line][color]'\n\nEach of them is optional. If not provided, the value from the style\ncycle is used. Exception: If ``line`` is given, but no ``marker``,\nthe data will be a line without markers.\n\nOther combinations such as ``[color][marker][line]`` are also\nsupported, but note that their parsing may be ambiguous.\n\n**Markers**\n\n=============   ===============================\ncharacter       description\n=============   ===============================\n``'.'``         point marker\n``','``         pixel marker\n``'o'``         circle marker\n``'v'``         triangle_down marker\n``'^'``         triangle_up marker\n``'<'``         triangle_left marker\n``'>'``         triangle_right marker\n``'1'``         tri_down marker\n``'2'``         tri_up marker\n``'3'``         tri_left marker\n``'4'``         tri_right marker\n``'8'``         octagon marker\n``'s'``         square marker\n``'p'``         pentagon marker\n``'P'``         plus (filled) marker\n``'*'``         star marker\n``'h'``         hexagon1 marker\n``'H'``         hexagon2 marker\n``'+'``         plus marker\n``'x'``         x marker\n``'X'``         x (filled) marker\n``'D'``         diamond marker\n``'d'``         thin_diamond marker\n``'|'``         vline marker\n``'_'``         hline marker\n=============   ===============================\n\n**Line Styles**\n\n=============    ===============================\ncharacter        description\n=============    ===============================\n``'-'``          solid line style\n``'--'``         dashed line style\n``'-.'``         dash-dot line style\n``':'``          dotted line style\n=============    ===============================\n\nExample format strings::\n\n    'b'    # blue markers with default shape\n    'or'   # red circles\n    '-g'   # green solid line\n    '--'   # dashed line with default color\n    '^k:'  # black triangle_up markers connected by a dotted line\n\n**Colors**\n\nThe supported color abbreviations are the single letter codes\n\n=============    ===============================\ncharacter        color\n=============    ===============================\n``'b'``          blue\n``'g'``          green\n``'r'``          red\n``'c'``          cyan\n``'m'``          magenta\n``'y'``          yellow\n``'k'``          black\n``'w'``          white\n=============    ===============================\n\nand the ``'CN'`` colors that index into the default property cycle.\n\nIf the color is the only part of the format string, you can\nadditionally use any  `matplotlib.colors` spec, e.g. full names\n(``'green'``) or hex strings (``'#008000'``).",
    "matplotlib.pyplot.plot_date": "[*Deprecated*] Plot coercing the axis to treat floats as dates.\n\n.. deprecated:: 3.9\n\n    This method exists for historic reasons and will be removed in version 3.11.\n\n    - ``datetime``-like data should directly be plotted using\n      `~.Axes.plot`.\n    -  If you need to plot plain numeric data as :ref:`date-format` or\n       need to set a timezone, call ``ax.xaxis.axis_date`` /\n       ``ax.yaxis.axis_date`` before `~.Axes.plot`. See\n       `.Axis.axis_date`.\n\nSimilar to `.plot`, this plots *y* vs. *x* as lines or markers.\nHowever, the axis labels are formatted as dates depending on *xdate*\nand *ydate*.  Note that `.plot` will work with `datetime` and\n`numpy.datetime64` objects without resorting to this method.\n\nParameters\n----------\nx, y : array-like\n    The coordinates of the data points. If *xdate* or *ydate* is\n    *True*, the respective values *x* or *y* are interpreted as\n    :ref:`Matplotlib dates <date-format>`.\n\nfmt : str, optional\n    The plot format string. For details, see the corresponding\n    parameter in `.plot`.\n\ntz : timezone string or `datetime.tzinfo`, default: :rc:`timezone`\n    The time zone to use in labeling dates.\n\nxdate : bool, default: True\n    If *True*, the *x*-axis will be interpreted as Matplotlib dates.\n\nydate : bool, default: False\n    If *True*, the *y*-axis will be interpreted as Matplotlib dates.\n\nReturns\n-------\nlist of `.Line2D`\n    Objects representing the plotted data.\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *x*, *y*\n**kwargs\n    Keyword arguments control the `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: :mpltype:`color`\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: :mpltype:`color` or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: :mpltype:`color`\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: :mpltype:`color`\n    markerfacecoloralt or mfcalt: :mpltype:`color`\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nmatplotlib.dates : Helper functions on dates.\nmatplotlib.dates.date2num : Convert dates to num.\nmatplotlib.dates.num2date : Convert num to dates.\nmatplotlib.dates.drange : Create an equally spaced sequence of dates.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.plot_date`.\n\nIf you are using custom date tickers and formatters, it may be\nnecessary to set the formatters/locators after the call to\n`.plot_date`. `.plot_date` will set the default tick locator to\n`.AutoDateLocator` (if the tick locator is not already set to a\n`.DateLocator` instance) and the default tick formatter to\n`.AutoDateFormatter` (if the tick formatter is not already set to a\n`.DateFormatter` instance).\n\n.. deprecated:: 3.9\n   Use plot instead.",
    "matplotlib.pyplot.polar": "\n    Make a polar plot.\n\n    call signature::\n\n      polar(theta, r, [fmt], **kwargs)\n\n    This is a convenience wrapper around `.pyplot.plot`. It ensures that the\n    current Axes is polar (or creates one if needed) and then passes all parameters\n    to ``.pyplot.plot``.\n\n    .. note::\n        When making polar plots using the :ref:`pyplot API <pyplot_interface>`,\n        ``polar()`` should typically be the first command because that makes sure\n        a polar Axes is created. Using other commands such as ``plt.title()``\n        before this can lead to the implicit creation of a rectangular Axes, in which\n        case a subsequent ``polar()`` call will fail.\n    ",
    "matplotlib.pyplot.prism": "\n    Set the colormap to 'prism'.\n\n    This changes the default colormap as well as the colormap of the current\n    image if there is one. See ``help(colormaps)`` for more information.\n    ",
    "matplotlib.pyplot.psd": "Plot the power spectral density.\n\nThe power spectral density :math:`P_{xx}` by Welch's average\nperiodogram method.  The vector *x* is divided into *NFFT* length\nsegments.  Each segment is detrended by function *detrend* and\nwindowed by function *window*.  *noverlap* gives the length of\nthe overlap between segments.  The :math:`|\\mathrm{fft}(i)|^2`\nof each segment :math:`i` are averaged to compute :math:`P_{xx}`,\nwith a scaling to correct for power loss due to windowing.\n\nIf len(*x*) < *NFFT*, it will be zero padded to *NFFT*.\n\nParameters\n----------\nx : 1-D array or sequence\n    Array or sequence containing the data\n\nFs : float, default: 2\n    The sampling frequency (samples per time unit).  It is used to calculate\n    the Fourier frequencies, *freqs*, in cycles per time unit.\n\nwindow : callable or ndarray, default: `.window_hanning`\n    A function or a vector of length *NFFT*.  To create window vectors see\n    `.window_hanning`, `.window_none`, `numpy.blackman`, `numpy.hamming`,\n    `numpy.bartlett`, `scipy.signal`, `scipy.signal.get_window`, etc.  If a\n    function is passed as the argument, it must take a data segment as an\n    argument and return the windowed version of the segment.\n\nsides : {'default', 'onesided', 'twosided'}, optional\n    Which sides of the spectrum to return. 'default' is one-sided for real\n    data and two-sided for complex data. 'onesided' forces the return of a\n    one-sided spectrum, while 'twosided' forces two-sided.\n\npad_to : int, optional\n    The number of points to which the data segment is padded when performing\n    the FFT.  This can be different from *NFFT*, which specifies the number\n    of data points used.  While not increasing the actual resolution of the\n    spectrum (the minimum distance between resolvable peaks), this can give\n    more points in the plot, allowing for more detail. This corresponds to\n    the *n* parameter in the call to `~numpy.fft.fft`. The default is None,\n    which sets *pad_to* equal to *NFFT*\n\nNFFT : int, default: 256\n    The number of data points used in each block for the FFT.  A power 2 is\n    most efficient.  This should *NOT* be used to get zero padding, or the\n    scaling of the result will be incorrect; use *pad_to* for this instead.\n\ndetrend : {'none', 'mean', 'linear'} or callable, default: 'none'\n    The function applied to each segment before fft-ing, designed to remove\n    the mean or linear trend.  Unlike in MATLAB, where the *detrend* parameter\n    is a vector, in Matplotlib it is a function.  The :mod:`~matplotlib.mlab`\n    module defines `.detrend_none`, `.detrend_mean`, and `.detrend_linear`,\n    but you can use a custom function as well.  You can also use a string to\n    choose one of the functions: 'none' calls `.detrend_none`. 'mean' calls\n    `.detrend_mean`. 'linear' calls `.detrend_linear`.\n\nscale_by_freq : bool, default: True\n    Whether the resulting density values should be scaled by the scaling\n    frequency, which gives density in units of 1/Hz.  This allows for\n    integration over the returned frequency values.  The default is True for\n    MATLAB compatibility.\n\nnoverlap : int, default: 0 (no overlap)\n    The number of points of overlap between segments.\n\nFc : int, default: 0\n    The center frequency of *x*, which offsets the x extents of the\n    plot to reflect the frequency range used when a signal is acquired\n    and then filtered and downsampled to baseband.\n\nreturn_line : bool, default: False\n    Whether to include the line object plotted in the returned values.\n\nReturns\n-------\nPxx : 1-D array\n    The values for the power spectrum :math:`P_{xx}` before scaling\n    (real valued).\n\nfreqs : 1-D array\n    The frequencies corresponding to the elements in *Pxx*.\n\nline : `~matplotlib.lines.Line2D`\n    The line created by this function.\n    Only returned if *return_line* is True.\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *x*\n\n**kwargs\n    Keyword arguments control the `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: :mpltype:`color`\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: :mpltype:`color` or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: :mpltype:`color`\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: :mpltype:`color`\n    markerfacecoloralt or mfcalt: :mpltype:`color`\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nspecgram\n    Differs in the default overlap; in not returning the mean of the\n    segment periodograms; in returning the times of the segments; and\n    in plotting a colormap instead of a line.\nmagnitude_spectrum\n    Plots the magnitude spectrum.\ncsd\n    Plots the spectral density between two signals.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.psd`.\n\nFor plotting, the power is plotted as\n:math:`10\\log_{10}(P_{xx})` for decibels, though *Pxx* itself\nis returned.\n\nReferences\n----------\nBendat & Piersol -- Random Data: Analysis and Measurement Procedures,\nJohn Wiley & Sons (1986)",
    "matplotlib.pyplot.quiver": "Plot a 2D field of arrows.\n\nCall signature::\n\n  quiver([X, Y], U, V, [C], /, **kwargs)\n\n*X*, *Y* define the arrow locations, *U*, *V* define the arrow directions, and\n*C* optionally sets the color. The arguments *X*, *Y*, *U*, *V*, *C* are\npositional-only.\n\n**Arrow length**\n\nThe default settings auto-scales the length of the arrows to a reasonable size.\nTo change this behavior see the *scale* and *scale_units* parameters.\n\n**Arrow shape**\n\nThe arrow shape is determined by *width*, *headwidth*, *headlength* and\n*headaxislength*. See the notes below.\n\n**Arrow styling**\n\nEach arrow is internally represented by a filled polygon with a default edge\nlinewidth of 0. As a result, an arrow is rather a filled area, not a line with\na head, and `.PolyCollection` properties like *linewidth*, *edgecolor*,\n*facecolor*, etc. act accordingly.\n\n\nParameters\n----------\nX, Y : 1D or 2D array-like, optional\n    The x and y coordinates of the arrow locations.\n\n    If not given, they will be generated as a uniform integer meshgrid based\n    on the dimensions of *U* and *V*.\n\n    If *X* and *Y* are 1D but *U*, *V* are 2D, *X*, *Y* are expanded to 2D\n    using ``X, Y = np.meshgrid(X, Y)``. In this case ``len(X)`` and ``len(Y)``\n    must match the column and row dimensions of *U* and *V*.\n\nU, V : 1D or 2D array-like\n    The x and y direction components of the arrow vectors. The interpretation\n    of these components (in data or in screen space) depends on *angles*.\n\n    *U* and *V* must have the same number of elements, matching the number of\n    arrow locations in *X*, *Y*. *U* and *V* may be masked. Locations masked\n    in any of *U*, *V*, and *C* will not be drawn.\n\nC : 1D or 2D array-like, optional\n    Numeric data that defines the arrow colors by colormapping via *norm* and\n    *cmap*.\n\n    This does not support explicit colors. If you want to set colors directly,\n    use *color* instead.  The size of *C* must match the number of arrow\n    locations.\n\nangles : {'uv', 'xy'} or array-like, default: 'uv'\n    Method for determining the angle of the arrows.\n\n    - 'uv':  Arrow directions are based on\n      :ref:`display coordinates <coordinate-systems>`; i.e. a 45\u00b0 angle will\n      always show up as diagonal on the screen, irrespective of figure or Axes\n      aspect ratio or Axes data ranges. This is useful when the arrows represent\n      a quantity whose direction is not tied to the x and y data coordinates.\n\n      If *U* == *V* the orientation of the arrow on the plot is 45 degrees\n      counter-clockwise from the horizontal axis (positive to the right).\n\n    - 'xy': Arrow direction in data coordinates, i.e. the arrows point from\n      (x, y) to (x+u, y+v). This is ideal for vector fields or gradient plots\n      where the arrows should directly represent movements or gradients in the\n      x and y directions.\n\n    - Arbitrary angles may be specified explicitly as an array of values\n      in degrees, counter-clockwise from the horizontal axis.\n\n      In this case *U*, *V* is only used to determine the length of the\n      arrows.\n\n      For example, ``angles=[30, 60, 90]`` will orient the arrows at 30, 60, and 90\n      degrees respectively, regardless of the *U* and *V* components.\n\n    Note: inverting a data axis will correspondingly invert the\n    arrows only with ``angles='xy'``.\n\npivot : {'tail', 'mid', 'middle', 'tip'}, default: 'tail'\n    The part of the arrow that is anchored to the *X*, *Y* grid. The arrow\n    rotates about this point.\n\n    'mid' is a synonym for 'middle'.\n\nscale : float, optional\n    Scales the length of the arrow inversely.\n\n    Number of data values represented by one unit of arrow length on the plot.\n    For example, if the data represents velocity in meters per second (m/s), the\n    scale parameter determines how many meters per second correspond to one unit of\n    arrow length relative to the width of the plot.\n    Smaller scale parameter makes the arrow longer.\n\n    By default, an autoscaling algorithm is used to scale the arrow length to a\n    reasonable size, which is based on the average vector length and the number of\n    vectors.\n\n    The arrow length unit is given by the *scale_units* parameter.\n\nscale_units : {'width', 'height', 'dots', 'inches', 'x', 'y', 'xy'}, default: 'width'\n\n    The physical image unit, which is used for rendering the scaled arrow data *U*, *V*.\n\n    The rendered arrow length is given by\n\n        length in x direction = $\\frac{u}{\\mathrm{scale}} \\mathrm{scale_unit}$\n\n        length in y direction = $\\frac{v}{\\mathrm{scale}} \\mathrm{scale_unit}$\n\n    For example, ``(u, v) = (0.5, 0)`` with ``scale=10, scale_unit=\"width\"`` results\n    in a horizontal arrow with a length of *0.5 / 10 * \"width\"*, i.e. 0.05 times the\n    Axes width.\n\n    Supported values are:\n\n    - 'width' or 'height': The arrow length is scaled relative to the width or height\n       of the Axes.\n       For example, ``scale_units='width', scale=1.0``, will result in an arrow length\n       of width of the Axes.\n\n    - 'dots': The arrow length of the arrows is in measured in display dots (pixels).\n\n    - 'inches': Arrow lengths are scaled based on the DPI (dots per inch) of the figure.\n       This ensures that the arrows have a consistent physical size on the figure,\n       in inches, regardless of data values or plot scaling.\n       For example, ``(u, v) = (1, 0)`` with ``scale_units='inches', scale=2`` results\n       in a 0.5 inch-long arrow.\n\n    - 'x' or 'y': The arrow length is scaled relative to the x or y axis units.\n       For example, ``(u, v) = (0, 1)`` with ``scale_units='x', scale=1`` results\n       in a vertical arrow with the length of 1 x-axis unit.\n\n    - 'xy': Arrow length will be same as 'x' or 'y' units.\n       This is useful for creating vectors in the x-y plane where u and v have\n       the same units as x and y. To plot vectors in the x-y plane with u and v having\n       the same units as x and y, use ``angles='xy', scale_units='xy', scale=1``.\n\n    Note: Setting *scale_units* without setting scale does not have any effect because\n    the scale units only differ by a constant factor and that is rescaled through\n    autoscaling.\n\nunits : {'width', 'height', 'dots', 'inches', 'x', 'y', 'xy'}, default: 'width'\n    Affects the arrow size (except for the length). In particular, the shaft\n    *width* is measured in multiples of this unit.\n\n    Supported values are:\n\n    - 'width', 'height': The width or height of the Axes.\n    - 'dots', 'inches': Pixels or inches based on the figure dpi.\n    - 'x', 'y', 'xy': *X*, *Y* or :math:`\\sqrt{X^2 + Y^2}` in data units.\n\n    The following table summarizes how these values affect the visible arrow\n    size under zooming and figure size changes:\n\n    =================  =================   ==================\n    units              zoom                figure size change\n    =================  =================   ==================\n    'x', 'y', 'xy'     arrow size scales   \u2014\n    'width', 'height'  \u2014                   arrow size scales\n    'dots', 'inches'   \u2014                   \u2014\n    =================  =================   ==================\n\nwidth : float, optional\n    Shaft width in arrow units. All head parameters are relative to *width*.\n\n    The default depends on choice of *units* above, and number of vectors;\n    a typical starting value is about 0.005 times the width of the plot.\n\nheadwidth : float, default: 3\n    Head width as multiple of shaft *width*. See the notes below.\n\nheadlength : float, default: 5\n    Head length as multiple of shaft *width*. See the notes below.\n\nheadaxislength : float, default: 4.5\n    Head length at shaft intersection as multiple of shaft *width*.\n    See the notes below.\n\nminshaft : float, default: 1\n    Length below which arrow scales, in units of head length. Do not\n    set this to less than 1, or small arrows will look terrible!\n\nminlength : float, default: 1\n    Minimum length as a multiple of shaft width; if an arrow length\n    is less than this, plot a dot (hexagon) of this diameter instead.\n\ncolor : :mpltype:`color` or list :mpltype:`color`, optional\n    Explicit color(s) for the arrows. If *C* has been set, *color* has no\n    effect.\n\n    This is a synonym for the `.PolyCollection` *facecolor* parameter.\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``.\n\n**kwargs : `~matplotlib.collections.PolyCollection` properties, optional\n    All other keyword arguments are passed on to `.PolyCollection`:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: array-like or scalar or None\n    animated: bool\n    antialiased or aa or antialiaseds: bool or list of bools\n    array: array-like or None\n    capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    clim: (vmin: float, vmax: float)\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    cmap: `.Colormap` or str or None\n    color: :mpltype:`color` or list of RGBA tuples\n    edgecolor or ec or edgecolors: :mpltype:`color` or list of :mpltype:`color` or 'face'\n    facecolor or facecolors or fc: :mpltype:`color` or list of :mpltype:`color`\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    gid: str\n    hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n    hatch_linewidth: unknown\n    in_layout: bool\n    joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    label: object\n    linestyle or dashes or linestyles or ls: str or tuple or list thereof\n    linewidth or linewidths or lw: float or list of floats\n    mouseover: bool\n    norm: `.Normalize` or str or None\n    offset_transform or transOffset: `.Transform`\n    offsets: (N, 2) or (2,) array-like\n    path_effects: list of `.AbstractPathEffect`\n    paths: list of array-like\n    picker: None or bool or float or callable\n    pickradius: float\n    rasterized: bool\n    sizes: `numpy.ndarray` or None\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    urls: list of str or None\n    verts: list of array-like\n    verts_and_codes: unknown\n    visible: bool\n    zorder: float\n\nReturns\n-------\n`~matplotlib.quiver.Quiver`\n\nSee Also\n--------\n.Axes.quiverkey : Add a key to a quiver plot.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.quiver`.\n\n\n**Arrow shape**\n\nThe arrow is drawn as a polygon using the nodes as shown below. The values\n*headwidth*, *headlength*, and *headaxislength* are in units of *width*.\n\n.. image:: /_static/quiver_sizes.svg\n   :width: 500px\n\nThe defaults give a slightly swept-back arrow. Here are some guidelines how to\nget other head shapes:\n\n- To make the head a triangle, make *headaxislength* the same as *headlength*.\n- To make the arrow more pointed, reduce *headwidth* or increase *headlength*\n  and *headaxislength*.\n- To make the head smaller relative to the shaft, scale down all the head\n  parameters proportionally.\n- To remove the head completely, set all *head* parameters to 0.\n- To get a diamond-shaped head, make *headaxislength* larger than *headlength*.\n- Warning: For *headaxislength* < (*headlength* / *headwidth*), the \"headaxis\"\n  nodes (i.e. the ones connecting the head with the shaft) will protrude out\n  of the head in forward direction so that the arrow head looks broken.",
    "matplotlib.pyplot.quiverkey": "Add a key to a quiver plot.\n\nThe positioning of the key depends on *X*, *Y*, *coordinates*, and\n*labelpos*.  If *labelpos* is 'N' or 'S', *X*, *Y* give the position of\nthe middle of the key arrow.  If *labelpos* is 'E', *X*, *Y* positions\nthe head, and if *labelpos* is 'W', *X*, *Y* positions the tail; in\neither of these two cases, *X*, *Y* is somewhere in the middle of the\narrow+label key object.\n\nParameters\n----------\nQ : `~matplotlib.quiver.Quiver`\n    A `.Quiver` object as returned by a call to `~.Axes.quiver()`.\nX, Y : float\n    The location of the key.\nU : float\n    The length of the key.\nlabel : str\n    The key label (e.g., length and units of the key).\nangle : float, default: 0\n    The angle of the key arrow, in degrees anti-clockwise from the\n    horizontal axis.\ncoordinates : {'axes', 'figure', 'data', 'inches'}, default: 'axes'\n    Coordinate system and units for *X*, *Y*: 'axes' and 'figure' are\n    normalized coordinate systems with (0, 0) in the lower left and\n    (1, 1) in the upper right; 'data' are the axes data coordinates\n    (used for the locations of the vectors in the quiver plot itself);\n    'inches' is position in the figure in inches, with (0, 0) at the\n    lower left corner.\ncolor : :mpltype:`color`\n    Overrides face and edge colors from *Q*.\nlabelpos : {'N', 'S', 'E', 'W'}\n    Position the label above, below, to the right, to the left of the\n    arrow, respectively.\nlabelsep : float, default: 0.1\n    Distance in inches between the arrow and the label.\nlabelcolor : :mpltype:`color`, default: :rc:`text.color`\n    Label color.\nfontproperties : dict, optional\n    A dictionary with keyword arguments accepted by the\n    `~matplotlib.font_manager.FontProperties` initializer:\n    *family*, *style*, *variant*, *size*, *weight*.\nzorder : float\n    The zorder of the key. The default is 0.1 above *Q*.\n**kwargs\n    Any additional keyword arguments are used to override vector\n    properties taken from *Q*.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.quiverkey`.\n",
    "matplotlib.pyplot.rc": "Set the current `.rcParams`.  *group* is the grouping for the rc, e.g.,\nfor ``lines.linewidth`` the group is ``lines``, for\n``axes.facecolor``, the group is ``axes``, and so on.  Group may\nalso be a list or tuple of group names, e.g., (*xtick*, *ytick*).\n*kwargs* is a dictionary attribute name/value pairs, e.g.,::\n\n  rc('lines', linewidth=2, color='r')\n\nsets the current `.rcParams` and is equivalent to::\n\n  rcParams['lines.linewidth'] = 2\n  rcParams['lines.color'] = 'r'\n\nThe following aliases are available to save typing for interactive users:\n\n=====   =================\nAlias   Property\n=====   =================\n'lw'    'linewidth'\n'ls'    'linestyle'\n'c'     'color'\n'fc'    'facecolor'\n'ec'    'edgecolor'\n'mew'   'markeredgewidth'\n'aa'    'antialiased'\n=====   =================\n\nThus you could abbreviate the above call as::\n\n      rc('lines', lw=2, c='r')\n\nNote you can use python's kwargs dictionary facility to store\ndictionaries of default parameters.  e.g., you can customize the\nfont rc as follows::\n\n  font = {'family' : 'monospace',\n          'weight' : 'bold',\n          'size'   : 'larger'}\n  rc('font', **font)  # pass in the font dict as kwargs\n\nThis enables you to easily switch between several configurations.  Use\n``matplotlib.style.use('default')`` or :func:`~matplotlib.rcdefaults` to\nrestore the default `.rcParams` after changes.\n\nNotes\n-----\n\n.. note::\n\n    This is equivalent to `matplotlib.rc`.\n\nSimilar functionality is available by using the normal dict interface, i.e.\n``rcParams.update({\"lines.linewidth\": 2, ...})`` (but ``rcParams.update``\ndoes not support abbreviations or grouping).",
    "matplotlib.pyplot.rc_context": "Return a context manager for temporarily changing rcParams.\n\nThe :rc:`backend` will not be reset by the context manager.\n\nrcParams changed both through the context manager invocation and\nin the body of the context will be reset on context exit.\n\nParameters\n----------\nrc : dict\n    The rcParams to temporarily set.\nfname : str or path-like\n    A file with Matplotlib rc settings. If both *fname* and *rc* are given,\n    settings from *rc* take precedence.\n\nSee Also\n--------\n:ref:`customizing-with-matplotlibrc-files`\n\nNotes\n-----\n\n.. note::\n\n    This is equivalent to `matplotlib.rc_context`.\n\nExamples\n--------\nPassing explicit values via a dict::\n\n    with mpl.rc_context({'interactive': False}):\n        fig, ax = plt.subplots()\n        ax.plot(range(3), range(3))\n        fig.savefig('example.png')\n        plt.close(fig)\n\nLoading settings from a file::\n\n     with mpl.rc_context(fname='print.rc'):\n         plt.plot(x, y)  # uses 'print.rc'\n\nSetting in the context body::\n\n    with mpl.rc_context():\n        # will be reset\n        mpl.rcParams['lines.linewidth'] = 5\n        plt.plot(x, y)",
    "matplotlib.pyplot.rcdefaults": "Restore the `.rcParams` from Matplotlib's internal default style.\n\nStyle-blacklisted `.rcParams` (defined in\n``matplotlib.style.core.STYLE_BLACKLIST``) are not updated.\n\nSee Also\n--------\nmatplotlib.rc_file_defaults\n    Restore the `.rcParams` from the rc file originally loaded by\n    Matplotlib.\nmatplotlib.style.use\n    Use a specific style file.  Call ``style.use('default')`` to restore\n    the default style.\n\nNotes\n-----\n\n.. note::\n\n    This is equivalent to `matplotlib.rcdefaults`.\n",
    "matplotlib.pyplot.rgrids": "\n    Get or set the radial gridlines on the current polar plot.\n\n    Call signatures::\n\n     lines, labels = rgrids()\n     lines, labels = rgrids(radii, labels=None, angle=22.5, fmt=None, **kwargs)\n\n    When called with no arguments, `.rgrids` simply returns the tuple\n    (*lines*, *labels*). When called with arguments, the labels will\n    appear at the specified radial distances and angle.\n\n    Parameters\n    ----------\n    radii : tuple with floats\n        The radii for the radial gridlines\n\n    labels : tuple with strings or None\n        The labels to use at each radial gridline. The\n        `matplotlib.ticker.ScalarFormatter` will be used if None.\n\n    angle : float\n        The angular position of the radius labels in degrees.\n\n    fmt : str or None\n        Format string used in `matplotlib.ticker.FormatStrFormatter`.\n        For example '%f'.\n\n    Returns\n    -------\n    lines : list of `.lines.Line2D`\n        The radial gridlines.\n\n    labels : list of `.text.Text`\n        The tick labels.\n\n    Other Parameters\n    ----------------\n    **kwargs\n        *kwargs* are optional `.Text` properties for the labels.\n\n    See Also\n    --------\n    .pyplot.thetagrids\n    .projections.polar.PolarAxes.set_rgrids\n    .Axis.get_gridlines\n    .Axis.get_ticklabels\n\n    Examples\n    --------\n    ::\n\n      # set the locations of the radial gridlines\n      lines, labels = rgrids( (0.25, 0.5, 1.0) )\n\n      # set the locations and labels of the radial gridlines\n      lines, labels = rgrids( (0.25, 0.5, 1.0), ('Tom', 'Dick', 'Harry' ))\n    ",
    "matplotlib.pyplot.savefig": "Save the current figure as an image or vector graphic to a file.\n\nCall signature::\n\n  savefig(fname, *, transparent=None, dpi='figure', format=None,\n          metadata=None, bbox_inches=None, pad_inches=0.1,\n          facecolor='auto', edgecolor='auto', backend=None,\n          **kwargs\n         )\n\nThe available output formats depend on the backend being used.\n\nParameters\n----------\nfname : str or path-like or binary file-like\n    A path, or a Python file-like object, or\n    possibly some backend-dependent object such as\n    `matplotlib.backends.backend_pdf.PdfPages`.\n\n    If *format* is set, it determines the output format, and the file\n    is saved as *fname*.  Note that *fname* is used verbatim, and there\n    is no attempt to make the extension, if any, of *fname* match\n    *format*, and no extension is appended.\n\n    If *format* is not set, then the format is inferred from the\n    extension of *fname*, if there is one.  If *format* is not\n    set and *fname* has no extension, then the file is saved with\n    :rc:`savefig.format` and the appropriate extension is appended to\n    *fname*.\n\nOther Parameters\n----------------\ntransparent : bool, default: :rc:`savefig.transparent`\n    If *True*, the Axes patches will all be transparent; the\n    Figure patch will also be transparent unless *facecolor*\n    and/or *edgecolor* are specified via kwargs.\n\n    If *False* has no effect and the color of the Axes and\n    Figure patches are unchanged (unless the Figure patch\n    is specified via the *facecolor* and/or *edgecolor* keyword\n    arguments in which case those colors are used).\n\n    The transparency of these patches will be restored to their\n    original values upon exit of this function.\n\n    This is useful, for example, for displaying\n    a plot on top of a colored background on a web page.\n\ndpi : float or 'figure', default: :rc:`savefig.dpi`\n    The resolution in dots per inch.  If 'figure', use the figure's\n    dpi value.\n\nformat : str\n    The file format, e.g. 'png', 'pdf', 'svg', ... The behavior when\n    this is unset is documented under *fname*.\n\nmetadata : dict, optional\n    Key/value pairs to store in the image metadata. The supported keys\n    and defaults depend on the image format and backend:\n\n    - 'png' with Agg backend: See the parameter ``metadata`` of\n      `~.FigureCanvasAgg.print_png`.\n    - 'pdf' with pdf backend: See the parameter ``metadata`` of\n      `~.backend_pdf.PdfPages`.\n    - 'svg' with svg backend: See the parameter ``metadata`` of\n      `~.FigureCanvasSVG.print_svg`.\n    - 'eps' and 'ps' with PS backend: Only 'Creator' is supported.\n\n    Not supported for 'pgf', 'raw', and 'rgba' as those formats do not support\n    embedding metadata.\n    Does not currently support 'jpg', 'tiff', or 'webp', but may include\n    embedding EXIF metadata in the future.\n\nbbox_inches : str or `.Bbox`, default: :rc:`savefig.bbox`\n    Bounding box in inches: only the given portion of the figure is\n    saved.  If 'tight', try to figure out the tight bbox of the figure.\n\npad_inches : float or 'layout', default: :rc:`savefig.pad_inches`\n    Amount of padding in inches around the figure when bbox_inches is\n    'tight'. If 'layout' use the padding from the constrained or\n    compressed layout engine; ignored if one of those engines is not in\n    use.\n\nfacecolor : :mpltype:`color` or 'auto', default: :rc:`savefig.facecolor`\n    The facecolor of the figure.  If 'auto', use the current figure\n    facecolor.\n\nedgecolor : :mpltype:`color` or 'auto', default: :rc:`savefig.edgecolor`\n    The edgecolor of the figure.  If 'auto', use the current figure\n    edgecolor.\n\nbackend : str, optional\n    Use a non-default backend to render the file, e.g. to render a\n    png file with the \"cairo\" backend rather than the default \"agg\",\n    or a pdf file with the \"pgf\" backend rather than the default\n    \"pdf\".  Note that the default backend is normally sufficient.  See\n    :ref:`the-builtin-backends` for a list of valid backends for each\n    file format.  Custom backends can be referenced as \"module://...\".\n\norientation : {'landscape', 'portrait'}\n    Currently only supported by the postscript backend.\n\npapertype : str\n    One of 'letter', 'legal', 'executive', 'ledger', 'a0' through\n    'a10', 'b0' through 'b10'. Only supported for postscript\n    output.\n\nbbox_extra_artists : list of `~matplotlib.artist.Artist`, optional\n    A list of extra artists that will be considered when the\n    tight bbox is calculated.\n\npil_kwargs : dict, optional\n    Additional keyword arguments that are passed to\n    `PIL.Image.Image.save` when saving the figure.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.Figure.savefig`.\n",
    "matplotlib.pyplot.sca": "\n    Set the current Axes to *ax* and the current Figure to the parent of *ax*.\n    ",
    "matplotlib.pyplot.scatter": "A scatter plot of *y* vs. *x* with varying marker size and/or color.\n\nParameters\n----------\nx, y : float or array-like, shape (n, )\n    The data positions.\n\ns : float or array-like, shape (n, ), optional\n    The marker size in points**2 (typographic points are 1/72 in.).\n    Default is ``rcParams['lines.markersize'] ** 2``.\n\n    The linewidth and edgecolor can visually interact with the marker\n    size, and can lead to artifacts if the marker size is smaller than\n    the linewidth.\n\n    If the linewidth is greater than 0 and the edgecolor is anything\n    but *'none'*, then the effective size of the marker will be\n    increased by half the linewidth because the stroke will be centered\n    on the edge of the shape.\n\n    To eliminate the marker edge either set *linewidth=0* or\n    *edgecolor='none'*.\n\nc : array-like or list of :mpltype:`color` or :mpltype:`color`, optional\n    The marker colors. Possible values:\n\n    - A scalar or sequence of n numbers to be mapped to colors using\n      *cmap* and *norm*.\n    - A 2D array in which the rows are RGB or RGBA.\n    - A sequence of colors of length n.\n    - A single color format string.\n\n    Note that *c* should not be a single numeric RGB or RGBA sequence\n    because that is indistinguishable from an array of values to be\n    colormapped. If you want to specify the same RGB or RGBA value for\n    all points, use a 2D array with a single row.  Otherwise,\n    value-matching will have precedence in case of a size matching with\n    *x* and *y*.\n\n    If you wish to specify a single color for all points\n    prefer the *color* keyword argument.\n\n    Defaults to `None`. In that case the marker color is determined\n    by the value of *color*, *facecolor* or *facecolors*. In case\n    those are not specified or `None`, the marker color is determined\n    by the next color of the ``Axes``' current \"shape and fill\" color\n    cycle. This cycle defaults to :rc:`axes.prop_cycle`.\n\nmarker : `~.markers.MarkerStyle`, default: :rc:`scatter.marker`\n    The marker style. *marker* can be either an instance of the class\n    or the text shorthand for a particular marker.\n    See :mod:`matplotlib.markers` for more information about marker\n    styles.\n\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    The Colormap instance or registered colormap name used to map scalar data\n    to colors.\n\n    This parameter is ignored if *c* is RGB(A).\n\nnorm : str or `~matplotlib.colors.Normalize`, optional\n    The normalization method used to scale scalar data to the [0, 1] range\n    before mapping to colors using *cmap*. By default, a linear scaling is\n    used, mapping the lowest value to 0 and the highest to 1.\n\n    If given, this can be one of the following:\n\n    - An instance of `.Normalize` or one of its subclasses\n      (see :ref:`colormapnorms`).\n    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n      list of available scales, call `matplotlib.scale.get_scale_names()`.\n      In that case, a suitable `.Normalize` subclass is dynamically generated\n      and instantiated.\n\n    This parameter is ignored if *c* is RGB(A).\n\nvmin, vmax : float, optional\n    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n    the data range that the colormap covers. By default, the colormap covers\n    the complete value range of the supplied data. It is an error to use\n    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n    name together with *vmin*/*vmax* is acceptable).\n\n    This parameter is ignored if *c* is RGB(A).\n\nalpha : float, default: None\n    The alpha blending value, between 0 (transparent) and 1 (opaque).\n\nlinewidths : float or array-like, default: :rc:`lines.linewidth`\n    The linewidth of the marker edges. Note: The default *edgecolors*\n    is 'face'. You may want to change this as well.\n\nedgecolors : {'face', 'none', *None*} or :mpltype:`color` or list of :mpltype:`color`, default: :rc:`scatter.edgecolors`\n    The edge color of the marker. Possible values:\n\n    - 'face': The edge color will always be the same as the face color.\n    - 'none': No patch boundary will be drawn.\n    - A color or sequence of colors.\n\n    For non-filled markers, *edgecolors* is ignored. Instead, the color\n    is determined like with 'face', i.e. from *c*, *colors*, or\n    *facecolors*.\n\ncolorizer : `~matplotlib.colorizer.Colorizer` or None, default: None\n    The Colorizer object used to map color to data. If None, a Colorizer\n    object is created from a *norm* and *cmap*.\n\n    This parameter is ignored if *c* is RGB(A).\n\nplotnonfinite : bool, default: False\n    Whether to plot points with nonfinite *c* (i.e. ``inf``, ``-inf``\n    or ``nan``). If ``True`` the points are drawn with the *bad*\n    colormap color (see `.Colormap.set_bad`).\n\nReturns\n-------\n`~matplotlib.collections.PathCollection`\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *x*, *y*, *s*, *linewidths*, *edgecolors*, *c*, *facecolor*, *facecolors*, *color*\n**kwargs : `~matplotlib.collections.PathCollection` properties\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: array-like or scalar or None\n    animated: bool\n    antialiased or aa or antialiaseds: bool or list of bools\n    array: array-like or None\n    capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    clim: (vmin: float, vmax: float)\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    cmap: `.Colormap` or str or None\n    color: :mpltype:`color` or list of RGBA tuples\n    edgecolor or ec or edgecolors: :mpltype:`color` or list of :mpltype:`color` or 'face'\n    facecolor or facecolors or fc: :mpltype:`color` or list of :mpltype:`color`\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    gid: str\n    hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n    hatch_linewidth: unknown\n    in_layout: bool\n    joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    label: object\n    linestyle or dashes or linestyles or ls: str or tuple or list thereof\n    linewidth or linewidths or lw: float or list of floats\n    mouseover: bool\n    norm: `.Normalize` or str or None\n    offset_transform or transOffset: `.Transform`\n    offsets: (N, 2) or (2,) array-like\n    path_effects: list of `.AbstractPathEffect`\n    paths: unknown\n    picker: None or bool or float or callable\n    pickradius: float\n    rasterized: bool\n    sizes: `numpy.ndarray` or None\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    urls: list of str or None\n    visible: bool\n    zorder: float\n\nSee Also\n--------\nplot : To plot scatter plots when markers are identical in size and\n    color.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.scatter`.\n\n* The `.plot` function will be faster for scatterplots where markers\n  don't vary in size or color.\n\n* Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which\n  case all masks will be combined and only unmasked points will be\n  plotted.\n\n* Fundamentally, scatter works with 1D arrays; *x*, *y*, *s*, and *c*\n  may be input as N-D arrays, but within scatter they will be\n  flattened. The exception is *c*, which will be flattened only if its\n  size matches the size of *x* and *y*.",
    "matplotlib.pyplot.sci": "\n        Set the current image.\n\n        This image will be the target of colormap functions like\n        ``pyplot.viridis``, and other functions such as `~.pyplot.clim`.  The\n        current image is an attribute of the current Axes.\n        ",
    "matplotlib.pyplot.semilogx": "Make a plot with log scaling on the x-axis.\n\nCall signatures::\n\n    semilogx([x], y, [fmt], data=None, **kwargs)\n    semilogx([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThis is just a thin wrapper around `.plot` which additionally changes\nthe x-axis to log scaling. All the concepts and parameters of plot can\nbe used here as well.\n\nThe additional parameters *base*, *subs*, and *nonpositive* control the\nx-axis properties. They are just forwarded to `.Axes.set_xscale`.\n\nParameters\n----------\nbase : float, default: 10\n    Base of the x logarithm.\n\nsubs : array-like, optional\n    The location of the minor xticks. If *None*, reasonable locations\n    are automatically chosen depending on the number of decades in the\n    plot. See `.Axes.set_xscale` for details.\n\nnonpositive : {'mask', 'clip'}, default: 'clip'\n    Non-positive values in x can be masked as invalid, or clipped to a\n    very small positive number.\n\n**kwargs\n    All parameters supported by `.plot`.\n\nReturns\n-------\nlist of `.Line2D`\n    Objects representing the plotted data.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.semilogx`.\n",
    "matplotlib.pyplot.semilogy": "Make a plot with log scaling on the y-axis.\n\nCall signatures::\n\n    semilogy([x], y, [fmt], data=None, **kwargs)\n    semilogy([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThis is just a thin wrapper around `.plot` which additionally changes\nthe y-axis to log scaling. All the concepts and parameters of plot can\nbe used here as well.\n\nThe additional parameters *base*, *subs*, and *nonpositive* control the\ny-axis properties. They are just forwarded to `.Axes.set_yscale`.\n\nParameters\n----------\nbase : float, default: 10\n    Base of the y logarithm.\n\nsubs : array-like, optional\n    The location of the minor yticks. If *None*, reasonable locations\n    are automatically chosen depending on the number of decades in the\n    plot. See `.Axes.set_yscale` for details.\n\nnonpositive : {'mask', 'clip'}, default: 'clip'\n    Non-positive values in y can be masked as invalid, or clipped to a\n    very small positive number.\n\n**kwargs\n    All parameters supported by `.plot`.\n\nReturns\n-------\nlist of `.Line2D`\n    Objects representing the plotted data.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.semilogy`.\n",
    "matplotlib.pyplot.set_cmap": "\n    Set the default colormap, and applies it to the current image if any.\n\n    Parameters\n    ----------\n    cmap : `~matplotlib.colors.Colormap` or str\n        A colormap instance or the name of a registered colormap.\n\n    See Also\n    --------\n    colormaps\n    get_cmap\n    ",
    "matplotlib.pyplot.set_loglevel": "Configure Matplotlib's logging levels.\n\nMatplotlib uses the standard library `logging` framework under the root\nlogger 'matplotlib'.  This is a helper function to:\n\n- set Matplotlib's root logger level\n- set the root logger handler's level, creating the handler\n  if it does not exist yet\n\nTypically, one should call ``set_loglevel(\"info\")`` or\n``set_loglevel(\"debug\")`` to get additional debugging information.\n\nUsers or applications that are installing their own logging handlers\nmay want to directly manipulate ``logging.getLogger('matplotlib')`` rather\nthan use this function.\n\nParameters\n----------\nlevel : {\"notset\", \"debug\", \"info\", \"warning\", \"error\", \"critical\"}\n    The log level of the handler.\n\nNotes\n-----\n\n.. note::\n\n    This is equivalent to `matplotlib.set_loglevel`.\n\nThe first time this function is called, an additional handler is attached\nto Matplotlib's root handler; this handler is reused every time and this\nfunction simply manipulates the logger and handler's level.",
    "matplotlib.pyplot.setp": "Set one or more properties on an `.Artist`, or list allowed values.\n\nParameters\n----------\nobj : `~matplotlib.artist.Artist` or list of `.Artist`\n    The artist(s) whose properties are being set or queried.  When setting\n    properties, all artists are affected; when querying the allowed values,\n    only the first instance in the sequence is queried.\n\n    For example, two lines can be made thicker and red with a single call:\n\n    >>> x = arange(0, 1, 0.01)\n    >>> lines = plot(x, sin(2*pi*x), x, sin(4*pi*x))\n    >>> setp(lines, linewidth=2, color='r')\n\nfile : file-like, default: `sys.stdout`\n    Where `setp` writes its output when asked to list allowed values.\n\n    >>> with open('output.log') as file:\n    ...     setp(line, file=file)\n\n    The default, ``None``, means `sys.stdout`.\n\n*args, **kwargs\n    The properties to set.  The following combinations are supported:\n\n    - Set the linestyle of a line to be dashed:\n\n      >>> line, = plot([1, 2, 3])\n      >>> setp(line, linestyle='--')\n\n    - Set multiple properties at once:\n\n      >>> setp(line, linewidth=2, color='r')\n\n    - List allowed values for a line's linestyle:\n\n      >>> setp(line, 'linestyle')\n      linestyle: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n\n    - List all properties that can be set, and their allowed values:\n\n      >>> setp(line)\n      agg_filter: a filter function, ...\n      [long output listing omitted]\n\n    `setp` also supports MATLAB style string/value pairs.  For example, the\n    following are equivalent:\n\n    >>> setp(lines, 'linewidth', 2, 'color', 'r')  # MATLAB style\n    >>> setp(lines, linewidth=2, color='r')        # Python style\n\nSee Also\n--------\ngetp\n\nNotes\n-----\n\n.. note::\n\n    This is equivalent to `matplotlib.artist.setp`.\n",
    "matplotlib.pyplot.show": "\n    Display all open figures.\n\n    Parameters\n    ----------\n    block : bool, optional\n        Whether to wait for all figures to be closed before returning.\n\n        If `True` block and run the GUI main loop until all figure windows\n        are closed.\n\n        If `False` ensure that all figure windows are displayed and return\n        immediately.  In this case, you are responsible for ensuring\n        that the event loop is running to have responsive figures.\n\n        Defaults to True in non-interactive mode and to False in interactive\n        mode (see `.pyplot.isinteractive`).\n\n    See Also\n    --------\n    ion : Enable interactive mode, which shows / updates the figure after\n          every plotting command, so that calling ``show()`` is not necessary.\n    ioff : Disable interactive mode.\n    savefig : Save the figure to an image file instead of showing it on screen.\n\n    Notes\n    -----\n    **Saving figures to file and showing a window at the same time**\n\n    If you want an image file as well as a user interface window, use\n    `.pyplot.savefig` before `.pyplot.show`. At the end of (a blocking)\n    ``show()`` the figure is closed and thus unregistered from pyplot. Calling\n    `.pyplot.savefig` afterwards would save a new and thus empty figure. This\n    limitation of command order does not apply if the show is non-blocking or\n    if you keep a reference to the figure and use `.Figure.savefig`.\n\n    **Auto-show in jupyter notebooks**\n\n    The jupyter backends (activated via ``%matplotlib inline``,\n    ``%matplotlib notebook``, or ``%matplotlib widget``), call ``show()`` at\n    the end of every cell by default. Thus, you usually don't have to call it\n    explicitly there.\n    ",
    "matplotlib.pyplot.specgram": "Plot a spectrogram.\n\nCompute and plot a spectrogram of data in *x*.  Data are split into\n*NFFT* length segments and the spectrum of each section is\ncomputed.  The windowing function *window* is applied to each\nsegment, and the amount of overlap of each segment is\nspecified with *noverlap*. The spectrogram is plotted as a colormap\n(using imshow).\n\nParameters\n----------\nx : 1-D array or sequence\n    Array or sequence containing the data.\n\nFs : float, default: 2\n    The sampling frequency (samples per time unit).  It is used to calculate\n    the Fourier frequencies, *freqs*, in cycles per time unit.\n\nwindow : callable or ndarray, default: `.window_hanning`\n    A function or a vector of length *NFFT*.  To create window vectors see\n    `.window_hanning`, `.window_none`, `numpy.blackman`, `numpy.hamming`,\n    `numpy.bartlett`, `scipy.signal`, `scipy.signal.get_window`, etc.  If a\n    function is passed as the argument, it must take a data segment as an\n    argument and return the windowed version of the segment.\n\nsides : {'default', 'onesided', 'twosided'}, optional\n    Which sides of the spectrum to return. 'default' is one-sided for real\n    data and two-sided for complex data. 'onesided' forces the return of a\n    one-sided spectrum, while 'twosided' forces two-sided.\n\npad_to : int, optional\n    The number of points to which the data segment is padded when performing\n    the FFT.  This can be different from *NFFT*, which specifies the number\n    of data points used.  While not increasing the actual resolution of the\n    spectrum (the minimum distance between resolvable peaks), this can give\n    more points in the plot, allowing for more detail. This corresponds to\n    the *n* parameter in the call to `~numpy.fft.fft`. The default is None,\n    which sets *pad_to* equal to *NFFT*\n\nNFFT : int, default: 256\n    The number of data points used in each block for the FFT.  A power 2 is\n    most efficient.  This should *NOT* be used to get zero padding, or the\n    scaling of the result will be incorrect; use *pad_to* for this instead.\n\ndetrend : {'none', 'mean', 'linear'} or callable, default: 'none'\n    The function applied to each segment before fft-ing, designed to remove\n    the mean or linear trend.  Unlike in MATLAB, where the *detrend* parameter\n    is a vector, in Matplotlib it is a function.  The :mod:`~matplotlib.mlab`\n    module defines `.detrend_none`, `.detrend_mean`, and `.detrend_linear`,\n    but you can use a custom function as well.  You can also use a string to\n    choose one of the functions: 'none' calls `.detrend_none`. 'mean' calls\n    `.detrend_mean`. 'linear' calls `.detrend_linear`.\n\nscale_by_freq : bool, default: True\n    Whether the resulting density values should be scaled by the scaling\n    frequency, which gives density in units of 1/Hz.  This allows for\n    integration over the returned frequency values.  The default is True for\n    MATLAB compatibility.\n\nmode : {'default', 'psd', 'magnitude', 'angle', 'phase'}\n    What sort of spectrum to use.  Default is 'psd', which takes the\n    power spectral density.  'magnitude' returns the magnitude\n    spectrum.  'angle' returns the phase spectrum without unwrapping.\n    'phase' returns the phase spectrum with unwrapping.\n\nnoverlap : int, default: 128\n    The number of points of overlap between blocks.\n\nscale : {'default', 'linear', 'dB'}\n    The scaling of the values in the *spec*.  'linear' is no scaling.\n    'dB' returns the values in dB scale.  When *mode* is 'psd',\n    this is dB power (10 * log10).  Otherwise, this is dB amplitude\n    (20 * log10). 'default' is 'dB' if *mode* is 'psd' or\n    'magnitude' and 'linear' otherwise.  This must be 'linear'\n    if *mode* is 'angle' or 'phase'.\n\nFc : int, default: 0\n    The center frequency of *x*, which offsets the x extents of the\n    plot to reflect the frequency range used when a signal is acquired\n    and then filtered and downsampled to baseband.\n\ncmap : `.Colormap`, default: :rc:`image.cmap`\n\nxextent : *None* or (xmin, xmax)\n    The image extent along the x-axis. The default sets *xmin* to the\n    left border of the first bin (*spectrum* column) and *xmax* to the\n    right border of the last bin. Note that for *noverlap>0* the width\n    of the bins is smaller than those of the segments.\n\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *x*\n\nvmin, vmax : float, optional\n    vmin and vmax define the data range that the colormap covers.\n    By default, the colormap covers the complete value range of the\n    data.\n\n**kwargs\n    Additional keyword arguments are passed on to `~.axes.Axes.imshow`\n    which makes the specgram image. The origin keyword argument\n    is not supported.\n\nReturns\n-------\nspectrum : 2D array\n    Columns are the periodograms of successive segments.\n\nfreqs : 1-D array\n    The frequencies corresponding to the rows in *spectrum*.\n\nt : 1-D array\n    The times corresponding to midpoints of segments (i.e., the columns\n    in *spectrum*).\n\nim : `.AxesImage`\n    The image created by imshow containing the spectrogram.\n\nSee Also\n--------\npsd\n    Differs in the default overlap; in returning the mean of the\n    segment periodograms; in not returning times; and in generating a\n    line plot instead of colormap.\nmagnitude_spectrum\n    A single spectrum, similar to having a single segment when *mode*\n    is 'magnitude'. Plots a line instead of a colormap.\nangle_spectrum\n    A single spectrum, similar to having a single segment when *mode*\n    is 'angle'. Plots a line instead of a colormap.\nphase_spectrum\n    A single spectrum, similar to having a single segment when *mode*\n    is 'phase'. Plots a line instead of a colormap.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.specgram`.\n\nThe parameters *detrend* and *scale_by_freq* do only apply when *mode*\nis set to 'psd'.",
    "matplotlib.pyplot.spring": "\n    Set the colormap to 'spring'.\n\n    This changes the default colormap as well as the colormap of the current\n    image if there is one. See ``help(colormaps)`` for more information.\n    ",
    "matplotlib.pyplot.spy": "Plot the sparsity pattern of a 2D array.\n\nThis visualizes the non-zero values of the array.\n\nTwo plotting styles are available: image and marker. Both\nare available for full arrays, but only the marker style\nworks for `scipy.sparse.spmatrix` instances.\n\n**Image style**\n\nIf *marker* and *markersize* are *None*, `~.Axes.imshow` is used. Any\nextra remaining keyword arguments are passed to this method.\n\n**Marker style**\n\nIf *Z* is a `scipy.sparse.spmatrix` or *marker* or *markersize* are\n*None*, a `.Line2D` object will be returned with the value of marker\ndetermining the marker type, and any remaining keyword arguments\npassed to `~.Axes.plot`.\n\nParameters\n----------\nZ : (M, N) array-like\n    The array to be plotted.\n\nprecision : float or 'present', default: 0\n    If *precision* is 0, any non-zero value will be plotted. Otherwise,\n    values of :math:`|Z| > precision` will be plotted.\n\n    For `scipy.sparse.spmatrix` instances, you can also\n    pass 'present'. In this case any value present in the array\n    will be plotted, even if it is identically zero.\n\naspect : {'equal', 'auto', None} or float, default: 'equal'\n    The aspect ratio of the Axes.  This parameter is particularly\n    relevant for images since it determines whether data pixels are\n    square.\n\n    This parameter is a shortcut for explicitly calling\n    `.Axes.set_aspect`. See there for further details.\n\n    - 'equal': Ensures an aspect ratio of 1. Pixels will be square.\n    - 'auto': The Axes is kept fixed and the aspect is adjusted so\n      that the data fit in the Axes. In general, this will result in\n      non-square pixels.\n    - *None*: Use :rc:`image.aspect`.\n\norigin : {'upper', 'lower'}, default: :rc:`image.origin`\n    Place the [0, 0] index of the array in the upper left or lower left\n    corner of the Axes. The convention 'upper' is typically used for\n    matrices and images.\n\nReturns\n-------\n`~matplotlib.image.AxesImage` or `.Line2D`\n    The return type depends on the plotting style (see above).\n\nOther Parameters\n----------------\n**kwargs\n    The supported additional parameters depend on the plotting style.\n\n    For the image style, you can pass the following additional\n    parameters of `~.Axes.imshow`:\n\n    - *cmap*\n    - *alpha*\n    - *url*\n    - any `.Artist` properties (passed on to the `.AxesImage`)\n\n    For the marker style, you can pass any `.Line2D` property except\n    for *linestyle*:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: :mpltype:`color`\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: :mpltype:`color` or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: :mpltype:`color`\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: :mpltype:`color`\n    markerfacecoloralt or mfcalt: :mpltype:`color`\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.spy`.\n",
    "matplotlib.pyplot.stackplot": "Draw a stacked area plot or a streamgraph.\n\nParameters\n----------\nx : (N,) array-like\n\ny : (M, N) array-like\n    The data is assumed to be unstacked. Each of the following\n    calls is legal::\n\n        stackplot(x, y)           # where y has shape (M, N)\n        stackplot(x, y1, y2, y3)  # where y1, y2, y3, y4 have length N\n\nbaseline : {'zero', 'sym', 'wiggle', 'weighted_wiggle'}\n    Method used to calculate the baseline:\n\n    - ``'zero'``: Constant zero baseline, i.e. a simple stacked plot.\n    - ``'sym'``:  Symmetric around zero and is sometimes called\n      'ThemeRiver'.\n    - ``'wiggle'``: Minimizes the sum of the squared slopes.\n    - ``'weighted_wiggle'``: Does the same but weights to account for\n      size of each layer. It is also called 'Streamgraph'-layout. More\n      details can be found at http://leebyron.com/streamgraph/.\n\nlabels : list of str, optional\n    A sequence of labels to assign to each data series. If unspecified,\n    then no labels will be applied to artists.\n\ncolors : list of :mpltype:`color`, optional\n    A sequence of colors to be cycled through and used to color the stacked\n    areas. The sequence need not be exactly the same length as the number\n    of provided *y*, in which case the colors will repeat from the\n    beginning.\n\n    If not specified, the colors from the Axes property cycle will be used.\n\nhatch : list of str, default: None\n    A sequence of hatching styles.  See\n    :doc:`/gallery/shapes_and_collections/hatch_style_reference`.\n    The sequence will be cycled through for filling the\n    stacked areas from bottom to top.\n    It need not be exactly the same length as the number\n    of provided *y*, in which case the styles will repeat from the\n    beginning.\n\n    .. versionadded:: 3.9\n       Support for list input\n\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``.\n\n**kwargs\n    All other keyword arguments are passed to `.Axes.fill_between`.\n\nReturns\n-------\nlist of `.PolyCollection`\n    A list of `.PolyCollection` instances, one for each element in the\n    stacked area plot.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.stackplot`.\n",
    "matplotlib.pyplot.stairs": "Draw a stepwise constant function as a line or a filled plot.\n\n*edges* define the x-axis positions of the steps. *values* the function values\nbetween these steps. Depending on *fill*, the function is drawn either as a\ncontinuous line with vertical segments at the edges, or as a filled area.\n\nParameters\n----------\nvalues : array-like\n    The step heights.\n\nedges : array-like\n    The step positions, with ``len(edges) == len(vals) + 1``,\n    between which the curve takes on vals values.\n\norientation : {'vertical', 'horizontal'}, default: 'vertical'\n    The direction of the steps. Vertical means that *values* are along\n    the y-axis, and edges are along the x-axis.\n\nbaseline : float, array-like or None, default: 0\n    The bottom value of the bounding edges or when\n    ``fill=True``, position of lower edge. If *fill* is\n    True or an array is passed to *baseline*, a closed\n    path is drawn.\n\n    If None, then drawn as an unclosed Path.\n\nfill : bool, default: False\n    Whether the area under the step curve should be filled.\n\n    Passing both ``fill=True` and ``baseline=None`` will likely result in\n    undesired filling: the first and last points will be connected\n    with a straight line and the fill will be between this line and the stairs.\n\n\nReturns\n-------\nStepPatch : `~matplotlib.patches.StepPatch`\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``.\n\n**kwargs\n    `~matplotlib.patches.StepPatch` properties\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.stairs`.\n",
    "matplotlib.pyplot.stem": "Create a stem plot.\n\nA stem plot draws lines perpendicular to a baseline at each location\n*locs* from the baseline to *heads*, and places a marker there. For\nvertical stem plots (the default), the *locs* are *x* positions, and\nthe *heads* are *y* values. For horizontal stem plots, the *locs* are\n*y* positions, and the *heads* are *x* values.\n\nCall signature::\n\n  stem([locs,] heads, linefmt=None, markerfmt=None, basefmt=None)\n\nThe *locs*-positions are optional. *linefmt* may be provided as\npositional, but all other formats must be provided as keyword\narguments.\n\nParameters\n----------\nlocs : array-like, default: (0, 1, ..., len(heads) - 1)\n    For vertical stem plots, the x-positions of the stems.\n    For horizontal stem plots, the y-positions of the stems.\n\nheads : array-like\n    For vertical stem plots, the y-values of the stem heads.\n    For horizontal stem plots, the x-values of the stem heads.\n\nlinefmt : str, optional\n    A string defining the color and/or linestyle of the vertical lines:\n\n    =========  =============\n    Character  Line Style\n    =========  =============\n    ``'-'``    solid line\n    ``'--'``   dashed line\n    ``'-.'``   dash-dot line\n    ``':'``    dotted line\n    =========  =============\n\n    Default: 'C0-', i.e. solid line with the first color of the color\n    cycle.\n\n    Note: Markers specified through this parameter (e.g. 'x') will be\n    silently ignored. Instead, markers should be specified using\n    *markerfmt*.\n\nmarkerfmt : str, optional\n    A string defining the color and/or shape of the markers at the stem\n    heads. If the marker is not given, use the marker 'o', i.e. filled\n    circles. If the color is not given, use the color from *linefmt*.\n\nbasefmt : str, default: 'C3-' ('C2-' in classic mode)\n    A format string defining the properties of the baseline.\n\norientation : {'vertical', 'horizontal'}, default: 'vertical'\n    The orientation of the stems.\n\nbottom : float, default: 0\n    The y/x-position of the baseline (depending on *orientation*).\n\nlabel : str, optional\n    The label to use for the stems in legends.\n\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``.\n\nReturns\n-------\n`.StemContainer`\n    The container may be treated like a tuple\n    (*markerline*, *stemlines*, *baseline*)\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.stem`.\n\n.. seealso::\n    The MATLAB function\n    `stem <https://www.mathworks.com/help/matlab/ref/stem.html>`_\n    which inspired this method.",
    "matplotlib.pyplot.step": "Make a step plot.\n\nCall signatures::\n\n    step(x, y, [fmt], *, data=None, where='pre', **kwargs)\n    step(x, y, [fmt], x2, y2, [fmt2], ..., *, where='pre', **kwargs)\n\nThis is just a thin wrapper around `.plot` which changes some\nformatting options. Most of the concepts and parameters of plot can be\nused here as well.\n\n.. note::\n\n    This method uses a standard plot with a step drawstyle: The *x*\n    values are the reference positions and steps extend left/right/both\n    directions depending on *where*.\n\n    For the common case where you know the values and edges of the\n    steps, use `~.Axes.stairs` instead.\n\nParameters\n----------\nx : array-like\n    1D sequence of x positions. It is assumed, but not checked, that\n    it is uniformly increasing.\n\ny : array-like\n    1D sequence of y levels.\n\nfmt : str, optional\n    A format string, e.g. 'g' for a green line. See `.plot` for a more\n    detailed description.\n\n    Note: While full format strings are accepted, it is recommended to\n    only specify the color. Line styles are currently ignored (use\n    the keyword argument *linestyle* instead). Markers are accepted\n    and plotted on the given positions, however, this is a rarely\n    needed feature for step plots.\n\nwhere : {'pre', 'post', 'mid'}, default: 'pre'\n    Define where the steps should be placed:\n\n    - 'pre': The y value is continued constantly to the left from\n      every *x* position, i.e. the interval ``(x[i-1], x[i]]`` has the\n      value ``y[i]``.\n    - 'post': The y value is continued constantly to the right from\n      every *x* position, i.e. the interval ``[x[i], x[i+1])`` has the\n      value ``y[i]``.\n    - 'mid': Steps occur half-way between the *x* positions.\n\ndata : indexable object, optional\n    An object with labelled data. If given, provide the label names to\n    plot in *x* and *y*.\n\n**kwargs\n    Additional parameters are the same as those for `.plot`.\n\nReturns\n-------\nlist of `.Line2D`\n    Objects representing the plotted data.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.step`.\n",
    "matplotlib.pyplot.streamplot": "Draw streamlines of a vector flow.\n\nParameters\n----------\nx, y : 1D/2D arrays\n    Evenly spaced strictly increasing arrays to make a grid.  If 2D, all\n    rows of *x* must be equal and all columns of *y* must be equal; i.e.,\n    they must be as if generated by ``np.meshgrid(x_1d, y_1d)``.\nu, v : 2D arrays\n    *x* and *y*-velocities. The number of rows and columns must match\n    the length of *y* and *x*, respectively.\ndensity : float or (float, float)\n    Controls the closeness of streamlines. When ``density = 1``, the domain\n    is divided into a 30x30 grid. *density* linearly scales this grid.\n    Each cell in the grid can have, at most, one traversing streamline.\n    For different densities in each direction, use a tuple\n    (density_x, density_y).\nlinewidth : float or 2D array\n    The width of the streamlines. With a 2D array the line width can be\n    varied across the grid. The array must have the same shape as *u*\n    and *v*.\ncolor : :mpltype:`color` or 2D array\n    The streamline color. If given an array, its values are converted to\n    colors using *cmap* and *norm*.  The array must have the same shape\n    as *u* and *v*.\ncmap, norm\n    Data normalization and colormapping parameters for *color*; only used\n    if *color* is an array of floats. See `~.Axes.imshow` for a detailed\n    description.\narrowsize : float\n    Scaling factor for the arrow size.\narrowstyle : str\n    Arrow style specification.\n    See `~matplotlib.patches.FancyArrowPatch`.\nminlength : float\n    Minimum length of streamline in axes coordinates.\nstart_points : (N, 2) array\n    Coordinates of starting points for the streamlines in data coordinates\n    (the same coordinates as the *x* and *y* arrays).\nzorder : float\n    The zorder of the streamlines and arrows.\n    Artists with lower zorder values are drawn first.\nmaxlength : float\n    Maximum length of streamline in axes coordinates.\nintegration_direction : {'forward', 'backward', 'both'}, default: 'both'\n    Integrate the streamline in forward, backward or both directions.\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *x*, *y*, *u*, *v*, *start_points*\nbroken_streamlines : boolean, default: True\n    If False, forces streamlines to continue until they\n    leave the plot domain.  If True, they may be terminated if they\n    come too close to another streamline.\n\nReturns\n-------\nStreamplotSet\n    Container object with attributes\n\n    - ``lines``: `.LineCollection` of streamlines\n\n    - ``arrows``: `.PatchCollection` containing `.FancyArrowPatch`\n      objects representing the arrows half-way along streamlines.\n\n    This container will probably change in the future to allow changes\n    to the colormap, alpha, etc. for both lines and arrows, but these\n    changes should be backward compatible.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.streamplot`.\n",
    "matplotlib.pyplot.subplot": "Add an Axes to the current figure or retrieve an existing Axes.\n\nThis is a wrapper of `.Figure.add_subplot` which provides additional\nbehavior when working with the implicit API (see the notes section).\n\nCall signatures::\n\n   subplot(nrows, ncols, index, **kwargs)\n   subplot(pos, **kwargs)\n   subplot(**kwargs)\n   subplot(ax)\n\nParameters\n----------\n*args : int, (int, int, *index*), or `.SubplotSpec`, default: (1, 1, 1)\n    The position of the subplot described by one of\n\n    - Three integers (*nrows*, *ncols*, *index*). The subplot will take the\n      *index* position on a grid with *nrows* rows and *ncols* columns.\n      *index* starts at 1 in the upper left corner and increases to the\n      right. *index* can also be a two-tuple specifying the (*first*,\n      *last*) indices (1-based, and including *last*) of the subplot, e.g.,\n      ``fig.add_subplot(3, 1, (1, 2))`` makes a subplot that spans the\n      upper 2/3 of the figure.\n    - A 3-digit integer. The digits are interpreted as if given separately\n      as three single-digit integers, i.e. ``fig.add_subplot(235)`` is the\n      same as ``fig.add_subplot(2, 3, 5)``. Note that this can only be used\n      if there are no more than 9 subplots.\n    - A `.SubplotSpec`.\n\nprojection : {None, 'aitoff', 'hammer', 'lambert', 'mollweide', 'polar', 'rectilinear', str}, optional\n    The projection type of the subplot (`~.axes.Axes`). *str* is the name\n    of a custom projection, see `~matplotlib.projections`. The default\n    None results in a 'rectilinear' projection.\n\npolar : bool, default: False\n    If True, equivalent to projection='polar'.\n\nsharex, sharey : `~matplotlib.axes.Axes`, optional\n    Share the x or y `~matplotlib.axis` with sharex and/or sharey. The\n    axis will have the same limits, ticks, and scale as the axis of the\n    shared Axes.\n\nlabel : str\n    A label for the returned Axes.\n\nReturns\n-------\n`~.axes.Axes`\n\n    The Axes of the subplot. The returned Axes can actually be an instance\n    of a subclass, such as `.projections.polar.PolarAxes` for polar\n    projections.\n\nOther Parameters\n----------------\n**kwargs\n    This method also takes the keyword arguments for the returned Axes\n    base class; except for the *figure* argument. The keyword arguments\n    for the rectilinear base class `~.axes.Axes` can be found in\n    the following table but there might also be other keyword\n    arguments if another projection is used.\n\n    Properties:\n    adjustable: {'box', 'datalim'}\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    anchor: (float, float) or {'C', 'SW', 'S', 'SE', 'E', 'NE', ...}\n    animated: bool\n    aspect: {'auto', 'equal'} or float\n    autoscale_on: bool\n    autoscalex_on: unknown\n    autoscaley_on: unknown\n    axes_locator: Callable[[Axes, Renderer], Bbox]\n    axisbelow: bool or 'line'\n    box_aspect: float or None\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    facecolor or fc: :mpltype:`color`\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    forward_navigation_events: bool or \"auto\"\n    frame_on: bool\n    gid: str\n    in_layout: bool\n    label: object\n    mouseover: bool\n    navigate: bool\n    navigate_mode: unknown\n    path_effects: list of `.AbstractPathEffect`\n    picker: None or bool or float or callable\n    position: [left, bottom, width, height] or `~matplotlib.transforms.Bbox`\n    prop_cycle: `~cycler.Cycler`\n    rasterization_zorder: float or None\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    subplotspec: unknown\n    title: str\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    visible: bool\n    xbound: (lower: float, upper: float)\n    xlabel: str\n    xlim: (left: float, right: float)\n    xmargin: float greater than -0.5\n    xscale: unknown\n    xticklabels: unknown\n    xticks: unknown\n    ybound: (lower: float, upper: float)\n    ylabel: str\n    ylim: (bottom: float, top: float)\n    ymargin: float greater than -0.5\n    yscale: unknown\n    yticklabels: unknown\n    yticks: unknown\n    zorder: float\n\nNotes\n-----\nCreating a new Axes will delete any preexisting Axes that\noverlaps with it beyond sharing a boundary::\n\n    import matplotlib.pyplot as plt\n    # plot a line, implicitly creating a subplot(111)\n    plt.plot([1, 2, 3])\n    # now create a subplot which represents the top plot of a grid\n    # with 2 rows and 1 column. Since this subplot will overlap the\n    # first, the plot (and its Axes) previously created, will be removed\n    plt.subplot(211)\n\nIf you do not want this behavior, use the `.Figure.add_subplot` method\nor the `.pyplot.axes` function instead.\n\nIf no *kwargs* are passed and there exists an Axes in the location\nspecified by *args* then that Axes will be returned rather than a new\nAxes being created.\n\nIf *kwargs* are passed and there exists an Axes in the location\nspecified by *args*, the projection type is the same, and the\n*kwargs* match with the existing Axes, then the existing Axes is\nreturned.  Otherwise a new Axes is created with the specified\nparameters.  We save a reference to the *kwargs* which we use\nfor this comparison.  If any of the values in *kwargs* are\nmutable we will not detect the case where they are mutated.\nIn these cases we suggest using `.Figure.add_subplot` and the\nexplicit Axes API rather than the implicit pyplot API.\n\nSee Also\n--------\n.Figure.add_subplot\n.pyplot.subplots\n.pyplot.axes\n.Figure.subplots\n\nExamples\n--------\n::\n\n    plt.subplot(221)\n\n    # equivalent but more general\n    ax1 = plt.subplot(2, 2, 1)\n\n    # add a subplot with no frame\n    ax2 = plt.subplot(222, frameon=False)\n\n    # add a polar subplot\n    plt.subplot(223, projection='polar')\n\n    # add a red subplot that shares the x-axis with ax1\n    plt.subplot(224, sharex=ax1, facecolor='red')\n\n    # delete ax2 from the figure\n    plt.delaxes(ax2)\n\n    # add ax2 to the figure again\n    plt.subplot(ax2)\n\n    # make the first Axes \"current\" again\n    plt.subplot(221)",
    "matplotlib.pyplot.subplot2grid": "\n    Create a subplot at a specific location inside a regular grid.\n\n    Parameters\n    ----------\n    shape : (int, int)\n        Number of rows and of columns of the grid in which to place axis.\n    loc : (int, int)\n        Row number and column number of the axis location within the grid.\n    rowspan : int, default: 1\n        Number of rows for the axis to span downwards.\n    colspan : int, default: 1\n        Number of columns for the axis to span to the right.\n    fig : `.Figure`, optional\n        Figure to place the subplot in. Defaults to the current figure.\n    **kwargs\n        Additional keyword arguments are handed to `~.Figure.add_subplot`.\n\n    Returns\n    -------\n    `~.axes.Axes`\n\n        The Axes of the subplot. The returned Axes can actually be an instance\n        of a subclass, such as `.projections.polar.PolarAxes` for polar\n        projections.\n\n    Notes\n    -----\n    The following call ::\n\n        ax = subplot2grid((nrows, ncols), (row, col), rowspan, colspan)\n\n    is identical to ::\n\n        fig = gcf()\n        gs = fig.add_gridspec(nrows, ncols)\n        ax = fig.add_subplot(gs[row:row+rowspan, col:col+colspan])\n    ",
    "matplotlib.pyplot.subplot_mosaic": "\n    Build a layout of Axes based on ASCII art or nested lists.\n\n    This is a helper function to build complex GridSpec layouts visually.\n\n    See :ref:`mosaic`\n    for an example and full API documentation\n\n    Parameters\n    ----------\n    mosaic : list of list of {hashable or nested} or str\n\n        A visual layout of how you want your Axes to be arranged\n        labeled as strings.  For example ::\n\n           x = [['A panel', 'A panel', 'edge'],\n                ['C panel', '.',       'edge']]\n\n        produces 4 Axes:\n\n        - 'A panel' which is 1 row high and spans the first two columns\n        - 'edge' which is 2 rows high and is on the right edge\n        - 'C panel' which in 1 row and 1 column wide in the bottom left\n        - a blank space 1 row and 1 column wide in the bottom center\n\n        Any of the entries in the layout can be a list of lists\n        of the same form to create nested layouts.\n\n        If input is a str, then it must be of the form ::\n\n          '''\n          AAE\n          C.E\n          '''\n\n        where each character is a column and each line is a row.\n        This only allows only single character Axes labels and does\n        not allow nesting but is very terse.\n\n    sharex, sharey : bool, default: False\n        If True, the x-axis (*sharex*) or y-axis (*sharey*) will be shared\n        among all subplots.  In that case, tick label visibility and axis units\n        behave as for `subplots`.  If False, each subplot's x- or y-axis will\n        be independent.\n\n    width_ratios : array-like of length *ncols*, optional\n        Defines the relative widths of the columns. Each column gets a\n        relative width of ``width_ratios[i] / sum(width_ratios)``.\n        If not given, all columns will have the same width.  Convenience\n        for ``gridspec_kw={'width_ratios': [...]}``.\n\n    height_ratios : array-like of length *nrows*, optional\n        Defines the relative heights of the rows. Each row gets a\n        relative height of ``height_ratios[i] / sum(height_ratios)``.\n        If not given, all rows will have the same height. Convenience\n        for ``gridspec_kw={'height_ratios': [...]}``.\n\n    empty_sentinel : object, optional\n        Entry in the layout to mean \"leave this space empty\".  Defaults\n        to ``'.'``. Note, if *layout* is a string, it is processed via\n        `inspect.cleandoc` to remove leading white space, which may\n        interfere with using white-space as the empty sentinel.\n\n    subplot_kw : dict, optional\n        Dictionary with keywords passed to the `.Figure.add_subplot` call\n        used to create each subplot.  These values may be overridden by\n        values in *per_subplot_kw*.\n\n    per_subplot_kw : dict, optional\n        A dictionary mapping the Axes identifiers or tuples of identifiers\n        to a dictionary of keyword arguments to be passed to the\n        `.Figure.add_subplot` call used to create each subplot.  The values\n        in these dictionaries have precedence over the values in\n        *subplot_kw*.\n\n        If *mosaic* is a string, and thus all keys are single characters,\n        it is possible to use a single string instead of a tuple as keys;\n        i.e. ``\"AB\"`` is equivalent to ``(\"A\", \"B\")``.\n\n        .. versionadded:: 3.7\n\n    gridspec_kw : dict, optional\n        Dictionary with keywords passed to the `.GridSpec` constructor used\n        to create the grid the subplots are placed on.\n\n    **fig_kw\n        All additional keyword arguments are passed to the\n        `.pyplot.figure` call.\n\n    Returns\n    -------\n    fig : `.Figure`\n       The new figure\n\n    dict[label, Axes]\n       A dictionary mapping the labels to the Axes objects.  The order of\n       the Axes is left-to-right and top-to-bottom of their position in the\n       total layout.\n\n    ",
    "matplotlib.pyplot.subplot_tool": "\n    Launch a subplot tool window for a figure.\n\n    Returns\n    -------\n    `matplotlib.widgets.SubplotTool`\n    ",
    "matplotlib.pyplot.subplots": "\n    Create a figure and a set of subplots.\n\n    This utility wrapper makes it convenient to create common layouts of\n    subplots, including the enclosing figure object, in a single call.\n\n    Parameters\n    ----------\n    nrows, ncols : int, default: 1\n        Number of rows/columns of the subplot grid.\n\n    sharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n        Controls sharing of properties among x (*sharex*) or y (*sharey*)\n        axes:\n\n        - True or 'all': x- or y-axis will be shared among all subplots.\n        - False or 'none': each subplot x- or y-axis will be independent.\n        - 'row': each subplot row will share an x- or y-axis.\n        - 'col': each subplot column will share an x- or y-axis.\n\n        When subplots have a shared x-axis along a column, only the x tick\n        labels of the bottom subplot are created. Similarly, when subplots\n        have a shared y-axis along a row, only the y tick labels of the first\n        column subplot are created. To later turn other subplots' ticklabels\n        on, use `~matplotlib.axes.Axes.tick_params`.\n\n        When subplots have a shared axis that has units, calling\n        `.Axis.set_units` will update each axis with the new units.\n\n        Note that it is not possible to unshare axes.\n\n    squeeze : bool, default: True\n        - If True, extra dimensions are squeezed out from the returned\n          array of `~matplotlib.axes.Axes`:\n\n          - if only one subplot is constructed (nrows=ncols=1), the\n            resulting single Axes object is returned as a scalar.\n          - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n            object array of Axes objects.\n          - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n\n        - If False, no squeezing at all is done: the returned Axes object is\n          always a 2D array containing Axes instances, even if it ends up\n          being 1x1.\n\n    width_ratios : array-like of length *ncols*, optional\n        Defines the relative widths of the columns. Each column gets a\n        relative width of ``width_ratios[i] / sum(width_ratios)``.\n        If not given, all columns will have the same width.  Equivalent\n        to ``gridspec_kw={'width_ratios': [...]}``.\n\n    height_ratios : array-like of length *nrows*, optional\n        Defines the relative heights of the rows. Each row gets a\n        relative height of ``height_ratios[i] / sum(height_ratios)``.\n        If not given, all rows will have the same height. Convenience\n        for ``gridspec_kw={'height_ratios': [...]}``.\n\n    subplot_kw : dict, optional\n        Dict with keywords passed to the\n        `~matplotlib.figure.Figure.add_subplot` call used to create each\n        subplot.\n\n    gridspec_kw : dict, optional\n        Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n        constructor used to create the grid the subplots are placed on.\n\n    **fig_kw\n        All additional keyword arguments are passed to the\n        `.pyplot.figure` call.\n\n    Returns\n    -------\n    fig : `.Figure`\n\n    ax : `~matplotlib.axes.Axes` or array of Axes\n        *ax* can be either a single `~.axes.Axes` object, or an array of Axes\n        objects if more than one subplot was created.  The dimensions of the\n        resulting array can be controlled with the squeeze keyword, see above.\n\n        Typical idioms for handling the return value are::\n\n            # using the variable ax for single a Axes\n            fig, ax = plt.subplots()\n\n            # using the variable axs for multiple Axes\n            fig, axs = plt.subplots(2, 2)\n\n            # using tuple unpacking for multiple Axes\n            fig, (ax1, ax2) = plt.subplots(1, 2)\n            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\n        The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n        because for the latter it's not clear if it refers to a single\n        `~.axes.Axes` instance or a collection of these.\n\n    See Also\n    --------\n    .pyplot.figure\n    .pyplot.subplot\n    .pyplot.axes\n    .Figure.subplots\n    .Figure.add_subplot\n\n    Examples\n    --------\n    ::\n\n        # First create some toy data:\n        x = np.linspace(0, 2*np.pi, 400)\n        y = np.sin(x**2)\n\n        # Create just a figure and only one subplot\n        fig, ax = plt.subplots()\n        ax.plot(x, y)\n        ax.set_title('Simple plot')\n\n        # Create two subplots and unpack the output array immediately\n        f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n        ax1.plot(x, y)\n        ax1.set_title('Sharing Y axis')\n        ax2.scatter(x, y)\n\n        # Create four polar Axes and access them through the returned array\n        fig, axs = plt.subplots(2, 2, subplot_kw=dict(projection=\"polar\"))\n        axs[0, 0].plot(x, y)\n        axs[1, 1].scatter(x, y)\n\n        # Share a X axis with each column of subplots\n        plt.subplots(2, 2, sharex='col')\n\n        # Share a Y axis with each row of subplots\n        plt.subplots(2, 2, sharey='row')\n\n        # Share both X and Y axes with all subplots\n        plt.subplots(2, 2, sharex='all', sharey='all')\n\n        # Note that this is the same as\n        plt.subplots(2, 2, sharex=True, sharey=True)\n\n        # Create figure number 10 with a single subplot\n        # and clears it if it already exists.\n        fig, ax = plt.subplots(num=10, clear=True)\n\n    ",
    "matplotlib.pyplot.subplots_adjust": "Adjust the subplot layout parameters.\n\nUnset parameters are left unmodified; initial values are given by\n:rc:`figure.subplot.[name]`.\n\n.. plot:: _embedded_plots/figure_subplots_adjust.py\n\nParameters\n----------\nleft : float, optional\n    The position of the left edge of the subplots,\n    as a fraction of the figure width.\nright : float, optional\n    The position of the right edge of the subplots,\n    as a fraction of the figure width.\nbottom : float, optional\n    The position of the bottom edge of the subplots,\n    as a fraction of the figure height.\ntop : float, optional\n    The position of the top edge of the subplots,\n    as a fraction of the figure height.\nwspace : float, optional\n    The width of the padding between subplots,\n    as a fraction of the average Axes width.\nhspace : float, optional\n    The height of the padding between subplots,\n    as a fraction of the average Axes height.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.Figure.subplots_adjust`.\n",
    "matplotlib.pyplot.summer": "\n    Set the colormap to 'summer'.\n\n    This changes the default colormap as well as the colormap of the current\n    image if there is one. See ``help(colormaps)`` for more information.\n    ",
    "matplotlib.pyplot.suptitle": "Add a centered super title to the figure.\n\nParameters\n----------\nt : str\n    The super title text.\nx : float, default: 0.5\n    The x location of the text in figure coordinates.\ny : float, default: 0.98\n    The y location of the text in figure coordinates.\nhorizontalalignment, ha : {'center', 'left', 'right'}, default: center\n    The horizontal alignment of the text relative to (*x*, *y*).\nverticalalignment, va : {'top', 'center', 'bottom', 'baseline'}, default: top\n    The vertical alignment of the text relative to (*x*, *y*).\nfontsize, size : default: :rc:`figure.titlesize`\n    The font size of the text. See `.Text.set_size` for possible\n    values.\nfontweight, weight : default: :rc:`figure.titleweight`\n    The font weight of the text. See `.Text.set_weight` for possible\n    values.\n\nReturns\n-------\ntext\n    The `.Text` instance of the super title.\n\nOther Parameters\n----------------\nfontproperties : None or dict, optional\n    A dict of font properties. If *fontproperties* is given the\n    default values for font size and weight are taken from the\n    `.FontProperties` defaults. :rc:`figure.titlesize` and\n    :rc:`figure.titleweight` are ignored in this case.\n\n**kwargs\n    Additional kwargs are `matplotlib.text.Text` properties.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.Figure.suptitle`.\n",
    "matplotlib.pyplot.switch_backend": "\n    Set the pyplot backend.\n\n    Switching to an interactive backend is possible only if no event loop for\n    another interactive backend has started.  Switching to and from\n    non-interactive backends is always possible.\n\n    If the new backend is different than the current backend then all open\n    Figures will be closed via ``plt.close('all')``.\n\n    Parameters\n    ----------\n    newbackend : str\n        The case-insensitive name of the backend to use.\n\n    ",
    "matplotlib.pyplot.table": "Add a table to an `~.axes.Axes`.\n\nAt least one of *cellText* or *cellColours* must be specified. These\nparameters must be 2D lists, in which the outer lists define the rows and\nthe inner list define the column values per row. Each row must have the\nsame number of elements.\n\nThe table can optionally have row and column headers, which are configured\nusing *rowLabels*, *rowColours*, *rowLoc* and *colLabels*, *colColours*,\n*colLoc* respectively.\n\nFor finer grained control over tables, use the `.Table` class and add it to\nthe Axes with `.Axes.add_table`.\n\nParameters\n----------\ncellText : 2D list of str or pandas.DataFrame, optional\n    The texts to place into the table cells.\n\n    *Note*: Line breaks in the strings are currently not accounted for and\n    will result in the text exceeding the cell boundaries.\n\ncellColours : 2D list of :mpltype:`color`, optional\n    The background colors of the cells.\n\ncellLoc : {'right', 'center', 'left'}\n    The alignment of the text within the cells.\n\ncolWidths : list of float, optional\n    The column widths in units of the axes. If not given, all columns will\n    have a width of *1 / ncols*.\n\nrowLabels : list of str, optional\n    The text of the row header cells.\n\nrowColours : list of :mpltype:`color`, optional\n    The colors of the row header cells.\n\nrowLoc : {'left', 'center', 'right'}\n    The text alignment of the row header cells.\n\ncolLabels : list of str, optional\n    The text of the column header cells.\n\ncolColours : list of :mpltype:`color`, optional\n    The colors of the column header cells.\n\ncolLoc : {'center', 'left', 'right'}\n    The text alignment of the column header cells.\n\nloc : str, default: 'bottom'\n    The position of the cell with respect to *ax*. This must be one of\n    the `~.Table.codes`.\n\nbbox : `.Bbox` or [xmin, ymin, width, height], optional\n    A bounding box to draw the table into. If this is not *None*, this\n    overrides *loc*.\n\nedges : {'closed', 'open', 'horizontal', 'vertical'} or substring of 'BRTL'\n    The cell edges to be drawn with a line. See also\n    `~.Cell.visible_edges`.\n\nReturns\n-------\n`~matplotlib.table.Table`\n    The created table.\n\nOther Parameters\n----------------\n**kwargs\n    `.Table` properties.\n\nProperties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fontsize: float\n    gid: str\n    in_layout: bool\n    label: object\n    mouseover: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: None or bool or float or callable\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    visible: bool\n    zorder: float\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.table`.\n",
    "matplotlib.pyplot.text": "Add text to the Axes.\n\nAdd the text *s* to the Axes at location *x*, *y* in data coordinates,\nwith a default ``horizontalalignment`` on the ``left`` and\n``verticalalignment`` at the ``baseline``. See\n:doc:`/gallery/text_labels_and_annotations/text_alignment`.\n\nParameters\n----------\nx, y : float\n    The position to place the text. By default, this is in data\n    coordinates. The coordinate system can be changed using the\n    *transform* parameter.\n\ns : str\n    The text.\n\nfontdict : dict, default: None\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``text(..., **fontdict)``.\n\n    A dictionary to override the default text properties. If fontdict\n    is None, the defaults are determined by `.rcParams`.\n\nReturns\n-------\n`.Text`\n    The created `.Text` instance.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties.\n    Other miscellaneous text parameters.\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased: bool\n    backgroundcolor: :mpltype:`color`\n    bbox: dict with properties for `.patches.FancyBboxPatch`\n    clip_box: unknown\n    clip_on: unknown\n    clip_path: unknown\n    color or c: :mpltype:`color`\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    fontfamily or family or fontname: {FONTNAME, 'serif', 'sans-serif', 'cursive', 'fantasy', 'monospace'}\n    fontproperties or font or font_properties: `.font_manager.FontProperties` or `str` or `pathlib.Path`\n    fontsize or size: float or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n    fontstretch or stretch: {a numeric value in range 0-1000, 'ultra-condensed', 'extra-condensed', 'condensed', 'semi-condensed', 'normal', 'semi-expanded', 'expanded', 'extra-expanded', 'ultra-expanded'}\n    fontstyle or style: {'normal', 'italic', 'oblique'}\n    fontvariant or variant: {'normal', 'small-caps'}\n    fontweight or weight: {a numeric value in range 0-1000, 'ultralight', 'light', 'normal', 'regular', 'book', 'medium', 'roman', 'semibold', 'demibold', 'demi', 'bold', 'heavy', 'extra bold', 'black'}\n    gid: str\n    horizontalalignment or ha: {'left', 'center', 'right'}\n    in_layout: bool\n    label: object\n    linespacing: float (multiple of font size)\n    math_fontfamily: str\n    mouseover: bool\n    multialignment or ma: {'left', 'right', 'center'}\n    parse_math: bool\n    path_effects: list of `.AbstractPathEffect`\n    picker: None or bool or float or callable\n    position: (float, float)\n    rasterized: bool\n    rotation: float or {'vertical', 'horizontal'}\n    rotation_mode: {None, 'default', 'anchor'}\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    text: object\n    transform: `~matplotlib.transforms.Transform`\n    transform_rotates_text: bool\n    url: str\n    usetex: bool, default: :rc:`text.usetex`\n    verticalalignment or va: {'baseline', 'bottom', 'center', 'center_baseline', 'top'}\n    visible: bool\n    wrap: bool\n    x: float\n    y: float\n    zorder: float\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.text`.\n\nExamples\n--------\nIndividual keyword arguments can be used to override any given\nparameter::\n\n    >>> text(x, y, s, fontsize=12)\n\nThe default transform specifies that text is in data coords,\nalternatively, you can specify text in axis coords ((0, 0) is\nlower-left and (1, 1) is upper-right).  The example below places\ntext in the center of the Axes::\n\n    >>> text(0.5, 0.5, 'matplotlib', horizontalalignment='center',\n    ...      verticalalignment='center', transform=ax.transAxes)\n\nYou can put a rectangular box around the text instance (e.g., to\nset a background color) by using the keyword *bbox*.  *bbox* is\na dictionary of `~matplotlib.patches.Rectangle`\nproperties.  For example::\n\n    >>> text(x, y, s, bbox=dict(facecolor='red', alpha=0.5))",
    "matplotlib.pyplot.thetagrids": "\n    Get or set the theta gridlines on the current polar plot.\n\n    Call signatures::\n\n     lines, labels = thetagrids()\n     lines, labels = thetagrids(angles, labels=None, fmt=None, **kwargs)\n\n    When called with no arguments, `.thetagrids` simply returns the tuple\n    (*lines*, *labels*). When called with arguments, the labels will\n    appear at the specified angles.\n\n    Parameters\n    ----------\n    angles : tuple with floats, degrees\n        The angles of the theta gridlines.\n\n    labels : tuple with strings or None\n        The labels to use at each radial gridline. The\n        `.projections.polar.ThetaFormatter` will be used if None.\n\n    fmt : str or None\n        Format string used in `matplotlib.ticker.FormatStrFormatter`.\n        For example '%f'. Note that the angle in radians will be used.\n\n    Returns\n    -------\n    lines : list of `.lines.Line2D`\n        The theta gridlines.\n\n    labels : list of `.text.Text`\n        The tick labels.\n\n    Other Parameters\n    ----------------\n    **kwargs\n        *kwargs* are optional `.Text` properties for the labels.\n\n    See Also\n    --------\n    .pyplot.rgrids\n    .projections.polar.PolarAxes.set_thetagrids\n    .Axis.get_gridlines\n    .Axis.get_ticklabels\n\n    Examples\n    --------\n    ::\n\n      # set the locations of the angular gridlines\n      lines, labels = thetagrids(range(45, 360, 90))\n\n      # set the locations and labels of the angular gridlines\n      lines, labels = thetagrids(range(45, 360, 90), ('NE', 'NW', 'SW', 'SE'))\n    ",
    "matplotlib.pyplot.tick_params": "Change the appearance of ticks, tick labels, and gridlines.\n\nTick properties that are not explicitly set using the keyword\narguments remain unchanged unless *reset* is True. For the current\nstyle settings, see `.Axis.get_tick_params`.\n\nParameters\n----------\naxis : {'x', 'y', 'both'}, default: 'both'\n    The axis to which the parameters are applied.\nwhich : {'major', 'minor', 'both'}, default: 'major'\n    The group of ticks to which the parameters are applied.\nreset : bool, default: False\n    Whether to reset the ticks to defaults before updating them.\n\nOther Parameters\n----------------\ndirection : {'in', 'out', 'inout'}\n    Puts ticks inside the Axes, outside the Axes, or both.\nlength : float\n    Tick length in points.\nwidth : float\n    Tick width in points.\ncolor : :mpltype:`color`\n    Tick color.\npad : float\n    Distance in points between tick and label.\nlabelsize : float or str\n    Tick label font size in points or as a string (e.g., 'large').\nlabelcolor : :mpltype:`color`\n    Tick label color.\nlabelfontfamily : str\n    Tick label font.\ncolors : :mpltype:`color`\n    Tick color and label color.\nzorder : float\n    Tick and label zorder.\nbottom, top, left, right : bool\n    Whether to draw the respective ticks.\nlabelbottom, labeltop, labelleft, labelright : bool\n    Whether to draw the respective tick labels.\nlabelrotation : float\n    Tick label rotation\ngrid_color : :mpltype:`color`\n    Gridline color.\ngrid_alpha : float\n    Transparency of gridlines: 0 (transparent) to 1 (opaque).\ngrid_linewidth : float\n    Width of gridlines in points.\ngrid_linestyle : str\n    Any valid `.Line2D` line style spec.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.tick_params`.\n\nExamples\n--------\n::\n\n    ax.tick_params(direction='out', length=6, width=2, colors='r',\n                   grid_color='r', grid_alpha=0.5)\n\nThis will make all major ticks be red, pointing out of the box,\nand with dimensions 6 points by 2 points.  Tick labels will\nalso be red.  Gridlines will be red and translucent.",
    "matplotlib.pyplot.ticklabel_format": "Configure the `.ScalarFormatter` used by default for linear Axes.\n\nIf a parameter is not set, the corresponding property of the formatter\nis left unchanged.\n\nParameters\n----------\naxis : {'x', 'y', 'both'}, default: 'both'\n    The axis to configure.  Only major ticks are affected.\n\nstyle : {'sci', 'scientific', 'plain'}\n    Whether to use scientific notation.\n    The formatter default is to use scientific notation.\n    'sci' is equivalent to 'scientific'.\n\nscilimits : pair of ints (m, n)\n    Scientific notation is used only for numbers outside the range\n    10\\ :sup:`m` to 10\\ :sup:`n` (and only if the formatter is\n    configured to use scientific notation at all).  Use (0, 0) to\n    include all numbers.  Use (m, m) where m != 0 to fix the order of\n    magnitude to 10\\ :sup:`m`.\n    The formatter default is :rc:`axes.formatter.limits`.\n\nuseOffset : bool or float\n    If True, the offset is calculated as needed.\n    If False, no offset is used.\n    If a numeric value, it sets the offset.\n    The formatter default is :rc:`axes.formatter.useoffset`.\n\nuseLocale : bool\n    Whether to format the number using the current locale or using the\n    C (English) locale.  This affects e.g. the decimal separator.  The\n    formatter default is :rc:`axes.formatter.use_locale`.\n\nuseMathText : bool\n    Render the offset and scientific notation in mathtext.\n    The formatter default is :rc:`axes.formatter.use_mathtext`.\n\nRaises\n------\nAttributeError\n    If the current formatter is not a `.ScalarFormatter`.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.ticklabel_format`.\n",
    "matplotlib.pyplot.tight_layout": "Adjust the padding between and around subplots.\n\nTo exclude an artist on the Axes from the bounding box calculation\nthat determines the subplot parameters (i.e. legend, or annotation),\nset ``a.set_in_layout(False)`` for that artist.\n\nParameters\n----------\npad : float, default: 1.08\n    Padding between the figure edge and the edges of subplots,\n    as a fraction of the font size.\nh_pad, w_pad : float, default: *pad*\n    Padding (height/width) between edges of adjacent subplots,\n    as a fraction of the font size.\nrect : tuple (left, bottom, right, top), default: (0, 0, 1, 1)\n    A rectangle in normalized figure coordinates into which the whole\n    subplots area (including labels) will fit.\n\nSee Also\n--------\n.Figure.set_layout_engine\n.pyplot.tight_layout\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.Figure.tight_layout`.\n",
    "matplotlib.pyplot.title": "Set a title for the Axes.\n\nSet one of the three available Axes titles. The available titles\nare positioned above the Axes in the center, flush with the left\nedge, and flush with the right edge.\n\nParameters\n----------\nlabel : str\n    Text to use for the title\n\nfontdict : dict\n\n    .. admonition:: Discouraged\n\n       The use of *fontdict* is discouraged. Parameters should be passed as\n       individual keyword arguments or using dictionary-unpacking\n       ``set_title(..., **fontdict)``.\n\n    A dictionary controlling the appearance of the title text,\n    the default *fontdict* is::\n\n       {'fontsize': rcParams['axes.titlesize'],\n        'fontweight': rcParams['axes.titleweight'],\n        'color': rcParams['axes.titlecolor'],\n        'verticalalignment': 'baseline',\n        'horizontalalignment': loc}\n\nloc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n    Which title to set.\n\ny : float, default: :rc:`axes.titley`\n    Vertical Axes location for the title (1.0 is the top).  If\n    None (the default) and :rc:`axes.titley` is also None, y is\n    determined automatically to avoid decorators on the Axes.\n\npad : float, default: :rc:`axes.titlepad`\n    The offset of the title from the top of the Axes, in points.\n\nReturns\n-------\n`.Text`\n    The matplotlib text instance representing the title\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    Other keyword arguments are text properties, see `.Text` for a list\n    of valid text properties.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.set_title`.\n",
    "matplotlib.pyplot.tricontour": "Draw contour lines on an unstructured triangular grid.\n\nCall signatures::\n\n    tricontour(triangulation, z, [levels], ...)\n    tricontour(x, y, z, [levels], *, [triangles=triangles], [mask=mask], ...)\n\nThe triangular grid can be specified either by passing a `.Triangulation`\nobject as the first parameter, or by passing the points *x*, *y* and\noptionally the *triangles* and a *mask*. See `.Triangulation` for an\nexplanation of these parameters. If neither of *triangulation* or\n*triangles* are given, the triangulation is calculated on the fly.\n\nIt is possible to pass *triangles* positionally, i.e.\n``tricontour(x, y, triangles, z, ...)``. However, this is discouraged. For more\nclarity, pass *triangles* via keyword argument.\n\nParameters\n----------\ntriangulation : `.Triangulation`, optional\n    An already created triangular grid.\n\nx, y, triangles, mask\n    Parameters defining the triangular grid. See `.Triangulation`.\n    This is mutually exclusive with specifying *triangulation*.\n\nz : array-like\n    The height values over which the contour is drawn.  Color-mapping is\n    controlled by *cmap*, *norm*, *vmin*, and *vmax*.\n\n    .. note::\n        All values in *z* must be finite. Hence, nan and inf values must\n        either be removed or `~.Triangulation.set_mask` be used.\n\nlevels : int or array-like, optional\n    Determines the number and positions of the contour lines / regions.\n\n    If an int *n*, use `~matplotlib.ticker.MaxNLocator`, which tries to\n    automatically choose no more than *n+1* \"nice\" contour levels between\n    between minimum and maximum numeric values of *Z*.\n\n    If array-like, draw contour lines at the specified levels.  The values must\n    be in increasing order.\n\nReturns\n-------\n`~matplotlib.tri.TriContourSet`\n\nOther Parameters\n----------------\ncolors : :mpltype:`color` or list of :mpltype:`color`, optional\n    The colors of the levels, i.e., the contour lines.\n\n    The sequence is cycled for the levels in ascending order. If the sequence\n    is shorter than the number of levels, it is repeated.\n\n    As a shortcut, single color strings may be used in place of one-element\n    lists, i.e. ``'red'`` instead of ``['red']`` to color all levels with the\n    same color. This shortcut does only work for color strings, not for other\n    ways of specifying colors.\n\n    By default (value *None*), the colormap specified by *cmap* will be used.\n\nalpha : float, default: 1\n    The alpha blending value, between 0 (transparent) and 1 (opaque).\n\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    The Colormap instance or registered colormap name used to map scalar data\n    to colors.\n\n    This parameter is ignored if *colors* is set.\n\nnorm : str or `~matplotlib.colors.Normalize`, optional\n    The normalization method used to scale scalar data to the [0, 1] range\n    before mapping to colors using *cmap*. By default, a linear scaling is\n    used, mapping the lowest value to 0 and the highest to 1.\n\n    If given, this can be one of the following:\n\n    - An instance of `.Normalize` or one of its subclasses\n      (see :ref:`colormapnorms`).\n    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n      list of available scales, call `matplotlib.scale.get_scale_names()`.\n      In that case, a suitable `.Normalize` subclass is dynamically generated\n      and instantiated.\n\n    This parameter is ignored if *colors* is set.\n\nvmin, vmax : float, optional\n    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n    the data range that the colormap covers. By default, the colormap covers\n    the complete value range of the supplied data. It is an error to use\n    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n    name together with *vmin*/*vmax* is acceptable).\n\n    If *vmin* or *vmax* are not given, the default color scaling is based on\n    *levels*.\n\n    This parameter is ignored if *colors* is set.\n\norigin : {*None*, 'upper', 'lower', 'image'}, default: None\n    Determines the orientation and exact position of *z* by specifying the\n    position of ``z[0, 0]``.  This is only relevant, if *X*, *Y* are not given.\n\n    - *None*: ``z[0, 0]`` is at X=0, Y=0 in the lower left corner.\n    - 'lower': ``z[0, 0]`` is at X=0.5, Y=0.5 in the lower left corner.\n    - 'upper': ``z[0, 0]`` is at X=N+0.5, Y=0.5 in the upper left corner.\n    - 'image': Use the value from :rc:`image.origin`.\n\nextent : (x0, x1, y0, y1), optional\n    If *origin* is not *None*, then *extent* is interpreted as in `.imshow`: it\n    gives the outer pixel boundaries. In this case, the position of z[0, 0] is\n    the center of the pixel, not a corner. If *origin* is *None*, then\n    (*x0*, *y0*) is the position of z[0, 0], and (*x1*, *y1*) is the position\n    of z[-1, -1].\n\n    This argument is ignored if *X* and *Y* are specified in the call to\n    contour.\n\nlocator : ticker.Locator subclass, optional\n    The locator is used to determine the contour levels if they are not given\n    explicitly via *levels*.\n    Defaults to `~.ticker.MaxNLocator`.\n\nextend : {'neither', 'both', 'min', 'max'}, default: 'neither'\n    Determines the ``tricontour``-coloring of values that are outside the\n    *levels* range.\n\n    If 'neither', values outside the *levels* range are not colored.  If 'min',\n    'max' or 'both', color the values below, above or below and above the\n    *levels* range.\n\n    Values below ``min(levels)`` and above ``max(levels)`` are mapped to the\n    under/over values of the `.Colormap`. Note that most colormaps do not have\n    dedicated colors for these by default, so that the over and under values\n    are the edge values of the colormap.  You may want to set these values\n    explicitly using `.Colormap.set_under` and `.Colormap.set_over`.\n\n    .. note::\n\n        An existing `.TriContourSet` does not get notified if properties of its\n        colormap are changed. Therefore, an explicit call to\n        `.ContourSet.changed()` is needed after modifying the colormap. The\n        explicit call can be left out, if a colorbar is assigned to the\n        `.TriContourSet` because it internally calls `.ContourSet.changed()`.\n\nxunits, yunits : registered units, optional\n    Override axis units by specifying an instance of a\n    :class:`matplotlib.units.ConversionInterface`.\n\nantialiased : bool, optional\n    Enable antialiasing, overriding the defaults.  For\n    filled contours, the default is *True*.  For line contours,\n    it is taken from :rc:`lines.antialiased`.\n\nlinewidths : float or array-like, default: :rc:`contour.linewidth`\n    The line width of the contour lines.\n\n    If a number, all levels will be plotted with this linewidth.\n\n    If a sequence, the levels in ascending order will be plotted with\n    the linewidths in the order specified.\n\n    If None, this falls back to :rc:`lines.linewidth`.\n\nlinestyles : {*None*, 'solid', 'dashed', 'dashdot', 'dotted'}, optional\n    If *linestyles* is *None*, the default is 'solid' unless the lines are\n    monochrome.  In that case, negative contours will take their linestyle\n    from :rc:`contour.negative_linestyle` setting.\n\n    *linestyles* can also be an iterable of the above strings specifying a\n    set of linestyles to be used. If this iterable is shorter than the\n    number of contour levels it will be repeated as necessary.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.tricontour`.\n",
    "matplotlib.pyplot.tricontourf": "Draw contour regions on an unstructured triangular grid.\n\nCall signatures::\n\n    tricontourf(triangulation, z, [levels], ...)\n    tricontourf(x, y, z, [levels], *, [triangles=triangles], [mask=mask], ...)\n\nThe triangular grid can be specified either by passing a `.Triangulation`\nobject as the first parameter, or by passing the points *x*, *y* and\noptionally the *triangles* and a *mask*. See `.Triangulation` for an\nexplanation of these parameters. If neither of *triangulation* or\n*triangles* are given, the triangulation is calculated on the fly.\n\nIt is possible to pass *triangles* positionally, i.e.\n``tricontourf(x, y, triangles, z, ...)``. However, this is discouraged. For more\nclarity, pass *triangles* via keyword argument.\n\nParameters\n----------\ntriangulation : `.Triangulation`, optional\n    An already created triangular grid.\n\nx, y, triangles, mask\n    Parameters defining the triangular grid. See `.Triangulation`.\n    This is mutually exclusive with specifying *triangulation*.\n\nz : array-like\n    The height values over which the contour is drawn.  Color-mapping is\n    controlled by *cmap*, *norm*, *vmin*, and *vmax*.\n\n    .. note::\n        All values in *z* must be finite. Hence, nan and inf values must\n        either be removed or `~.Triangulation.set_mask` be used.\n\nlevels : int or array-like, optional\n    Determines the number and positions of the contour lines / regions.\n\n    If an int *n*, use `~matplotlib.ticker.MaxNLocator`, which tries to\n    automatically choose no more than *n+1* \"nice\" contour levels between\n    between minimum and maximum numeric values of *Z*.\n\n    If array-like, draw contour lines at the specified levels.  The values must\n    be in increasing order.\n\nReturns\n-------\n`~matplotlib.tri.TriContourSet`\n\nOther Parameters\n----------------\ncolors : :mpltype:`color` or list of :mpltype:`color`, optional\n    The colors of the levels, i.e., the contour regions.\n\n    The sequence is cycled for the levels in ascending order. If the sequence\n    is shorter than the number of levels, it is repeated.\n\n    As a shortcut, single color strings may be used in place of one-element\n    lists, i.e. ``'red'`` instead of ``['red']`` to color all levels with the\n    same color. This shortcut does only work for color strings, not for other\n    ways of specifying colors.\n\n    By default (value *None*), the colormap specified by *cmap* will be used.\n\nalpha : float, default: 1\n    The alpha blending value, between 0 (transparent) and 1 (opaque).\n\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    The Colormap instance or registered colormap name used to map scalar data\n    to colors.\n\n    This parameter is ignored if *colors* is set.\n\nnorm : str or `~matplotlib.colors.Normalize`, optional\n    The normalization method used to scale scalar data to the [0, 1] range\n    before mapping to colors using *cmap*. By default, a linear scaling is\n    used, mapping the lowest value to 0 and the highest to 1.\n\n    If given, this can be one of the following:\n\n    - An instance of `.Normalize` or one of its subclasses\n      (see :ref:`colormapnorms`).\n    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n      list of available scales, call `matplotlib.scale.get_scale_names()`.\n      In that case, a suitable `.Normalize` subclass is dynamically generated\n      and instantiated.\n\n    This parameter is ignored if *colors* is set.\n\nvmin, vmax : float, optional\n    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n    the data range that the colormap covers. By default, the colormap covers\n    the complete value range of the supplied data. It is an error to use\n    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n    name together with *vmin*/*vmax* is acceptable).\n\n    If *vmin* or *vmax* are not given, the default color scaling is based on\n    *levels*.\n\n    This parameter is ignored if *colors* is set.\n\norigin : {*None*, 'upper', 'lower', 'image'}, default: None\n    Determines the orientation and exact position of *z* by specifying the\n    position of ``z[0, 0]``.  This is only relevant, if *X*, *Y* are not given.\n\n    - *None*: ``z[0, 0]`` is at X=0, Y=0 in the lower left corner.\n    - 'lower': ``z[0, 0]`` is at X=0.5, Y=0.5 in the lower left corner.\n    - 'upper': ``z[0, 0]`` is at X=N+0.5, Y=0.5 in the upper left corner.\n    - 'image': Use the value from :rc:`image.origin`.\n\nextent : (x0, x1, y0, y1), optional\n    If *origin* is not *None*, then *extent* is interpreted as in `.imshow`: it\n    gives the outer pixel boundaries. In this case, the position of z[0, 0] is\n    the center of the pixel, not a corner. If *origin* is *None*, then\n    (*x0*, *y0*) is the position of z[0, 0], and (*x1*, *y1*) is the position\n    of z[-1, -1].\n\n    This argument is ignored if *X* and *Y* are specified in the call to\n    contour.\n\nlocator : ticker.Locator subclass, optional\n    The locator is used to determine the contour levels if they are not given\n    explicitly via *levels*.\n    Defaults to `~.ticker.MaxNLocator`.\n\nextend : {'neither', 'both', 'min', 'max'}, default: 'neither'\n    Determines the ``tricontourf``-coloring of values that are outside the\n    *levels* range.\n\n    If 'neither', values outside the *levels* range are not colored.  If 'min',\n    'max' or 'both', color the values below, above or below and above the\n    *levels* range.\n\n    Values below ``min(levels)`` and above ``max(levels)`` are mapped to the\n    under/over values of the `.Colormap`. Note that most colormaps do not have\n    dedicated colors for these by default, so that the over and under values\n    are the edge values of the colormap.  You may want to set these values\n    explicitly using `.Colormap.set_under` and `.Colormap.set_over`.\n\n    .. note::\n\n        An existing `.TriContourSet` does not get notified if properties of its\n        colormap are changed. Therefore, an explicit call to\n        `.ContourSet.changed()` is needed after modifying the colormap. The\n        explicit call can be left out, if a colorbar is assigned to the\n        `.TriContourSet` because it internally calls `.ContourSet.changed()`.\n\nxunits, yunits : registered units, optional\n    Override axis units by specifying an instance of a\n    :class:`matplotlib.units.ConversionInterface`.\n\nantialiased : bool, optional\n    Enable antialiasing, overriding the defaults.  For\n    filled contours, the default is *True*.  For line contours,\n    it is taken from :rc:`lines.antialiased`.\n\nhatches : list[str], optional\n    A list of crosshatch patterns to use on the filled areas.\n    If None, no hatching will be added to the contour.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.tricontourf`.\n\n`.tricontourf` fills intervals that are closed at the top; that is, for\nboundaries *z1* and *z2*, the filled region is::\n\n    z1 < Z <= z2\n\nexcept for the lowest interval, which is closed on both sides (i.e. it\nincludes the lowest value).",
    "matplotlib.pyplot.tripcolor": "Create a pseudocolor plot of an unstructured triangular grid.\n\nCall signatures::\n\n  tripcolor(triangulation, c, *, ...)\n  tripcolor(x, y, c, *, [triangles=triangles], [mask=mask], ...)\n\nThe triangular grid can be specified either by passing a `.Triangulation`\nobject as the first parameter, or by passing the points *x*, *y* and\noptionally the *triangles* and a *mask*. See `.Triangulation` for an\nexplanation of these parameters.\n\nIt is possible to pass the triangles positionally, i.e.\n``tripcolor(x, y, triangles, c, ...)``. However, this is discouraged.\nFor more clarity, pass *triangles* via keyword argument.\n\nIf neither of *triangulation* or *triangles* are given, the triangulation\nis calculated on the fly. In this case, it does not make sense to provide\ncolors at the triangle faces via *c* or *facecolors* because there are\nmultiple possible triangulations for a group of points and you don't know\nwhich triangles will be constructed.\n\nParameters\n----------\ntriangulation : `.Triangulation`\n    An already created triangular grid.\nx, y, triangles, mask\n    Parameters defining the triangular grid. See `.Triangulation`.\n    This is mutually exclusive with specifying *triangulation*.\nc : array-like\n    The color values, either for the points or for the triangles. Which one\n    is automatically inferred from the length of *c*, i.e. does it match\n    the number of points or the number of triangles. If there are the same\n    number of points and triangles in the triangulation it is assumed that\n    color values are defined at points; to force the use of color values at\n    triangles use the keyword argument ``facecolors=c`` instead of just\n    ``c``.\n    This parameter is position-only.\nfacecolors : array-like, optional\n    Can be used alternatively to *c* to specify colors at the triangle\n    faces. This parameter takes precedence over *c*.\nshading : {'flat', 'gouraud'}, default: 'flat'\n    If  'flat' and the color values *c* are defined at points, the color\n    values used for each triangle are from the mean c of the triangle's\n    three points. If *shading* is 'gouraud' then color values must be\n    defined at points.\ncmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n    The Colormap instance or registered colormap name used to map scalar data\n    to colors.\n\nnorm : str or `~matplotlib.colors.Normalize`, optional\n    The normalization method used to scale scalar data to the [0, 1] range\n    before mapping to colors using *cmap*. By default, a linear scaling is\n    used, mapping the lowest value to 0 and the highest to 1.\n\n    If given, this can be one of the following:\n\n    - An instance of `.Normalize` or one of its subclasses\n      (see :ref:`colormapnorms`).\n    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n      list of available scales, call `matplotlib.scale.get_scale_names()`.\n      In that case, a suitable `.Normalize` subclass is dynamically generated\n      and instantiated.\n\nvmin, vmax : float, optional\n    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n    the data range that the colormap covers. By default, the colormap covers\n    the complete value range of the supplied data. It is an error to use\n    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n    name together with *vmin*/*vmax* is acceptable).\n\ncolorizer : `~matplotlib.colorizer.Colorizer` or None, default: None\n    The Colorizer object used to map color to data. If None, a Colorizer\n    object is created from a *norm* and *cmap*.\n\nReturns\n-------\n`~matplotlib.collections.PolyCollection` or `~matplotlib.collections.TriMesh`\n    The result depends on *shading*: For ``shading='flat'`` the result is a\n    `.PolyCollection`, for ``shading='gouraud'`` the result is a `.TriMesh`.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.collections.Collection` properties\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: array-like or scalar or None\n    animated: bool\n    antialiased or aa or antialiaseds: bool or list of bools\n    array: array-like or None\n    capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    clim: (vmin: float, vmax: float)\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    cmap: `.Colormap` or str or None\n    color: :mpltype:`color` or list of RGBA tuples\n    edgecolor or ec or edgecolors: :mpltype:`color` or list of :mpltype:`color` or 'face'\n    facecolor or facecolors or fc: :mpltype:`color` or list of :mpltype:`color`\n    figure: `~matplotlib.figure.Figure` or `~matplotlib.figure.SubFigure`\n    gid: str\n    hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n    hatch_linewidth: unknown\n    in_layout: bool\n    joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    label: object\n    linestyle or dashes or linestyles or ls: str or tuple or list thereof\n    linewidth or linewidths or lw: float or list of floats\n    mouseover: bool\n    norm: `.Normalize` or str or None\n    offset_transform or transOffset: `.Transform`\n    offsets: (N, 2) or (2,) array-like\n    path_effects: list of `.AbstractPathEffect`\n    paths: unknown\n    picker: None or bool or float or callable\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    urls: list of str or None\n    visible: bool\n    zorder: float\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.tripcolor`.\n",
    "matplotlib.pyplot.triplot": "Draw an unstructured triangular grid as lines and/or markers.\n\nCall signatures::\n\n  triplot(triangulation, ...)\n  triplot(x, y, [triangles], *, [mask=mask], ...)\n\nThe triangular grid can be specified either by passing a `.Triangulation`\nobject as the first parameter, or by passing the points *x*, *y* and\noptionally the *triangles* and a *mask*. If neither of *triangulation* or\n*triangles* are given, the triangulation is calculated on the fly.\n\nParameters\n----------\ntriangulation : `.Triangulation`\n    An already created triangular grid.\nx, y, triangles, mask\n    Parameters defining the triangular grid. See `.Triangulation`.\n    This is mutually exclusive with specifying *triangulation*.\nother_parameters\n    All other args and kwargs are forwarded to `~.Axes.plot`.\n\nReturns\n-------\nlines : `~matplotlib.lines.Line2D`\n    The drawn triangles edges.\nmarkers : `~matplotlib.lines.Line2D`\n    The drawn marker nodes.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.triplot`.\n",
    "matplotlib.pyplot.twinx": "\n    Make and return a second Axes that shares the *x*-axis.  The new Axes will\n    overlay *ax* (or the current Axes if *ax* is *None*), and its ticks will be\n    on the right.\n\n    Examples\n    --------\n    :doc:`/gallery/subplots_axes_and_figures/two_scales`\n    ",
    "matplotlib.pyplot.twiny": "\n    Make and return a second Axes that shares the *y*-axis.  The new Axes will\n    overlay *ax* (or the current Axes if *ax* is *None*), and its ticks will be\n    on the top.\n\n    Examples\n    --------\n    :doc:`/gallery/subplots_axes_and_figures/two_scales`\n    ",
    "matplotlib.pyplot.uninstall_repl_displayhook": "Disconnect from the display hook of the current shell.",
    "matplotlib.pyplot.violinplot": "Make a violin plot.\n\nMake a violin plot for each column of *dataset* or each vector in\nsequence *dataset*.  Each filled area extends to represent the\nentire data range, with optional lines at the mean, the median,\nthe minimum, the maximum, and user-specified quantiles.\n\nParameters\n----------\ndataset : Array or a sequence of vectors.\n    The input data.\n\npositions : array-like, default: [1, 2, ..., n]\n    The positions of the violins; i.e. coordinates on the x-axis for\n    vertical violins (or y-axis for horizontal violins).\n\nvert : bool, optional\n    .. deprecated:: 3.10\n        Use *orientation* instead.\n\n        If this is given during the deprecation period, it overrides\n        the *orientation* parameter.\n\n    If True, plots the violins vertically.\n    If False, plots the violins horizontally.\n\norientation : {'vertical', 'horizontal'}, default: 'vertical'\n    If 'horizontal', plots the violins horizontally.\n    Otherwise, plots the violins vertically.\n\n    .. versionadded:: 3.10\n\nwidths : float or array-like, default: 0.5\n    The maximum width of each violin in units of the *positions* axis.\n    The default is 0.5, which is half the available space when using default\n    *positions*.\n\nshowmeans : bool, default: False\n    Whether to show the mean with a line.\n\nshowextrema : bool, default: True\n    Whether to show extrema with a line.\n\nshowmedians : bool, default: False\n    Whether to show the median with a line.\n\nquantiles : array-like, default: None\n    If not None, set a list of floats in interval [0, 1] for each violin,\n    which stands for the quantiles that will be rendered for that\n    violin.\n\npoints : int, default: 100\n    The number of points to evaluate each of the gaussian kernel density\n    estimations at.\n\nbw_method : {'scott', 'silverman'} or float or callable, default: 'scott'\n    The method used to calculate the estimator bandwidth.  If a\n    float, this will be used directly as `kde.factor`.  If a\n    callable, it should take a `matplotlib.mlab.GaussianKDE` instance as\n    its only parameter and return a float.\n\nside : {'both', 'low', 'high'}, default: 'both'\n    'both' plots standard violins. 'low'/'high' only\n    plots the side below/above the positions value.\n\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *dataset*\n\nReturns\n-------\ndict\n    A dictionary mapping each component of the violinplot to a\n    list of the corresponding collection instances created. The\n    dictionary has the following keys:\n\n    - ``bodies``: A list of the `~.collections.PolyCollection`\n      instances containing the filled area of each violin.\n\n    - ``cmeans``: A `~.collections.LineCollection` instance that marks\n      the mean values of each of the violin's distribution.\n\n    - ``cmins``: A `~.collections.LineCollection` instance that marks\n      the bottom of each violin's distribution.\n\n    - ``cmaxes``: A `~.collections.LineCollection` instance that marks\n      the top of each violin's distribution.\n\n    - ``cbars``: A `~.collections.LineCollection` instance that marks\n      the centers of each violin's distribution.\n\n    - ``cmedians``: A `~.collections.LineCollection` instance that\n      marks the median values of each of the violin's distribution.\n\n    - ``cquantiles``: A `~.collections.LineCollection` instance created\n      to identify the quantile values of each of the violin's\n      distribution.\n\nSee Also\n--------\n.Axes.violin : Draw a violin from pre-computed statistics.\nboxplot : Draw a box and whisker plot.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.violinplot`.\n",
    "matplotlib.pyplot.viridis": "\n    Set the colormap to 'viridis'.\n\n    This changes the default colormap as well as the colormap of the current\n    image if there is one. See ``help(colormaps)`` for more information.\n    ",
    "matplotlib.pyplot.vlines": "Plot vertical lines at each *x* from *ymin* to *ymax*.\n\nParameters\n----------\nx : float or array-like\n    x-indexes where to plot the lines.\n\nymin, ymax : float or array-like\n    Respective beginning and end of each line. If scalars are\n    provided, all lines will have the same length.\n\ncolors : :mpltype:`color` or list of color, default: :rc:`lines.color`\n\nlinestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, default: 'solid'\n\nlabel : str, default: ''\n\nReturns\n-------\n`~matplotlib.collections.LineCollection`\n\nOther Parameters\n----------------\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *x*, *ymin*, *ymax*, *colors*\n**kwargs : `~matplotlib.collections.LineCollection` properties.\n\nSee Also\n--------\nhlines : horizontal lines\naxvline : vertical line across the Axes\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.vlines`.\n",
    "matplotlib.pyplot.waitforbuttonpress": "Blocking call to interact with the figure.\n\nWait for user input and return True if a key was pressed, False if a\nmouse button was pressed and None if no input was given within\n*timeout* seconds.  Negative values deactivate *timeout*.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.Figure.waitforbuttonpress`.\n",
    "matplotlib.pyplot.winter": "\n    Set the colormap to 'winter'.\n\n    This changes the default colormap as well as the colormap of the current\n    image if there is one. See ``help(colormaps)`` for more information.\n    ",
    "matplotlib.pyplot.xcorr": "Plot the cross correlation between *x* and *y*.\n\nThe correlation with lag k is defined as\n:math:`\\sum_n x[n+k] \\cdot y^*[n]`, where :math:`y^*` is the complex\nconjugate of :math:`y`.\n\nParameters\n----------\nx, y : array-like of length n\n    Neither *x* nor *y* are run through Matplotlib's unit conversion, so\n    these should be unit-less arrays.\n\ndetrend : callable, default: `.mlab.detrend_none` (no detrending)\n    A detrending function applied to *x* and *y*.  It must have the\n    signature ::\n\n        detrend(x: np.ndarray) -> np.ndarray\n\nnormed : bool, default: True\n    If ``True``, input vectors are normalised to unit length.\n\nusevlines : bool, default: True\n    Determines the plot style.\n\n    If ``True``, vertical lines are plotted from 0 to the xcorr value\n    using `.Axes.vlines`. Additionally, a horizontal line is plotted\n    at y=0 using `.Axes.axhline`.\n\n    If ``False``, markers are plotted at the xcorr values using\n    `.Axes.plot`.\n\nmaxlags : int, default: 10\n    Number of lags to show. If None, will return all ``2 * len(x) - 1``\n    lags.\n\nReturns\n-------\nlags : array (length ``2*maxlags+1``)\n    The lag vector.\nc : array  (length ``2*maxlags+1``)\n    The auto correlation vector.\nline : `.LineCollection` or `.Line2D`\n    `.Artist` added to the Axes of the correlation:\n\n    - `.LineCollection` if *usevlines* is True.\n    - `.Line2D` if *usevlines* is False.\nb : `~matplotlib.lines.Line2D` or None\n    Horizontal line at 0 if *usevlines* is True\n    None *usevlines* is False.\n\nOther Parameters\n----------------\nlinestyle : `~matplotlib.lines.Line2D` property, optional\n    The linestyle for plotting the data points.\n    Only used if *usevlines* is ``False``.\n\nmarker : str, default: 'o'\n    The marker for plotting the data points.\n    Only used if *usevlines* is ``False``.\n\ndata : indexable object, optional\n    If given, the following parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` if ``s`` is a key in ``data``:\n\n    *x*, *y*\n\n**kwargs\n    Additional parameters are passed to `.Axes.vlines` and\n    `.Axes.axhline` if *usevlines* is ``True``; otherwise they are\n    passed to `.Axes.plot`.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.xcorr`.\n\nThe cross correlation is performed with `numpy.correlate` with\n``mode = \"full\"``.",
    "matplotlib.pyplot.xkcd": "\n    Turn on `xkcd <https://xkcd.com/>`_ sketch-style drawing mode.\n\n    This will only have an effect on things drawn after this function is called.\n\n    For best results, install the `xkcd script <https://github.com/ipython/xkcd-font/>`_\n    font; xkcd fonts are not packaged with Matplotlib.\n\n    Parameters\n    ----------\n    scale : float, optional\n        The amplitude of the wiggle perpendicular to the source line.\n    length : float, optional\n        The length of the wiggle along the line.\n    randomness : float, optional\n        The scale factor by which the length is shrunken or expanded.\n\n    Notes\n    -----\n    This function works by a number of rcParams, so it will probably\n    override others you have set before.\n\n    If you want the effects of this function to be temporary, it can\n    be used as a context manager, for example::\n\n        with plt.xkcd():\n            # This figure will be in XKCD-style\n            fig1 = plt.figure()\n            # ...\n\n        # This figure will be in regular style\n        fig2 = plt.figure()\n    ",
    "matplotlib.pyplot.xlabel": "Set the label for the x-axis.\n\nParameters\n----------\nxlabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *x* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.set_xlabel`.\n",
    "matplotlib.pyplot.xlim": "\n    Get or set the x limits of the current Axes.\n\n    Call signatures::\n\n        left, right = xlim()  # return the current xlim\n        xlim((left, right))   # set the xlim to left, right\n        xlim(left, right)     # set the xlim to left, right\n\n    If you do not specify args, you can pass *left* or *right* as kwargs,\n    i.e.::\n\n        xlim(right=3)  # adjust the right leaving left unchanged\n        xlim(left=1)  # adjust the left leaving right unchanged\n\n    Setting limits turns autoscaling off for the x-axis.\n\n    Returns\n    -------\n    left, right\n        A tuple of the new x-axis limits.\n\n    Notes\n    -----\n    Calling this function with no arguments (e.g. ``xlim()``) is the pyplot\n    equivalent of calling `~.Axes.get_xlim` on the current Axes.\n    Calling this function with arguments is the pyplot equivalent of calling\n    `~.Axes.set_xlim` on the current Axes. All arguments are passed though.\n    ",
    "matplotlib.pyplot.xscale": "Set the xaxis' scale.\n\nParameters\n----------\nvalue : {\"linear\", \"log\", \"symlog\", \"logit\", ...} or `.ScaleBase`\n    The axis scale type to apply.\n\n**kwargs\n    Different keyword arguments are accepted, depending on the scale.\n    See the respective class keyword arguments:\n\n    - `matplotlib.scale.LinearScale`\n    - `matplotlib.scale.LogScale`\n    - `matplotlib.scale.SymmetricalLogScale`\n    - `matplotlib.scale.LogitScale`\n    - `matplotlib.scale.FuncScale`\n    - `matplotlib.scale.AsinhScale`\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.set_xscale`.\n\nBy default, Matplotlib supports the above-mentioned scales.\nAdditionally, custom scales may be registered using\n`matplotlib.scale.register_scale`. These scales can then also\nbe used here.",
    "matplotlib.pyplot.xticks": "\n    Get or set the current tick locations and labels of the x-axis.\n\n    Pass no arguments to return the current values without modifying them.\n\n    Parameters\n    ----------\n    ticks : array-like, optional\n        The list of xtick locations.  Passing an empty list removes all xticks.\n    labels : array-like, optional\n        The labels to place at the given *ticks* locations.  This argument can\n        only be passed if *ticks* is passed as well.\n    minor : bool, default: False\n        If ``False``, get/set the major ticks/labels; if ``True``, the minor\n        ticks/labels.\n    **kwargs\n        `.Text` properties can be used to control the appearance of the labels.\n\n        .. warning::\n\n            This only sets the properties of the current ticks, which is\n            only sufficient if you either pass *ticks*, resulting in a\n            fixed list of ticks, or if the plot is static.\n\n            Ticks are not guaranteed to be persistent. Various operations\n            can create, delete and modify the Tick instances. There is an\n            imminent risk that these settings can get lost if you work on\n            the figure further (including also panning/zooming on a\n            displayed figure).\n\n            Use `~.pyplot.tick_params` instead if possible.\n\n\n    Returns\n    -------\n    locs\n        The list of xtick locations.\n    labels\n        The list of xlabel `.Text` objects.\n\n    Notes\n    -----\n    Calling this function with no arguments (e.g. ``xticks()``) is the pyplot\n    equivalent of calling `~.Axes.get_xticks` and `~.Axes.get_xticklabels` on\n    the current Axes.\n    Calling this function with arguments is the pyplot equivalent of calling\n    `~.Axes.set_xticks` and `~.Axes.set_xticklabels` on the current Axes.\n\n    Examples\n    --------\n    >>> locs, labels = xticks()  # Get the current locations and labels.\n    >>> xticks(np.arange(0, 1, step=0.2))  # Set label locations.\n    >>> xticks(np.arange(3), ['Tom', 'Dick', 'Sue'])  # Set text labels.\n    >>> xticks([0, 1, 2], ['January', 'February', 'March'],\n    ...        rotation=20)  # Set text labels and properties.\n    >>> xticks([])  # Disable xticks.\n    ",
    "matplotlib.pyplot.ylabel": "Set the label for the y-axis.\n\nParameters\n----------\nylabel : str\n    The label text.\n\nlabelpad : float, default: :rc:`axes.labelpad`\n    Spacing in points from the Axes bounding box including ticks\n    and tick labels.  If None, the previous value is left as is.\n\nloc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n    The label position. This is a high-level alternative for passing\n    parameters *y* and *horizontalalignment*.\n\nOther Parameters\n----------------\n**kwargs : `~matplotlib.text.Text` properties\n    `.Text` properties control the appearance of the label.\n\nSee Also\n--------\ntext : Documents the properties supported by `.Text`.\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.set_ylabel`.\n",
    "matplotlib.pyplot.ylim": "\n    Get or set the y-limits of the current Axes.\n\n    Call signatures::\n\n        bottom, top = ylim()  # return the current ylim\n        ylim((bottom, top))   # set the ylim to bottom, top\n        ylim(bottom, top)     # set the ylim to bottom, top\n\n    If you do not specify args, you can alternatively pass *bottom* or\n    *top* as kwargs, i.e.::\n\n        ylim(top=3)  # adjust the top leaving bottom unchanged\n        ylim(bottom=1)  # adjust the bottom leaving top unchanged\n\n    Setting limits turns autoscaling off for the y-axis.\n\n    Returns\n    -------\n    bottom, top\n        A tuple of the new y-axis limits.\n\n    Notes\n    -----\n    Calling this function with no arguments (e.g. ``ylim()``) is the pyplot\n    equivalent of calling `~.Axes.get_ylim` on the current Axes.\n    Calling this function with arguments is the pyplot equivalent of calling\n    `~.Axes.set_ylim` on the current Axes. All arguments are passed though.\n    ",
    "matplotlib.pyplot.yscale": "Set the yaxis' scale.\n\nParameters\n----------\nvalue : {\"linear\", \"log\", \"symlog\", \"logit\", ...} or `.ScaleBase`\n    The axis scale type to apply.\n\n**kwargs\n    Different keyword arguments are accepted, depending on the scale.\n    See the respective class keyword arguments:\n\n    - `matplotlib.scale.LinearScale`\n    - `matplotlib.scale.LogScale`\n    - `matplotlib.scale.SymmetricalLogScale`\n    - `matplotlib.scale.LogitScale`\n    - `matplotlib.scale.FuncScale`\n    - `matplotlib.scale.AsinhScale`\n\nNotes\n-----\n\n.. note::\n\n    This is the :ref:`pyplot wrapper <pyplot_interface>` for `.axes.Axes.set_yscale`.\n\nBy default, Matplotlib supports the above-mentioned scales.\nAdditionally, custom scales may be registered using\n`matplotlib.scale.register_scale`. These scales can then also\nbe used here.",
    "matplotlib.pyplot.yticks": "\n    Get or set the current tick locations and labels of the y-axis.\n\n    Pass no arguments to return the current values without modifying them.\n\n    Parameters\n    ----------\n    ticks : array-like, optional\n        The list of ytick locations.  Passing an empty list removes all yticks.\n    labels : array-like, optional\n        The labels to place at the given *ticks* locations.  This argument can\n        only be passed if *ticks* is passed as well.\n    minor : bool, default: False\n        If ``False``, get/set the major ticks/labels; if ``True``, the minor\n        ticks/labels.\n    **kwargs\n        `.Text` properties can be used to control the appearance of the labels.\n\n        .. warning::\n\n            This only sets the properties of the current ticks, which is\n            only sufficient if you either pass *ticks*, resulting in a\n            fixed list of ticks, or if the plot is static.\n\n            Ticks are not guaranteed to be persistent. Various operations\n            can create, delete and modify the Tick instances. There is an\n            imminent risk that these settings can get lost if you work on\n            the figure further (including also panning/zooming on a\n            displayed figure).\n\n            Use `~.pyplot.tick_params` instead if possible.\n\n    Returns\n    -------\n    locs\n        The list of ytick locations.\n    labels\n        The list of ylabel `.Text` objects.\n\n    Notes\n    -----\n    Calling this function with no arguments (e.g. ``yticks()``) is the pyplot\n    equivalent of calling `~.Axes.get_yticks` and `~.Axes.get_yticklabels` on\n    the current Axes.\n    Calling this function with arguments is the pyplot equivalent of calling\n    `~.Axes.set_yticks` and `~.Axes.set_yticklabels` on the current Axes.\n\n    Examples\n    --------\n    >>> locs, labels = yticks()  # Get the current locations and labels.\n    >>> yticks(np.arange(0, 1, step=0.2))  # Set label locations.\n    >>> yticks(np.arange(3), ['Tom', 'Dick', 'Sue'])  # Set text labels.\n    >>> yticks([0, 1, 2], ['January', 'February', 'March'],\n    ...        rotation=45)  # Set text labels and properties.\n    >>> yticks([])  # Disable yticks.\n    ",
    "seaborn.FacetGrid": "Multi-plot grid for plotting conditional relationships.",
    "seaborn.JointGrid": "Grid for drawing a bivariate plot with marginal univariate plots.\n\n    Many plots can be drawn by using the figure-level interface :func:`jointplot`.\n    Use this class directly when you need more flexibility.\n\n    ",
    "seaborn.PairGrid": "Subplot grid for plotting pairwise relationships in a dataset.\n\n    This object maps each variable in a dataset onto a column and row in a\n    grid of multiple axes. Different axes-level plotting functions can be\n    used to draw bivariate plots in the upper and lower triangles, and the\n    marginal distribution of each variable can be shown on the diagonal.\n\n    Several different common plots can be generated in a single line using\n    :func:`pairplot`. Use :class:`PairGrid` when you need more flexibility.\n\n    See the :ref:`tutorial <grid_tutorial>` for more information.\n\n    ",
    "seaborn.axes_style": "\n    Get the parameters that control the general style of the plots.\n\n    The style parameters control properties like the color of the background and\n    whether a grid is enabled by default. This is accomplished using the\n    matplotlib rcParams system.\n\n    The options are illustrated in the\n    :doc:`aesthetics tutorial <../tutorial/aesthetics>`.\n\n    This function can also be used as a context manager to temporarily\n    alter the global defaults. See :func:`set_theme` or :func:`set_style`\n    to modify the global defaults for all plots.\n\n    Parameters\n    ----------\n    style : None, dict, or one of {darkgrid, whitegrid, dark, white, ticks}\n        A dictionary of parameters or the name of a preconfigured style.\n    rc : dict, optional\n        Parameter mappings to override the values in the preset seaborn\n        style dictionaries. This only updates parameters that are\n        considered part of the style definition.\n\n    Examples\n    --------\n\n    .. include:: ../docstrings/axes_style.rst\n\n    ",
    "seaborn.barplot": "Show point estimates and errors as rectangular bars.\n\nA bar plot represents an aggregate or statistical estimate for a numeric\nvariable with the height of each rectangle and indicates the uncertainty\naround that estimate using an error bar. Bar plots include 0 in the\naxis range, and they are a good choice when 0 is a meaningful value\nfor the variable to take.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \nestimator : string or callable that maps vector -> scalar\n    Statistical function to estimate within each categorical bin.\nerrorbar : string, (string, number) tuple, callable or None\n    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple\n    with a method name and a level parameter, or a function that maps from a\n    vector to a (min, max) interval, or None to hide errorbar. See the\n    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.\n\n    .. versionadded:: v0.12.0\nn_boot : int\n    Number of bootstrap samples used to compute confidence intervals.\nseed : int, `numpy.random.Generator`, or `numpy.random.RandomState`\n    Seed or random number generator for reproducible bootstrapping.\nunits : name of variable in `data` or vector data\n    Identifier of sampling units; used by the errorbar function to\n    perform a multilevel bootstrap and account for repeated measures\nweights : name of variable in `data` or vector data\n    Data values or column used to compute weighted statistics.\n    Note that the use of weights may limit other statistical options.\n\n    .. versionadded:: v0.13.1    \norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nsaturation : float\n    Proportion of the original saturation to draw fill colors in. Large\n    patches often look better with desaturated colors, but set this to\n    `1` if you want the colors to perfectly match the input values.    \nfill : bool\n    If True, use a solid patch. Otherwise, draw as line art.\n\n    .. versionadded:: v0.13.0    \nhue_norm : tuple or :class:`matplotlib.colors.Normalize` object\n    Normalization in data units for colormap applied to the `hue`\n    variable when it is numeric. Not relevant if `hue` is categorical.\n\n    .. versionadded:: v0.12.0    \nwidth : float\n    Width allotted to each element on the orient axis. When `native_scale=True`,\n    it is relative to the minimum distance between two values in the native scale.    \ndodge : \"auto\" or bool\n    When hue mapping is used, whether elements should be narrowed and shifted along\n    the orient axis to eliminate overlap. If `\"auto\"`, set to `True` when the\n    orient variable is crossed with the categorical variable or `False` otherwise.\n\n    .. versionchanged:: 0.13.0\n\n        Added `\"auto\"` mode as a new default.    \ngap : float\n    Shrink on the orient axis by this factor to add a gap between dodged elements.\n\n    .. versionadded:: 0.13.0    \nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\n\n    .. versionadded:: v0.13.0    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \ncapsize : float\n    Width of the \"caps\" on error bars, relative to bar spacing.    \nerr_kws : dict\n    Parameters of :class:`matplotlib.lines.Line2D`, for the error bar artists.\n\n    .. versionadded:: v0.13.0    \nci : float\n    Level of the confidence interval to show, in [0, 100].\n\n    .. deprecated:: v0.12.0\n        Use `errorbar=(\"ci\", ...)`.    \nerrcolor : matplotlib color\n    Color used for the error bar lines.\n\n    .. deprecated:: 0.13.0\n        Use `err_kws={'color': ...}`.    \nerrwidth : float\n    Thickness of error bar lines (and caps), in points.\n\n    .. deprecated:: 0.13.0\n        Use `err_kws={'linewidth': ...}`.    \nax : matplotlib Axes\n    Axes object to draw the plot onto, otherwise uses the current Axes.    \nkwargs : key, value mappings\n    Other parameters are passed through to :class:`matplotlib.patches.Rectangle`.\n\nReturns\n-------\nax : matplotlib Axes\n    Returns the Axes object with the plot drawn onto it.    \n\nSee Also\n--------\ncountplot : Show the counts of observations in each categorical bin.    \npointplot : Show point estimates and confidence intervals using dots.    \ncatplot : Combine a categorical plot with a :class:`FacetGrid`.    \n\nNotes\n-----\n\nFor datasets where 0 is not a meaningful value, a :func:`pointplot` will\nallow you to focus on differences between levels of one or more categorical\nvariables.\n\nIt is also important to keep in mind that a bar plot shows only the mean (or\nother aggregate) value, but it is often more informative to show the\ndistribution of values at each level of the categorical variables. In those\ncases, approaches such as a :func:`boxplot` or :func:`violinplot` may be\nmore appropriate.\n\nExamples\n--------\n.. include:: ../docstrings/barplot.rst\n\n",
    "seaborn.blend_palette": "Make a palette that blends between a list of colors.\n\n    Parameters\n    ----------\n    colors : sequence of colors in various formats interpreted by `input`\n        hex code, html color name, or tuple in `input` space.\n    n_colors : int, optional\n        Number of colors in the palette.\n    as_cmap : bool, optional\n        If True, return a :class:`matplotlib.colors.ListedColormap`.\n\n    Returns\n    -------\n    palette\n        list of RGB tuples or :class:`matplotlib.colors.ListedColormap`\n\n    Examples\n    --------\n    .. include: ../docstrings/blend_palette.rst\n\n    ",
    "seaborn.boxenplot": "Draw an enhanced box plot for larger datasets.\n\nThis style of plot was originally named a \"letter value\" plot because it\nshows a large number of quantiles that are defined as \"letter values\".  It\nis similar to a box plot in plotting a nonparametric representation of a\ndistribution in which all features correspond to actual observations. By\nplotting more quantiles, it provides more information about the shape of\nthe distribution, particularly in the tails.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nsaturation : float\n    Proportion of the original saturation to draw fill colors in. Large\n    patches often look better with desaturated colors, but set this to\n    `1` if you want the colors to perfectly match the input values.    \nfill : bool\n    If True, use a solid patch. Otherwise, draw as line art.\n\n    .. versionadded:: v0.13.0    \ndodge : \"auto\" or bool\n    When hue mapping is used, whether elements should be narrowed and shifted along\n    the orient axis to eliminate overlap. If `\"auto\"`, set to `True` when the\n    orient variable is crossed with the categorical variable or `False` otherwise.\n\n    .. versionchanged:: 0.13.0\n\n        Added `\"auto\"` mode as a new default.    \nwidth : float\n    Width allotted to each element on the orient axis. When `native_scale=True`,\n    it is relative to the minimum distance between two values in the native scale.    \ngap : float\n    Shrink on the orient axis by this factor to add a gap between dodged elements.\n\n    .. versionadded:: 0.13.0    \nlinewidth : float\n    Width of the lines that frame the plot elements.    \nlinecolor : color\n    Color to use for line elements, when `fill` is True.\n\n    .. versionadded:: v0.13.0    \nwidth_method : {\"exponential\", \"linear\", \"area\"}\n    Method to use for the width of the letter value boxes:\n\n    - `\"exponential\"`: Represent the corresponding percentile\n    - `\"linear\"`: Decrease by a constant amount for each box\n    - `\"area\"`: Represent the density of data points in that box\nk_depth : {\"tukey\", \"proportion\", \"trustworthy\", \"full\"} or int\n    The number of levels to compute and draw in each tail:\n\n    - `\"tukey\"`: Use log2(n) - 3 levels, covering similar range as boxplot whiskers\n    - `\"proportion\"`: Leave approximately `outlier_prop` fliers\n    - `\"trusthworthy\"`: Extend to level with confidence of at least `trust_alpha`\n    - `\"full\"`: Use log2(n) + 1 levels and extend to most extreme points\noutlier_prop : float\n    Proportion of data expected to be outliers; used when `k_depth=\"proportion\"`.\ntrust_alpha : float\n    Confidence threshold for most extreme level; used when `k_depth=\"trustworthy\"`.\nshowfliers : bool\n    If False, suppress the plotting of outliers.\nhue_norm : tuple or :class:`matplotlib.colors.Normalize` object\n    Normalization in data units for colormap applied to the `hue`\n    variable when it is numeric. Not relevant if `hue` is categorical.\n\n    .. versionadded:: v0.12.0    \nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\n\n    .. versionadded:: v0.13.0    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \nbox_kws: dict\n    Keyword arguments for the box artists; passed to\n    :class:`matplotlib.patches.Rectangle`.\n\n    .. versionadded:: v0.12.0\nline_kws: dict\n    Keyword arguments for the line denoting the median; passed to\n    :meth:`matplotlib.axes.Axes.plot`.\n\n    .. versionadded:: v0.12.0\nflier_kws: dict\n    Keyword arguments for the scatter denoting the outlier observations;\n    passed to :meth:`matplotlib.axes.Axes.scatter`.\n\n    .. versionadded:: v0.12.0\nax : matplotlib Axes\n    Axes object to draw the plot onto, otherwise uses the current Axes.    \nkwargs : key, value mappings\n    Other keyword arguments are passed to :class:`matplotlib.patches.Rectangle`,\n    superceded by those in `box_kws`.\n\nReturns\n-------\nax : matplotlib Axes\n    Returns the Axes object with the plot drawn onto it.    \n\nSee Also\n--------\nviolinplot : A combination of boxplot and kernel density estimation.    \nboxplot : A traditional box-and-whisker plot with a similar API.    \ncatplot : Combine a categorical plot with a :class:`FacetGrid`.    \n\nNotes\n-----\n\nFor a more extensive explanation, you can read the paper that introduced the plot:\nhttps://vita.had.co.nz/papers/letter-value-plot.html\n\nExamples\n--------\n.. include:: ../docstrings/boxenplot.rst\n\n",
    "seaborn.boxplot": "Draw a box plot to show distributions with respect to categories.\n\nA box plot (or box-and-whisker plot) shows the distribution of quantitative\ndata in a way that facilitates comparisons between variables or across\nlevels of a categorical variable. The box shows the quartiles of the\ndataset while the whiskers extend to show the rest of the distribution,\nexcept for points that are determined to be \"outliers\" using a method\nthat is a function of the inter-quartile range.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nsaturation : float\n    Proportion of the original saturation to draw fill colors in. Large\n    patches often look better with desaturated colors, but set this to\n    `1` if you want the colors to perfectly match the input values.    \nfill : bool\n    If True, use a solid patch. Otherwise, draw as line art.\n\n    .. versionadded:: v0.13.0    \ndodge : \"auto\" or bool\n    When hue mapping is used, whether elements should be narrowed and shifted along\n    the orient axis to eliminate overlap. If `\"auto\"`, set to `True` when the\n    orient variable is crossed with the categorical variable or `False` otherwise.\n\n    .. versionchanged:: 0.13.0\n\n        Added `\"auto\"` mode as a new default.    \nwidth : float\n    Width allotted to each element on the orient axis. When `native_scale=True`,\n    it is relative to the minimum distance between two values in the native scale.    \ngap : float\n    Shrink on the orient axis by this factor to add a gap between dodged elements.\n\n    .. versionadded:: 0.13.0    \nwhis : float or pair of floats\n    Paramater that controls whisker length. If scalar, whiskers are drawn\n    to the farthest datapoint within *whis * IQR* from the nearest hinge.\n    If a tuple, it is interpreted as percentiles that whiskers represent.\nlinecolor : color\n    Color to use for line elements, when `fill` is True.\n\n    .. versionadded:: v0.13.0    \nlinewidth : float\n    Width of the lines that frame the plot elements.    \nfliersize : float\n    Size of the markers used to indicate outlier observations.\nhue_norm : tuple or :class:`matplotlib.colors.Normalize` object\n    Normalization in data units for colormap applied to the `hue`\n    variable when it is numeric. Not relevant if `hue` is categorical.\n\n    .. versionadded:: v0.12.0    \nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\n\n    .. versionadded:: v0.13.0    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \nax : matplotlib Axes\n    Axes object to draw the plot onto, otherwise uses the current Axes.    \nkwargs : key, value mappings\n    Other keyword arguments are passed through to\n    :meth:`matplotlib.axes.Axes.boxplot`.\n\nReturns\n-------\nax : matplotlib Axes\n    Returns the Axes object with the plot drawn onto it.    \n\nSee Also\n--------\nviolinplot : A combination of boxplot and kernel density estimation.    \nstripplot : A scatterplot where one variable is categorical. Can be used\n            in conjunction with other plots to show each observation.    \nswarmplot : A categorical scatterplot where the points do not overlap. Can\n            be used with other plots to show each observation.    \ncatplot : Combine a categorical plot with a :class:`FacetGrid`.    \n\nExamples\n--------\n.. include:: ../docstrings/boxplot.rst\n\n",
    "seaborn.catplot": "Figure-level interface for drawing categorical plots onto a FacetGrid.\n\nThis function provides access to several axes-level functions that\nshow the relationship between a numerical and one or more categorical\nvariables using one of several visual representations. The `kind`\nparameter selects the underlying axes-level function to use.\n\nCategorical scatterplots:\n\n- :func:`stripplot` (with `kind=\"strip\"`; the default)\n- :func:`swarmplot` (with `kind=\"swarm\"`)\n\nCategorical distribution plots:\n\n- :func:`boxplot` (with `kind=\"box\"`)\n- :func:`violinplot` (with `kind=\"violin\"`)\n- :func:`boxenplot` (with `kind=\"boxen\"`)\n\nCategorical estimate plots:\n\n- :func:`pointplot` (with `kind=\"point\"`)\n- :func:`barplot` (with `kind=\"bar\"`)\n- :func:`countplot` (with `kind=\"count\"`)\n\nExtra keyword arguments are passed to the underlying function, so you\nshould refer to the documentation for each to see kind-specific options.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nAfter plotting, the :class:`FacetGrid` with the plot is returned and can\nbe used directly to tweak supporting plot details or add other layers.\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \nrow, col : names of variables in `data` or vector data\n    Categorical variables that will determine the faceting of the grid.\nkind : str\n    The kind of plot to draw, corresponds to the name of a categorical\n    axes-level plotting function. Options are: \"strip\", \"swarm\", \"box\", \"violin\",\n    \"boxen\", \"point\", \"bar\", or \"count\".\nestimator : string or callable that maps vector -> scalar\n    Statistical function to estimate within each categorical bin.\nerrorbar : string, (string, number) tuple, callable or None\n    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple\n    with a method name and a level parameter, or a function that maps from a\n    vector to a (min, max) interval, or None to hide errorbar. See the\n    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.\n\n    .. versionadded:: v0.12.0\nn_boot : int\n    Number of bootstrap samples used to compute confidence intervals.\nseed : int, `numpy.random.Generator`, or `numpy.random.RandomState`\n    Seed or random number generator for reproducible bootstrapping.\nunits : name of variable in `data` or vector data\n    Identifier of sampling units; used by the errorbar function to\n    perform a multilevel bootstrap and account for repeated measures\nweights : name of variable in `data` or vector data\n    Data values or column used to compute weighted statistics.\n    Note that the use of weights may limit other statistical options.\n\n    .. versionadded:: v0.13.1    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \nrow_order, col_order : lists of strings\n    Order to organize the rows and/or columns of the grid in; otherwise the\n    orders are inferred from the data objects.\ncol_wrap : int\n    \"Wrap\" the column variable at this width, so that the column facets\n    span multiple rows. Incompatible with a ``row`` facet.    \nheight : scalar\n    Height (in inches) of each facet. See also: ``aspect``.    \naspect : scalar\n    Aspect ratio of each facet, so that ``aspect * height`` gives the width\n    of each facet in inches.    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nhue_norm : tuple or :class:`matplotlib.colors.Normalize` object\n    Normalization in data units for colormap applied to the `hue`\n    variable when it is numeric. Not relevant if `hue` is categorical.\n\n    .. versionadded:: v0.12.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \nlegend_out : bool\n    If ``True``, the figure size will be extended, and the legend will be\n    drawn outside the plot on the center right.    \nshare{x,y} : bool, 'col', or 'row' optional\n    If true, the facets will share y axes across columns and/or x axes\n    across rows.    \nmargin_titles : bool\n    If ``True``, the titles for the row variable are drawn to the right of\n    the last column. This option is experimental and may not work in all\n    cases.    \nfacet_kws : dict\n    Dictionary of other keyword arguments to pass to :class:`FacetGrid`.\nkwargs : key, value pairings\n    Other keyword arguments are passed through to the underlying plotting\n    function.\n\nReturns\n-------\n:class:`FacetGrid`\n    Returns the :class:`FacetGrid` object with the plot on it for further\n    tweaking.\n\nExamples\n--------\n.. include:: ../docstrings/catplot.rst\n\n",
    "seaborn.choose_colorbrewer_palette": "Select a palette from the ColorBrewer set.\n\n    These palettes are built into matplotlib and can be used by name in\n    many seaborn functions, or by passing the object returned by this function.\n\n    Parameters\n    ----------\n    data_type : {'sequential', 'diverging', 'qualitative'}\n        This describes the kind of data you want to visualize. See the seaborn\n        color palette docs for more information about how to choose this value.\n        Note that you can pass substrings (e.g. 'q' for 'qualitative.\n\n    as_cmap : bool\n        If True, the return value is a matplotlib colormap rather than a\n        list of discrete colors.\n\n    Returns\n    -------\n    pal or cmap : list of colors or matplotlib colormap\n        Object that can be passed to plotting functions.\n\n    See Also\n    --------\n    dark_palette : Create a sequential palette with dark low values.\n    light_palette : Create a sequential palette with bright low values.\n    diverging_palette : Create a diverging palette from selected colors.\n    cubehelix_palette : Create a sequential palette or colormap using the\n                        cubehelix system.\n\n\n    ",
    "seaborn.choose_cubehelix_palette": "Launch an interactive widget to create a sequential cubehelix palette.\n\n    This corresponds with the :func:`cubehelix_palette` function. This kind\n    of palette is good for data that range between relatively uninteresting\n    low values and interesting high values. The cubehelix system allows the\n    palette to have more hue variance across the range, which can be helpful\n    for distinguishing a wider range of values.\n\n    Requires IPython 2+ and must be used in the notebook.\n\n    Parameters\n    ----------\n    as_cmap : bool\n        If True, the return value is a matplotlib colormap rather than a\n        list of discrete colors.\n\n    Returns\n    -------\n    pal or cmap : list of colors or matplotlib colormap\n        Object that can be passed to plotting functions.\n\n    See Also\n    --------\n    cubehelix_palette : Create a sequential palette or colormap using the\n                        cubehelix system.\n\n    ",
    "seaborn.choose_dark_palette": "Launch an interactive widget to create a dark sequential palette.\n\n    This corresponds with the :func:`dark_palette` function. This kind\n    of palette is good for data that range between relatively uninteresting\n    low values and interesting high values.\n\n    Requires IPython 2+ and must be used in the notebook.\n\n    Parameters\n    ----------\n    input : {'husl', 'hls', 'rgb'}\n        Color space for defining the seed value. Note that the default is\n        different than the default input for :func:`dark_palette`.\n    as_cmap : bool\n        If True, the return value is a matplotlib colormap rather than a\n        list of discrete colors.\n\n    Returns\n    -------\n    pal or cmap : list of colors or matplotlib colormap\n        Object that can be passed to plotting functions.\n\n    See Also\n    --------\n    dark_palette : Create a sequential palette with dark low values.\n    light_palette : Create a sequential palette with bright low values.\n    cubehelix_palette : Create a sequential palette or colormap using the\n                        cubehelix system.\n\n    ",
    "seaborn.choose_diverging_palette": "Launch an interactive widget to choose a diverging color palette.\n\n    This corresponds with the :func:`diverging_palette` function. This kind\n    of palette is good for data that range between interesting low values\n    and interesting high values with a meaningful midpoint. (For example,\n    change scores relative to some baseline value).\n\n    Requires IPython 2+ and must be used in the notebook.\n\n    Parameters\n    ----------\n    as_cmap : bool\n        If True, the return value is a matplotlib colormap rather than a\n        list of discrete colors.\n\n    Returns\n    -------\n    pal or cmap : list of colors or matplotlib colormap\n        Object that can be passed to plotting functions.\n\n    See Also\n    --------\n    diverging_palette : Create a diverging color palette or colormap.\n    choose_colorbrewer_palette : Interactively choose palettes from the\n                                 colorbrewer set, including diverging palettes.\n\n    ",
    "seaborn.choose_light_palette": "Launch an interactive widget to create a light sequential palette.\n\n    This corresponds with the :func:`light_palette` function. This kind\n    of palette is good for data that range between relatively uninteresting\n    low values and interesting high values.\n\n    Requires IPython 2+ and must be used in the notebook.\n\n    Parameters\n    ----------\n    input : {'husl', 'hls', 'rgb'}\n        Color space for defining the seed value. Note that the default is\n        different than the default input for :func:`light_palette`.\n    as_cmap : bool\n        If True, the return value is a matplotlib colormap rather than a\n        list of discrete colors.\n\n    Returns\n    -------\n    pal or cmap : list of colors or matplotlib colormap\n        Object that can be passed to plotting functions.\n\n    See Also\n    --------\n    light_palette : Create a sequential palette with bright low values.\n    dark_palette : Create a sequential palette with dark low values.\n    cubehelix_palette : Create a sequential palette or colormap using the\n                        cubehelix system.\n\n    ",
    "seaborn.clustermap": "\n    Plot a matrix dataset as a hierarchically-clustered heatmap.\n\n    This function requires scipy to be available.\n\n    Parameters\n    ----------\n    data : 2D array-like\n        Rectangular data for clustering. Cannot contain NAs.\n    pivot_kws : dict, optional\n        If `data` is a tidy dataframe, can provide keyword arguments for\n        pivot to create a rectangular dataframe.\n    method : str, optional\n        Linkage method to use for calculating clusters. See\n        :func:`scipy.cluster.hierarchy.linkage` documentation for more\n        information.\n    metric : str, optional\n        Distance metric to use for the data. See\n        :func:`scipy.spatial.distance.pdist` documentation for more options.\n        To use different metrics (or methods) for rows and columns, you may\n        construct each linkage matrix yourself and provide them as\n        `{row,col}_linkage`.\n    z_score : int or None, optional\n        Either 0 (rows) or 1 (columns). Whether or not to calculate z-scores\n        for the rows or the columns. Z scores are: z = (x - mean)/std, so\n        values in each row (column) will get the mean of the row (column)\n        subtracted, then divided by the standard deviation of the row (column).\n        This ensures that each row (column) has mean of 0 and variance of 1.\n    standard_scale : int or None, optional\n        Either 0 (rows) or 1 (columns). Whether or not to standardize that\n        dimension, meaning for each row or column, subtract the minimum and\n        divide each by its maximum.\n    figsize : tuple of (width, height), optional\n        Overall size of the figure.\n    cbar_kws : dict, optional\n        Keyword arguments to pass to `cbar_kws` in :func:`heatmap`, e.g. to\n        add a label to the colorbar.\n    {row,col}_cluster : bool, optional\n        If ``True``, cluster the {rows, columns}.\n    {row,col}_linkage : :class:`numpy.ndarray`, optional\n        Precomputed linkage matrix for the rows or columns. See\n        :func:`scipy.cluster.hierarchy.linkage` for specific formats.\n    {row,col}_colors : list-like or pandas DataFrame/Series, optional\n        List of colors to label for either the rows or columns. Useful to evaluate\n        whether samples within a group are clustered together. Can use nested lists or\n        DataFrame for multiple color levels of labeling. If given as a\n        :class:`pandas.DataFrame` or :class:`pandas.Series`, labels for the colors are\n        extracted from the DataFrames column names or from the name of the Series.\n        DataFrame/Series colors are also matched to the data by their index, ensuring\n        colors are drawn in the correct order.\n    mask : bool array or DataFrame, optional\n        If passed, data will not be shown in cells where `mask` is True.\n        Cells with missing values are automatically masked. Only used for\n        visualizing, not for calculating.\n    {dendrogram,colors}_ratio : float, or pair of floats, optional\n        Proportion of the figure size devoted to the two marginal elements. If\n        a pair is given, they correspond to (row, col) ratios.\n    cbar_pos : tuple of (left, bottom, width, height), optional\n        Position of the colorbar axes in the figure. Setting to ``None`` will\n        disable the colorbar.\n    tree_kws : dict, optional\n        Parameters for the :class:`matplotlib.collections.LineCollection`\n        that is used to plot the lines of the dendrogram tree.\n    kwargs : other keyword arguments\n        All other keyword arguments are passed to :func:`heatmap`.\n\n    Returns\n    -------\n    :class:`ClusterGrid`\n        A :class:`ClusterGrid` instance.\n\n    See Also\n    --------\n    heatmap : Plot rectangular data as a color-encoded matrix.\n\n    Notes\n    -----\n    The returned object has a ``savefig`` method that should be used if you\n    want to save the figure object without clipping the dendrograms.\n\n    To access the reordered row indices, use:\n    ``clustergrid.dendrogram_row.reordered_ind``\n\n    Column indices, use:\n    ``clustergrid.dendrogram_col.reordered_ind``\n\n    Examples\n    --------\n\n    .. include:: ../docstrings/clustermap.rst\n\n    ",
    "seaborn.color_palette": "Return a list of colors or continuous colormap defining a palette.\n\n    Possible ``palette`` values include:\n        - Name of a seaborn palette (deep, muted, bright, pastel, dark, colorblind)\n        - Name of matplotlib colormap\n        - 'husl' or 'hls'\n        - 'ch:<cubehelix arguments>'\n        - 'light:<color>', 'dark:<color>', 'blend:<color>,<color>',\n        - A sequence of colors in any format matplotlib accepts\n\n    Calling this function with ``palette=None`` will return the current\n    matplotlib color cycle.\n\n    This function can also be used in a ``with`` statement to temporarily\n    set the color cycle for a plot or set of plots.\n\n    See the :ref:`tutorial <palette_tutorial>` for more information.\n\n    Parameters\n    ----------\n    palette : None, string, or sequence, optional\n        Name of palette or None to return current palette. If a sequence, input\n        colors are used but possibly cycled and desaturated.\n    n_colors : int, optional\n        Number of colors in the palette. If ``None``, the default will depend\n        on how ``palette`` is specified. Named palettes default to 6 colors,\n        but grabbing the current palette or passing in a list of colors will\n        not change the number of colors unless this is specified. Asking for\n        more colors than exist in the palette will cause it to cycle. Ignored\n        when ``as_cmap`` is True.\n    desat : float, optional\n        Proportion to desaturate each color by.\n    as_cmap : bool\n        If True, return a :class:`matplotlib.colors.ListedColormap`.\n\n    Returns\n    -------\n    list of RGB tuples or :class:`matplotlib.colors.ListedColormap`\n\n    See Also\n    --------\n    set_palette : Set the default color cycle for all plots.\n    set_color_codes : Reassign color codes like ``\"b\"``, ``\"g\"``, etc. to\n                      colors from one of the seaborn palettes.\n\n    Examples\n    --------\n\n    .. include:: ../docstrings/color_palette.rst\n\n    ",
    "seaborn.countplot": "Show the counts of observations in each categorical bin using bars.\n\nA count plot can be thought of as a histogram across a categorical, instead\nof quantitative, variable. The basic API and options are identical to those\nfor :func:`barplot`, so you can compare counts across nested variables.\n\nNote that :func:`histplot` function offers similar functionality with additional\nfeatures (e.g. bar stacking), although its default behavior is somewhat different.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nsaturation : float\n    Proportion of the original saturation to draw fill colors in. Large\n    patches often look better with desaturated colors, but set this to\n    `1` if you want the colors to perfectly match the input values.    \nhue_norm : tuple or :class:`matplotlib.colors.Normalize` object\n    Normalization in data units for colormap applied to the `hue`\n    variable when it is numeric. Not relevant if `hue` is categorical.\n\n    .. versionadded:: v0.12.0    \nstat : {'count', 'percent', 'proportion', 'probability'}\n    Statistic to compute; when not `'count'`, bar heights will be normalized so that\n    they sum to 100 (for `'percent'`) or 1 (otherwise) across the plot.\n\n    .. versionadded:: v0.13.0\nwidth : float\n    Width allotted to each element on the orient axis. When `native_scale=True`,\n    it is relative to the minimum distance between two values in the native scale.    \ndodge : \"auto\" or bool\n    When hue mapping is used, whether elements should be narrowed and shifted along\n    the orient axis to eliminate overlap. If `\"auto\"`, set to `True` when the\n    orient variable is crossed with the categorical variable or `False` otherwise.\n\n    .. versionchanged:: 0.13.0\n\n        Added `\"auto\"` mode as a new default.    \nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\n\n    .. versionadded:: v0.13.0    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \nax : matplotlib Axes\n    Axes object to draw the plot onto, otherwise uses the current Axes.    \nkwargs : key, value mappings\n    Other parameters are passed through to :class:`matplotlib.patches.Rectangle`.\n\nReturns\n-------\nax : matplotlib Axes\n    Returns the Axes object with the plot drawn onto it.    \n\nSee Also\n--------\nhistplot : Bin and count observations with additional options.\nbarplot : Show point estimates and confidence intervals using bars.    \ncatplot : Combine a categorical plot with a :class:`FacetGrid`.    \n\nExamples\n--------\n.. include:: ../docstrings/countplot.rst\n\n",
    "seaborn.crayon_palette": "Make a palette with color names from Crayola crayons.\n\n    Colors are taken from here:\n    https://en.wikipedia.org/wiki/List_of_Crayola_crayon_colors\n\n    This is just a simple wrapper around the `seaborn.crayons` dictionary.\n\n    Parameters\n    ----------\n    colors : list of strings\n        List of keys in the `seaborn.crayons` dictionary.\n\n    Returns\n    -------\n    palette\n        A list of colors as RGB tuples.\n\n    See Also\n    --------\n    xkcd_palette : Make a palette with named colors from the XKCD color survey.\n\n    ",
    "seaborn.cubehelix_palette": "Make a sequential palette from the cubehelix system.\n\n    This produces a colormap with linearly-decreasing (or increasing)\n    brightness. That means that information will be preserved if printed to\n    black and white or viewed by someone who is colorblind.  \"cubehelix\" is\n    also available as a matplotlib-based palette, but this function gives the\n    user more control over the look of the palette and has a different set of\n    defaults.\n\n    In addition to using this function, it is also possible to generate a\n    cubehelix palette generally in seaborn using a string starting with\n    `ch:` and containing other parameters (e.g. `\"ch:s=.25,r=-.5\"`).\n\n    Parameters\n    ----------\n    n_colors : int\n        Number of colors in the palette.\n    start : float, 0 <= start <= 3\n        The hue value at the start of the helix.\n    rot : float\n        Rotations around the hue wheel over the range of the palette.\n    gamma : float 0 <= gamma\n        Nonlinearity to emphasize dark (gamma < 1) or light (gamma > 1) colors.\n    hue : float, 0 <= hue <= 1\n        Saturation of the colors.\n    dark : float 0 <= dark <= 1\n        Intensity of the darkest color in the palette.\n    light : float 0 <= light <= 1\n        Intensity of the lightest color in the palette.\n    reverse : bool\n        If True, the palette will go from dark to light.\n    as_cmap : bool\n        If True, return a :class:`matplotlib.colors.ListedColormap`.\n\n    Returns\n    -------\n    palette\n        list of RGB tuples or :class:`matplotlib.colors.ListedColormap`\n\n    See Also\n    --------\n    choose_cubehelix_palette : Launch an interactive widget to select cubehelix\n                               palette parameters.\n    dark_palette : Create a sequential palette with dark low values.\n    light_palette : Create a sequential palette with bright low values.\n\n    References\n    ----------\n    Green, D. A. (2011). \"A colour scheme for the display of astronomical\n    intensity images\". Bulletin of the Astromical Society of India, Vol. 39,\n    p. 289-295.\n\n    Examples\n    --------\n    .. include:: ../docstrings/cubehelix_palette.rst\n\n    ",
    "seaborn.dark_palette": "Make a sequential palette that blends from dark to ``color``.\n\n    This kind of palette is good for data that range between relatively\n    uninteresting low values and interesting high values.\n\n    The ``color`` parameter can be specified in a number of ways, including\n    all options for defining a color in matplotlib and several additional\n    color spaces that are handled by seaborn. You can also use the database\n    of named colors from the XKCD color survey.\n\n    If you are using the IPython notebook, you can also choose this palette\n    interactively with the :func:`choose_dark_palette` function.\n\n    Parameters\n    ----------\n    color : base color for high values\n        hex, rgb-tuple, or html color name\n    n_colors : int, optional\n        number of colors in the palette\n    reverse : bool, optional\n        if True, reverse the direction of the blend\n    as_cmap : bool, optional\n        If True, return a :class:`matplotlib.colors.ListedColormap`.\n    input : {'rgb', 'hls', 'husl', xkcd'}\n        Color space to interpret the input color. The first three options\n        apply to tuple inputs and the latter applies to string inputs.\n\n    Returns\n    -------\n    palette\n        list of RGB tuples or :class:`matplotlib.colors.ListedColormap`\n\n    See Also\n    --------\n    light_palette : Create a sequential palette with bright low values.\n    diverging_palette : Create a diverging palette with two colors.\n\n    Examples\n    --------\n    .. include:: ../docstrings/dark_palette.rst\n\n    ",
    "seaborn.desaturate": "Decrease the saturation channel of a color by some percent.\n\n    Parameters\n    ----------\n    color : matplotlib color\n        hex, rgb-tuple, or html color name\n    prop : float\n        saturation channel of color will be multiplied by this value\n\n    Returns\n    -------\n    new_color : rgb tuple\n        desaturated color code in RGB tuple representation\n\n    ",
    "seaborn.despine": "Remove the top and right spines from plot(s).\n\n    fig : matplotlib figure, optional\n        Figure to despine all axes of, defaults to the current figure.\n    ax : matplotlib axes, optional\n        Specific axes object to despine. Ignored if fig is provided.\n    top, right, left, bottom : boolean, optional\n        If True, remove that spine.\n    offset : int or dict, optional\n        Absolute distance, in points, spines should be moved away\n        from the axes (negative values move spines inward). A single value\n        applies to all spines; a dict can be used to set offset values per\n        side.\n    trim : bool, optional\n        If True, limit spines to the smallest and largest major tick\n        on each non-despined axis.\n\n    Returns\n    -------\n    None\n\n    ",
    "seaborn.displot": "Figure-level interface for drawing distribution plots onto a FacetGrid.\n\nThis function provides access to several approaches for visualizing the\nunivariate or bivariate distribution of data, including subsets of data\ndefined by semantic mapping and faceting across multiple subplots. The\n``kind`` parameter selects the approach to use:\n\n- :func:`histplot` (with ``kind=\"hist\"``; the default)\n- :func:`kdeplot` (with ``kind=\"kde\"``)\n- :func:`ecdfplot` (with ``kind=\"ecdf\"``; univariate-only)\n\nAdditionally, a :func:`rugplot` can be added to any kind of plot to show\nindividual observations.\n\nExtra keyword arguments are passed to the underlying function, so you should\nrefer to the documentation for each to understand the complete set of options\nfor making plots with this interface.\n\nSee the :doc:`distribution plots tutorial <../tutorial/distributions>` for a more\nin-depth discussion of the relative strengths and weaknesses of each approach.\nThe distinction between figure-level and axes-level functions is explained\nfurther in the :doc:`user guide <../tutorial/function_overview>`.\n\nParameters\n----------\ndata : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n    Input data structure. Either a long-form collection of vectors that can be\n    assigned to named variables or a wide-form dataset that will be internally\n    reshaped.\nx, y : vectors or keys in ``data``\n    Variables that specify positions on the x and y axes.\nhue : vector or key in ``data``\n    Semantic variable that is mapped to determine the color of plot elements.\nrow, col : vectors or keys in ``data``\n    Variables that define subsets to plot on different facets.    \nweights : vector or key in ``data``\n    Observation weights used for computing the distribution function.\nkind : {\"hist\", \"kde\", \"ecdf\"}\n    Approach for visualizing the data. Selects the underlying plotting function\n    and determines the additional set of valid parameters.\nrug : bool\n    If True, show each observation with marginal ticks (as in :func:`rugplot`).\nrug_kws : dict\n    Parameters to control the appearance of the rug plot.\nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\nlegend : bool\n    If False, suppress the legend for semantic variables.\npalette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n    Method for choosing the colors to use when mapping the ``hue`` semantic.\n    String values are passed to :func:`color_palette`. List or dict values\n    imply categorical mapping, while a colormap object implies numeric mapping.\nhue_order : vector of strings\n    Specify the order of processing and plotting for categorical levels of the\n    ``hue`` semantic.\nhue_norm : tuple or :class:`matplotlib.colors.Normalize`\n    Either a pair of values that set the normalization range in data units\n    or an object that will map from data units into a [0, 1] interval. Usage\n    implies numeric mapping.\ncolor : :mod:`matplotlib color <matplotlib.colors>`\n    Single color specification for when hue mapping is not used. Otherwise, the\n    plot will try to hook into the matplotlib property cycle.\ncol_wrap : int\n    \"Wrap\" the column variable at this width, so that the column facets\n    span multiple rows. Incompatible with a ``row`` facet.    \n{row,col}_order : vector of strings\n    Specify the order in which levels of the ``row`` and/or ``col`` variables\n    appear in the grid of subplots.    \nheight : scalar\n    Height (in inches) of each facet. See also: ``aspect``.    \naspect : scalar\n    Aspect ratio of each facet, so that ``aspect * height`` gives the width\n    of each facet in inches.    \nfacet_kws : dict\n    Additional parameters passed to :class:`FacetGrid`.\n\nkwargs\n    Other keyword arguments are documented with the relevant axes-level function:\n\n    - :func:`histplot` (with ``kind=\"hist\"``)\n    - :func:`kdeplot` (with ``kind=\"kde\"``)\n    - :func:`ecdfplot` (with ``kind=\"ecdf\"``)\n\nReturns\n-------\n:class:`FacetGrid`\n    An object managing one or more subplots that correspond to conditional data\n    subsets with convenient methods for batch-setting of axes attributes.\n\nSee Also\n--------\nhistplot : Plot a histogram of binned counts with optional normalization or smoothing.\nkdeplot : Plot univariate or bivariate distributions using kernel density estimation.\nrugplot : Plot a tick at each observation value along the x and/or y axes.\necdfplot : Plot empirical cumulative distribution functions.\njointplot : Draw a bivariate plot with univariate marginal distributions.\n\nExamples\n--------\n\nSee the API documentation for the axes-level functions for more details\nabout the breadth of options available for each plot kind.\n\n.. include:: ../docstrings/displot.rst\n\n",
    "seaborn.distplot": "\n    DEPRECATED\n\n    This function has been deprecated and will be removed in seaborn v0.14.0.\n    It has been replaced by :func:`histplot` and :func:`displot`, two functions\n    with a modern API and many more capabilities.\n\n    For a guide to updating, please see this notebook:\n\n    https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n    ",
    "seaborn.diverging_palette": "Make a diverging palette between two HUSL colors.\n\n    If you are using the IPython notebook, you can also choose this palette\n    interactively with the :func:`choose_diverging_palette` function.\n\n    Parameters\n    ----------\n    h_neg, h_pos : float in [0, 359]\n        Anchor hues for negative and positive extents of the map.\n    s : float in [0, 100], optional\n        Anchor saturation for both extents of the map.\n    l : float in [0, 100], optional\n        Anchor lightness for both extents of the map.\n    sep : int, optional\n        Size of the intermediate region.\n    n : int, optional\n        Number of colors in the palette (if not returning a cmap)\n    center : {\"light\", \"dark\"}, optional\n        Whether the center of the palette is light or dark\n    as_cmap : bool, optional\n        If True, return a :class:`matplotlib.colors.ListedColormap`.\n\n    Returns\n    -------\n    palette\n        list of RGB tuples or :class:`matplotlib.colors.ListedColormap`\n\n    See Also\n    --------\n    dark_palette : Create a sequential palette with dark values.\n    light_palette : Create a sequential palette with light values.\n\n    Examples\n    --------\n    .. include: ../docstrings/diverging_palette.rst\n\n    ",
    "seaborn.dogplot": "Who's a good boy?",
    "seaborn.ecdfplot": "Plot empirical cumulative distribution functions.\n\nAn ECDF represents the proportion or count of observations falling below each\nunique value in a dataset. Compared to a histogram or density plot, it has the\nadvantage that each observation is visualized directly, meaning that there are\nno binning or smoothing parameters that need to be adjusted. It also aids direct\ncomparisons between multiple distributions. A downside is that the relationship\nbetween the appearance of the plot and the basic properties of the distribution\n(such as its central tendency, variance, and the presence of any bimodality)\nmay not be as intuitive.\n\nMore information is provided in the :ref:`user guide <tutorial_ecdf>`.\n\nParameters\n----------\ndata : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n    Input data structure. Either a long-form collection of vectors that can be\n    assigned to named variables or a wide-form dataset that will be internally\n    reshaped.\nx, y : vectors or keys in ``data``\n    Variables that specify positions on the x and y axes.\nhue : vector or key in ``data``\n    Semantic variable that is mapped to determine the color of plot elements.\nweights : vector or key in ``data``\n    If provided, weight the contribution of the corresponding data points\n    towards the cumulative distribution using these values.\nstat : {{\"proportion\", \"percent\", \"count\"}}\n    Distribution statistic to compute.\ncomplementary : bool\n    If True, use the complementary CDF (1 - CDF)\npalette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n    Method for choosing the colors to use when mapping the ``hue`` semantic.\n    String values are passed to :func:`color_palette`. List or dict values\n    imply categorical mapping, while a colormap object implies numeric mapping.\nhue_order : vector of strings\n    Specify the order of processing and plotting for categorical levels of the\n    ``hue`` semantic.\nhue_norm : tuple or :class:`matplotlib.colors.Normalize`\n    Either a pair of values that set the normalization range in data units\n    or an object that will map from data units into a [0, 1] interval. Usage\n    implies numeric mapping.\nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\nlegend : bool\n    If False, suppress the legend for semantic variables.\nax : :class:`matplotlib.axes.Axes`\n    Pre-existing axes for the plot. Otherwise, call :func:`matplotlib.pyplot.gca`\n    internally.\nkwargs\n    Other keyword arguments are passed to :meth:`matplotlib.axes.Axes.plot`.\n\nReturns\n-------\n:class:`matplotlib.axes.Axes`\n    The matplotlib axes containing the plot.\n\nSee Also\n--------\ndisplot : Figure-level interface to distribution plot functions.\nhistplot : Plot a histogram of binned counts with optional normalization or smoothing.\nkdeplot : Plot univariate or bivariate distributions using kernel density estimation.\nrugplot : Plot a tick at each observation value along the x and/or y axes.\n\nExamples\n--------\n\n.. include:: ../docstrings/ecdfplot.rst\n\n",
    "seaborn.get_data_home": "Return a path to the cache directory for example datasets.\n\n    This directory is used by :func:`load_dataset`.\n\n    If the ``data_home`` argument is not provided, it will use a directory\n    specified by the `SEABORN_DATA` environment variable (if it exists)\n    or otherwise default to an OS-appropriate user cache location.\n\n    ",
    "seaborn.get_dataset_names": "Report available example datasets, useful for reporting issues.\n\n    Requires an internet connection.\n\n    ",
    "seaborn.heatmap": "Plot rectangular data as a color-encoded matrix.\n\n    This is an Axes-level function and will draw the heatmap into the\n    currently-active Axes if none is provided to the ``ax`` argument.  Part of\n    this Axes space will be taken and used to plot a colormap, unless ``cbar``\n    is False or a separate Axes is provided to ``cbar_ax``.\n\n    Parameters\n    ----------\n    data : rectangular dataset\n        2D dataset that can be coerced into an ndarray. If a Pandas DataFrame\n        is provided, the index/column information will be used to label the\n        columns and rows.\n    vmin, vmax : floats, optional\n        Values to anchor the colormap, otherwise they are inferred from the\n        data and other keyword arguments.\n    cmap : matplotlib colormap name or object, or list of colors, optional\n        The mapping from data values to color space. If not provided, the\n        default will depend on whether ``center`` is set.\n    center : float, optional\n        The value at which to center the colormap when plotting divergent data.\n        Using this parameter will change the default ``cmap`` if none is\n        specified.\n    robust : bool, optional\n        If True and ``vmin`` or ``vmax`` are absent, the colormap range is\n        computed with robust quantiles instead of the extreme values.\n    annot : bool or rectangular dataset, optional\n        If True, write the data value in each cell. If an array-like with the\n        same shape as ``data``, then use this to annotate the heatmap instead\n        of the data. Note that DataFrames will match on position, not index.\n    fmt : str, optional\n        String formatting code to use when adding annotations.\n    annot_kws : dict of key, value mappings, optional\n        Keyword arguments for :meth:`matplotlib.axes.Axes.text` when ``annot``\n        is True.\n    linewidths : float, optional\n        Width of the lines that will divide each cell.\n    linecolor : color, optional\n        Color of the lines that will divide each cell.\n    cbar : bool, optional\n        Whether to draw a colorbar.\n    cbar_kws : dict of key, value mappings, optional\n        Keyword arguments for :meth:`matplotlib.figure.Figure.colorbar`.\n    cbar_ax : matplotlib Axes, optional\n        Axes in which to draw the colorbar, otherwise take space from the\n        main Axes.\n    square : bool, optional\n        If True, set the Axes aspect to \"equal\" so each cell will be\n        square-shaped.\n    xticklabels, yticklabels : \"auto\", bool, list-like, or int, optional\n        If True, plot the column names of the dataframe. If False, don't plot\n        the column names. If list-like, plot these alternate labels as the\n        xticklabels. If an integer, use the column names but plot only every\n        n label. If \"auto\", try to densely plot non-overlapping labels.\n    mask : bool array or DataFrame, optional\n        If passed, data will not be shown in cells where ``mask`` is True.\n        Cells with missing values are automatically masked.\n    ax : matplotlib Axes, optional\n        Axes in which to draw the plot, otherwise use the currently-active\n        Axes.\n    kwargs : other keyword arguments\n        All other keyword arguments are passed to\n        :meth:`matplotlib.axes.Axes.pcolormesh`.\n\n    Returns\n    -------\n    ax : matplotlib Axes\n        Axes object with the heatmap.\n\n    See Also\n    --------\n    clustermap : Plot a matrix using hierarchical clustering to arrange the\n                 rows and columns.\n\n    Examples\n    --------\n\n    .. include:: ../docstrings/heatmap.rst\n\n    ",
    "seaborn.histplot": "Plot univariate or bivariate histograms to show distributions of datasets.\n\nA histogram is a classic visualization tool that represents the distribution\nof one or more variables by counting the number of observations that fall within\ndiscrete bins.\n\nThis function can normalize the statistic computed within each bin to estimate\nfrequency, density or probability mass, and it can add a smooth curve obtained\nusing a kernel density estimate, similar to :func:`kdeplot`.\n\nMore information is provided in the :ref:`user guide <tutorial_hist>`.\n\nParameters\n----------\ndata : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n    Input data structure. Either a long-form collection of vectors that can be\n    assigned to named variables or a wide-form dataset that will be internally\n    reshaped.\nx, y : vectors or keys in ``data``\n    Variables that specify positions on the x and y axes.\nhue : vector or key in ``data``\n    Semantic variable that is mapped to determine the color of plot elements.\nweights : vector or key in ``data``\n    If provided, weight the contribution of the corresponding data points\n    towards the count in each bin by these factors.\nstat : str\n    Aggregate statistic to compute in each bin.\n    \n    - `count`: show the number of observations in each bin\n    - `frequency`: show the number of observations divided by the bin width\n    - `probability` or `proportion`: normalize such that bar heights sum to 1\n    - `percent`: normalize such that bar heights sum to 100\n    - `density`: normalize such that the total area of the histogram equals 1\nbins : str, number, vector, or a pair of such values\n    Generic bin parameter that can be the name of a reference rule,\n    the number of bins, or the breaks of the bins.\n    Passed to :func:`numpy.histogram_bin_edges`.\nbinwidth : number or pair of numbers\n    Width of each bin, overrides ``bins`` but can be used with\n    ``binrange``.\nbinrange : pair of numbers or a pair of pairs\n    Lowest and highest value for bin edges; can be used either\n    with ``bins`` or ``binwidth``. Defaults to data extremes.\ndiscrete : bool\n    If True, default to ``binwidth=1`` and draw the bars so that they are\n    centered on their corresponding data points. This avoids \"gaps\" that may\n    otherwise appear when using discrete (integer) data.\ncumulative : bool\n    If True, plot the cumulative counts as bins increase.\ncommon_bins : bool\n    If True, use the same bins when semantic variables produce multiple\n    plots. If using a reference rule to determine the bins, it will be computed\n    with the full dataset.\ncommon_norm : bool\n    If True and using a normalized statistic, the normalization will apply over\n    the full dataset. Otherwise, normalize each histogram independently.\nmultiple : {\"layer\", \"dodge\", \"stack\", \"fill\"}\n    Approach to resolving multiple elements when semantic mapping creates subsets.\n    Only relevant with univariate data.\nelement : {\"bars\", \"step\", \"poly\"}\n    Visual representation of the histogram statistic.\n    Only relevant with univariate data.\nfill : bool\n    If True, fill in the space under the histogram.\n    Only relevant with univariate data.\nshrink : number\n    Scale the width of each bar relative to the binwidth by this factor.\n    Only relevant with univariate data.\nkde : bool\n    If True, compute a kernel density estimate to smooth the distribution\n    and show on the plot as (one or more) line(s).\n    Only relevant with univariate data.\nkde_kws : dict\n    Parameters that control the KDE computation, as in :func:`kdeplot`.\nline_kws : dict\n    Parameters that control the KDE visualization, passed to\n    :meth:`matplotlib.axes.Axes.plot`.\nthresh : number or None\n    Cells with a statistic less than or equal to this value will be transparent.\n    Only relevant with bivariate data.\npthresh : number or None\n    Like ``thresh``, but a value in [0, 1] such that cells with aggregate counts\n    (or other statistics, when used) up to this proportion of the total will be\n    transparent.\npmax : number or None\n    A value in [0, 1] that sets that saturation point for the colormap at a value\n    such that cells below constitute this proportion of the total count (or\n    other statistic, when used).\ncbar : bool\n    If True, add a colorbar to annotate the color mapping in a bivariate plot.\n    Note: Does not currently support plots with a ``hue`` variable well.\ncbar_ax : :class:`matplotlib.axes.Axes`\n    Pre-existing axes for the colorbar.\ncbar_kws : dict\n    Additional parameters passed to :meth:`matplotlib.figure.Figure.colorbar`.\npalette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n    Method for choosing the colors to use when mapping the ``hue`` semantic.\n    String values are passed to :func:`color_palette`. List or dict values\n    imply categorical mapping, while a colormap object implies numeric mapping.\nhue_order : vector of strings\n    Specify the order of processing and plotting for categorical levels of the\n    ``hue`` semantic.\nhue_norm : tuple or :class:`matplotlib.colors.Normalize`\n    Either a pair of values that set the normalization range in data units\n    or an object that will map from data units into a [0, 1] interval. Usage\n    implies numeric mapping.\ncolor : :mod:`matplotlib color <matplotlib.colors>`\n    Single color specification for when hue mapping is not used. Otherwise, the\n    plot will try to hook into the matplotlib property cycle.\nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\nlegend : bool\n    If False, suppress the legend for semantic variables.\nax : :class:`matplotlib.axes.Axes`\n    Pre-existing axes for the plot. Otherwise, call :func:`matplotlib.pyplot.gca`\n    internally.\nkwargs\n    Other keyword arguments are passed to one of the following matplotlib\n    functions:\n\n    - :meth:`matplotlib.axes.Axes.bar` (univariate, element=\"bars\")\n    - :meth:`matplotlib.axes.Axes.fill_between` (univariate, other element, fill=True)\n    - :meth:`matplotlib.axes.Axes.plot` (univariate, other element, fill=False)\n    - :meth:`matplotlib.axes.Axes.pcolormesh` (bivariate)\n\nReturns\n-------\n:class:`matplotlib.axes.Axes`\n    The matplotlib axes containing the plot.\n\nSee Also\n--------\ndisplot : Figure-level interface to distribution plot functions.\nkdeplot : Plot univariate or bivariate distributions using kernel density estimation.\nrugplot : Plot a tick at each observation value along the x and/or y axes.\necdfplot : Plot empirical cumulative distribution functions.\njointplot : Draw a bivariate plot with univariate marginal distributions.\n\nNotes\n-----\n\nThe choice of bins for computing and plotting a histogram can exert\nsubstantial influence on the insights that one is able to draw from the\nvisualization. If the bins are too large, they may erase important features.\nOn the other hand, bins that are too small may be dominated by random\nvariability, obscuring the shape of the true underlying distribution. The\ndefault bin size is determined using a reference rule that depends on the\nsample size and variance. This works well in many cases, (i.e., with\n\"well-behaved\" data) but it fails in others. It is always a good to try\ndifferent bin sizes to be sure that you are not missing something important.\nThis function allows you to specify bins in several different ways, such as\nby setting the total number of bins to use, the width of each bin, or the\nspecific locations where the bins should break.\n\nExamples\n--------\n\n.. include:: ../docstrings/histplot.rst\n\n",
    "seaborn.hls_palette": "\n    Return hues with constant lightness and saturation in the HLS system.\n\n    The hues are evenly sampled along a circular path. The resulting palette will be\n    appropriate for categorical or cyclical data.\n\n    The `h`, `l`, and `s` values should be between 0 and 1.\n\n    .. note::\n        While the separation of the resulting colors will be mathematically\n        constant, the HLS system does not construct a perceptually-uniform space,\n        so their apparent intensity will vary.\n\n    Parameters\n    ----------\n    n_colors : int\n        Number of colors in the palette.\n    h : float\n        The value of the first hue.\n    l : float\n        The lightness value.\n    s : float\n        The saturation intensity.\n    as_cmap : bool\n        If True, return a matplotlib colormap object.\n\n    Returns\n    -------\n    palette\n        list of RGB tuples or :class:`matplotlib.colors.ListedColormap`\n\n    See Also\n    --------\n    husl_palette : Make a palette using evenly spaced hues in the HUSL system.\n\n    Examples\n    --------\n    .. include:: ../docstrings/hls_palette.rst\n\n    ",
    "seaborn.husl_palette": "\n    Return hues with constant lightness and saturation in the HUSL system.\n\n    The hues are evenly sampled along a circular path. The resulting palette will be\n    appropriate for categorical or cyclical data.\n\n    The `h`, `l`, and `s` values should be between 0 and 1.\n\n    This function is similar to :func:`hls_palette`, but it uses a nonlinear color\n    space that is more perceptually uniform.\n\n    Parameters\n    ----------\n    n_colors : int\n        Number of colors in the palette.\n    h : float\n        The value of the first hue.\n    l : float\n        The lightness value.\n    s : float\n        The saturation intensity.\n    as_cmap : bool\n        If True, return a matplotlib colormap object.\n\n    Returns\n    -------\n    palette\n        list of RGB tuples or :class:`matplotlib.colors.ListedColormap`\n\n    See Also\n    --------\n    hls_palette : Make a palette using evenly spaced hues in the HSL system.\n\n    Examples\n    --------\n    .. include:: ../docstrings/husl_palette.rst\n\n    ",
    "seaborn.jointplot": "Draw a plot of two variables with bivariate and univariate graphs.\n\nThis function provides a convenient interface to the :class:`JointGrid`\nclass, with several canned plot kinds. This is intended to be a fairly\nlightweight wrapper; if you need more flexibility, you should use\n:class:`JointGrid` directly.\n\nParameters\n----------\ndata : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n    Input data structure. Either a long-form collection of vectors that can be\n    assigned to named variables or a wide-form dataset that will be internally\n    reshaped.\nx, y : vectors or keys in ``data``\n    Variables that specify positions on the x and y axes.\nhue : vector or key in ``data``\n    Semantic variable that is mapped to determine the color of plot elements.\nkind : { \"scatter\" | \"kde\" | \"hist\" | \"hex\" | \"reg\" | \"resid\" }\n    Kind of plot to draw. See the examples for references to the underlying functions.\nheight : numeric\n    Size of the figure (it will be square).\nratio : numeric\n    Ratio of joint axes height to marginal axes height.\nspace : numeric\n    Space between the joint and marginal axes\ndropna : bool\n    If True, remove observations that are missing from ``x`` and ``y``.\n{x, y}lim : pairs of numbers\n    Axis limits to set before plotting.\ncolor : :mod:`matplotlib color <matplotlib.colors>`\n    Single color specification for when hue mapping is not used. Otherwise, the\n    plot will try to hook into the matplotlib property cycle.\npalette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n    Method for choosing the colors to use when mapping the ``hue`` semantic.\n    String values are passed to :func:`color_palette`. List or dict values\n    imply categorical mapping, while a colormap object implies numeric mapping.\nhue_order : vector of strings\n    Specify the order of processing and plotting for categorical levels of the\n    ``hue`` semantic.\nhue_norm : tuple or :class:`matplotlib.colors.Normalize`\n    Either a pair of values that set the normalization range in data units\n    or an object that will map from data units into a [0, 1] interval. Usage\n    implies numeric mapping.\nmarginal_ticks : bool\n    If False, suppress ticks on the count/density axis of the marginal plots.\n{joint, marginal}_kws : dicts\n    Additional keyword arguments for the plot components.\nkwargs\n    Additional keyword arguments are passed to the function used to\n    draw the plot on the joint Axes, superseding items in the\n    ``joint_kws`` dictionary.\n\nReturns\n-------\n:class:`JointGrid`\n    An object managing multiple subplots that correspond to joint and marginal axes\n    for plotting a bivariate relationship or distribution.\n\nSee Also\n--------\nJointGrid : Set up a figure with joint and marginal views on bivariate data.\nPairGrid : Set up a figure with joint and marginal views on multiple variables.\njointplot : Draw multiple bivariate plots with univariate marginal distributions.\n\nExamples\n--------\n\n.. include:: ../docstrings/jointplot.rst\n\n",
    "seaborn.kdeplot": "Plot univariate or bivariate distributions using kernel density estimation.\n\nA kernel density estimate (KDE) plot is a method for visualizing the\ndistribution of observations in a dataset, analogous to a histogram. KDE\nrepresents the data using a continuous probability density curve in one or\nmore dimensions.\n\nThe approach is explained further in the :ref:`user guide <tutorial_kde>`.\n\nRelative to a histogram, KDE can produce a plot that is less cluttered and\nmore interpretable, especially when drawing multiple distributions. But it\nhas the potential to introduce distortions if the underlying distribution is\nbounded or not smooth. Like a histogram, the quality of the representation\nalso depends on the selection of good smoothing parameters.\n\nParameters\n----------\ndata : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n    Input data structure. Either a long-form collection of vectors that can be\n    assigned to named variables or a wide-form dataset that will be internally\n    reshaped.\nx, y : vectors or keys in ``data``\n    Variables that specify positions on the x and y axes.\nhue : vector or key in ``data``\n    Semantic variable that is mapped to determine the color of plot elements.\nweights : vector or key in ``data``\n    If provided, weight the kernel density estimation using these values.\npalette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n    Method for choosing the colors to use when mapping the ``hue`` semantic.\n    String values are passed to :func:`color_palette`. List or dict values\n    imply categorical mapping, while a colormap object implies numeric mapping.\nhue_order : vector of strings\n    Specify the order of processing and plotting for categorical levels of the\n    ``hue`` semantic.\nhue_norm : tuple or :class:`matplotlib.colors.Normalize`\n    Either a pair of values that set the normalization range in data units\n    or an object that will map from data units into a [0, 1] interval. Usage\n    implies numeric mapping.\ncolor : :mod:`matplotlib color <matplotlib.colors>`\n    Single color specification for when hue mapping is not used. Otherwise, the\n    plot will try to hook into the matplotlib property cycle.\nfill : bool or None\n    If True, fill in the area under univariate density curves or between\n    bivariate contours. If None, the default depends on ``multiple``.\nmultiple : {{\"layer\", \"stack\", \"fill\"}}\n    Method for drawing multiple elements when semantic mapping creates subsets.\n    Only relevant with univariate data.\ncommon_norm : bool\n    If True, scale each conditional density by the number of observations\n    such that the total area under all densities sums to 1. Otherwise,\n    normalize each density independently.\ncommon_grid : bool\n    If True, use the same evaluation grid for each kernel density estimate.\n    Only relevant with univariate data.\ncumulative : bool, optional\n    If True, estimate a cumulative distribution function. Requires scipy.\nbw_method : string, scalar, or callable, optional\n    Method for determining the smoothing bandwidth to use; passed to\n    :class:`scipy.stats.gaussian_kde`.\nbw_adjust : number, optional\n    Factor that multiplicatively scales the value chosen using\n    ``bw_method``. Increasing will make the curve smoother. See Notes.\nwarn_singular : bool\n    If True, issue a warning when trying to estimate the density of data\n    with zero variance.\nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\nlevels : int or vector\n    Number of contour levels or values to draw contours at. A vector argument\n    must have increasing values in [0, 1]. Levels correspond to iso-proportions\n    of the density: e.g., 20% of the probability mass will lie below the\n    contour drawn for 0.2. Only relevant with bivariate data.\nthresh : number in [0, 1]\n    Lowest iso-proportion level at which to draw a contour line. Ignored when\n    ``levels`` is a vector. Only relevant with bivariate data.\ngridsize : int\n    Number of points on each dimension of the evaluation grid.\ncut : number, optional\n    Factor, multiplied by the smoothing bandwidth, that determines how\n    far the evaluation grid extends past the extreme datapoints. When\n    set to 0, truncate the curve at the data limits.\nclip : pair of numbers or None, or a pair of such pairs\n    Do not evaluate the density outside of these limits.\nlegend : bool\n    If False, suppress the legend for semantic variables.\ncbar : bool\n    If True, add a colorbar to annotate the color mapping in a bivariate plot.\n    Note: Does not currently support plots with a ``hue`` variable well.\ncbar_ax : :class:`matplotlib.axes.Axes`\n    Pre-existing axes for the colorbar.\ncbar_kws : dict\n    Additional parameters passed to :meth:`matplotlib.figure.Figure.colorbar`.\nax : :class:`matplotlib.axes.Axes`\n    Pre-existing axes for the plot. Otherwise, call :func:`matplotlib.pyplot.gca`\n    internally.\nkwargs\n    Other keyword arguments are passed to one of the following matplotlib\n    functions:\n\n    - :meth:`matplotlib.axes.Axes.plot` (univariate, ``fill=False``),\n    - :meth:`matplotlib.axes.Axes.fill_between` (univariate, ``fill=True``),\n    - :meth:`matplotlib.axes.Axes.contour` (bivariate, ``fill=False``),\n    - :meth:`matplotlib.axes.contourf` (bivariate, ``fill=True``).\n\nReturns\n-------\n:class:`matplotlib.axes.Axes`\n    The matplotlib axes containing the plot.\n\nSee Also\n--------\ndisplot : Figure-level interface to distribution plot functions.\nhistplot : Plot a histogram of binned counts with optional normalization or smoothing.\necdfplot : Plot empirical cumulative distribution functions.\njointplot : Draw a bivariate plot with univariate marginal distributions.\nviolinplot : Draw an enhanced boxplot using kernel density estimation.\n\nNotes\n-----\n\nThe *bandwidth*, or standard deviation of the smoothing kernel, is an\nimportant parameter. Misspecification of the bandwidth can produce a\ndistorted representation of the data. Much like the choice of bin width in a\nhistogram, an over-smoothed curve can erase true features of a\ndistribution, while an under-smoothed curve can create false features out of\nrandom variability. The rule-of-thumb that sets the default bandwidth works\nbest when the true distribution is smooth, unimodal, and roughly bell-shaped.\nIt is always a good idea to check the default behavior by using ``bw_adjust``\nto increase or decrease the amount of smoothing.\n\nBecause the smoothing algorithm uses a Gaussian kernel, the estimated density\ncurve can extend to values that do not make sense for a particular dataset.\nFor example, the curve may be drawn over negative values when smoothing data\nthat are naturally positive. The ``cut`` and ``clip`` parameters can be used\nto control the extent of the curve, but datasets that have many observations\nclose to a natural boundary may be better served by a different visualization\nmethod.\n\nSimilar considerations apply when a dataset is naturally discrete or \"spiky\"\n(containing many repeated observations of the same value). Kernel density\nestimation will always produce a smooth curve, which would be misleading\nin these situations.\n\nThe units on the density axis are a common source of confusion. While kernel\ndensity estimation produces a probability distribution, the height of the curve\nat each point gives a density, not a probability. A probability can be obtained\nonly by integrating the density across a range. The curve is normalized so\nthat the integral over all possible values is 1, meaning that the scale of\nthe density axis depends on the data values.\n\nExamples\n--------\n\n.. include:: ../docstrings/kdeplot.rst\n\n",
    "seaborn.light_palette": "Make a sequential palette that blends from light to ``color``.\n\n    The ``color`` parameter can be specified in a number of ways, including\n    all options for defining a color in matplotlib and several additional\n    color spaces that are handled by seaborn. You can also use the database\n    of named colors from the XKCD color survey.\n\n    If you are using a Jupyter notebook, you can also choose this palette\n    interactively with the :func:`choose_light_palette` function.\n\n    Parameters\n    ----------\n    color : base color for high values\n        hex code, html color name, or tuple in `input` space.\n    n_colors : int, optional\n        number of colors in the palette\n    reverse : bool, optional\n        if True, reverse the direction of the blend\n    as_cmap : bool, optional\n        If True, return a :class:`matplotlib.colors.ListedColormap`.\n    input : {'rgb', 'hls', 'husl', xkcd'}\n        Color space to interpret the input color. The first three options\n        apply to tuple inputs and the latter applies to string inputs.\n\n    Returns\n    -------\n    palette\n        list of RGB tuples or :class:`matplotlib.colors.ListedColormap`\n\n    See Also\n    --------\n    dark_palette : Create a sequential palette with dark low values.\n    diverging_palette : Create a diverging palette with two colors.\n\n    Examples\n    --------\n    .. include:: ../docstrings/light_palette.rst\n\n    ",
    "seaborn.lineplot": "Draw a line plot with possibility of several semantic groupings.\n\nThe relationship between `x` and `y` can be shown for different subsets\nof the data using the `hue`, `size`, and `style` parameters. These\nparameters control what visual semantics are used to identify the different\nsubsets. It is possible to show up to three dimensions independently by\nusing all three semantic types, but this style of plot can be hard to\ninterpret and is often ineffective. Using redundant semantics (i.e. both\n`hue` and `style` for the same variable) can be helpful for making\ngraphics more accessible.\n\nSee the :ref:`tutorial <relational_tutorial>` for more information.\n\nThe default treatment of the `hue` (and to a lesser extent, `size`)\nsemantic, if present, depends on whether the variable is inferred to\nrepresent \"numeric\" or \"categorical\" data. In particular, numeric variables\nare represented with a sequential colormap by default, and the legend\nentries show regular \"ticks\" with values that may or may not exist in the\ndata. This behavior can be controlled through various parameters, as\ndescribed and illustrated below.\n\nBy default, the plot aggregates over multiple `y` values at each value of\n`x` and shows an estimate of the central tendency and a confidence\ninterval for that estimate.\n\nParameters\n----------\ndata : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n    Input data structure. Either a long-form collection of vectors that can be\n    assigned to named variables or a wide-form dataset that will be internally\n    reshaped.\nx, y : vectors or keys in ``data``\n    Variables that specify positions on the x and y axes.\nhue : vector or key in `data`\n    Grouping variable that will produce lines with different colors.\n    Can be either categorical or numeric, although color mapping will\n    behave differently in latter case.\nsize : vector or key in `data`\n    Grouping variable that will produce lines with different widths.\n    Can be either categorical or numeric, although size mapping will\n    behave differently in latter case.\nstyle : vector or key in `data`\n    Grouping variable that will produce lines with different dashes\n    and/or markers. Can have a numeric dtype but will always be treated\n    as categorical.\nunits : vector or key in `data`\n    Grouping variable identifying sampling units. When used, a separate\n    line will be drawn for each unit with appropriate semantics, but no\n    legend entry will be added. Useful for showing distribution of\n    experimental replicates when exact identities are not needed.\nweights : vector or key in `data`\n    Data values or column used to compute weighted estimation.\n    Note that use of weights currently limits the choice of statistics\n    to a 'mean' estimator and 'ci' errorbar.\npalette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n    Method for choosing the colors to use when mapping the ``hue`` semantic.\n    String values are passed to :func:`color_palette`. List or dict values\n    imply categorical mapping, while a colormap object implies numeric mapping.\nhue_order : vector of strings\n    Specify the order of processing and plotting for categorical levels of the\n    ``hue`` semantic.\nhue_norm : tuple or :class:`matplotlib.colors.Normalize`\n    Either a pair of values that set the normalization range in data units\n    or an object that will map from data units into a [0, 1] interval. Usage\n    implies numeric mapping.\nsizes : list, dict, or tuple\n    An object that determines how sizes are chosen when `size` is used.\n    List or dict arguments should provide a size for each unique data value,\n    which forces a categorical interpretation. The argument may also be a\n    min, max tuple.\nsize_order : list\n    Specified order for appearance of the `size` variable levels,\n    otherwise they are determined from the data. Not relevant when the\n    `size` variable is numeric.\nsize_norm : tuple or Normalize object\n    Normalization in data units for scaling plot objects when the\n    `size` variable is numeric.\ndashes : boolean, list, or dictionary\n    Object determining how to draw the lines for different levels of the\n    `style` variable. Setting to `True` will use default dash codes, or\n    you can pass a list of dash codes or a dictionary mapping levels of the\n    `style` variable to dash codes. Setting to `False` will use solid\n    lines for all subsets. Dashes are specified as in matplotlib: a tuple\n    of `(segment, gap)` lengths, or an empty string to draw a solid line.\nmarkers : boolean, list, or dictionary\n    Object determining how to draw the markers for different levels of the\n    `style` variable. Setting to `True` will use default markers, or\n    you can pass a list of markers or a dictionary mapping levels of the\n    `style` variable to markers. Setting to `False` will draw\n    marker-less lines.  Markers are specified as in matplotlib.\nstyle_order : list\n    Specified order for appearance of the `style` variable levels\n    otherwise they are determined from the data. Not relevant when the\n    `style` variable is numeric.\nestimator : name of pandas method or callable or None\n    Method for aggregating across multiple observations of the `y`\n    variable at the same `x` level. If `None`, all observations will\n    be drawn.\nerrorbar : string, (string, number) tuple, or callable\n    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple\n    with a method name and a level parameter, or a function that maps from a\n    vector to a (min, max) interval, or None to hide errorbar. See the\n    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.\nn_boot : int\n    Number of bootstraps to use for computing the confidence interval.\nseed : int, numpy.random.Generator, or numpy.random.RandomState\n    Seed or random number generator for reproducible bootstrapping.\norient : \"x\" or \"y\"\n    Dimension along which the data are sorted / aggregated. Equivalently,\n    the \"independent variable\" of the resulting function.\nsort : boolean\n    If True, the data will be sorted by the x and y variables, otherwise\n    lines will connect points in the order they appear in the dataset.\nerr_style : \"band\" or \"bars\"\n    Whether to draw the confidence intervals with translucent error bands\n    or discrete error bars.\nerr_kws : dict of keyword arguments\n    Additional parameters to control the aesthetics of the error bars. The\n    kwargs are passed either to :meth:`matplotlib.axes.Axes.fill_between`\n    or :meth:`matplotlib.axes.Axes.errorbar`, depending on `err_style`.\nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\nci : int or \"sd\" or None\n    Size of the confidence interval to draw when aggregating.\n\n    .. deprecated:: 0.12.0\n        Use the new `errorbar` parameter for more flexibility.\n\nax : :class:`matplotlib.axes.Axes`\n    Pre-existing axes for the plot. Otherwise, call :func:`matplotlib.pyplot.gca`\n    internally.\nkwargs : key, value mappings\n    Other keyword arguments are passed down to\n    :meth:`matplotlib.axes.Axes.plot`.\n\nReturns\n-------\n:class:`matplotlib.axes.Axes`\n    The matplotlib axes containing the plot.\n\nSee Also\n--------\nscatterplot : Plot data using points.\npointplot : Plot point estimates and CIs using markers and lines.\n\nExamples\n--------\n\n.. include:: ../docstrings/lineplot.rst\n\n",
    "seaborn.lmplot": "Plot data and regression model fits across a FacetGrid.\n\nThis function combines :func:`regplot` and :class:`FacetGrid`. It is\nintended as a convenient interface to fit regression models across\nconditional subsets of a dataset.\n\nWhen thinking about how to assign variables to different facets, a general\nrule is that it makes sense to use ``hue`` for the most important\ncomparison, followed by ``col`` and ``row``. However, always think about\nyour particular dataset and the goals of the visualization you are\ncreating.\n\nThere are a number of mutually exclusive options for estimating the\nregression model. See the :ref:`tutorial <regression_tutorial>` for more\ninformation.    \n\nThe parameters to this function span most of the options in\n:class:`FacetGrid`, although there may be occasional cases where you will\nwant to use that class and :func:`regplot` directly.\n\nParameters\n----------\ndata : DataFrame\n    Tidy (\"long-form\") dataframe where each column is a variable and each\n    row is an observation.    \nx, y : strings, optional\n    Input variables; these should be column names in ``data``.\nhue, col, row : strings\n    Variables that define subsets of the data, which will be drawn on\n    separate facets in the grid. See the ``*_order`` parameters to control\n    the order of levels of this variable.\npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \ncol_wrap : int\n    \"Wrap\" the column variable at this width, so that the column facets\n    span multiple rows. Incompatible with a ``row`` facet.    \nheight : scalar\n    Height (in inches) of each facet. See also: ``aspect``.    \naspect : scalar\n    Aspect ratio of each facet, so that ``aspect * height`` gives the width\n    of each facet in inches.    \nmarkers : matplotlib marker code or list of marker codes, optional\n    Markers for the scatterplot. If a list, each marker in the list will be\n    used for each level of the ``hue`` variable.\nshare{x,y} : bool, 'col', or 'row' optional\n    If true, the facets will share y axes across columns and/or x axes\n    across rows.    \n\n    .. deprecated:: 0.12.0\n        Pass using the `facet_kws` dictionary.\n\n{hue,col,row}_order : lists, optional\n    Order for the levels of the faceting variables. By default, this will\n    be the order that the levels appear in ``data`` or, if the variables\n    are pandas categoricals, the category order.\nlegend : bool, optional\n    If ``True`` and there is a ``hue`` variable, add a legend.\nlegend_out : bool\n    If ``True``, the figure size will be extended, and the legend will be\n    drawn outside the plot on the center right.    \n\n    .. deprecated:: 0.12.0\n        Pass using the `facet_kws` dictionary.\n\nx_estimator : callable that maps vector -> scalar, optional\n    Apply this function to each unique value of ``x`` and plot the\n    resulting estimate. This is useful when ``x`` is a discrete variable.\n    If ``x_ci`` is given, this estimate will be bootstrapped and a\n    confidence interval will be drawn.    \nx_bins : int or vector, optional\n    Bin the ``x`` variable into discrete bins and then estimate the central\n    tendency and a confidence interval. This binning only influences how\n    the scatterplot is drawn; the regression is still fit to the original\n    data.  This parameter is interpreted either as the number of\n    evenly-sized (not necessary spaced) bins or the positions of the bin\n    centers. When this parameter is used, it implies that the default of\n    ``x_estimator`` is ``numpy.mean``.    \nx_ci : \"ci\", \"sd\", int in [0, 100] or None, optional\n    Size of the confidence interval used when plotting a central tendency\n    for discrete values of ``x``. If ``\"ci\"``, defer to the value of the\n    ``ci`` parameter. If ``\"sd\"``, skip bootstrapping and show the\n    standard deviation of the observations in each bin.    \nscatter : bool, optional\n    If ``True``, draw a scatterplot with the underlying observations (or\n    the ``x_estimator`` values).    \nfit_reg : bool, optional\n    If ``True``, estimate and plot a regression model relating the ``x``\n    and ``y`` variables.    \nci : int in [0, 100] or None, optional\n    Size of the confidence interval for the regression estimate. This will\n    be drawn using translucent bands around the regression line. The\n    confidence interval is estimated using a bootstrap; for large\n    datasets, it may be advisable to avoid that computation by setting\n    this parameter to None.    \nn_boot : int, optional\n    Number of bootstrap resamples used to estimate the ``ci``. The default\n    value attempts to balance time and stability; you may want to increase\n    this value for \"final\" versions of plots.    \nunits : variable name in ``data``, optional\n    If the ``x`` and ``y`` observations are nested within sampling units,\n    those can be specified here. This will be taken into account when\n    computing the confidence intervals by performing a multilevel bootstrap\n    that resamples both units and observations (within unit). This does not\n    otherwise influence how the regression is estimated or drawn.    \nseed : int, numpy.random.Generator, or numpy.random.RandomState, optional\n    Seed or random number generator for reproducible bootstrapping.    \norder : int, optional\n    If ``order`` is greater than 1, use ``numpy.polyfit`` to estimate a\n    polynomial regression.    \nlogistic : bool, optional\n    If ``True``, assume that ``y`` is a binary variable and use\n    ``statsmodels`` to estimate a logistic regression model. Note that this\n    is substantially more computationally intensive than linear regression,\n    so you may wish to decrease the number of bootstrap resamples\n    (``n_boot``) or set ``ci`` to None.    \nlowess : bool, optional\n    If ``True``, use ``statsmodels`` to estimate a nonparametric lowess\n    model (locally weighted linear regression). Note that confidence\n    intervals cannot currently be drawn for this kind of model.    \nrobust : bool, optional\n    If ``True``, use ``statsmodels`` to estimate a robust regression. This\n    will de-weight outliers. Note that this is substantially more\n    computationally intensive than standard linear regression, so you may\n    wish to decrease the number of bootstrap resamples (``n_boot``) or set\n    ``ci`` to None.    \nlogx : bool, optional\n    If ``True``, estimate a linear regression of the form y ~ log(x), but\n    plot the scatterplot and regression model in the input space. Note that\n    ``x`` must be positive for this to work.    \n{x,y}_partial : strings in ``data`` or matrices\n    Confounding variables to regress out of the ``x`` or ``y`` variables\n    before plotting.    \ntruncate : bool, optional\n    If ``True``, the regression line is bounded by the data limits. If\n    ``False``, it extends to the ``x`` axis limits.\n\n{x,y}_jitter : floats, optional\n    Add uniform random noise of this size to either the ``x`` or ``y``\n    variables. The noise is added to a copy of the data after fitting the\n    regression, and only influences the look of the scatterplot. This can\n    be helpful when plotting variables that take discrete values.    \n{scatter,line}_kws : dictionaries\n    Additional keyword arguments to pass to ``plt.scatter`` and\n    ``plt.plot``.    \nfacet_kws : dict\n    Dictionary of keyword arguments for :class:`FacetGrid`.\n\nSee Also\n--------\nregplot : Plot data and a conditional model fit.\nFacetGrid : Subplot grid for plotting conditional relationships.\npairplot : Combine :func:`regplot` and :class:`PairGrid` (when used with\n           ``kind=\"reg\"``).\n\nNotes\n-----\n\nThe :func:`regplot` and :func:`lmplot` functions are closely related, but\nthe former is an axes-level function while the latter is a figure-level\nfunction that combines :func:`regplot` and :class:`FacetGrid`.    \n\nExamples\n--------\n\n.. include:: ../docstrings/lmplot.rst\n\n",
    "seaborn.load_dataset": "Load an example dataset from the online repository (requires internet).\n\n    This function provides quick access to a small number of example datasets\n    that are useful for documenting seaborn or generating reproducible examples\n    for bug reports. It is not necessary for normal usage.\n\n    Note that some of the datasets have a small amount of preprocessing applied\n    to define a proper ordering for categorical variables.\n\n    Use :func:`get_dataset_names` to see a list of available datasets.\n\n    Parameters\n    ----------\n    name : str\n        Name of the dataset (``{name}.csv`` on\n        https://github.com/mwaskom/seaborn-data).\n    cache : boolean, optional\n        If True, try to load from the local cache first, and save to the cache\n        if a download is required.\n    data_home : string, optional\n        The directory in which to cache data; see :func:`get_data_home`.\n    kws : keys and values, optional\n        Additional keyword arguments are passed to passed through to\n        :func:`pandas.read_csv`.\n\n    Returns\n    -------\n    df : :class:`pandas.DataFrame`\n        Tabular data, possibly with some preprocessing applied.\n\n    ",
    "seaborn.move_legend": "\n    Recreate a plot's legend at a new location.\n\n    The name is a slight misnomer. Matplotlib legends do not expose public\n    control over their position parameters. So this function creates a new legend,\n    copying over the data from the original object, which is then removed.\n\n    Parameters\n    ----------\n    obj : the object with the plot\n        This argument can be either a seaborn or matplotlib object:\n\n        - :class:`seaborn.FacetGrid` or :class:`seaborn.PairGrid`\n        - :class:`matplotlib.axes.Axes` or :class:`matplotlib.figure.Figure`\n\n    loc : str or int\n        Location argument, as in :meth:`matplotlib.axes.Axes.legend`.\n\n    kwargs\n        Other keyword arguments are passed to :meth:`matplotlib.axes.Axes.legend`.\n\n    Examples\n    --------\n\n    .. include:: ../docstrings/move_legend.rst\n\n    ",
    "seaborn.mpl_palette": "\n    Return a palette or colormap from the matplotlib registry.\n\n    For continuous palettes, evenly-spaced discrete samples are chosen while\n    excluding the minimum and maximum value in the colormap to provide better\n    contrast at the extremes.\n\n    For qualitative palettes (e.g. those from colorbrewer), exact values are\n    indexed (rather than interpolated), but fewer than `n_colors` can be returned\n    if the palette does not define that many.\n\n    Parameters\n    ----------\n    name : string\n        Name of the palette. This should be a named matplotlib colormap.\n    n_colors : int\n        Number of discrete colors in the palette.\n\n    Returns\n    -------\n    list of RGB tuples or :class:`matplotlib.colors.ListedColormap`\n\n    Examples\n    --------\n    .. include:: ../docstrings/mpl_palette.rst\n\n    ",
    "seaborn.pairplot": "Plot pairwise relationships in a dataset.\n\n    By default, this function will create a grid of Axes such that each numeric\n    variable in ``data`` will by shared across the y-axes across a single row and\n    the x-axes across a single column. The diagonal plots are treated\n    differently: a univariate distribution plot is drawn to show the marginal\n    distribution of the data in each column.\n\n    It is also possible to show a subset of variables or plot different\n    variables on the rows and columns.\n\n    This is a high-level interface for :class:`PairGrid` that is intended to\n    make it easy to draw a few common styles. You should use :class:`PairGrid`\n    directly if you need more flexibility.\n\n    Parameters\n    ----------\n    data : `pandas.DataFrame`\n        Tidy (long-form) dataframe where each column is a variable and\n        each row is an observation.\n    hue : name of variable in ``data``\n        Variable in ``data`` to map plot aspects to different colors.\n    hue_order : list of strings\n        Order for the levels of the hue variable in the palette\n    palette : dict or seaborn color palette\n        Set of colors for mapping the ``hue`` variable. If a dict, keys\n        should be values  in the ``hue`` variable.\n    vars : list of variable names\n        Variables within ``data`` to use, otherwise use every column with\n        a numeric datatype.\n    {x, y}_vars : lists of variable names\n        Variables within ``data`` to use separately for the rows and\n        columns of the figure; i.e. to make a non-square plot.\n    kind : {'scatter', 'kde', 'hist', 'reg'}\n        Kind of plot to make.\n    diag_kind : {'auto', 'hist', 'kde', None}\n        Kind of plot for the diagonal subplots. If 'auto', choose based on\n        whether or not ``hue`` is used.\n    markers : single matplotlib marker code or list\n        Either the marker to use for all scatterplot points or a list of markers\n        with a length the same as the number of levels in the hue variable so that\n        differently colored points will also have different scatterplot\n        markers.\n    height : scalar\n        Height (in inches) of each facet.\n    aspect : scalar\n        Aspect * height gives the width (in inches) of each facet.\n    corner : bool\n        If True, don't add axes to the upper (off-diagonal) triangle of the\n        grid, making this a \"corner\" plot.\n    dropna : boolean\n        Drop missing values from the data before plotting.\n    {plot, diag, grid}_kws : dicts\n        Dictionaries of keyword arguments. ``plot_kws`` are passed to the\n        bivariate plotting function, ``diag_kws`` are passed to the univariate\n        plotting function, and ``grid_kws`` are passed to the :class:`PairGrid`\n        constructor.\n\n    Returns\n    -------\n    grid : :class:`PairGrid`\n        Returns the underlying :class:`PairGrid` instance for further tweaking.\n\n    See Also\n    --------\n    PairGrid : Subplot grid for more flexible plotting of pairwise relationships.\n    JointGrid : Grid for plotting joint and marginal distributions of two variables.\n\n    Examples\n    --------\n\n    .. include:: ../docstrings/pairplot.rst\n\n    ",
    "seaborn.palplot": "Plot the values in a color palette as a horizontal array.\n\n    Parameters\n    ----------\n    pal : sequence of matplotlib colors\n        colors, i.e. as returned by seaborn.color_palette()\n    size :\n        scaling factor for size of plot\n\n    ",
    "seaborn.plotting_context": "\n    Get the parameters that control the scaling of plot elements.\n\n    These parameters correspond to label size, line thickness, etc. For more\n    information, see the :doc:`aesthetics tutorial <../tutorial/aesthetics>`.\n\n    The base context is \"notebook\", and the other contexts are \"paper\", \"talk\",\n    and \"poster\", which are version of the notebook parameters scaled by different\n    values. Font elements can also be scaled independently of (but relative to)\n    the other values.\n\n    This function can also be used as a context manager to temporarily\n    alter the global defaults. See :func:`set_theme` or :func:`set_context`\n    to modify the global defaults for all plots.\n\n    Parameters\n    ----------\n    context : None, dict, or one of {paper, notebook, talk, poster}\n        A dictionary of parameters or the name of a preconfigured set.\n    font_scale : float, optional\n        Separate scaling factor to independently scale the size of the\n        font elements.\n    rc : dict, optional\n        Parameter mappings to override the values in the preset seaborn\n        context dictionaries. This only updates parameters that are\n        considered part of the context definition.\n\n    Examples\n    --------\n\n    .. include:: ../docstrings/plotting_context.rst\n\n    ",
    "seaborn.pointplot": "Show point estimates and errors using lines with markers.\n\nA point plot represents an estimate of central tendency for a numeric\nvariable by the position of the dot and provides some indication of the\nuncertainty around that estimate using error bars.\n\nPoint plots can be more useful than bar plots for focusing comparisons\nbetween different levels of one or more categorical variables. They are\nparticularly adept at showing interactions: how the relationship between\nlevels of one categorical variable changes across levels of a second\ncategorical variable. The lines that join each point from the same `hue`\nlevel allow interactions to be judged by differences in slope, which is\neasier for the eyes than comparing the heights of several groups of points\nor bars.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \nestimator : string or callable that maps vector -> scalar\n    Statistical function to estimate within each categorical bin.\nerrorbar : string, (string, number) tuple, callable or None\n    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple\n    with a method name and a level parameter, or a function that maps from a\n    vector to a (min, max) interval, or None to hide errorbar. See the\n    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.\n\n    .. versionadded:: v0.12.0\nn_boot : int\n    Number of bootstrap samples used to compute confidence intervals.\nseed : int, `numpy.random.Generator`, or `numpy.random.RandomState`\n    Seed or random number generator for reproducible bootstrapping.\nunits : name of variable in `data` or vector data\n    Identifier of sampling units; used by the errorbar function to\n    perform a multilevel bootstrap and account for repeated measures\nweights : name of variable in `data` or vector data\n    Data values or column used to compute weighted statistics.\n    Note that the use of weights may limit other statistical options.\n\n    .. versionadded:: v0.13.1    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nmarkers : string or list of strings\n    Markers to use for each of the `hue` levels.\nlinestyles : string or list of strings\n    Line styles to use for each of the `hue` levels.\ndodge : bool or float\n    Amount to separate the points for each level of the `hue` variable along\n    the categorical axis. Setting to `True` will apply a small default.\nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\n\n    .. versionadded:: v0.13.0    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncapsize : float\n    Width of the \"caps\" on error bars, relative to bar spacing.    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \nerr_kws : dict\n    Parameters of :class:`matplotlib.lines.Line2D`, for the error bar artists.\n\n    .. versionadded:: v0.13.0    \nci : float\n    Level of the confidence interval to show, in [0, 100].\n\n    .. deprecated:: v0.12.0\n        Use `errorbar=(\"ci\", ...)`.    \nerrwidth : float\n    Thickness of error bar lines (and caps), in points.\n\n    .. deprecated:: 0.13.0\n        Use `err_kws={'linewidth': ...}`.    \njoin : bool\n    If `True`, connect point estimates with a line.\n\n    .. deprecated:: v0.13.0\n        Set `linestyle=\"none\"` to remove the lines between the points.\nscale : float\n    Scale factor for the plot elements.\n\n    .. deprecated:: v0.13.0\n        Control element sizes with :class:`matplotlib.lines.Line2D` parameters.\nax : matplotlib Axes\n    Axes object to draw the plot onto, otherwise uses the current Axes.    \nkwargs : key, value mappings\n    Other parameters are passed through to :class:`matplotlib.lines.Line2D`.\n\n    .. versionadded:: v0.13.0\n\nReturns\n-------\nax : matplotlib Axes\n    Returns the Axes object with the plot drawn onto it.    \n\nSee Also\n--------\nbarplot : Show point estimates and confidence intervals using bars.    \ncatplot : Combine a categorical plot with a :class:`FacetGrid`.    \n\nNotes\n-----\nIt is important to keep in mind that a point plot shows only the mean (or\nother estimator) value, but in many cases it may be more informative to\nshow the distribution of values at each level of the categorical variables.\nIn that case, other approaches such as a box or violin plot may be more\nappropriate.\n\nExamples\n--------\n.. include:: ../docstrings/pointplot.rst\n\n",
    "seaborn.regplot": "Plot data and a linear regression model fit.\n\nThere are a number of mutually exclusive options for estimating the\nregression model. See the :ref:`tutorial <regression_tutorial>` for more\ninformation.    \n\nParameters\n----------\nx, y: string, series, or vector array\n    Input variables. If strings, these should correspond with column names\n    in ``data``. When pandas objects are used, axes will be labeled with\n    the series name.\ndata : DataFrame\n    Tidy (\"long-form\") dataframe where each column is a variable and each\n    row is an observation.    \nx_estimator : callable that maps vector -> scalar, optional\n    Apply this function to each unique value of ``x`` and plot the\n    resulting estimate. This is useful when ``x`` is a discrete variable.\n    If ``x_ci`` is given, this estimate will be bootstrapped and a\n    confidence interval will be drawn.    \nx_bins : int or vector, optional\n    Bin the ``x`` variable into discrete bins and then estimate the central\n    tendency and a confidence interval. This binning only influences how\n    the scatterplot is drawn; the regression is still fit to the original\n    data.  This parameter is interpreted either as the number of\n    evenly-sized (not necessary spaced) bins or the positions of the bin\n    centers. When this parameter is used, it implies that the default of\n    ``x_estimator`` is ``numpy.mean``.    \nx_ci : \"ci\", \"sd\", int in [0, 100] or None, optional\n    Size of the confidence interval used when plotting a central tendency\n    for discrete values of ``x``. If ``\"ci\"``, defer to the value of the\n    ``ci`` parameter. If ``\"sd\"``, skip bootstrapping and show the\n    standard deviation of the observations in each bin.    \nscatter : bool, optional\n    If ``True``, draw a scatterplot with the underlying observations (or\n    the ``x_estimator`` values).    \nfit_reg : bool, optional\n    If ``True``, estimate and plot a regression model relating the ``x``\n    and ``y`` variables.    \nci : int in [0, 100] or None, optional\n    Size of the confidence interval for the regression estimate. This will\n    be drawn using translucent bands around the regression line. The\n    confidence interval is estimated using a bootstrap; for large\n    datasets, it may be advisable to avoid that computation by setting\n    this parameter to None.    \nn_boot : int, optional\n    Number of bootstrap resamples used to estimate the ``ci``. The default\n    value attempts to balance time and stability; you may want to increase\n    this value for \"final\" versions of plots.    \nunits : variable name in ``data``, optional\n    If the ``x`` and ``y`` observations are nested within sampling units,\n    those can be specified here. This will be taken into account when\n    computing the confidence intervals by performing a multilevel bootstrap\n    that resamples both units and observations (within unit). This does not\n    otherwise influence how the regression is estimated or drawn.    \nseed : int, numpy.random.Generator, or numpy.random.RandomState, optional\n    Seed or random number generator for reproducible bootstrapping.    \norder : int, optional\n    If ``order`` is greater than 1, use ``numpy.polyfit`` to estimate a\n    polynomial regression.    \nlogistic : bool, optional\n    If ``True``, assume that ``y`` is a binary variable and use\n    ``statsmodels`` to estimate a logistic regression model. Note that this\n    is substantially more computationally intensive than linear regression,\n    so you may wish to decrease the number of bootstrap resamples\n    (``n_boot``) or set ``ci`` to None.    \nlowess : bool, optional\n    If ``True``, use ``statsmodels`` to estimate a nonparametric lowess\n    model (locally weighted linear regression). Note that confidence\n    intervals cannot currently be drawn for this kind of model.    \nrobust : bool, optional\n    If ``True``, use ``statsmodels`` to estimate a robust regression. This\n    will de-weight outliers. Note that this is substantially more\n    computationally intensive than standard linear regression, so you may\n    wish to decrease the number of bootstrap resamples (``n_boot``) or set\n    ``ci`` to None.    \nlogx : bool, optional\n    If ``True``, estimate a linear regression of the form y ~ log(x), but\n    plot the scatterplot and regression model in the input space. Note that\n    ``x`` must be positive for this to work.    \n{x,y}_partial : strings in ``data`` or matrices\n    Confounding variables to regress out of the ``x`` or ``y`` variables\n    before plotting.    \ntruncate : bool, optional\n    If ``True``, the regression line is bounded by the data limits. If\n    ``False``, it extends to the ``x`` axis limits.\n\n{x,y}_jitter : floats, optional\n    Add uniform random noise of this size to either the ``x`` or ``y``\n    variables. The noise is added to a copy of the data after fitting the\n    regression, and only influences the look of the scatterplot. This can\n    be helpful when plotting variables that take discrete values.    \nlabel : string\n    Label to apply to either the scatterplot or regression line (if\n    ``scatter`` is ``False``) for use in a legend.\ncolor : matplotlib color\n    Color to apply to all plot elements; will be superseded by colors\n    passed in ``scatter_kws`` or ``line_kws``.\nmarker : matplotlib marker code\n    Marker to use for the scatterplot glyphs.\n{scatter,line}_kws : dictionaries\n    Additional keyword arguments to pass to ``plt.scatter`` and\n    ``plt.plot``.    \nax : matplotlib Axes, optional\n    Axes object to draw the plot onto, otherwise uses the current Axes.\n\nReturns\n-------\nax : matplotlib Axes\n    The Axes object containing the plot.\n\nSee Also\n--------\nlmplot : Combine :func:`regplot` and :class:`FacetGrid` to plot multiple\n         linear relationships in a dataset.\njointplot : Combine :func:`regplot` and :class:`JointGrid` (when used with\n            ``kind=\"reg\"``).\npairplot : Combine :func:`regplot` and :class:`PairGrid` (when used with\n           ``kind=\"reg\"``).\nresidplot : Plot the residuals of a linear regression model.\n\nNotes\n-----\n\nThe :func:`regplot` and :func:`lmplot` functions are closely related, but\nthe former is an axes-level function while the latter is a figure-level\nfunction that combines :func:`regplot` and :class:`FacetGrid`.    \n\n\nIt's also easy to combine :func:`regplot` and :class:`JointGrid` or\n:class:`PairGrid` through the :func:`jointplot` and :func:`pairplot`\nfunctions, although these do not directly accept all of :func:`regplot`'s\nparameters.\n\nExamples\n--------\n\n.. include:: ../docstrings/regplot.rst\n\n",
    "seaborn.relplot": "Figure-level interface for drawing relational plots onto a FacetGrid.\n\nThis function provides access to several different axes-level functions\nthat show the relationship between two variables with semantic mappings\nof subsets. The `kind` parameter selects the underlying axes-level\nfunction to use:\n\n- :func:`scatterplot` (with `kind=\"scatter\"`; the default)\n- :func:`lineplot` (with `kind=\"line\"`)\n\nExtra keyword arguments are passed to the underlying function, so you\nshould refer to the documentation for each to see kind-specific options.\n\nThe relationship between `x` and `y` can be shown for different subsets\nof the data using the `hue`, `size`, and `style` parameters. These\nparameters control what visual semantics are used to identify the different\nsubsets. It is possible to show up to three dimensions independently by\nusing all three semantic types, but this style of plot can be hard to\ninterpret and is often ineffective. Using redundant semantics (i.e. both\n`hue` and `style` for the same variable) can be helpful for making\ngraphics more accessible.\n\nSee the :ref:`tutorial <relational_tutorial>` for more information.\n\nThe default treatment of the `hue` (and to a lesser extent, `size`)\nsemantic, if present, depends on whether the variable is inferred to\nrepresent \"numeric\" or \"categorical\" data. In particular, numeric variables\nare represented with a sequential colormap by default, and the legend\nentries show regular \"ticks\" with values that may or may not exist in the\ndata. This behavior can be controlled through various parameters, as\ndescribed and illustrated below.\n\nAfter plotting, the :class:`FacetGrid` with the plot is returned and can\nbe used directly to tweak supporting plot details or add other layers.\n\nParameters\n----------\ndata : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n    Input data structure. Either a long-form collection of vectors that can be\n    assigned to named variables or a wide-form dataset that will be internally\n    reshaped.\nx, y : vectors or keys in ``data``\n    Variables that specify positions on the x and y axes.\nhue : vector or key in `data`\n    Grouping variable that will produce elements with different colors.\n    Can be either categorical or numeric, although color mapping will\n    behave differently in latter case.\nsize : vector or key in `data`\n    Grouping variable that will produce elements with different sizes.\n    Can be either categorical or numeric, although size mapping will\n    behave differently in latter case.\nstyle : vector or key in `data`\n    Grouping variable that will produce elements with different styles.\n    Can have a numeric dtype but will always be treated as categorical.\nunits : vector or key in `data`\n    Grouping variable identifying sampling units. When used, a separate\n    line will be drawn for each unit with appropriate semantics, but no\n    legend entry will be added. Useful for showing distribution of\n    experimental replicates when exact identities are not needed.\nweights : vector or key in `data`\n    Data values or column used to compute weighted estimation.\n    Note that use of weights currently limits the choice of statistics\n    to a 'mean' estimator and 'ci' errorbar.\nrow, col : vectors or keys in ``data``\n    Variables that define subsets to plot on different facets.    \ncol_wrap : int\n    \"Wrap\" the column variable at this width, so that the column facets\n    span multiple rows. Incompatible with a ``row`` facet.    \nrow_order, col_order : lists of strings\n    Order to organize the rows and/or columns of the grid in, otherwise the\n    orders are inferred from the data objects.\npalette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n    Method for choosing the colors to use when mapping the ``hue`` semantic.\n    String values are passed to :func:`color_palette`. List or dict values\n    imply categorical mapping, while a colormap object implies numeric mapping.\nhue_order : vector of strings\n    Specify the order of processing and plotting for categorical levels of the\n    ``hue`` semantic.\nhue_norm : tuple or :class:`matplotlib.colors.Normalize`\n    Either a pair of values that set the normalization range in data units\n    or an object that will map from data units into a [0, 1] interval. Usage\n    implies numeric mapping.\nsizes : list, dict, or tuple\n    An object that determines how sizes are chosen when `size` is used.\n    List or dict arguments should provide a size for each unique data value,\n    which forces a categorical interpretation. The argument may also be a\n    min, max tuple.\nsize_order : list\n    Specified order for appearance of the `size` variable levels,\n    otherwise they are determined from the data. Not relevant when the\n    `size` variable is numeric.\nsize_norm : tuple or Normalize object\n    Normalization in data units for scaling plot objects when the\n    `size` variable is numeric.\nstyle_order : list\n    Specified order for appearance of the `style` variable levels\n    otherwise they are determined from the data. Not relevant when the\n    `style` variable is numeric.\ndashes : boolean, list, or dictionary\n    Object determining how to draw the lines for different levels of the\n    `style` variable. Setting to `True` will use default dash codes, or\n    you can pass a list of dash codes or a dictionary mapping levels of the\n    `style` variable to dash codes. Setting to `False` will use solid\n    lines for all subsets. Dashes are specified as in matplotlib: a tuple\n    of `(segment, gap)` lengths, or an empty string to draw a solid line.\nmarkers : boolean, list, or dictionary\n    Object determining how to draw the markers for different levels of the\n    `style` variable. Setting to `True` will use default markers, or\n    you can pass a list of markers or a dictionary mapping levels of the\n    `style` variable to markers. Setting to `False` will draw\n    marker-less lines.  Markers are specified as in matplotlib.\nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\nkind : string\n    Kind of plot to draw, corresponding to a seaborn relational plot.\n    Options are `\"scatter\"` or `\"line\"`.\nheight : scalar\n    Height (in inches) of each facet. See also: ``aspect``.    \naspect : scalar\n    Aspect ratio of each facet, so that ``aspect * height`` gives the width\n    of each facet in inches.    \nfacet_kws : dict\n    Dictionary of other keyword arguments to pass to :class:`FacetGrid`.\nkwargs : key, value pairings\n    Other keyword arguments are passed through to the underlying plotting\n    function.\n\nReturns\n-------\n:class:`FacetGrid`\n    An object managing one or more subplots that correspond to conditional data\n    subsets with convenient methods for batch-setting of axes attributes.\n\nExamples\n--------\n\n.. include:: ../docstrings/relplot.rst\n\n",
    "seaborn.reset_defaults": "Restore all RC params to default settings.",
    "seaborn.reset_orig": "Restore all RC params to original settings (respects custom rc).",
    "seaborn.residplot": "Plot the residuals of a linear regression.\n\n    This function will regress y on x (possibly as a robust or polynomial\n    regression) and then draw a scatterplot of the residuals. You can\n    optionally fit a lowess smoother to the residual plot, which can\n    help in determining if there is structure to the residuals.\n\n    Parameters\n    ----------\n    data : DataFrame, optional\n        DataFrame to use if `x` and `y` are column names.\n    x : vector or string\n        Data or column name in `data` for the predictor variable.\n    y : vector or string\n        Data or column name in `data` for the response variable.\n    {x, y}_partial : vectors or string(s) , optional\n        These variables are treated as confounding and are removed from\n        the `x` or `y` variables before plotting.\n    lowess : boolean, optional\n        Fit a lowess smoother to the residual scatterplot.\n    order : int, optional\n        Order of the polynomial to fit when calculating the residuals.\n    robust : boolean, optional\n        Fit a robust linear regression when calculating the residuals.\n    dropna : boolean, optional\n        If True, ignore observations with missing data when fitting and\n        plotting.\n    label : string, optional\n        Label that will be used in any plot legends.\n    color : matplotlib color, optional\n        Color to use for all elements of the plot.\n    {scatter, line}_kws : dictionaries, optional\n        Additional keyword arguments passed to scatter() and plot() for drawing\n        the components of the plot.\n    ax : matplotlib axis, optional\n        Plot into this axis, otherwise grab the current axis or make a new\n        one if not existing.\n\n    Returns\n    -------\n    ax: matplotlib axes\n        Axes with the regression plot.\n\n    See Also\n    --------\n    regplot : Plot a simple linear regression model.\n    jointplot : Draw a :func:`residplot` with univariate marginal distributions\n                (when used with ``kind=\"resid\"``).\n\n    Examples\n    --------\n\n    .. include:: ../docstrings/residplot.rst\n\n    ",
    "seaborn.rugplot": "Plot marginal distributions by drawing ticks along the x and y axes.\n\nThis function is intended to complement other plots by showing the location\nof individual observations in an unobtrusive way.\n\nParameters\n----------\ndata : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n    Input data structure. Either a long-form collection of vectors that can be\n    assigned to named variables or a wide-form dataset that will be internally\n    reshaped.\nx, y : vectors or keys in ``data``\n    Variables that specify positions on the x and y axes.\nhue : vector or key in ``data``\n    Semantic variable that is mapped to determine the color of plot elements.\nheight : float\n    Proportion of axes extent covered by each rug element. Can be negative.\nexpand_margins : bool\n    If True, increase the axes margins by the height of the rug to avoid\n    overlap with other elements.\npalette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n    Method for choosing the colors to use when mapping the ``hue`` semantic.\n    String values are passed to :func:`color_palette`. List or dict values\n    imply categorical mapping, while a colormap object implies numeric mapping.\nhue_order : vector of strings\n    Specify the order of processing and plotting for categorical levels of the\n    ``hue`` semantic.\nhue_norm : tuple or :class:`matplotlib.colors.Normalize`\n    Either a pair of values that set the normalization range in data units\n    or an object that will map from data units into a [0, 1] interval. Usage\n    implies numeric mapping.\nlegend : bool\n    If False, do not add a legend for semantic variables.\nax : :class:`matplotlib.axes.Axes`\n    Pre-existing axes for the plot. Otherwise, call :func:`matplotlib.pyplot.gca`\n    internally.\nkwargs\n    Other keyword arguments are passed to\n    :meth:`matplotlib.collections.LineCollection`\n\nReturns\n-------\n:class:`matplotlib.axes.Axes`\n    The matplotlib axes containing the plot.\n\nExamples\n--------\n\n.. include:: ../docstrings/rugplot.rst\n\n",
    "seaborn.saturate": "Return a fully saturated color with the same hue.\n\n    Parameters\n    ----------\n    color : matplotlib color\n        hex, rgb-tuple, or html color name\n\n    Returns\n    -------\n    new_color : rgb tuple\n        saturated color code in RGB tuple representation\n\n    ",
    "seaborn.scatterplot": "Draw a scatter plot with possibility of several semantic groupings.\n\nThe relationship between `x` and `y` can be shown for different subsets\nof the data using the `hue`, `size`, and `style` parameters. These\nparameters control what visual semantics are used to identify the different\nsubsets. It is possible to show up to three dimensions independently by\nusing all three semantic types, but this style of plot can be hard to\ninterpret and is often ineffective. Using redundant semantics (i.e. both\n`hue` and `style` for the same variable) can be helpful for making\ngraphics more accessible.\n\nSee the :ref:`tutorial <relational_tutorial>` for more information.\n\nThe default treatment of the `hue` (and to a lesser extent, `size`)\nsemantic, if present, depends on whether the variable is inferred to\nrepresent \"numeric\" or \"categorical\" data. In particular, numeric variables\nare represented with a sequential colormap by default, and the legend\nentries show regular \"ticks\" with values that may or may not exist in the\ndata. This behavior can be controlled through various parameters, as\ndescribed and illustrated below.\n\nParameters\n----------\ndata : :class:`pandas.DataFrame`, :class:`numpy.ndarray`, mapping, or sequence\n    Input data structure. Either a long-form collection of vectors that can be\n    assigned to named variables or a wide-form dataset that will be internally\n    reshaped.\nx, y : vectors or keys in ``data``\n    Variables that specify positions on the x and y axes.\nhue : vector or key in `data`\n    Grouping variable that will produce points with different colors.\n    Can be either categorical or numeric, although color mapping will\n    behave differently in latter case.\nsize : vector or key in `data`\n    Grouping variable that will produce points with different sizes.\n    Can be either categorical or numeric, although size mapping will\n    behave differently in latter case.\nstyle : vector or key in `data`\n    Grouping variable that will produce points with different markers.\n    Can have a numeric dtype but will always be treated as categorical.\npalette : string, list, dict, or :class:`matplotlib.colors.Colormap`\n    Method for choosing the colors to use when mapping the ``hue`` semantic.\n    String values are passed to :func:`color_palette`. List or dict values\n    imply categorical mapping, while a colormap object implies numeric mapping.\nhue_order : vector of strings\n    Specify the order of processing and plotting for categorical levels of the\n    ``hue`` semantic.\nhue_norm : tuple or :class:`matplotlib.colors.Normalize`\n    Either a pair of values that set the normalization range in data units\n    or an object that will map from data units into a [0, 1] interval. Usage\n    implies numeric mapping.\nsizes : list, dict, or tuple\n    An object that determines how sizes are chosen when `size` is used.\n    List or dict arguments should provide a size for each unique data value,\n    which forces a categorical interpretation. The argument may also be a\n    min, max tuple.\nsize_order : list\n    Specified order for appearance of the `size` variable levels,\n    otherwise they are determined from the data. Not relevant when the\n    `size` variable is numeric.\nsize_norm : tuple or Normalize object\n    Normalization in data units for scaling plot objects when the\n    `size` variable is numeric.\nmarkers : boolean, list, or dictionary\n    Object determining how to draw the markers for different levels of the\n    `style` variable. Setting to `True` will use default markers, or\n    you can pass a list of markers or a dictionary mapping levels of the\n    `style` variable to markers. Setting to `False` will draw\n    marker-less lines.  Markers are specified as in matplotlib.\nstyle_order : list\n    Specified order for appearance of the `style` variable levels\n    otherwise they are determined from the data. Not relevant when the\n    `style` variable is numeric.\nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\nax : :class:`matplotlib.axes.Axes`\n    Pre-existing axes for the plot. Otherwise, call :func:`matplotlib.pyplot.gca`\n    internally.\nkwargs : key, value mappings\n    Other keyword arguments are passed down to\n    :meth:`matplotlib.axes.Axes.scatter`.\n\nReturns\n-------\n:class:`matplotlib.axes.Axes`\n    The matplotlib axes containing the plot.\n\nSee Also\n--------\nlineplot : Plot data using lines.\nstripplot : Plot a categorical scatter with jitter.\nswarmplot : Plot a categorical scatter with non-overlapping points.\n\nExamples\n--------\n\n.. include:: ../docstrings/scatterplot.rst\n\n",
    "seaborn.set": "\n    Alias for :func:`set_theme`, which is the preferred interface.\n\n    This function may be removed in the future.\n    ",
    "seaborn.set_color_codes": "Change how matplotlib color shorthands are interpreted.\n\n    Calling this will change how shorthand codes like \"b\" or \"g\"\n    are interpreted by matplotlib in subsequent plots.\n\n    Parameters\n    ----------\n    palette : {deep, muted, pastel, dark, bright, colorblind}\n        Named seaborn palette to use as the source of colors.\n\n    See Also\n    --------\n    set : Color codes can be set through the high-level seaborn style\n          manager.\n    set_palette : Color codes can also be set through the function that\n                  sets the matplotlib color cycle.\n\n    ",
    "seaborn.set_context": "\n    Set the parameters that control the scaling of plot elements.\n\n    These parameters correspond to label size, line thickness, etc.\n    Calling this function modifies the global matplotlib `rcParams`. For more\n    information, see the :doc:`aesthetics tutorial <../tutorial/aesthetics>`.\n\n    The base context is \"notebook\", and the other contexts are \"paper\", \"talk\",\n    and \"poster\", which are version of the notebook parameters scaled by different\n    values. Font elements can also be scaled independently of (but relative to)\n    the other values.\n\n    See :func:`plotting_context` to get the parameter values.\n\n    Parameters\n    ----------\n    context : dict, or one of {paper, notebook, talk, poster}\n        A dictionary of parameters or the name of a preconfigured set.\n    font_scale : float, optional\n        Separate scaling factor to independently scale the size of the\n        font elements.\n    rc : dict, optional\n        Parameter mappings to override the values in the preset seaborn\n        context dictionaries. This only updates parameters that are\n        considered part of the context definition.\n\n    Examples\n    --------\n\n    .. include:: ../docstrings/set_context.rst\n\n    ",
    "seaborn.set_hls_values": "Independently manipulate the h, l, or s channels of a color.\n\n    Parameters\n    ----------\n    color : matplotlib color\n        hex, rgb-tuple, or html color name\n    h, l, s : floats between 0 and 1, or None\n        new values for each channel in hls space\n\n    Returns\n    -------\n    new_color : rgb tuple\n        new color code in RGB tuple representation\n\n    ",
    "seaborn.set_palette": "Set the matplotlib color cycle using a seaborn palette.\n\n    Parameters\n    ----------\n    palette : seaborn color palette | matplotlib colormap | hls | husl\n        Palette definition. Should be something :func:`color_palette` can process.\n    n_colors : int\n        Number of colors in the cycle. The default number of colors will depend\n        on the format of ``palette``, see the :func:`color_palette`\n        documentation for more information.\n    desat : float\n        Proportion to desaturate each color by.\n    color_codes : bool\n        If ``True`` and ``palette`` is a seaborn palette, remap the shorthand\n        color codes (e.g. \"b\", \"g\", \"r\", etc.) to the colors from this palette.\n\n    See Also\n    --------\n    color_palette : build a color palette or set the color cycle temporarily\n                    in a ``with`` statement.\n    set_context : set parameters to scale plot elements\n    set_style : set the default parameters for figure style\n\n    ",
    "seaborn.set_style": "\n    Set the parameters that control the general style of the plots.\n\n    The style parameters control properties like the color of the background and\n    whether a grid is enabled by default. This is accomplished using the\n    matplotlib rcParams system.\n\n    The options are illustrated in the\n    :doc:`aesthetics tutorial <../tutorial/aesthetics>`.\n\n    See :func:`axes_style` to get the parameter values.\n\n    Parameters\n    ----------\n    style : dict, or one of {darkgrid, whitegrid, dark, white, ticks}\n        A dictionary of parameters or the name of a preconfigured style.\n    rc : dict, optional\n        Parameter mappings to override the values in the preset seaborn\n        style dictionaries. This only updates parameters that are\n        considered part of the style definition.\n\n    Examples\n    --------\n\n    .. include:: ../docstrings/set_style.rst\n\n    ",
    "seaborn.set_theme": "\n    Set aspects of the visual theme for all matplotlib and seaborn plots.\n\n    This function changes the global defaults for all plots using the\n    matplotlib rcParams system. The themeing is decomposed into several distinct\n    sets of parameter values.\n\n    The options are illustrated in the :doc:`aesthetics <../tutorial/aesthetics>`\n    and :doc:`color palette <../tutorial/color_palettes>` tutorials.\n\n    Parameters\n    ----------\n    context : string or dict\n        Scaling parameters, see :func:`plotting_context`.\n    style : string or dict\n        Axes style parameters, see :func:`axes_style`.\n    palette : string or sequence\n        Color palette, see :func:`color_palette`.\n    font : string\n        Font family, see matplotlib font manager.\n    font_scale : float, optional\n        Separate scaling factor to independently scale the size of the\n        font elements.\n    color_codes : bool\n        If ``True`` and ``palette`` is a seaborn palette, remap the shorthand\n        color codes (e.g. \"b\", \"g\", \"r\", etc.) to the colors from this palette.\n    rc : dict or None\n        Dictionary of rc parameter mappings to override the above.\n\n    Examples\n    --------\n\n    .. include:: ../docstrings/set_theme.rst\n\n    ",
    "seaborn.stripplot": "Draw a categorical scatterplot using jitter to reduce overplotting.\n\nA strip plot can be drawn on its own, but it is also a good complement\nto a box or violin plot in cases where you want to show all observations\nalong with some representation of the underlying distribution.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \njitter : float, `True`/`1` is special-cased\n    Amount of jitter (only along the categorical axis) to apply. This\n    can be useful when you have many points and they overlap, so that\n    it is easier to see the distribution. You can specify the amount\n    of jitter (half the width of the uniform random variable support),\n    or use `True` for a good default.\ndodge : bool\n    When a `hue` variable is assigned, setting this to `True` will\n    separate the strips for different hue levels along the categorical\n    axis and narrow the amount of space allotedto each strip. Otherwise,\n    the points for each level will be plotted in the same strip.\norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nsize : float\n    Radius of the markers, in points.\nedgecolor : matplotlib color, \"gray\" is special-cased\n    Color of the lines around each point. If you pass `\"gray\"`, the\n    brightness is determined by the color palette used for the body\n    of the points. Note that `stripplot` has `linewidth=0` by default,\n    so edge colors are only visible with nonzero line width.\nlinewidth : float\n    Width of the lines that frame the plot elements.    \nhue_norm : tuple or :class:`matplotlib.colors.Normalize` object\n    Normalization in data units for colormap applied to the `hue`\n    variable when it is numeric. Not relevant if `hue` is categorical.\n\n    .. versionadded:: v0.12.0    \nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\n\n    .. versionadded:: v0.13.0    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \nax : matplotlib Axes\n    Axes object to draw the plot onto, otherwise uses the current Axes.    \nkwargs : key, value mappings\n    Other keyword arguments are passed through to\n    :meth:`matplotlib.axes.Axes.scatter`.\n\nReturns\n-------\nax : matplotlib Axes\n    Returns the Axes object with the plot drawn onto it.    \n\nSee Also\n--------\nswarmplot : A categorical scatterplot where the points do not overlap. Can\n            be used with other plots to show each observation.    \nboxplot : A traditional box-and-whisker plot with a similar API.    \nviolinplot : A combination of boxplot and kernel density estimation.    \ncatplot : Combine a categorical plot with a :class:`FacetGrid`.    \n\nExamples\n--------\n.. include:: ../docstrings/stripplot.rst\n\n",
    "seaborn.swarmplot": "Draw a categorical scatterplot with points adjusted to be non-overlapping.\n\nThis function is similar to :func:`stripplot`, but the points are adjusted\n(only along the categorical axis) so that they don't overlap. This gives a\nbetter representation of the distribution of values, but it does not scale\nwell to large numbers of observations. This style of plot is sometimes\ncalled a \"beeswarm\".\n\nA swarm plot can be drawn on its own, but it is also a good complement\nto a box or violin plot in cases where you want to show all observations\nalong with some representation of the underlying distribution.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \ndodge : bool\n    When a `hue` variable is assigned, setting this to `True` will\n    separate the swarms for different hue levels along the categorical\n    axis and narrow the amount of space allotedto each strip. Otherwise,\n    the points for each level will be plotted in the same swarm.\norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nsize : float\n    Radius of the markers, in points.\nedgecolor : matplotlib color, \"gray\" is special-cased\n    Color of the lines around each point. If you pass `\"gray\"`, the\n    brightness is determined by the color palette used for the body\n    of the points.\nlinewidth : float\n    Width of the lines that frame the plot elements.    \nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\n\n    .. versionadded:: v0.13.0    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \nax : matplotlib Axes\n    Axes object to draw the plot onto, otherwise uses the current Axes.    \nkwargs : key, value mappings\n    Other keyword arguments are passed through to\n    :meth:`matplotlib.axes.Axes.scatter`.\n\nReturns\n-------\nax : matplotlib Axes\n    Returns the Axes object with the plot drawn onto it.    \n\nSee Also\n--------\nboxplot : A traditional box-and-whisker plot with a similar API.    \nviolinplot : A combination of boxplot and kernel density estimation.    \nstripplot : A scatterplot where one variable is categorical. Can be used\n            in conjunction with other plots to show each observation.    \ncatplot : Combine a categorical plot with a :class:`FacetGrid`.    \n\nExamples\n--------\n.. include:: ../docstrings/swarmplot.rst\n\n",
    "seaborn.violinplot": "Draw a patch representing a KDE and add observations or box plot statistics.\n\nA violin plot plays a similar role as a box-and-whisker plot. It shows the\ndistribution of data points after grouping by one (or more) variables.\nUnlike a box plot, each violin is drawn using a kernel density estimate\nof the underlying distribution.\n\nSee the :ref:`tutorial <categorical_tutorial>` for more information.\n\n.. note::\n    By default, this function treats one of the variables as categorical\n    and draws data at ordinal positions (0, 1, ... n) on the relevant axis.\n    As of version 0.13.0, this can be disabled by setting `native_scale=True`.\n\n\nParameters\n----------\ndata : DataFrame, Series, dict, array, or list of arrays\n    Dataset for plotting. If `x` and `y` are absent, this is\n    interpreted as wide-form. Otherwise it is expected to be long-form.    \nx, y, hue : names of variables in `data` or vector data\n    Inputs for plotting long-form data. See examples for interpretation.    \norder, hue_order : lists of strings\n    Order to plot the categorical levels in; otherwise the levels are\n    inferred from the data objects.    \norient : \"v\" | \"h\" | \"x\" | \"y\"\n    Orientation of the plot (vertical or horizontal). This is usually\n    inferred based on the type of the input variables, but it can be used\n    to resolve ambiguity when both `x` and `y` are numeric or when\n    plotting wide-form data.\n\n    .. versionchanged:: v0.13.0\n        Added 'x'/'y' as options, equivalent to 'v'/'h'.    \ncolor : matplotlib color\n    Single color for the elements in the plot.    \npalette : palette name, list, or dict\n    Colors to use for the different levels of the ``hue`` variable. Should\n    be something that can be interpreted by :func:`color_palette`, or a\n    dictionary mapping hue levels to matplotlib colors.    \nsaturation : float\n    Proportion of the original saturation to draw fill colors in. Large\n    patches often look better with desaturated colors, but set this to\n    `1` if you want the colors to perfectly match the input values.    \nfill : bool\n    If True, use a solid patch. Otherwise, draw as line art.\n\n    .. versionadded:: v0.13.0    \ninner : {\"box\", \"quart\", \"point\", \"stick\", None}\n    Representation of the data in the violin interior. One of the following:\n\n    - `\"box\"`: draw a miniature box-and-whisker plot\n    - `\"quart\"`: show the quartiles of the data\n    - `\"point\"` or `\"stick\"`: show each observation\nsplit : bool\n    Show an un-mirrored distribution, alternating sides when using `hue`.\n\n    .. versionchanged:: v0.13.0\n        Previously, this option required a `hue` variable with exactly two levels.\nwidth : float\n    Width allotted to each element on the orient axis. When `native_scale=True`,\n    it is relative to the minimum distance between two values in the native scale.    \ndodge : \"auto\" or bool\n    When hue mapping is used, whether elements should be narrowed and shifted along\n    the orient axis to eliminate overlap. If `\"auto\"`, set to `True` when the\n    orient variable is crossed with the categorical variable or `False` otherwise.\n\n    .. versionchanged:: 0.13.0\n\n        Added `\"auto\"` mode as a new default.    \ngap : float\n    Shrink on the orient axis by this factor to add a gap between dodged elements.\n\n    .. versionadded:: 0.13.0    \nlinewidth : float\n    Width of the lines that frame the plot elements.    \nlinecolor : color\n    Color to use for line elements, when `fill` is True.\n\n    .. versionadded:: v0.13.0    \ncut : float\n    Distance, in units of bandwidth, to extend the density past extreme\n    datapoints. Set to 0 to limit the violin within the data range.\ngridsize : int\n    Number of points in the discrete grid used to evaluate the KDE.\nbw_method : {\"scott\", \"silverman\", float}\n    Either the name of a reference rule or the scale factor to use when\n    computing the kernel bandwidth. The actual kernel size will be\n    determined by multiplying the scale factor by the standard deviation of\n    the data within each group.\n\n    .. versionadded:: v0.13.0\nbw_adjust: float\n    Factor that scales the bandwidth to use more or less smoothing.\n\n    .. versionadded:: v0.13.0\ndensity_norm : {\"area\", \"count\", \"width\"}\n    Method that normalizes each density to determine the violin's width.\n    If `area`, each violin will have the same area. If `count`, the width\n    will be proportional to the number of observations. If `width`, each\n    violin will have the same width.\n\n    .. versionadded:: v0.13.0\ncommon_norm : bool\n    When `True`, normalize the density across all violins.\n\n    .. versionadded:: v0.13.0\nhue_norm : tuple or :class:`matplotlib.colors.Normalize` object\n    Normalization in data units for colormap applied to the `hue`\n    variable when it is numeric. Not relevant if `hue` is categorical.\n\n    .. versionadded:: v0.12.0    \nformatter : callable\n    Function for converting categorical data into strings. Affects both grouping\n    and tick labels.\n\n    .. versionadded:: v0.13.0    \nlog_scale : bool or number, or pair of bools or numbers\n    Set axis scale(s) to log. A single value sets the data axis for any numeric\n    axes in the plot. A pair of values sets each axis independently.\n    Numeric values are interpreted as the desired base (default 10).\n    When `None` or `False`, seaborn defers to the existing Axes scale.\n\n    .. versionadded:: v0.13.0    \nnative_scale : bool\n    When True, numeric or datetime values on the categorical axis will maintain\n    their original scaling rather than being converted to fixed indices.\n\n    .. versionadded:: v0.13.0    \nlegend : \"auto\", \"brief\", \"full\", or False\n    How to draw the legend. If \"brief\", numeric `hue` and `size`\n    variables will be represented with a sample of evenly spaced values.\n    If \"full\", every group will get an entry in the legend. If \"auto\",\n    choose between brief or full representation based on number of levels.\n    If `False`, no legend data is added and no legend is drawn.\n\n    .. versionadded:: v0.13.0    \nscale : {\"area\", \"count\", \"width\"}\n    .. deprecated:: v0.13.0\n        See `density_norm`.\nscale_hue : bool\n    .. deprecated:: v0.13.0\n        See `common_norm`.\nbw : {'scott', 'silverman', float}\n    .. deprecated:: v0.13.0\n        See `bw_method` and `bw_adjust`.\ninner_kws : dict of key, value mappings\n    Keyword arguments for the \"inner\" plot, passed to one of:\n\n    - :class:`matplotlib.collections.LineCollection` (with `inner=\"stick\"`)\n    - :meth:`matplotlib.axes.Axes.scatter` (with `inner=\"point\"`)\n    - :meth:`matplotlib.axes.Axes.plot` (with `inner=\"quart\"` or `inner=\"box\"`)\n\n    Additionally, with `inner=\"box\"`, the keywords `box_width`, `whis_width`,\n    and `marker` receive special handling for the components of the \"box\" plot.\n\n    .. versionadded:: v0.13.0\nax : matplotlib Axes\n    Axes object to draw the plot onto, otherwise uses the current Axes.    \nkwargs : key, value mappings\n    Keyword arguments for the violin patches, passsed through to\n    :meth:`matplotlib.axes.Axes.fill_between`.\n\nReturns\n-------\nax : matplotlib Axes\n    Returns the Axes object with the plot drawn onto it.    \n\nSee Also\n--------\nboxplot : A traditional box-and-whisker plot with a similar API.    \nstripplot : A scatterplot where one variable is categorical. Can be used\n            in conjunction with other plots to show each observation.    \nswarmplot : A categorical scatterplot where the points do not overlap. Can\n            be used with other plots to show each observation.    \ncatplot : Combine a categorical plot with a :class:`FacetGrid`.    \n\nExamples\n--------\n.. include:: ../docstrings/violinplot.rst\n\n",
    "seaborn.xkcd_palette": "Make a palette with color names from the xkcd color survey.\n\n    See xkcd for the full list of colors: https://xkcd.com/color/rgb/\n\n    This is just a simple wrapper around the `seaborn.xkcd_rgb` dictionary.\n\n    Parameters\n    ----------\n    colors : list of strings\n        List of keys in the `seaborn.xkcd_rgb` dictionary.\n\n    Returns\n    -------\n    palette\n        A list of colors as RGB tuples.\n\n    See Also\n    --------\n    crayon_palette : Make a palette with Crayola crayon colors.\n\n    ",
    "plotly.graph_objects.AngularAxis": "\n        plotly.graph_objs.AngularAxis is deprecated.\n    Please replace it with one of the following more specific types\n      - plotly.graph_objs.layout.AngularAxis\n      - plotly.graph_objs.layout.polar.AngularAxis\n\n    ",
    "plotly.graph_objects.Annotation": "\n        plotly.graph_objs.Annotation is deprecated.\n    Please replace it with one of the following more specific types\n      - plotly.graph_objs.layout.Annotation\n      - plotly.graph_objs.layout.scene.Annotation\n\n    ",
    "plotly.graph_objects.Annotations": "\n        plotly.graph_objs.Annotations is deprecated.\n    Please replace it with a list or tuple of instances of the following types\n      - plotly.graph_objs.layout.Annotation\n      - plotly.graph_objs.layout.scene.Annotation\n\n    ",
    "plotly.graph_objects.ColorBar": "\n        plotly.graph_objs.ColorBar is deprecated.\n    Please replace it with one of the following more specific types\n      - plotly.graph_objs.scatter.marker.ColorBar\n      - plotly.graph_objs.surface.ColorBar\n      - etc.\n\n    ",
    "plotly.graph_objects.Contours": "\n        plotly.graph_objs.Contours is deprecated.\n    Please replace it with one of the following more specific types\n      - plotly.graph_objs.contour.Contours\n      - plotly.graph_objs.surface.Contours\n      - etc.\n\n    ",
    "plotly.graph_objects.Data": "\n        plotly.graph_objs.Data is deprecated.\n    Please replace it with a list or tuple of instances of the following types\n      - plotly.graph_objs.Scatter\n      - plotly.graph_objs.Bar\n      - plotly.graph_objs.Area\n      - plotly.graph_objs.Histogram\n      - etc.\n\n    ",
    "plotly.graph_objects.ErrorX": "\n        plotly.graph_objs.ErrorX is deprecated.\n    Please replace it with one of the following more specific types\n      - plotly.graph_objs.scatter.ErrorX\n      - plotly.graph_objs.histogram.ErrorX\n      - etc.\n\n    ",
    "plotly.graph_objects.ErrorY": "\n        plotly.graph_objs.ErrorY is deprecated.\n    Please replace it with one of the following more specific types\n      - plotly.graph_objs.scatter.ErrorY\n      - plotly.graph_objs.histogram.ErrorY\n      - etc.\n\n    ",
    "plotly.graph_objects.ErrorZ": "\n        plotly.graph_objs.ErrorZ is deprecated.\n    Please replace it with one of the following more specific types\n      - plotly.graph_objs.scatter3d.ErrorZ\n\n    ",
    "plotly.graph_objects.Font": "\n        plotly.graph_objs.Font is deprecated.\n    Please replace it with one of the following more specific types\n      - plotly.graph_objs.layout.Font\n      - plotly.graph_objs.layout.hoverlabel.Font\n      - etc.\n\n    ",
    "plotly.graph_objects.Frames": "\n        plotly.graph_objs.Frames is deprecated.\n    Please replace it with a list or tuple of instances of the following types\n      - plotly.graph_objs.Frame\n\n    ",
    "plotly.graph_objects.Histogram2dcontour": "\n        plotly.graph_objs.Histogram2dcontour is deprecated.\n    Please replace it with one of the following more specific types\n      - plotly.graph_objs.Histogram2dContour\n\n    ",
    "plotly.graph_objects.Legend": "\n        plotly.graph_objs.Legend is deprecated.\n    Please replace it with one of the following more specific types\n      - plotly.graph_objs.layout.Legend\n\n    ",
    "plotly.graph_objects.Line": "\n        plotly.graph_objs.Line is deprecated.\n    Please replace it with one of the following more specific types\n      - plotly.graph_objs.scatter.Line\n      - plotly.graph_objs.layout.shape.Line\n      - etc.\n\n    ",
    "plotly.graph_objects.Margin": "\n        plotly.graph_objs.Margin is deprecated.\n    Please replace it with one of the following more specific types\n      - plotly.graph_objs.layout.Margin\n\n    ",
    "plotly.graph_objects.Marker": "\n        plotly.graph_objs.Marker is deprecated.\n    Please replace it with one of the following more specific types\n      - plotly.graph_objs.scatter.Marker\n      - plotly.graph_objs.histogram.selected.Marker\n      - etc.\n\n    ",
    "plotly.graph_objects.RadialAxis": "\n        plotly.graph_objs.RadialAxis is deprecated.\n    Please replace it with one of the following more specific types\n      - plotly.graph_objs.layout.RadialAxis\n      - plotly.graph_objs.layout.polar.RadialAxis\n\n    ",
    "plotly.graph_objects.Scene": "\n        plotly.graph_objs.Scene is deprecated.\n    Please replace it with one of the following more specific types\n      - plotly.graph_objs.layout.Scene\n\n    ",
    "plotly.graph_objects.Stream": "\n        plotly.graph_objs.Stream is deprecated.\n    Please replace it with one of the following more specific types\n      - plotly.graph_objs.scatter.Stream\n      - plotly.graph_objs.area.Stream\n\n    ",
    "plotly.graph_objects.Trace": "\n        plotly.graph_objs.Trace is deprecated.\n    Please replace it with one of the following more specific types\n      - plotly.graph_objs.Scatter\n      - plotly.graph_objs.Bar\n      - plotly.graph_objs.Area\n      - plotly.graph_objs.Histogram\n      - etc.\n\n    ",
    "plotly.graph_objects.XAxis": "\n        plotly.graph_objs.XAxis is deprecated.\n    Please replace it with one of the following more specific types\n      - plotly.graph_objs.layout.XAxis\n      - plotly.graph_objs.layout.scene.XAxis\n\n    ",
    "plotly.graph_objects.XBins": "\n        plotly.graph_objs.XBins is deprecated.\n    Please replace it with one of the following more specific types\n      - plotly.graph_objs.histogram.XBins\n      - plotly.graph_objs.histogram2d.XBins\n\n    ",
    "plotly.graph_objects.YAxis": "\n        plotly.graph_objs.YAxis is deprecated.\n    Please replace it with one of the following more specific types\n      - plotly.graph_objs.layout.YAxis\n      - plotly.graph_objs.layout.scene.YAxis\n\n    ",
    "plotly.graph_objects.YBins": "\n        plotly.graph_objs.YBins is deprecated.\n    Please replace it with one of the following more specific types\n      - plotly.graph_objs.histogram.YBins\n      - plotly.graph_objs.histogram2d.YBins\n\n    ",
    "plotly.graph_objects.ZAxis": "\n        plotly.graph_objs.ZAxis is deprecated.\n    Please replace it with one of the following more specific types\n      - plotly.graph_objs.layout.scene.ZAxis\n\n    ",
    "streamlit.components.v1": "\nThis directory contains the files and modules for the exposed API.\n",
    "streamlit.components.v1.declare_component": "Create a custom component and register it if there is a ``ScriptRunContext``.\n\n    The component is not registered when there is no ``ScriptRunContext``.\n    This can happen when a ``CustomComponent`` is executed as standalone\n    command (e.g. for testing).\n\n    To use this function, import it from the ``streamlit.components.v1``\n    module.\n\n    .. warning::\n        Using ``st.components.v1.declare_component`` directly (instead of\n        importing its module) is deprecated and will be disallowed in a later\n        version.\n\n    Parameters\n    ----------\n    name : str\n        A short, descriptive name for the component, like \"slider\".\n\n    path: str, Path, or None\n        The path to serve the component's frontend files from. The path should\n        be absolute. If ``path`` is ``None`` (default), Streamlit will serve\n        the component from the location in ``url``. Either ``path`` or ``url``\n        must be specified, but not both.\n\n    url: str or None\n        The URL that the component is served from. If ``url`` is ``None``\n        (default), Streamlit will serve the component from the location in\n        ``path``. Either ``path`` or ``url`` must be specified, but not both.\n\n    Returns\n    -------\n    CustomComponent\n        A ``CustomComponent`` that can be called like a function.\n        Calling the component will create a new instance of the component\n        in the Streamlit app.\n\n    ",
    "streamlit.components.v1.html": "Display an HTML string in an iframe.\n\n        To use this function, import it from the ``streamlit.components.v1``\n        module.\n\n        If you want to insert HTML text into your app without an iframe, try\n        ``st.html`` instead.\n\n        .. warning::\n            Using ``st.components.v1.html`` directly (instead of importing\n            its module) is deprecated and will be disallowed in a later version.\n\n        Parameters\n        ----------\n        html : str\n            The HTML string to embed in the iframe.\n\n        width : int\n            The width of the iframe in CSS pixels. By default, this is the\n            app's default element width.\n\n        height : int\n            The height of the frame in CSS pixels. By default, this is ``150``.\n\n        scrolling : bool\n            Whether to allow scrolling in the iframe. If this ``False``\n            (default), Streamlit crops any content larger than the iframe and\n            does not show a scrollbar. If this is ``True``, Streamlit shows a\n            scrollbar when the content is larger than the iframe.\n\n        Example\n        -------\n\n        >>> import streamlit.components.v1 as components\n        >>>\n        >>> components.html(\n        >>>     \"<p><span style='text-decoration: line-through double red;'>Oops</span>!</p>\"\n        >>> )\n\n        ",
    "streamlit.components.v1.iframe": "Load a remote URL in an iframe.\n\n        To use this function, import it from the ``streamlit.components.v1``\n        module.\n\n        .. warning::\n            Using ``st.components.v1.iframe`` directly (instead of importing\n            its module) is deprecated and will be disallowed in a later version.\n\n        Parameters\n        ----------\n        src : str\n            The URL of the page to embed.\n\n        width : int\n            The width of the iframe in CSS pixels. By default, this is the\n            app's default element width.\n\n        height : int\n            The height of the frame in CSS pixels. By default, this is ``150``.\n\n        scrolling : bool\n            Whether to allow scrolling in the iframe. If this ``False``\n            (default), Streamlit crops any content larger than the iframe and\n            does not show a scrollbar. If this is ``True``, Streamlit shows a\n            scrollbar when the content is larger than the iframe.\n\n        Example\n        -------\n\n        >>> import streamlit.components.v1 as components\n        >>>\n        >>> components.iframe(\"https://example.com\", height=500)\n\n        ",
    "mplfinance.make_addplot": "\n    Take data (pd.Series, pd.DataFrame, np.ndarray of floats, list of floats), and\n    kwargs (see valid_addplot_kwargs_table) and construct a correctly structured dict\n    to be passed into plot() using kwarg `addplot`.  \n    NOTE WELL: len(data) here must match the len(data) passed into plot()\n    ",
    "mplfinance.make_marketcolors": "\n    Create a 'marketcolors' dict that is structured as expected\n    by mplfinance._styles code:\n        up     = color for close >= open\n        down   = color for close  < open\n        edge   = color for edge of candlestick; if \"inherit\"\n                 then edge color will be same as up or down.\n        wick   = color for wick of candlestick; if \"inherit\"\n                 then wick color will be same as up or down.\n        alpha  = opacity, 0.0 to 1.0, of candlestick face.\n        ohlc   = color of ohlc bars when all the same color;\n                 if ohlc == \"inherit\" then use up/down colors.\n        volume = color of volume bars when all the same color;\n                 if volume == \"inherit\" then use up/down colors.\n    ",
    "mplfinance.plot": "\n    Given a Pandas DataFrame containing columns Open,High,Low,Close and optionally Volume\n    with a DatetimeIndex, plot the data.\n    Available plots include ohlc bars, candlestick, and line plots.\n    Also provide visually analysis in the form of common technical studies, such as:\n    moving averages, renko, etc.\n    Also provide ability to plot trading signals, and/or addtional user-defined data.\n    ",
    "mplfinance.show": "\n    Display all open figures.\n\n    Parameters\n    ----------\n    block : bool, optional\n        Whether to wait for all figures to be closed before returning.\n\n        If `True` block and run the GUI main loop until all figure windows\n        are closed.\n\n        If `False` ensure that all figure windows are displayed and return\n        immediately.  In this case, you are responsible for ensuring\n        that the event loop is running to have responsive figures.\n\n        Defaults to True in non-interactive mode and to False in interactive\n        mode (see `.pyplot.isinteractive`).\n\n    See Also\n    --------\n    ion : Enable interactive mode, which shows / updates the figure after\n          every plotting command, so that calling ``show()`` is not necessary.\n    ioff : Disable interactive mode.\n    savefig : Save the figure to an image file instead of showing it on screen.\n\n    Notes\n    -----\n    **Saving figures to file and showing a window at the same time**\n\n    If you want an image file as well as a user interface window, use\n    `.pyplot.savefig` before `.pyplot.show`. At the end of (a blocking)\n    ``show()`` the figure is closed and thus unregistered from pyplot. Calling\n    `.pyplot.savefig` afterwards would save a new and thus empty figure. This\n    limitation of command order does not apply if the show is non-blocking or\n    if you keep a reference to the figure and use `.Figure.savefig`.\n\n    **Auto-show in jupyter notebooks**\n\n    The jupyter backends (activated via ``%matplotlib inline``,\n    ``%matplotlib notebook``, or ``%matplotlib widget``), call ``show()`` at\n    the end of every cell by default. Thus, you usually don't have to call it\n    explicitly there.\n    ",
    "finplot": "\nFinancial data plotter with better defaults, api, behavior and performance than\nmpl_finance and plotly.\n\nLines up your time-series with a shared X-axis; ideal for volume, RSI, etc.\n\nZoom does something similar to what you'd normally expect for financial data,\nwhere the Y-axis is auto-scaled to highest high and lowest low in the active\nregion.\n",
    "finplot.ColorMap": "\n    ColorMap(pos, color, mapping=ColorMap.CLIP)\n\n    ColorMap stores a mapping of specific data values to colors, for example:\n\n        | 0.0 \u2192 black\n        | 0.2 \u2192 red\n        | 0.6 \u2192 yellow\n        | 1.0 \u2192 white\n\n    The colors for intermediate values are determined by interpolating between\n    the two nearest colors in RGB color space.\n\n    A ColorMap object provides access to the interpolated colors by indexing with a float value:\n    ``cm[0.5]`` returns a QColor corresponding to the center of ColorMap `cm`.\n    ",
    "finplot.Decimal": "Construct a new Decimal object. 'value' can be an integer, string, tuple,\nor another Decimal object. If no value is given, return Decimal('0'). The\ncontext does not affect the conversion and is only passed to determine if\nthe InvalidOperation trap is active.\n\n",
    "finplot.OrderedDict": "Dictionary that remembers insertion order",
    "finplot.PandasDataSource": "Candle sticks: create with five columns: time, open, close, hi, lo - in that order.\n       Volume bars: create with three columns: time, open, close, volume - in that order.\n       For all other types, time needs to be first, usually followed by one or more Y-columns.",
    "finplot._ax_overlay": "The scale parameter defines how \"high up\" on the initial plot this overlay will show.\n       The yaxis parameter can be one of [False, 'linear', 'log'].",
    "finplot._draw_line_extra_text": "Shows the proportions of this line height compared to the previous segment.",
    "finplot._improve_significants": "Force update of the EPS if we both have no bars/candles AND a log scale.\n       This is intended to fix the lower part of the grid on line plots on a log scale.",
    "finplot._repaint_candles": "Candles are only partially drawn, and therefore needs manual dirty reminder whenever it goes off-screen.",
    "finplot._set_max_zoom": "Set the relative allowed zoom level between axes groups, where the lowest-resolution\n       plot in each group uses max_zoom_points, while the others get a scale >=1 of their\n       respective highest zoom level.",
    "finplot.add_crosshair_info": "Callback when crosshair updated like so: info(ax,x,y,xtext,ytext); the info()\n       callback must return two values: xtext and ytext.",
    "finplot.autoviewrestore": "Restor functionality saves view zoom coordinates when closing a window, and\n       load them when creating the plot (with the same name) again.",
    "finplot.bar": "Bar plots are decoupled. Use volume_ocv() if you want a bar plot which relates to other time plots.",
    "finplot.ceil": "Return the ceiling of x as an Integral.\n\nThis is the smallest integer >= x.",
    "finplot.datetime": "datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\nThe year, month and day arguments are required. tzinfo may be None, or an\ninstance of a tzinfo subclass. The remaining arguments may be ints.\n",
    "finplot.defaultdict": "defaultdict(default_factory=None, /, [...]) --> dict with default factory\n\nThe default factory is called without arguments to produce\na new value when a key is not present, in __getitem__ only.\nA defaultdict compares equal to a dict with the same items.\nAll remaining arguments are treated the same as if they were\npassed to the dict constructor, including keyword arguments.\n",
    "finplot.floor": "Return the floor of x as an Integral.\n\nThis is the largest integer <= x.",
    "finplot.fmod": "Return fmod(x, y), according to platform C.\n\nx % y may differ.",
    "finplot.heatmap": "Expensive function. Only use on small data sets. See HeatmapItem for kwargs. Input datasrc\n       has x (time) in index or first column, y (price) as column names, and intensity (color) as\n       cell values.",
    "finplot.horiz_time_volume": "Draws multiple fixed horizontal volumes. The input format is:\n       [[time0, [(price0,volume0),(price1,volume1),...]], ...]\n\n       This chart needs to be plot last, so it knows if it controls\n       what time periods are shown, or if its using time already in\n       place by another plot.",
    "finplot.horizvol_colorfilter": "The sections argument is a (starting_index, color_name) array.",
    "finplot.literal_eval": "\n    Evaluate an expression node or a string containing only a Python\n    expression.  The string or node provided may only consist of the following\n    Python literal structures: strings, bytes, numbers, tuples, lists, dicts,\n    sets, booleans, and None.\n\n    Caution: A complex expression can overflow the C stack and cause a crash.\n    ",
    "finplot.partial": "partial(func, *args, **keywords) - new function with partial application\n    of the given arguments and keywords.\n",
    "finplot.partialmethod": "Method descriptor with partial application of the given arguments\n    and keywords.\n\n    Supports wrapping existing descriptors and handles non-descriptor\n    callables as instance methods.\n    ",
    "finplot.set_mouse_callback": "Callback when clicked like so: callback(x, y).",
    "finplot.timezone": "Fixed offset from UTC implementation of tzinfo.",
    "finplot.tzlocal": "\n    A :class:`tzinfo` subclass built around the ``time`` timezone functions.\n    ",
    "finplot.volume_colorfilter_section": "The sections argument is a (starting_index, color_name) array."
}